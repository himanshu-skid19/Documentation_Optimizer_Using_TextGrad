2025-04-18 18:16:13,413 - INFO - WandB initialized with run name: flan-t5-base-none
2025-04-18 18:16:13,413 - INFO - Loading dataset from processed_dataset
2025-04-18 18:16:14,321 - INFO - Loaded 1990 examples from processed_dataset/train.jsonl
2025-04-18 18:16:14,624 - INFO - Loaded 426 examples from processed_dataset/val.jsonl
2025-04-18 18:16:14,932 - INFO - Loaded 427 examples from processed_dataset/test.jsonl
2025-04-18 18:16:14,932 - INFO - Loading tokenizer: google/flan-t5-base
/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2025-04-18 18:16:15,334 - INFO - Not using quantization
2025-04-18 18:16:15,335 - INFO - Loading model: google/flan-t5-base
2025-04-18 18:16:18,468 - INFO - Setting up LoRA for efficient fine-tuning
trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3560841867092814
2025-04-18 18:16:18,784 - INFO - Tokenizing datasets
Map: 100%|████████████████████████████████████████████████████████████████████| 1990/1990 [00:00<00:00, 4248.94 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████| 427/427 [00:00<00:00, 3788.20 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████| 426/426 [00:00<00:00, 4043.99 examples/s]
2025-04-18 18:16:19,797 - INFO - Total non-ignored tokens in training set: 131378
2025-04-18 18:16:20,655 - INFO - Starting training...
/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                                             | 0/2480 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
                                                                                                                           
{'loss': 0.0, 'epoch': 0.01}
{'loss': 0.0, 'epoch': 0.02}
{'loss': 0.0, 'epoch': 0.02}
{'loss': 0.0, 'epoch': 0.03}
{'loss': 0.0, 'epoch': 0.04}
{'loss': 0.0, 'epoch': 0.05}
{'loss': 0.0, 'epoch': 0.06}
{'loss': 0.0, 'epoch': 0.06}
{'loss': 0.0, 'epoch': 0.07}
{'loss': 0.0, 'epoch': 0.08}
{'loss': 0.0, 'epoch': 0.09}
{'loss': 0.0, 'epoch': 0.1}
{'loss': 0.0, 'epoch': 0.1}
{'loss': 0.0, 'epoch': 0.11}
{'loss': 0.0, 'epoch': 0.12}
{'loss': 0.0, 'epoch': 0.13}
{'loss': 0.0, 'epoch': 0.14}
{'loss': 0.0, 'epoch': 0.14}
{'loss': 0.0, 'epoch': 0.15}
{'loss': 0.0, 'epoch': 0.16}
{'loss': 0.0, 'epoch': 0.17}
{'loss': 0.0, 'epoch': 0.18}
{'loss': 0.0, 'epoch': 0.18}
{'loss': 0.0, 'epoch': 0.19}
{'loss': 0.0, 'epoch': 0.2}
{'loss': 0.0, 'epoch': 0.21}
{'loss': 0.0, 'epoch': 0.22}
{'loss': 0.0, 'epoch': 0.22}
{'loss': 0.0, 'epoch': 0.23}
{'loss': 0.0, 'epoch': 0.24}
{'loss': 0.0, 'epoch': 0.25}
{'loss': 0.0, 'epoch': 0.26}
{'loss': 0.0, 'epoch': 0.27}
{'loss': 0.0, 'epoch': 0.27}
{'loss': 0.0, 'epoch': 0.28}
{'loss': 0.0, 'epoch': 0.29}
{'loss': 0.0, 'epoch': 0.3}
{'loss': 0.0, 'epoch': 0.31}
{'loss': 0.0, 'epoch': 0.31}
{'loss': 0.0, 'epoch': 0.32}
{'loss': 0.0, 'epoch': 0.33}
{'loss': 0.0, 'epoch': 0.34}
{'loss': 0.0, 'epoch': 0.35}
{'loss': 0.0, 'epoch': 0.35}
{'loss': 0.0, 'epoch': 0.36}
{'loss': 0.0, 'epoch': 0.37}
{'loss': 0.0, 'epoch': 0.38}
{'loss': 0.0, 'epoch': 0.39}
{'loss': 0.0, 'epoch': 0.39}
{'loss': 0.0, 'epoch': 0.4}
{'loss': 0.0, 'epoch': 0.41}
{'loss': 0.0, 'epoch': 0.42}
{'loss': 0.0, 'epoch': 0.43}
{'loss': 0.0, 'epoch': 0.43}
{'loss': 0.0, 'epoch': 0.44}
{'loss': 0.0, 'epoch': 0.45}
{'loss': 0.0, 'epoch': 0.46}
{'loss': 0.0, 'epoch': 0.47}
{'loss': 0.0, 'epoch': 0.47}
{'loss': 0.0, 'epoch': 0.48}
{'loss': 0.0, 'epoch': 0.49}
{'loss': 0.0, 'epoch': 0.5}
{'loss': 0.0, 'epoch': 0.51}
{'loss': 0.0, 'epoch': 0.51}
{'loss': 0.0, 'epoch': 0.52}
{'loss': 0.0, 'epoch': 0.53}
{'loss': 0.0, 'epoch': 0.54}
{'loss': 0.0, 'epoch': 0.55}
{'loss': 0.0, 'epoch': 0.55}
{'loss': 0.0, 'epoch': 0.56}
{'loss': 0.0, 'epoch': 0.57}
{'loss': 0.0, 'epoch': 0.58}
{'loss': 0.0, 'epoch': 0.59}
{'loss': 0.0, 'epoch': 0.59}
{'loss': 0.0, 'epoch': 0.6}
{'loss': 0.0, 'epoch': 0.61}
{'loss': 0.0, 'epoch': 0.62}
{'loss': 0.0, 'epoch': 0.63}
{'loss': 0.0, 'epoch': 0.63}
{'loss': 0.0, 'epoch': 0.64}
{'loss': 0.0, 'epoch': 0.65}
{'loss': 0.0, 'epoch': 0.66}
{'loss': 0.0, 'epoch': 0.67}
{'loss': 0.0, 'epoch': 0.67}
{'loss': 0.0, 'epoch': 0.68}
{'loss': 0.0, 'epoch': 0.69}
{'loss': 0.0, 'epoch': 0.7}
{'loss': 0.0, 'epoch': 0.71}
{'loss': 0.0, 'epoch': 0.71}
{'loss': 0.0, 'epoch': 0.72}
{'loss': 0.0, 'epoch': 0.73}
{'loss': 0.0, 'epoch': 0.74}
{'loss': 0.0, 'epoch': 0.75}
{'loss': 0.0, 'epoch': 0.76}
{'loss': 0.0, 'epoch': 0.76}
{'loss': 0.0, 'epoch': 0.77}
{'loss': 0.0, 'epoch': 0.78}
{'loss': 0.0, 'epoch': 0.79}
{'loss': 0.0, 'epoch': 0.8}
{'loss': 0.0, 'epoch': 0.8}
{'loss': 0.0, 'epoch': 0.81}
{'loss': 0.0, 'epoch': 0.82}
{'loss': 0.0, 'epoch': 0.83}
{'loss': 0.0, 'epoch': 0.84}
{'loss': 0.0, 'epoch': 0.84}
{'loss': 0.0, 'epoch': 0.85}
{'loss': 0.0, 'epoch': 0.86}
{'loss': 0.0, 'epoch': 0.87}
{'loss': 0.0, 'epoch': 0.88}
{'loss': 0.0, 'epoch': 0.88}
{'loss': 0.0, 'epoch': 0.89}
{'loss': 0.0, 'epoch': 0.9}
{'loss': 0.0, 'epoch': 0.91}
{'loss': 0.0, 'epoch': 0.92}
{'loss': 0.0, 'epoch': 0.92}
{'loss': 0.0, 'epoch': 0.93}
{'loss': 0.0, 'epoch': 0.94}
{'loss': 0.0, 'epoch': 0.95}
{'loss': 0.0, 'epoch': 0.96}
{'loss': 0.0, 'epoch': 0.96}
{'loss': 0.0, 'epoch': 0.97}
{'loss': 0.0, 'epoch': 0.98}
{'loss': 0.0, 'epoch': 0.99}
{'loss': 0.0, 'epoch': 1.0}
{'loss': 0.0, 'epoch': 1.0}
{'loss': 0.0, 'epoch': 1.0}
{'loss': 0.0, 'epoch': 1.01}
{'loss': 0.0, 'epoch': 1.02}
{'loss': 0.0, 'epoch': 1.03}
{'loss': 0.0, 'epoch': 1.04}
{'loss': 0.0, 'epoch': 1.04}
{'loss': 0.0, 'epoch': 1.05}
{'loss': 0.0, 'epoch': 1.06}
{'loss': 0.0, 'epoch': 1.07}
{'loss': 0.0, 'epoch': 1.08}
{'loss': 0.0, 'epoch': 1.08}
{'loss': 0.0, 'epoch': 1.09}
{'loss': 0.0, 'epoch': 1.1}
{'loss': 0.0, 'epoch': 1.11}
{'loss': 0.0, 'epoch': 1.12}
{'loss': 0.0, 'epoch': 1.12}
{'loss': 0.0, 'epoch': 1.13}
{'loss': 0.0, 'epoch': 1.14}
{'loss': 0.0, 'epoch': 1.15}
{'loss': 0.0, 'epoch': 1.16}
{'loss': 0.0, 'epoch': 1.16}
{'loss': 0.0, 'epoch': 1.17}
{'loss': 0.0, 'epoch': 1.18}
{'loss': 0.0, 'epoch': 1.19}
{'loss': 0.0, 'epoch': 1.2}
{'loss': 0.0, 'epoch': 1.2}
{'loss': 0.0, 'epoch': 1.21}
{'loss': 0.0, 'epoch': 1.22}
{'loss': 0.0, 'epoch': 1.23}
{'loss': 0.0, 'epoch': 1.24}
{'loss': 0.0, 'epoch': 1.24}
{'loss': 0.0, 'epoch': 1.25}
{'loss': 0.0, 'epoch': 1.26}
{'loss': 0.0, 'epoch': 1.27}
{'loss': 0.0, 'epoch': 1.28}
{'loss': 0.0, 'epoch': 1.29}
{'loss': 0.0, 'epoch': 1.29}
{'loss': 0.0, 'epoch': 1.3}
{'loss': 0.0, 'epoch': 1.31}
{'loss': 0.0, 'epoch': 1.32}
{'loss': 0.0, 'epoch': 1.33}
{'loss': 0.0, 'epoch': 1.33}
{'loss': 0.0, 'epoch': 1.34}
{'loss': 0.0, 'epoch': 1.35}
{'loss': 0.0, 'epoch': 1.36}
{'loss': 0.0, 'epoch': 1.37}
{'loss': 0.0, 'epoch': 1.37}
{'loss': 0.0, 'epoch': 1.38}
{'loss': 0.0, 'epoch': 1.39}
{'loss': 0.0, 'epoch': 1.4}
{'loss': 0.0, 'epoch': 1.41}
{'loss': 0.0, 'epoch': 1.41}
{'loss': 0.0, 'epoch': 1.42}
{'loss': 0.0, 'epoch': 1.43}
{'loss': 0.0, 'epoch': 1.44}
{'loss': 0.0, 'epoch': 1.45}
{'loss': 0.0, 'epoch': 1.45}
{'loss': 0.0, 'epoch': 1.46}
{'loss': 0.0, 'epoch': 1.47}
{'loss': 0.0, 'epoch': 1.48}
{'loss': 0.0, 'epoch': 1.49}
{'loss': 0.0, 'epoch': 1.49}
{'loss': 0.0, 'epoch': 1.5}
{'loss': 0.0, 'epoch': 1.51}
{'loss': 0.0, 'epoch': 1.52}
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 521, in <module>
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 477, in main
    # Don't disable token checking - we want errors to surface
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 2761, in training_step
    self.accelerator.backward(loss)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/accelerate/accelerator.py", line 2450, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 521, in <module>
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 477, in main
    # Don't disable token checking - we want errors to surface
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 2761, in training_step
    self.accelerator.backward(loss)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/accelerate/accelerator.py", line 2450, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
