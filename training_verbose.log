2025-04-18 18:20:45,004 - INFO - Loading dataset from processed_dataset
2025-04-18 18:20:45,004 - INFO - Loading processed_dataset/train.jsonl
2025-04-18 18:20:45,012 - INFO - First line of train data: {"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, shell, available_events):\n        \n        \n        self.shell = shell\n        self.callbacks = {n:[] for n in available_events}```\n\nUpdated code:\n```python\ndef __init__(self, shell, available_events, print_on_error=True):\n        \n        \n        self.shell = shell\n        self.callbacks = {n:[] for n in available_events}\n        self.print_on_error = print_on_error\n```\n\nOriginal documentation:\n```python\nInitialise the :class:`CallbackManager`.\n\n        Parameters\n        ----------\n        shell\n            The :class:`~IPython.core.interactiveshell.InteractiveShell` instance\n        available_events\n            An iterable of names for callback events.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialise the :class:`CallbackManager`.\n\n        Parameters\n        ----------\n        shell\n            The :class:`~IPython.core.interactiveshell.InteractiveShell` instance\n        available_events\n            An iterable of names for callback events.\n        print_on_error:\n            A boolean flag to set whether the EventManager will print a warning which a event errors.", "file": "IPython_core_events.py", "function": "EventManager.__init__", "project": "ipython", "stats": {"code_additions": 2, "code_deletions": 1, "doc_additions": 2, "doc_deletions": 0, "code1_len": 147, "code2_len": 213, "doc1_len": 254, "doc2_len": 380, "doc_change_ratio": 0.1111111111111111, "code_change_ratio": 0.2727272727272727, "doc_just_formatted": false, "total_code_changes": 3, "total_doc_changes": 2}}

2025-04-18 18:20:45,945 - INFO - Loaded 1990 examples from processed_dataset/train.jsonl
2025-04-18 18:20:45,945 - INFO - Loading processed_dataset/val.jsonl
2025-04-18 18:20:45,953 - INFO - First line of val data: {"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef numpy_to_tensor(value):\n    \n    if isinstance(value, torch_np.ndarray):\n        return value.tensor\n    elif isinstance(value, (tuple, list)):\n        return type(value)(numpy_to_tensor(obj) for obj in value)\n    else:\n        return value```\n\nUpdated code:\n```python\ndef numpy_to_tensor(value):\n    \n    if isinstance(value, np.ndarray):\n        return torch.as_tensor(value)\n    if isinstance(value, tnp.ndarray):\n        return value.tensor\n    elif isinstance(value, (tuple, list)):\n        return type(value)(numpy_to_tensor(obj) for obj in value)\n    else:\n        return value\n```\n\nOriginal documentation:\n```python\nConvert torch_np.ndarray to tensor, leave other types intact. If a list/tuple, loop through it to convert.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert tnp.ndarray to tensor, leave other types intact. If a list/tuple, loop through it to convert.", "file": "torch__dynamo_utils.py", "function": "numpy_to_tensor", "project": "pytorch", "stats": {"code_additions": 3, "code_deletions": 1, "doc_additions": 1, "doc_deletions": 1, "code1_len": 244, "code2_len": 315, "doc1_len": 106, "doc2_len": 101, "doc_change_ratio": 1.0, "code_change_ratio": 0.2222222222222222, "doc_just_formatted": false, "total_code_changes": 4, "total_doc_changes": 2}}

2025-04-18 18:20:46,254 - INFO - Loaded 426 examples from processed_dataset/val.jsonl
2025-04-18 18:20:46,254 - INFO - Loading processed_dataset/test.jsonl
2025-04-18 18:20:46,262 - INFO - First line of test data: {"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_interpolation(self, s):\n        \n        \n        \n        if s is None: s = rcParams['image.interpolation']\n        s = s.lower()\n        if s not in self._interpd:\n            raise ValueError('Illegal interpolation string')\n        self._interpolation = s```\n\nUpdated code:\n```python\ndef set_interpolation(self, s):\n        \n        \n\n        \n        if s is None: s = rcParams['image.interpolation']\n        s = s.lower()\n        if s not in self._interpd:\n            raise ValueError('Illegal interpolation string')\n        self._interpolation = s\n```\n\nOriginal documentation:\n```python\nSet the interpolation method the image uses when resizing.\n\n        ACCEPTS: ['bicubic' | 'bilinear' | 'blackman100' | 'blackman256' |\n        'blackman64', 'nearest' | 'sinc144' | 'sinc256' | 'sinc64' |\n        'spline16' | 'spline36']\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the interpolation method the image uses when resizing.\n\n        ACCEPTS: ['nearest' | 'bilinear' | 'bicubic' | 'spline16' |\n          'spline36' | 'hanning' | 'hamming' | 'hermite' | 'kaiser' |\n          'quadric' | 'catrom' | 'gaussian' | 'bessel' | 'mitchell' |\n          'sinc' | 'lanczos' | ]", "file": "lib_matplotlib_image.py", "function": "AxesImage.set_interpolation", "project": "matplotlib", "stats": {"code_additions": 1, "code_deletions": 0, "doc_additions": 4, "doc_deletions": 3, "code1_len": 266, "code2_len": 267, "doc1_len": 236, "doc2_len": 300, "doc_change_ratio": 0.6363636363636364, "code_change_ratio": 0.05263157894736842, "doc_just_formatted": false, "total_code_changes": 1, "total_doc_changes": 7}}

2025-04-18 18:20:46,564 - INFO - Loaded 427 examples from processed_dataset/test.jsonl
2025-04-18 18:20:46,564 - INFO - Loading tokenizer: google/flan-t5-base
2025-04-18 18:20:47,443 - INFO - Not using quantization
2025-04-18 18:20:47,443 - INFO - Loading model: google/flan-t5-base
2025-04-18 18:20:50,087 - INFO - Model type: T5ForConditionalGeneration
2025-04-18 18:20:50,088 - INFO - Model parameters: 247577856
2025-04-18 18:20:50,088 - INFO - Setting up LoRA for fine-tuning
2025-04-18 18:20:50,088 - INFO - Detected Flan-T5 model, using q,v target modules
2025-04-18 18:20:50,441 - INFO - Trainable parameters: 1769472 (0.71% of total)
2025-04-18 18:20:50,443 - INFO - Tokenizing datasets
2025-04-18 18:20:50,443 - INFO - Tokenizing train dataset with 1990 examples
2025-04-18 18:20:50,455 - INFO - Sample input length: 877
2025-04-18 18:20:50,455 - INFO - Sample target length: 380
2025-04-18 18:20:50,626 - INFO - Batch tokenized with 65866 non-padding tokens in labels
2025-04-18 18:20:50,706 - INFO - Sample input length: 1344
2025-04-18 18:20:50,706 - INFO - Sample target length: 302
2025-04-18 18:20:50,879 - INFO - Batch tokenized with 65512 non-padding tokens in labels
2025-04-18 18:20:50,965 - INFO - Tokenized train dataset has 1990 examples
2025-04-18 18:20:50,965 - INFO - Tokenizing test dataset with 427 examples
2025-04-18 18:20:50,978 - INFO - Sample input length: 1032
2025-04-18 18:20:50,978 - INFO - Sample target length: 300
2025-04-18 18:20:51,057 - INFO - Batch tokenized with 28843 non-padding tokens in labels
2025-04-18 18:20:51,092 - INFO - Tokenized test dataset has 427 examples
2025-04-18 18:20:51,093 - INFO - Tokenizing validation dataset with 426 examples
2025-04-18 18:20:51,102 - INFO - Sample input length: 928
2025-04-18 18:20:51,102 - INFO - Sample target length: 101
2025-04-18 18:20:51,176 - INFO - Batch tokenized with 28180 non-padding tokens in labels
2025-04-18 18:20:51,213 - INFO - Tokenized validation dataset has 426 examples
2025-04-18 18:20:51,530 - INFO - Total non-ignored tokens in training set: 131378
2025-04-18 18:20:51,531 - INFO - Average non-ignored tokens per example: 66.02
2025-04-18 18:20:52,686 - INFO - Starting training...
2025-04-18 18:20:52,813 - INFO - Number of non-padded label tokens: 186
2025-04-18 18:20:52,813 - INFO - Labels device: cpu, Model device: cuda:0
2025-04-18 18:20:52,829 - ERROR - Training failed with error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
2025-04-18 18:20:52,832 - ERROR - Traceback (most recent call last):
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 495, in main
    train_result = trainer.train()
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 176, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 159, in compute_loss
    outputs = model(**inputs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/peft/peft_model.py", line 1080, in forward
    return self.base_model(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1683, in forward
    encoder_outputs = self.encoder(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 988, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)

2025-04-18 18:20:52,832 - INFO - Saving model to ./api-docs-model-fixed
2025-04-18 18:20:52,868 - INFO - Training script completed!
2025-04-18 18:21:20,929 - INFO - WandB initialized with run name: flan-t5-base-debug
2025-04-18 18:21:20,929 - INFO - Loading dataset from processed_dataset
2025-04-18 18:21:20,930 - INFO - Loading processed_dataset/train.jsonl
2025-04-18 18:21:20,930 - INFO - First line of train data: {"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, shell, available_events):\n        \n        \n        self.shell = shell\n        self.callbacks = {n:[] for n in available_events}```\n\nUpdated code:\n```python\ndef __init__(self, shell, available_events, print_on_error=True):\n        \n        \n        self.shell = shell\n        self.callbacks = {n:[] for n in available_events}\n        self.print_on_error = print_on_error\n```\n\nOriginal documentation:\n```python\nInitialise the :class:`CallbackManager`.\n\n        Parameters\n        ----------\n        shell\n            The :class:`~IPython.core.interactiveshell.InteractiveShell` instance\n        available_events\n            An iterable of names for callback events.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialise the :class:`CallbackManager`.\n\n        Parameters\n        ----------\n        shell\n            The :class:`~IPython.core.interactiveshell.InteractiveShell` instance\n        available_events\n            An iterable of names for callback events.\n        print_on_error:\n            A boolean flag to set whether the EventManager will print a warning which a event errors.", "file": "IPython_core_events.py", "function": "EventManager.__init__", "project": "ipython", "stats": {"code_additions": 2, "code_deletions": 1, "doc_additions": 2, "doc_deletions": 0, "code1_len": 147, "code2_len": 213, "doc1_len": 254, "doc2_len": 380, "doc_change_ratio": 0.1111111111111111, "code_change_ratio": 0.2727272727272727, "doc_just_formatted": false, "total_code_changes": 3, "total_doc_changes": 2}}

2025-04-18 18:21:21,839 - INFO - Loaded 1990 examples from processed_dataset/train.jsonl
2025-04-18 18:21:21,840 - INFO - Loading processed_dataset/val.jsonl
2025-04-18 18:21:21,840 - INFO - First line of val data: {"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef numpy_to_tensor(value):\n    \n    if isinstance(value, torch_np.ndarray):\n        return value.tensor\n    elif isinstance(value, (tuple, list)):\n        return type(value)(numpy_to_tensor(obj) for obj in value)\n    else:\n        return value```\n\nUpdated code:\n```python\ndef numpy_to_tensor(value):\n    \n    if isinstance(value, np.ndarray):\n        return torch.as_tensor(value)\n    if isinstance(value, tnp.ndarray):\n        return value.tensor\n    elif isinstance(value, (tuple, list)):\n        return type(value)(numpy_to_tensor(obj) for obj in value)\n    else:\n        return value\n```\n\nOriginal documentation:\n```python\nConvert torch_np.ndarray to tensor, leave other types intact. If a list/tuple, loop through it to convert.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert tnp.ndarray to tensor, leave other types intact. If a list/tuple, loop through it to convert.", "file": "torch__dynamo_utils.py", "function": "numpy_to_tensor", "project": "pytorch", "stats": {"code_additions": 3, "code_deletions": 1, "doc_additions": 1, "doc_deletions": 1, "code1_len": 244, "code2_len": 315, "doc1_len": 106, "doc2_len": 101, "doc_change_ratio": 1.0, "code_change_ratio": 0.2222222222222222, "doc_just_formatted": false, "total_code_changes": 4, "total_doc_changes": 2}}

2025-04-18 18:21:22,136 - INFO - Loaded 426 examples from processed_dataset/val.jsonl
2025-04-18 18:21:22,136 - INFO - Loading processed_dataset/test.jsonl
2025-04-18 18:21:22,137 - INFO - First line of test data: {"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_interpolation(self, s):\n        \n        \n        \n        if s is None: s = rcParams['image.interpolation']\n        s = s.lower()\n        if s not in self._interpd:\n            raise ValueError('Illegal interpolation string')\n        self._interpolation = s```\n\nUpdated code:\n```python\ndef set_interpolation(self, s):\n        \n        \n\n        \n        if s is None: s = rcParams['image.interpolation']\n        s = s.lower()\n        if s not in self._interpd:\n            raise ValueError('Illegal interpolation string')\n        self._interpolation = s\n```\n\nOriginal documentation:\n```python\nSet the interpolation method the image uses when resizing.\n\n        ACCEPTS: ['bicubic' | 'bilinear' | 'blackman100' | 'blackman256' |\n        'blackman64', 'nearest' | 'sinc144' | 'sinc256' | 'sinc64' |\n        'spline16' | 'spline36']\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the interpolation method the image uses when resizing.\n\n        ACCEPTS: ['nearest' | 'bilinear' | 'bicubic' | 'spline16' |\n          'spline36' | 'hanning' | 'hamming' | 'hermite' | 'kaiser' |\n          'quadric' | 'catrom' | 'gaussian' | 'bessel' | 'mitchell' |\n          'sinc' | 'lanczos' | ]", "file": "lib_matplotlib_image.py", "function": "AxesImage.set_interpolation", "project": "matplotlib", "stats": {"code_additions": 1, "code_deletions": 0, "doc_additions": 4, "doc_deletions": 3, "code1_len": 266, "code2_len": 267, "doc1_len": 236, "doc2_len": 300, "doc_change_ratio": 0.6363636363636364, "code_change_ratio": 0.05263157894736842, "doc_just_formatted": false, "total_code_changes": 1, "total_doc_changes": 7}}

2025-04-18 18:21:22,429 - INFO - Loaded 427 examples from processed_dataset/test.jsonl
2025-04-18 18:21:22,430 - INFO - Loading tokenizer: google/flan-t5-base
2025-04-18 18:21:22,823 - INFO - Not using quantization
2025-04-18 18:21:22,823 - INFO - Loading model: google/flan-t5-base
2025-04-18 18:21:25,061 - INFO - Model type: T5ForConditionalGeneration
2025-04-18 18:21:25,062 - INFO - Model parameters: 247577856
2025-04-18 18:21:25,062 - INFO - Setting up LoRA for fine-tuning
2025-04-18 18:21:25,062 - INFO - Detected Flan-T5 model, using q,v target modules
2025-04-18 18:21:25,408 - INFO - Trainable parameters: 1769472 (0.71% of total)
2025-04-18 18:21:25,411 - INFO - Tokenizing datasets
2025-04-18 18:21:25,411 - INFO - Tokenizing train dataset with 1990 examples
2025-04-18 18:21:25,422 - INFO - Sample input length: 877
2025-04-18 18:21:25,423 - INFO - Sample target length: 380
2025-04-18 18:21:25,589 - INFO - Batch tokenized with 65866 non-padding tokens in labels
2025-04-18 18:21:25,667 - INFO - Sample input length: 1344
2025-04-18 18:21:25,668 - INFO - Sample target length: 302
2025-04-18 18:21:25,835 - INFO - Batch tokenized with 65512 non-padding tokens in labels
2025-04-18 18:21:25,926 - INFO - Tokenized train dataset has 1990 examples
2025-04-18 18:21:25,926 - INFO - Tokenizing test dataset with 427 examples
2025-04-18 18:21:25,939 - INFO - Sample input length: 1032
2025-04-18 18:21:25,939 - INFO - Sample target length: 300
2025-04-18 18:21:26,017 - INFO - Batch tokenized with 28843 non-padding tokens in labels
2025-04-18 18:21:26,058 - INFO - Tokenized test dataset has 427 examples
2025-04-18 18:21:26,058 - INFO - Tokenizing validation dataset with 426 examples
2025-04-18 18:21:26,069 - INFO - Sample input length: 928
2025-04-18 18:21:26,069 - INFO - Sample target length: 101
2025-04-18 18:21:26,143 - INFO - Batch tokenized with 28180 non-padding tokens in labels
2025-04-18 18:21:26,180 - INFO - Tokenized validation dataset has 426 examples
2025-04-18 18:21:26,493 - INFO - Total non-ignored tokens in training set: 131378
2025-04-18 18:21:26,493 - INFO - Average non-ignored tokens per example: 66.02
2025-04-18 18:21:27,690 - INFO - Starting training...
2025-04-18 18:21:27,837 - INFO - Number of non-padded label tokens: 186
2025-04-18 18:21:27,837 - INFO - Labels device: cpu, Model device: cuda:0
2025-04-18 18:21:27,842 - ERROR - Training failed with error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
2025-04-18 18:21:27,843 - ERROR - Traceback (most recent call last):
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 562, in main
    train_result = trainer.train()
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 209, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 184, in compute_loss
    outputs = model(**inputs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/peft/peft_model.py", line 1080, in forward
    return self.base_model(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1683, in forward
    encoder_outputs = self.encoder(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 988, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)

2025-04-18 18:21:27,844 - INFO - Saving model to ./api-docs-model-fixed
2025-04-18 18:21:29,516 - INFO - Training script completed!
2025-04-18 18:23:13,632 - INFO - WandB initialized with run name: flan-t5-base-device-fix
2025-04-18 18:23:13,632 - INFO - Using device: cuda
2025-04-18 18:23:13,632 - INFO - Loading dataset from processed_dataset
2025-04-18 18:23:14,611 - INFO - Loaded 1990 examples from processed_dataset/train.jsonl
2025-04-18 18:23:14,921 - INFO - Loaded 426 examples from processed_dataset/val.jsonl
2025-04-18 18:23:15,229 - INFO - Loaded 427 examples from processed_dataset/test.jsonl
2025-04-18 18:23:15,230 - INFO - Loading tokenizer: google/flan-t5-base
2025-04-18 18:23:15,690 - INFO - Not using quantization
2025-04-18 18:23:15,690 - INFO - Loading model: google/flan-t5-base
2025-04-18 18:23:19,152 - INFO - Setting up LoRA for fine-tuning
2025-04-18 18:23:19,565 - INFO - Model device after LoRA setup: cuda:0
2025-04-18 18:23:19,566 - INFO - Tokenizing datasets
2025-04-18 18:23:20,592 - INFO - Total non-ignored tokens in training set: 131378
2025-04-18 18:23:20,619 - INFO - Starting training...
2025-04-18 18:23:20,620 - INFO - Model device before training: cuda:0
2025-04-18 18:23:20,621 - INFO - Sample batch input_ids device: cuda:0
2025-04-18 18:23:20,621 - INFO - Sample batch attention_mask device: cuda:0
2025-04-18 18:23:20,621 - INFO - Sample batch labels device: cuda:0
2025-04-18 18:23:20,621 - INFO - Sample batch decoder_input_ids device: cuda:0
2025-04-18 18:23:20,770 - ERROR - Training failed with error: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned
2025-04-18 18:23:20,772 - ERROR - Traceback (most recent call last):
  File "/home/himanshu-skid19/Desktop/Advanced ML lab/Documentation_Optimizer_Using_TextGrad/train.py", line 504, in main
    train_result = trainer.train()
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/transformers/trainer.py", line 1907, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 680, in _next_data
    data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 60, in pin_memory
    return type(data)({k: pin_memory(sample, device) for k, sample in data.items()})  # type: ignore[call-arg]
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 60, in <dictcomp>
    return type(data)({k: pin_memory(sample, device) for k, sample in data.items()})  # type: ignore[call-arg]
  File "/home/himanshu-skid19/miniconda3/envs/ml-proj/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 55, in pin_memory
    return data.pin_memory(device)
RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned

2025-04-18 18:23:20,772 - INFO - Saving model to ./api-docs-model-fixed
2025-04-18 18:23:23,290 - INFO - Training script completed!
