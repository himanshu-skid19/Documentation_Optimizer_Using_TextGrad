{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get(self):\n        \n        \n        context = {\n            \"job_id\": self.job_id,\n            \"node_id\": self.node_id,\n        }\n        if self.worker.mode == ray.worker.WORKER_MODE:\n            if self.task_id is not None:\n                context[\"task_id\"] = self.task_id\n            if self.actor_id is not None:\n                context[\"actor_id\"] = self.actor_id\n\n        return context```\n\nUpdated code:\n```python\ndef get(self):\n        \n        \n        context = {\n            \"job_id\": self.job_id,\n            \"node_id\": self.node_id,\n            \"namespace\": self.namespace,\n        }\n        if self.worker.mode == ray.worker.WORKER_MODE:\n            if self.task_id is not None:\n                context[\"task_id\"] = self.task_id\n            if self.actor_id is not None:\n                context[\"actor_id\"] = self.actor_id\n\n        return context\n```\n\nOriginal documentation:\n```python\nGet a dictionary of the current context.\n\n        Fields that are not available (e.g., actor ID inside a task) won't be\n        included in the field.\n\n        Returns:\n            dict: Dictionary of the current context.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get a dictionary of the current context.\n\n\n\n        Returns:\n            dict: Dictionary of the current context.", "file": "python_ray_runtime_context.py", "function": "RuntimeContext.get", "project": "ray", "change_stats": {"code_changes": 3, "doc_changes": 5, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef context_agent(query: str, n: int):\n    \n    \n\n    \n    query_embedding = get_ada_embedding(query)\n    results = index.query(query_embedding, top_k=n, include_metadata=True, namespace=OBJECTIVE)\n    # print(\"***** RESULTS *****\")\n    # print(results)\n    sorted_results = sorted(results.matches, key=lambda x: x.score, reverse=True)\n    return [(str(item.metadata[\"task\"])) for item in sorted_results]```\n\nUpdated code:\n```python\ndef context_agent(query: str, top_results_num: int):\n    \n    \n\n    \n    count = collection.count()\n    if count == 0:\n        return []\n    results = collection.query(\n        query_texts=query, n_results=min(top_results_num, count), include=[\"metadatas\"]\n    )\n    # print(\"***** RESULTS *****\")\n    # print(results)\n    return [item[\"task\"] for item in results[\"metadatas\"][0]]\n```\n\nOriginal documentation:\n```python\nRetrieves context for a given query from an index of tasks.\n\n    Args:\n        query (str): The query or objective for retrieving context.\n        n (int): The number of top results to retrieve.\n\n    Returns:\n        list: A list of tasks as context for the given query, sorted by relevance.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieves context for a given query from an index of tasks.\n\n    Args:\n        query (str): The query or objective for retrieving context.\n        top_results_num (int): The number of top results to retrieve.\n\n    Returns:\n        list: A list of tasks as context for the given query, sorted by relevance.", "file": "babyagi.py", "function": "context_agent", "project": "babyagi", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _setup_recorder(self) -> None | int:\n        \n        tries = 1\n\n        while tries <= self.db_max_retries:\n            try:\n                self._setup_connection()\n                return migration.get_schema_version(self.get_session)\n            except UnsupportedDialect:\n                break\n            except Exception as err:  # pylint: disable=broad-except\n                _LOGGER.exception(\n                    \"Error during connection setup: %s (retrying in %s seconds)\",\n                    err,\n                    self.db_retry_wait,\n                )\n            tries += 1\n            time.sleep(self.db_retry_wait)\n\n        return None```\n\nUpdated code:\n```python\ndef _setup_recorder(self) -> bool:\n        \n        tries = 1\n\n        while tries <= self.db_max_retries:\n            try:\n                self._setup_connection()\n                return True\n            except UnsupportedDialect:\n                break\n            except Exception as err:  # pylint: disable=broad-except\n                _LOGGER.exception(\n                    \"Error during connection setup: %s (retrying in %s seconds)\",\n                    err,\n                    self.db_retry_wait,\n                )\n            tries += 1\n            time.sleep(self.db_retry_wait)\n\n        return False\n```\n\nOriginal documentation:\n```python\nCreate connect to the database and get the schema version.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a connection to the database.", "file": "homeassistant_components_recorder_core.py", "function": "Recorder._setup_recorder", "project": "core", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef stock_institute_hold_detail(stock: str = \"600433\", quarter: str = \"20201\") -> pd.DataFrame:\n    \n    \n    \n    url = \"http://vip.stock.finance.sina.com.cn/q/api/jsonp.php/var%20details=/ComStockHoldService.getJGCGDetail\"\n    params = {\n        \"symbol\": stock,\n        \"quarter\": quarter,\n        }\n    r = requests.get(url, params=params)\n    text_data = r.text\n    json_data = demjson.decode(text_data[text_data.find(\"{\"):-2])\n    big_df = pd.DataFrame()\n    for item in json_data[\"data\"].keys():\n        inner_temp_df = pd.DataFrame(json_data[\"data\"][item]).T.iloc[:-1, :]\n        inner_temp_df.reset_index(inplace=True)\n        big_df = big_df.append(inner_temp_df, ignore_index=True)\n    if not big_df.empty:\n        big_df[\"index\"] = big_df[\"index\"].str.split(\"_\", expand=True)[0]\n        big_df.rename(columns={\"index\": \"institute\"}, inplace=True)\n        big_df = big_df.iloc[:, :12]\n        big_df.columns = [\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\",\n                          \"\u6301\u80a1\u673a\u6784\u4ee3\u7801\",\n                          \"\u6301\u80a1\u673a\u6784\u7b80\u79f0\",\n                          \"\u6301\u80a1\u673a\u6784\u5168\u79f0\",\n                          \"\u6301\u80a1\u6570\",\n                          \"\u6700\u65b0\u6301\u80a1\u6570\",\n                          \"\u6301\u80a1\u6bd4\u4f8b\",\n                          \"\u6700\u65b0\u6301\u80a1\u6bd4\u4f8b\",\n                          \"\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\",\n                          \"\u6700\u65b0\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\",\n                          \"\u6301\u80a1\u6bd4\u4f8b\u589e\u5e45\",\n                          \"\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\u589e\u5e45\",\n                          ]\n        big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"] = big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"].str.replace(\"fund\", \"\u57fa\u91d1\")\n        big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"] = big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"].str.replace(\"socialSecurity\", \"\u5168\u56fd\u793e\u4fdd\")\n        big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"] = big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"].str.replace(\"qfii\", \"QFII\")\n        return big_df\n    else:\n        return None```\n\nUpdated code:\n```python\ndef stock_institute_hold_detail(\n    stock: str = \"600433\", quarter: str = \"20201\"\n) -> pd.DataFrame:\n    \n    \n    \n    url = \"https://vip.stock.finance.sina.com.cn/q/api/jsonp.php/var%20details=/ComStockHoldService.getJGCGDetail\"\n    params = {\n        \"symbol\": stock,\n        \"quarter\": quarter,\n    }\n    r = requests.get(url, params=params)\n    text_data = r.text\n    json_data = demjson.decode(text_data[text_data.find(\"{\") : -2])\n    big_df = pd.DataFrame()\n    for item in json_data[\"data\"].keys():\n        inner_temp_df = pd.DataFrame(json_data[\"data\"][item]).T.iloc[:-1, :]\n        inner_temp_df.reset_index(inplace=True)\n        big_df = pd.concat([big_df, inner_temp_df], ignore_index=True)\n    if not big_df.empty:\n        big_df[\"index\"] = big_df[\"index\"].str.split(\"_\", expand=True)[0]\n        big_df.rename(columns={\"index\": \"institute\"}, inplace=True)\n        big_df = big_df.iloc[:, :12]\n        big_df.columns = [\n            \"\u6301\u80a1\u673a\u6784\u7c7b\u578b\",\n            \"\u6301\u80a1\u673a\u6784\u4ee3\u7801\",\n            \"\u6301\u80a1\u673a\u6784\u7b80\u79f0\",\n            \"\u6301\u80a1\u673a\u6784\u5168\u79f0\",\n            \"\u6301\u80a1\u6570\",\n            \"\u6700\u65b0\u6301\u80a1\u6570\",\n            \"\u6301\u80a1\u6bd4\u4f8b\",\n            \"\u6700\u65b0\u6301\u80a1\u6bd4\u4f8b\",\n            \"\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\",\n            \"\u6700\u65b0\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\",\n            \"\u6301\u80a1\u6bd4\u4f8b\u589e\u5e45\",\n            \"\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\u589e\u5e45\",\n        ]\n        big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"] = big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"].str.replace(\"fund\", \"\u57fa\u91d1\")\n        big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"] = big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"].str.replace(\"socialSecurity\", \"\u5168\u56fd\u793e\u4fdd\")\n        big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"] = big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"].str.replace(\"qfii\", \"QFII\")\n        big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"] = big_df[\"\u6301\u80a1\u673a\u6784\u7c7b\u578b\"].str.replace(\"insurance\", \"\u4fdd\u9669\")\n\n        big_df['\u6301\u80a1\u6570'] = pd.to_numeric(big_df['\u6301\u80a1\u6570'], errors=\"coerce\")\n        big_df['\u6700\u65b0\u6301\u80a1\u6570'] = pd.to_numeric(big_df['\u6700\u65b0\u6301\u80a1\u6570'], errors=\"coerce\")\n        big_df['\u6301\u80a1\u6bd4\u4f8b'] = pd.to_numeric(big_df['\u6301\u80a1\u6bd4\u4f8b'], errors=\"coerce\")\n        big_df['\u6700\u65b0\u6301\u80a1\u6bd4\u4f8b'] = pd.to_numeric(big_df['\u6700\u65b0\u6301\u80a1\u6bd4\u4f8b'], errors=\"coerce\")\n        big_df['\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b'] = pd.to_numeric(big_df['\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b'], errors=\"coerce\")\n        big_df['\u6700\u65b0\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b'] = pd.to_numeric(big_df['\u6700\u65b0\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b'], errors=\"coerce\")\n        big_df['\u6301\u80a1\u6bd4\u4f8b\u589e\u5e45'] = pd.to_numeric(big_df['\u6301\u80a1\u6bd4\u4f8b\u589e\u5e45'], errors=\"coerce\")\n        big_df['\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\u589e\u5e45'] = pd.to_numeric(big_df['\u5360\u6d41\u901a\u80a1\u6bd4\u4f8b\u589e\u5e45'], errors=\"coerce\")\n\n        return big_df\n    else:\n        return None\n```\n\nOriginal documentation:\n```python\n\u65b0\u6d6a\u8d22\u7ecf-\u80a1\u7968-\u673a\u6784\u6301\u80a1\u8be6\u60c5\n    http://vip.stock.finance.sina.com.cn/q/go.php/vComStockHold/kind/jgcg/index.phtml\n    :param stock: \u80a1\u7968\u4ee3\u7801\n    :type stock: str\n    :param quarter: \u4ece 2005 \u5e74\u5f00\u59cb, {\"\u4e00\u5b63\u62a5\":1, \"\u4e2d\u62a5\":2 \"\u4e09\u5b63\u62a5\":3 \"\u5e74\u62a5\":4}, e.g., \"20191\", \u5176\u4e2d\u7684 1 \u8868\u793a\u4e00\u5b63\u62a5; \"20193\", \u5176\u4e2d\u7684 3 \u8868\u793a\u4e09\u5b63\u62a5;\n    :type quarter: str\n    :return: \u6307\u5b9a\u80a1\u7968\u548c\u8d22\u62a5\u65f6\u95f4\u7684\u673a\u6784\u6301\u80a1\u6570\u636e\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u65b0\u6d6a\u8d22\u7ecf-\u80a1\u7968-\u673a\u6784\u6301\u80a1\u8be6\u60c5\n    https://vip.stock.finance.sina.com.cn/q/go.php/vComStockHold/kind/jgcg/index.phtml\n    :param stock: \u80a1\u7968\u4ee3\u7801\n    :type stock: str\n    :param quarter: \u4ece 2005 \u5e74\u5f00\u59cb, {\"\u4e00\u5b63\u62a5\":1, \"\u4e2d\u62a5\":2 \"\u4e09\u5b63\u62a5\":3 \"\u5e74\u62a5\":4}, e.g., \"20191\", \u5176\u4e2d\u7684 1 \u8868\u793a\u4e00\u5b63\u62a5; \"20193\", \u5176\u4e2d\u7684 3 \u8868\u793a\u4e09\u5b63\u62a5;\n    :type quarter: str\n    :return: \u6307\u5b9a\u80a1\u7968\u548c\u8d22\u62a5\u65f6\u95f4\u7684\u673a\u6784\u6301\u80a1\u6570\u636e\n    :rtype: pandas.DataFrame", "file": "akshare_stock_fundamental_stock_hold.py", "function": "stock_institute_hold_detail", "project": "akshare", "change_stats": {"code_changes": 52, "doc_changes": 4, "total_changes": 56}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef decorrelated_batch_normalization(x, **kwargs):\n    \n\n      # NOQA\n    groups, eps, running_mean, running_projection, decay = \\\n        argument.parse_kwargs(\n            kwargs, ('groups', 16), ('eps', 2e-5), ('running_mean', None),\n            ('running_projection', None), ('decay', 0.9))\n\n    f = DecorrelatedBatchNormalizationFunction(groups, eps, running_mean,\n                                               running_projection,\n                                               decay)\n    return f.apply((x,))[0]```\n\nUpdated code:\n```python\ndef decorrelated_batch_normalization(x, **kwargs):\n    \n\n    \n    groups, eps, running_mean, running_projection, decay = \\\n        argument.parse_kwargs(\n            kwargs, ('groups', 16), ('eps', 2e-5), ('running_mean', None),\n            ('running_projection', None), ('decay', 0.9))\n\n    f = DecorrelatedBatchNormalization(\n        groups, eps, running_mean, running_projection, decay)\n    return f.apply((x,))[0]\n```\n\nOriginal documentation:\n```python\nDecorrelated batch normalization function.\n\n    It takes the input variable ``x`` and normalizes it using\n    batch statistics to make the output zero-mean and decorrelated.\n\n    Args:\n        x (:class:`~chainer.Variable`): Input variable.\n        groups (int): Number of groups to use for group whitening.\n        eps (float): Epsilon value for numerical stability.\n        running_mean (:ref:`ndarray`): Expected value of the mean. This is a\n            running average of the mean over several mini-batches using\n            the decay parameter. If ``None``, the expected mean is initialized\n            to zero.\n        running_projection (:ref:`ndarray`):\n            Expected value of the project matrix. This is a\n            running average of the projection over several mini-batches using\n            the decay parameter. If ``None``, the expected projected is\n            initialized to the identity matrix.\n        decay (float): Decay rate of moving average. It is used during\n            training.\n\n    Returns:\n        ~chainer.Variable: The output variable which has the same shape as\n        :math:`x`.\n\n    See: `Decorrelated Batch Normalization <https://arxiv.org/abs/1804.08450>`_\n\n    .. seealso:: :class:`links.DecorrelatedBatchNormalization`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "decorrelated_batch_normalization(x, groups=16, eps2e-5,\nrunning_mean=None, running_projection=None, decay=0.9)\n\n    Decorrelated batch normalization function.\n\n    It takes the input variable ``x`` and normalizes it using\n    batch statistics to make the output zero-mean and decorrelated.\n\n    Args:\n        x (:class:`~chainer.Variable`): Input variable.\n        groups (int): Number of groups to use for group whitening.\n        eps (float): Epsilon value for numerical stability.\n        running_mean (:ref:`ndarray`): Expected value of the mean. This is a\n            running average of the mean over several mini-batches using\n            the decay parameter. If ``None``, the expected mean is initialized\n            to zero.\n        running_projection (:ref:`ndarray`):\n            Expected value of the project matrix. This is a\n            running average of the projection over several mini-batches using\n            the decay parameter. If ``None``, the expected projected is\n            initialized to the identity matrix.\n        decay (float): Decay rate of moving average. It is used during\n            training.\n\n    Returns:\n        ~chainer.Variable: The output variable which has the same shape as\n        :math:`x`.\n\n    See: `Decorrelated Batch Normalization <https://arxiv.org/abs/1804.08450>`_\n\n    .. seealso:: :class:`links.DecorrelatedBatchNormalization`", "file": "chainer_", "function": "decorrelated_batch_normalization", "project": "chainer", "change_stats": {"code_changes": 9, "doc_changes": 7, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, query, key, value, mask):\n        \n        \n        n_batch = query.size(0)\n        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch, head, time1, time2)\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n            scores = scores.masked_fill(mask == 0, MIN_VALUE)\n        self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(self.attn)\n        x = torch.matmul(p_attn, v)  # (batch, head, time1, d_k)\n        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)  # (batch, time1, d_model)\n        return self.linear_out(x)```\n\nUpdated code:\n```python\ndef forward(self, query, key, value, mask):\n        \n        \n        n_batch = query.size(0)\n        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch, head, time1, time2)\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n            scores = scores.masked_fill(~mask, MIN_VALUE)\n        self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n        if mask is not None:\n            self.attn = self.attn.masked_fill(~mask, 0.0)\n\n        p_attn = self.dropout(self.attn)\n        x = torch.matmul(p_attn, v)  # (batch, head, time1, d_k)\n        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)  # (batch, time1, d_model)\n        return self.linear_out(x)\n```\n\nOriginal documentation:\n```python\nCompute 'Scaled Dot Product Attention'\n\n        :param torch.Tensor query: (batch, time1, size)\n        :param torch.Tensor key: (batch, time2, size)\n        :param torch.Tensor value: (batch, time2, size)\n        :param torch.Tensor mask: (batch, time1)\n        :param torch.nn.Dropout dropout:\n        :return torch.Tensor: attentined and transformed `value` (batch, time1, d_model)\n             weighted by the query dot key attention (batch, head, time1, time2)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute 'Scaled Dot Product Attention'\n\n        :param torch.Tensor query: (batch, time1, size)\n        :param torch.Tensor key: (batch, time2, size)\n        :param torch.Tensor value: (batch, time2, size)\n        :param torch.Tensor mask: (batch, time1, time2)\n        :param torch.nn.Dropout dropout:\n        :return torch.Tensor: attentined and transformed `value` (batch, time1, d_model)\n             weighted by the query dot key attention (batch, head, time1, time2)", "file": "espnet_nets_pytorch_backend_transformer_attention.py", "function": "MultiHeadedAttention.forward", "project": "espnet", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cancel(self):\n        \n        \n        # TODO(edoakes): when actor task cancellation is supported, we should cancel\n        # the scheduled actor task here if the assign request task is done.\n        self._assign_request_task.cancel()```\n\nUpdated code:\n```python\ndef cancel(self):\n        \n        \n        if not self._assign_request_task.done():\n            self._assign_request_task.cancel()\n        elif self._assign_request_task.exception() is None:\n            ray.cancel(self._assign_request_task.result())\n```\n\nOriginal documentation:\n```python\nAttempt to cancel the `DeploymentHandle` call.\n\n        This is best effort and will only successfully cancel the call if it has not yet\n        been assigned to a replica actor. If the call is successfully cancelled,\n        subsequent operations on the ref will raise an `asyncio.CancelledError` (or a\n        `concurrent.futures.CancelledError` if using synchronous methods like\n        `.result()`).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Attempt to cancel the `DeploymentHandle` call.\n\n        This is best effort.\n\n        - If the request hasn't been assigned to a replica actor, the assignment will be\n          cancelled.\n        - If the request has been assigned to a replica actor, `ray.cancel` will be\n          called on the object ref, attempting to cancel the request and any downstream\n          requests it makes.\n\n        If the request is successfully cancelled, subsequent operations on the ref will\n        raise an exception:\n\n            - If the request was cancelled before assignment, they'll raise\n              `asyncio.CancelledError` (or a `concurrent.futures.CancelledError` for\n              synchronous methods like `.result()`.).\n            - If the request was cancelled after assignment, they'll raise\n              `ray.exceptions.TaskCancelledError`.", "file": "python_ray_serve_handle.py", "function": "_DeploymentResponseBase.cancel", "project": "ray", "change_stats": {"code_changes": 9, "doc_changes": 23, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef qr(a, mode=\"full\"):\n    \n    \n\n    \n\n    x = [[2, 1], [3, 4]]\n    if isinstance(numpy.linalg.qr(x, mode), tuple):\n        return QRFull(mode)(a)\n    else:\n        return QRIncomplete(mode)(a)```\n\nUpdated code:\n```python\ndef qr(a, mode=\"reduced\"):\n    \n    \n\n    \n\n    x = [[2, 1], [3, 4]]\n    if isinstance(numpy.linalg.qr(x, mode), tuple):\n        return QRFull(mode)(a)\n    else:\n        return QRIncomplete(mode)(a)\n```\n\nOriginal documentation:\n```python\nComputes the QR decomposition of a matrix.\n    Factor the matrix a as qr, where q\n    is orthonormal and r is upper-triangular.\n\n    Parameters\n    ----------\n    a : array_like, shape (M, N)\n        Matrix to be factored.\n\n    mode : {'reduced', 'complete', 'r', 'raw', 'full', 'economic'}, optional\n        If K = min(M, N), then\n\n        'reduced'\n          returns q, r with dimensions (M, K), (K, N)\n\n        'complete'\n           returns q, r with dimensions (M, M), (M, N)\n\n        'r'\n          returns r only with dimensions (K, N)\n\n        'raw'\n          returns h, tau with dimensions (N, M), (K,)\n\n        'full'\n          alias of 'reduced', deprecated (default)\n\n        'economic'\n          returns h from 'raw', deprecated.\n\n        The options 'reduced', 'complete', and 'raw' are new in numpy\n        1.8, see the notes for more information. The default is\n        'reduced' and to maintain backward compatibility with earlier\n        versions of numpy both it and the old default 'full' can be\n        omitted. Note that array h returned in 'raw' mode is\n        transposed for calling Fortran. The 'economic' mode is\n        deprecated. The modes 'full' and 'economic' may be passed\n        using only the first letter for backwards compatibility, but\n        all others must be spelled out.\n\n        Default mode is 'full' which is also default for numpy 1.6.1.\n\n        :note: Default mode was left to full as full and reduced are\n           both doing the same thing in the new numpy version but only\n           full works on the old previous numpy version.\n\n    Returns\n    -------\n    q : matrix of float or complex, optional\n        A matrix with orthonormal columns. When mode = 'complete' the\n        result is an orthogonal/unitary matrix depending on whether or\n        not a is real/complex. The determinant may be either +/- 1 in\n        that case.\n    r : matrix of float or complex, optional\n        The upper-triangular matrix.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes the QR decomposition of a matrix.\n    Factor the matrix a as qr, where q\n    is orthonormal and r is upper-triangular.\n\n    Parameters\n    ----------\n    a : array_like, shape (M, N)\n        Matrix to be factored.\n\n    mode : {'reduced', 'complete', 'r', 'raw'}, optional\n        If K = min(M, N), then\n\n        'reduced'\n          returns q, r with dimensions (M, K), (K, N)\n\n        'complete'\n           returns q, r with dimensions (M, M), (M, N)\n\n        'r'\n          returns r only with dimensions (K, N)\n\n        'raw'\n          returns h, tau with dimensions (N, M), (K,)\n\n        The options 'reduced', 'complete', and 'raw' are new in numpy\n        1.8, see the notes for more information. The default is\n        'reduced' and to maintain backward compatibility with earlier\n        versions of numpy it can be\n        omitted. Note that array h returned in 'raw' mode is\n        transposed for calling Fortran.\n\n        Default mode is 'reduced' which is also default for numpy 1.8\n\n    Returns\n    -------\n    q : matrix of float or complex, optional\n        A matrix with orthonormal columns. When mode = 'complete' the\n        result is an orthogonal/unitary matrix depending on whether or\n        not a is real/complex. The determinant may be either +/- 1 in\n        that case.\n    r : matrix of float or complex, optional\n        The upper-triangular matrix.", "file": "theano_tensor_nlinalg.py", "function": "qr", "project": "Theano", "change_stats": {"code_changes": 4, "doc_changes": 23, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef json_dump(data, filename):\n    \n    \n    \n    with open(filename, 'w') as fh:\n        try:\n            json.dump(data, fh, cls=_JSONEncoder, indent=2)\n        except OSError as e:\n            _log.warning('Could not save font_manager cache {}'.format(e))```\n\nUpdated code:\n```python\ndef json_dump(data, filename):\n    \n    \n    \n    with cbook._lock_path(filename), open(filename, 'w') as fh:\n        try:\n            json.dump(data, fh, cls=_JSONEncoder, indent=2)\n        except OSError as e:\n            _log.warning('Could not save font_manager cache {}'.format(e))\n```\n\nOriginal documentation:\n```python\nDump `FontManager` *data* as JSON to the file named *filename*.\n\n    See Also\n    --------\n    json_load\n\n    Notes\n    -----\n    File paths that are children of the Matplotlib data path (typically, fonts\n    shipped with Matplotlib) are stored relative to that data path (to remain\n    valid across virtualenvs).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Dump `FontManager` *data* as JSON to the file named *filename*.\n\n    See Also\n    --------\n    json_load\n\n    Notes\n    -----\n    File paths that are children of the Matplotlib data path (typically, fonts\n    shipped with Matplotlib) are stored relative to that data path (to remain\n    valid across virtualenvs).\n\n    This function temporarily locks the output file to prevent multiple\n    processes from overwriting one another's output.", "file": "lib_matplotlib_font_manager.py", "function": "json_dump", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef flush(self, include_footers=False, callback=None):\n        \n        \n        chunk = b\"\".join(self._write_buffer)\n        self._write_buffer = []\n        if not self._headers_written:\n            self._headers_written = True\n            for transform in self._transforms:\n                self._status_code, self._headers, chunk = \\\n                    transform.transform_first_chunk(\n                        self._status_code, self._headers,\n                        chunk, include_footers)\n            # Ignore the chunk and only write the headers for HEAD requests\n            if self.request.method == \"HEAD\":\n                chunk = None\n\n            # Finalize the cookie headers (which have been stored in a side\n            # object so an outgoing cookie could be overwritten before it\n            # is sent).\n            if hasattr(self, \"_new_cookie\"):\n                for cookie in self._new_cookie.values():\n                    self.add_header(\"Set-Cookie\", cookie.OutputString(None))\n\n            start_line = httputil.ResponseStartLine('',\n                                                    self._status_code,\n                                                    self._reason)\n            return self.request.connection.write_headers(\n                start_line, self._headers, chunk, callback=callback)\n        else:\n            for transform in self._transforms:\n                chunk = transform.transform_chunk(chunk, include_footers)\n            # Ignore the chunk and only write the headers for HEAD requests\n            if self.request.method != \"HEAD\":\n                return self.request.connection.write(chunk, callback=callback)\n            else:\n                future = Future()\n                future.set_result(None)\n                return future```\n\nUpdated code:\n```python\ndef flush(self, include_footers=False):\n        \n        \n        chunk = b\"\".join(self._write_buffer)\n        self._write_buffer = []\n        if not self._headers_written:\n            self._headers_written = True\n            for transform in self._transforms:\n                self._status_code, self._headers, chunk = \\\n                    transform.transform_first_chunk(\n                        self._status_code, self._headers,\n                        chunk, include_footers)\n            # Ignore the chunk and only write the headers for HEAD requests\n            if self.request.method == \"HEAD\":\n                chunk = None\n\n            # Finalize the cookie headers (which have been stored in a side\n            # object so an outgoing cookie could be overwritten before it\n            # is sent).\n            if hasattr(self, \"_new_cookie\"):\n                for cookie in self._new_cookie.values():\n                    self.add_header(\"Set-Cookie\", cookie.OutputString(None))\n\n            start_line = httputil.ResponseStartLine('',\n                                                    self._status_code,\n                                                    self._reason)\n            return self.request.connection.write_headers(\n                start_line, self._headers, chunk)\n        else:\n            for transform in self._transforms:\n                chunk = transform.transform_chunk(chunk, include_footers)\n            # Ignore the chunk and only write the headers for HEAD requests\n            if self.request.method != \"HEAD\":\n                return self.request.connection.write(chunk)\n            else:\n                future = Future()\n                future.set_result(None)\n                return future\n```\n\nOriginal documentation:\n```python\nFlushes the current output buffer to the network.\n\n        The ``callback`` argument, if given, can be used for flow control:\n        it will be run when all flushed data has been written to the socket.\n        Note that only one flush callback can be outstanding at a time;\n        if another flush occurs before the previous flush's callback\n        has been run, the previous callback will be discarded.\n\n        .. versionchanged:: 4.0\n           Now returns a `.Future` if no callback is given.\n\n        .. deprecated:: 5.1\n\n           The ``callback`` argument is deprecated and will be removed in\n           Tornado 6.0.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Flushes the current output buffer to the network.\n\n        The ``callback`` argument, if given, can be used for flow control:\n        it will be run when all flushed data has been written to the socket.\n        Note that only one flush callback can be outstanding at a time;\n        if another flush occurs before the previous flush's callback\n        has been run, the previous callback will be discarded.\n\n        .. versionchanged:: 4.0\n           Now returns a `.Future` if no callback is given.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed.", "file": "tornado_web.py", "function": "RequestHandler.flush", "project": "tornado", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef setup_std_streams(self):\n          \n\n        std_streams = [self.stderr, self.stdout, self.logsfile]\n        for file in std_streams:\n            # create the file if it doesn't exist or clear it if it exists\n            try:\n                open(file,'w').close()\n            except FileNotFoundError:\n                os.mkdir(os.path.dirname(file))\n                open(file,'w').close()```\n\nUpdated code:\n```python\ndef setup_std_streams(self):\n          \n\n        std_streams = [self.stderr, self.stdout, self.logsfile]\n        for file in std_streams:\n            # we don't want to clear the logfile when we stop the daemon using -S\n            if '-S' in sys.argv and file == self.stdout:\n                continue\n            # create the file if it doesn't exist or clear it if it exists\n            try:\n                open(file,'w').close()\n            except (FileNotFoundError,NotADirectoryError):\n                os.mkdir(os.path.dirname(file))\n                open(file,'w').close()\n```\n\nOriginal documentation:\n```python\nCreate standard steam files and dirs and clear logs file\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create standard steam files and dirs and clear them", "file": "slips.py", "function": "Daemon.setup_std_streams", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, file=None, lineno_width=0, offset_width=0, label_width=0,\n                       line_offset=0):\n        \n        \n        self.file = file\n        self.lineno_width = lineno_width\n        self.offset_width = offset_width\n        self.label_width = label_width```\n\nUpdated code:\n```python\ndef __init__(self, file=None, lineno_width=0, offset_width=0, label_width=0,\n                       line_offset=0, show_caches=False):\n        \n\n        \n        self.file = file\n        self.lineno_width = lineno_width\n        self.offset_width = offset_width\n        self.label_width = label_width\n        self.show_caches = show_caches\n```\n\nOriginal documentation:\n```python\nCreate a Formatter\n\n        *file* where to write the output\n        *lineno_width* sets the width of the line number field (0 omits it)\n        *offset_width* sets the width of the instruction offset field\n        *label_width* sets the width of the label field\n\n        *line_offset* the line number (within the code unit)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a Formatter\n\n        *file* where to write the output\n        *lineno_width* sets the width of the line number field (0 omits it)\n        *offset_width* sets the width of the instruction offset field\n        *label_width* sets the width of the label field\n        *show_caches* is a boolean indicating whether to display cache lines", "file": "Lib_dis.py", "function": "Formatter.__init__", "project": "cpython", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_product_list_context(request, filter_set):\n    \n    \n    # Avoiding circular dependency\n    from ..filters import SORT_BY_FIELDS\n\n    qs = filter_set.qs\n    if not filter_set.form.is_valid():\n        qs = qs.none()\n    products_paginated = get_paginator_items(\n        qs, settings.PAGINATE_BY, request.GET.get(\"page\")\n    )\n    products_and_availability = list(\n        products_with_availability(\n            products_paginated,\n            request.discounts,\n            request.country,\n            request.currency,\n            request.extensions,\n        )\n    )\n    now_sorted_by = get_now_sorted_by(filter_set)\n    arg_sort_by = request.GET.get(\"sort_by\")\n    is_descending = arg_sort_by.startswith(\"-\") if arg_sort_by else False\n    return {\n        \"filter_set\": filter_set,\n        \"products\": products_and_availability,\n        \"products_paginated\": products_paginated,\n        \"sort_by_choices\": SORT_BY_FIELDS,\n        \"now_sorted_by\": now_sorted_by,\n        \"is_descending\": is_descending,\n    }```\n\nUpdated code:\n```python\ndef get_product_list_context(\n    request: \"HttpRequest\", filter_set: \"ProductCategoryFilter\"\n) -> dict:\n    \n    # Avoiding circular dependency\n    from ..filters import SORT_BY_FIELDS\n\n    qs = filter_set.qs\n    if not filter_set.form.is_valid():\n        qs = qs.none()\n    products_paginated = get_paginator_items(\n        qs, settings.PAGINATE_BY, request.GET.get(\"page\")\n    )\n    products_and_availability = list(\n        products_with_availability(\n            products_paginated,\n            request.discounts,\n            request.country,\n            request.currency,\n            request.extensions,\n        )\n    )\n    now_sorted_by = get_now_sorted_by(filter_set)\n    arg_sort_by = request.GET.get(\"sort_by\")\n    is_descending = arg_sort_by.startswith(\"-\") if arg_sort_by else False\n    return {\n        \"filter_set\": filter_set,\n        \"products\": products_and_availability,\n        \"products_paginated\": products_paginated,\n        \"sort_by_choices\": SORT_BY_FIELDS,\n        \"now_sorted_by\": now_sorted_by,\n        \"is_descending\": is_descending,\n    }\n```\n\nOriginal documentation:\n```python\nBuild a context from the given filter set.\n\n    :param request: request object\n    :param filter_set: filter set for product list\n    :return: context dictionary\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Build a context from the given filter set.", "file": "saleor_product_utils___init__.py", "function": "get_product_list_context", "project": "saleor", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _orthogonalize_gram_schmidt(matrix, epsilon=0):\n    \n    \n    \n    num_cols = matrix.shape[1]\n    for i in range(num_cols):\n        # Normalize the i'th column.\n        col = matrix[:, i : i + 1]\n        # If no epsilon is added here, division by zero may be caused by vanishing gradients.\n        # This epsilon is not needed if the input matrix covers the gradients of at least one entire layer in the neural network.\n        if epsilon == 0:\n            # Note that col ** 2 can underflow/overflow if we use FP16.\n            # May need to consider multiplying a scaling factor and dividing it later, or using bfloat16 instead.\n            try:\n                col /= torch.norm(col)\n            except ZeroDivisionError:\n                logger.error(\n                    \"The matrix to be orthogonalized has at least a column of all 0s. Please set a small value such as 1e-8 \"\n                    \"as `orthogonalization_epsilon` in PowerSGD state.\"\n                )\n                # Recover the values from NaNs to 0s.\n                col.fill_(0.0)\n        else:\n            col /= torch.norm(col) + epsilon\n        # Project it on the rest and remove it.\n        if i + 1 < num_cols:\n            rest = matrix[:, i + 1 :]\n            rest -= torch.sum(col * rest, dim=0) * col```\n\nUpdated code:\n```python\ndef _orthogonalize_gram_schmidt(matrices, epsilon=0):\n    \n    \n    \n    num_cols = matrices.shape[2]\n    for i in range(num_cols):\n        # Normalize the i'th column.\n        col = matrices[:, :, i : i + 1]\n        # If no epsilon is added here, division by zero may be caused by vanishing gradients.\n        # This epsilon is not needed if the input batch of matrices covers the gradients of at least one entire layer\n        # in the neural network.\n        if epsilon == 0:\n            # Note that col ** 2 can underflow/overflow if we use FP16.\n            # May need to consider multiplying a scaling factor and dividing it later, or using bfloat16 instead.\n            try:\n                col /= torch.norm(col, dim=1, keepdim=True)\n            except ZeroDivisionError:\n                logger.error(\n                    \"The matrices to be orthogonalized has at least a column of all 0s. Please set a small value such as 1e-8 \"\n                    \"as `orthogonalization_epsilon` in PowerSGD state.\"\n                )\n                # Recover the values from NaNs to 0s.\n                col.fill_(0.0)\n        else:\n            col /= torch.norm(col, dim=1, keepdim=True) + epsilon\n        # Project it on the rest and remove it.\n        if i + 1 < num_cols:\n            rest = matrices[:, :, i + 1 :]\n            rest -= torch.sum(col * rest, dim=1, keepdim=True) * col\n```\n\nOriginal documentation:\n```python\nApplies Gram-Schmidt procedure to orthogonalize a given 2D tensor.\n    If epsilon is 0, this is equivalent to `torch.qr(matrix, out=(matrix, _))`,\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Applies Gram-Schmidt procedure to orthogonalize a batch of matrices.\n    If epsilon is 0, this is equivalent to `torch.qr(matrices, out=(matrices, _))`,", "file": "torch_distributed_algorithms_ddp_comm_hooks_powerSGD_hook.py", "function": "_orthogonalize_gram_schmidt", "project": "pytorch", "change_stats": {"code_changes": 21, "doc_changes": 6, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef for_provider(\n        provider: ServiceProvider,\n        dispatch_table_factory: Callable[[ServiceProvider], DispatchTable] = None,\n        service_lifecycle_hook: ServiceLifecycleHook = None,\n        custom_service_name: Optional[str] = None,\n    ) -> \"Service\":\n        \n        \n        \n        # determine the service_lifecycle_hook\n        if service_lifecycle_hook is None:\n            if isinstance(provider, ServiceLifecycleHook):\n                service_lifecycle_hook = provider\n\n        # determine the delegate for injecting into the skeleton\n        delegate = dispatch_table_factory(provider) if dispatch_table_factory else provider\n        service_name = custom_service_name or provider.service\n        service = Service(\n            name=service_name,\n            skeleton=Skeleton(load_service(service_name), delegate),\n            lifecycle_hook=service_lifecycle_hook,\n        )\n        service._provider = provider\n\n        return service```\n\nUpdated code:\n```python\ndef for_provider(\n        provider: ServiceProvider,\n        dispatch_table_factory: Callable[[ServiceProvider], DispatchTable] = None,\n        service_lifecycle_hook: ServiceLifecycleHook = None,\n    ) -> \"Service\":\n        \n        \n        \n        # determine the service_lifecycle_hook\n        if service_lifecycle_hook is None:\n            if isinstance(provider, ServiceLifecycleHook):\n                service_lifecycle_hook = provider\n\n        # determine the delegate for injecting into the skeleton\n        delegate = dispatch_table_factory(provider) if dispatch_table_factory else provider\n        service = Service(\n            name=provider.service,\n            skeleton=Skeleton(load_service(provider.service), delegate),\n            lifecycle_hook=service_lifecycle_hook,\n        )\n        service._provider = provider\n\n        return service\n```\n\nOriginal documentation:\n```python\nFactory method for creating services for providers. This method hides a bunch of legacy code and\n        band-aids/adapters to make persistence visitors work, while providing compatibility with the legacy edge proxy.\n\n        :param provider: the service provider, i.e., the implementation of the generated ASF service API.\n        :param dispatch_table_factory: a `MotoFallbackDispatcher` or something similar that uses the provider to\n            create a dispatch table. this one's a bit clumsy.\n        :param service_lifecycle_hook: if left empty, the factory checks whether the provider is a ServiceLifecycleHook.\n        :param custom_service_name: allows defining a custom name for this service (instead of the one in the provider).\n        :return: a service instance\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Factory method for creating services for providers. This method hides a bunch of legacy code and\n        band-aids/adapters to make persistence visitors work, while providing compatibility with the legacy edge proxy.\n\n        :param provider: the service provider, i.e., the implementation of the generated ASF service API.\n        :param dispatch_table_factory: a `MotoFallbackDispatcher` or something similar that uses the provider to\n            create a dispatch table. this one's a bit clumsy.\n        :param service_lifecycle_hook: if left empty, the factory checks whether the provider is a ServiceLifecycleHook.\n        :return: a service instance", "file": "localstack_services_plugins.py", "function": "Service.for_provider", "project": "localstack", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef loss(self, model, guide, *args, **kwargs):\n        \n        \n        \n        elbo = 0.0\n        for weights, model_trace, guide_trace in self._get_traces(model, guide, *args, **kwargs):\n            elbo_particle = 0\n            for name, model_site in model_trace.nodes.items():\n                if model_site[\"type\"] != \"sample\":\n                    continue\n\n                # grab weights introduced by enumeration\n                cond_indep_stack = model_site[\"cond_indep_stack\"]\n                weight = weights.get_upstream(cond_indep_stack)\n                if weight is None:\n                    continue\n\n                log_r = model_site[\"batch_log_pdf\"]\n                if not model_site[\"is_observed\"]:\n                    log_r = log_r - guide_trace.nodes[name][\"batch_log_pdf\"]\n                elbo_particle += (log_r * weight).sum().item()\n\n            elbo += elbo_particle / self.num_particles\n\n        loss = -elbo\n        if is_nan(loss):\n            warnings.warn('Encountered NAN loss')\n        return loss```\n\nUpdated code:\n```python\ndef loss(self, model, guide, *args, **kwargs):\n        \n        \n        \n        elbo = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, *args, **kwargs):\n            elbo_particle = _compute_dice_elbo(model_trace, guide_trace)\n            if is_identically_zero(elbo_particle):\n                continue\n\n            elbo += elbo_particle.item() / self.num_particles\n\n        loss = -elbo\n        if is_nan(loss):\n            warnings.warn('Encountered NAN loss')\n        return loss\n```\n\nOriginal documentation:\n```python\n:returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Estimates the ELBO using ``num_particles`` many samples (particles).", "file": "pyro_infer_traceenum_elbo.py", "function": "TraceEnum_ELBO.loss", "project": "pyro", "change_stats": {"code_changes": 24, "doc_changes": 4, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef disable_popups(self, enable=True):\n        \n        \n        \n        if enable:\n            self.jsonrpc.setAccessibilityPatterns({\n                \"com.android.packageinstaller\": [u\"\u786e\u5b9a\", u\"\u5b89\u88c5\", u\"\u4e0b\u4e00\u6b65\", u\"\u597d\", u\"\u5141\u8bb8\", u\"\u6211\u77e5\u9053\"],\n                \"com.miui.securitycenter\": [u\"\u7ee7\u7eed\u5b89\u88c5\"], # xiaomi\n                \"com.lbe.security.miui\": [u\"\u5141\u8bb8\"], # xiaomi\n                \"android\": [u\"\u597d\", u\"\u5b89\u88c5\"], # vivo\n                \"com.huawei.systemmanager\": [u\"\u7acb\u5373\u5220\u9664\"], # huawei\n            })\n        else:\n            self.jsonrpc.setAccessibilityPatterns({})```\n\nUpdated code:\n```python\ndef disable_popups(self, enable=True):\n        \n        \n        \n        raise NotImplementedError()\n\n        if enable:\n            self.jsonrpc.setAccessibilityPatterns({\n                \"com.android.packageinstaller\": [u\"\u786e\u5b9a\", u\"\u5b89\u88c5\", u\"\u4e0b\u4e00\u6b65\", u\"\u597d\", u\"\u5141\u8bb8\", u\"\u6211\u77e5\u9053\"],\n                \"com.miui.securitycenter\": [u\"\u7ee7\u7eed\u5b89\u88c5\"], # xiaomi\n                \"com.lbe.security.miui\": [u\"\u5141\u8bb8\"], # xiaomi\n                \"android\": [u\"\u597d\", u\"\u5b89\u88c5\"], # vivo\n                \"com.huawei.systemmanager\": [u\"\u7acb\u5373\u5220\u9664\"], # huawei\n            })\n        else:\n            self.jsonrpc.setAccessibilityPatterns({})\n```\n\nOriginal documentation:\n```python\nAutomatic click all popups\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Automatic click all popups\n        TODO: need fix", "file": "uiautomator2___init__.py", "function": "UIAutomatorServer.disable_popups", "project": "uiautomator2", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef process_evaluation_score_confidence(\n        self,\n        reporter: str,\n        report_time: int,\n        key_type: str,\n        key: str,\n        evaluation: dict,\n    ):\n        \n        \n        \n\n        if evaluation is None:\n            self.print(\n                f'Peer {reporter} has no data to share about {key}', 2, 0\n            )\n            return\n\n        if type(evaluation) != dict:\n            self.print('Evaluation is not a dictionary', 0, 2)\n            # TODO: lower reputation\n            return\n\n        # check that fields are present and with correct type\n        try:\n            score = float(evaluation['score'])\n            confidence = float(evaluation['confidence'])\n        except KeyError:\n            self.print('Score or confidence are missing', 0, 2)\n            # TODO: lower reputation\n            return\n        except ValueError:\n            self.print('Score or confidence have wrong data type', 0, 2)\n            # TODO: lower reputation\n            return\n\n        # validate value ranges (must be from <0, 1>)\n        if score < -1 or score > 1:\n            self.print('Score value is out of bounds', 0, 2)\n            # TODO: lower reputation\n            return\n\n        if confidence < 0 or confidence > 1:\n            self.print('Confidence value is out of bounds', 0, 2)\n            # TODO: lower reputation\n            return\n\n        self.trustdb.insert_new_go_report(\n            reporter, key_type, key, score, confidence, report_time\n        )\n        result = (\n            f'Data processing ok: reporter {reporter}, report time {report_time}, key {key} ({key_type}), '\n            f'score {score}, confidence {confidence}'\n        )\n        self.print(result, 2, 0)\n        # print(f\"*** [debugging p2p] ***  stored a report about about  {key} from {reporter} in p2p_reports key in the db \")\n        # save all report info in the db\n        report_info = {\n            'reporter':reporter,\n            'report_time':report_time,\n        }\n        report_info.update(evaluation)\n        __database__.store_p2p_report(key, report_info)```\n\nUpdated code:\n```python\ndef process_evaluation_score_confidence(\n        self,\n        reporter: str,\n        report_time: int,\n        key_type: str,\n        key: str,\n        evaluation: dict,\n    ):\n        \n        \n        \n\n        if evaluation is None:\n            self.print(\n                f'Peer {reporter} has no data to share about {key}', 2, 0\n            )\n            return\n\n        if type(evaluation) != dict:\n            self.print('Evaluation is not a dictionary', 0, 2)\n            # TODO: lower reputation\n            return\n\n        # check that fields are present and with correct type\n        try:\n            score = float(evaluation['score'])\n            confidence = float(evaluation['confidence'])\n        except KeyError:\n            self.print('Score or confidence are missing', 0, 2)\n            # TODO: lower reputation\n            return\n        except ValueError:\n            self.print('Score or confidence have wrong data type', 0, 2)\n            # TODO: lower reputation\n            return\n\n        # validate value ranges (must be from <0, 1>)\n        if score < -1 or score > 1:\n            self.print('Score value is out of bounds', 0, 2)\n            # TODO: lower reputation\n            return\n\n        if confidence < 0 or confidence > 1:\n            self.print('Confidence value is out of bounds', 0, 2)\n            # TODO: lower reputation\n            return\n\n        self.trustdb.insert_new_go_report(\n            reporter, key_type, key, score, confidence, report_time\n        )\n        result = (\n            f'Data processing ok: reporter {reporter}, report time {report_time}, key {key} ({key_type}), '\n            f'score {score}, confidence {confidence}'\n        )\n        self.print(result, 2, 0)\n        # print(f\"*** [debugging p2p] ***  stored a report about about  {key} from {reporter} in p2p_reports key in the db \")\n        # save all report info in the db\n        # convert ts to human readable format\n        report_info = {\n            'reporter':reporter,\n            'report_time': utils.convert_format(report_time, utils.alerts_format),\n        }\n        report_info.update(evaluation)\n        __database__.store_p2p_report(key, report_info)\n```\n\nOriginal documentation:\n```python\nHandle reported score and confidence\n\n        Data is read from provided dictionary, and saved into the database.\n\n        :param reporter: The peer that sent the data\n        :param report_time: Time of receiving the data, provided by the go part\n        :param key_type: The type of key the peer is reporting (only \"ip\" is supported now)\n        :param key: The key itself\n        :param evaluation: Dictionary containing score and confidence values\n        :return: None, data is saved to the database\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Handle reported score and confidence from another peer\n\n        Data is read from provided dictionary, and saved into the database.\n\n        :param reporter: The peer that sent the data\n        :param report_time: Time of receiving the data, provided by the go part\n        :param key_type: The type of key the peer is reporting (only \"ip\" is supported now)\n        :param key: The key itself\n        :param evaluation: Dictionary containing score and confidence values\n        :return: None, data is saved to the database", "file": "modules_p2ptrust_utils_go_director.py", "function": "GoDirector.process_evaluation_score_confidence", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_token(request):\n    \n    \n    \n    request.META[\"CSRF_COOKIE_USED\"] = True\n    return request.META.get(\"CSRF_COOKIE\", None)```\n\nUpdated code:\n```python\ndef get_token(request):\n    \n    \n    \n    if \"CSRF_COOKIE\" not in request.META:\n        request.META[\"CSRF_COOKIE\"] = _get_new_csrf_key()\n    request.META[\"CSRF_COOKIE_USED\"] = True\n    return request.META[\"CSRF_COOKIE\"]\n```\n\nOriginal documentation:\n```python\nReturns the CSRF token required for a POST form. The token is an\n    alphanumeric value.\n\n    A side effect of calling this function is to make the csrf_protect\n    decorator and the CsrfViewMiddleware add a CSRF cookie and a 'Vary: Cookie'\n    header to the outgoing response.  For this reason, you may need to use this\n    function lazily, as is done by the csrf context processor.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the CSRF token required for a POST form. The token is an\n    alphanumeric value. A new token is created if one is not already set.\n\n    A side effect of calling this function is to make the csrf_protect\n    decorator and the CsrfViewMiddleware add a CSRF cookie and a 'Vary: Cookie'\n    header to the outgoing response.  For this reason, you may need to use this\n    function lazily, as is done by the csrf context processor.", "file": "django_middleware_csrf.py", "function": "get_token", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams_fn,\n               fpn_min_level=3,\n               fpn_max_level=7,\n               additional_layer_depth=256,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False,\n               override_base_feature_extractor_hyperparams=False):\n    \n    \n    super(SSDResnet101V1FpnFeatureExtractor, self).__init__(\n        is_training,\n        depth_multiplier,\n        min_depth,\n        pad_to_multiple,\n        conv_hyperparams_fn,\n        resnet_v1.resnet_v1_101,\n        'resnet_v1_101',\n        'fpn',\n        fpn_min_level,\n        fpn_max_level,\n        additional_layer_depth,\n        reuse_weights=reuse_weights,\n        use_explicit_padding=use_explicit_padding,\n        use_depthwise=use_depthwise,\n        override_base_feature_extractor_hyperparams=\n        override_base_feature_extractor_hyperparams)```\n\nUpdated code:\n```python\ndef __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams_fn,\n               fpn_min_level=3,\n               fpn_max_level=7,\n               additional_layer_depth=256,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False,\n               use_native_resize_op=False,\n               override_base_feature_extractor_hyperparams=False):\n    \n    \n    super(SSDResnet101V1FpnFeatureExtractor, self).__init__(\n        is_training,\n        depth_multiplier,\n        min_depth,\n        pad_to_multiple,\n        conv_hyperparams_fn,\n        resnet_v1.resnet_v1_101,\n        'resnet_v1_101',\n        'fpn',\n        fpn_min_level,\n        fpn_max_level,\n        additional_layer_depth,\n        reuse_weights=reuse_weights,\n        use_explicit_padding=use_explicit_padding,\n        use_depthwise=use_depthwise,\n        use_native_resize_op=use_native_resize_op,\n        override_base_feature_extractor_hyperparams=\n        override_base_feature_extractor_hyperparams)\n```\n\nOriginal documentation:\n```python\nSSD Resnet101 V1 FPN feature extractor based on Resnet v1 architecture.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d\n        and separable_conv2d ops in the layers that are added on top of the\n        base feature extractor.\n      fpn_min_level: the minimum level in feature pyramid networks.\n      fpn_max_level: the maximum level in feature pyramid networks.\n      additional_layer_depth: additional feature map layer channel depth.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False. UNUSED currently.\n      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.\n      override_base_feature_extractor_hyperparams: Whether to override\n        hyperparameters of the base feature extractor with the one from\n        `conv_hyperparams_fn`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "SSD Resnet101 V1 FPN feature extractor based on Resnet v1 architecture.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d\n        and separable_conv2d ops in the layers that are added on top of the\n        base feature extractor.\n      fpn_min_level: the minimum level in feature pyramid networks.\n      fpn_max_level: the maximum level in feature pyramid networks.\n      additional_layer_depth: additional feature map layer channel depth.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False. UNUSED currently.\n      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.\n      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize\n        to do upsampling in FPN. Default is false.\n      override_base_feature_extractor_hyperparams: Whether to override\n        hyperparameters of the base feature extractor with the one from\n        `conv_hyperparams_fn`.", "file": "research_object_detection_models_ssd_resnet_v1_fpn_feature_extractor.py", "function": "SSDResnet101V1FpnFeatureExtractor.__init__", "project": "models", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef line_nd(start, stop, *, endpoint=False, round=True):\n    \n    \n    start = np.asarray(start)\n    stop = np.asarray(stop)\n    npoints = int(np.ceil(np.max(np.abs(stop - start))))\n    if endpoint:\n        npoints += 1\n    coords = []\n    for dim in range(len(start)):\n        dimcoords = np.linspace(start[dim], stop[dim], npoints, endpoint)\n        if round:\n            dimcoords = _round_safe(dimcoords).astype(int)\n        coords.append(dimcoords)\n    return coords```\n\nUpdated code:\n```python\ndef line_nd(start, stop, *, endpoint=False, integer=True):\n    \n    \n    start = np.asarray(start)\n    stop = np.asarray(stop)\n    npoints = int(np.ceil(np.max(np.abs(stop - start))))\n    if endpoint:\n        npoints += 1\n    coords = []\n    for dim in range(len(start)):\n        dimcoords = np.linspace(start[dim], stop[dim], npoints, endpoint)\n        if integer:\n            dimcoords = _round_safe(dimcoords).astype(int)\n        coords.append(dimcoords)\n    return coords\n```\n\nOriginal documentation:\n```python\nDraw a single-pixel thick line in n dimensions.\n\n    The line produced will be ndim-connected. That is, two subsequent\n    pixels in the line will be either direct or diagonal neighbours in\n    n dimensions.\n\n    Parameters\n    ----------\n    start : array-like, shape (N,)\n        The start coordinates of the line.\n    stop : array-like, shape (N,)\n        The end coordinates of the line.\n    endpoint : bool, optional\n        Whether to include the endpoint in the returned line. Defaults\n        to False, which allows for easy drawing of multi-point paths.\n    round : bool, optional\n        Whether to round the coordinates to integer. If True (default),\n        the returned coordinates can be used to directly index into an\n        array. `False` could be used for e.g. vector drawing.\n\n    Returns\n    -------\n    coords : tuple of arrays\n        The coordinates of points on the line.\n\n    Examples\n    --------\n    >>> lin = line_nd((1, 1), (5, 2.5), endpoint=False)\n    >>> lin\n    [array([1, 2, 3, 4]), array([1, 1, 2, 2])]\n    >>> im = np.zeros((6, 5), dtype=int)\n    >>> im[lin] = 1\n    >>> im\n    array([[0, 0, 0, 0, 0],\n           [0, 1, 0, 0, 0],\n           [0, 1, 0, 0, 0],\n           [0, 0, 1, 0, 0],\n           [0, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0]])\n    >>> line_nd([2, 1, 1], [5, 5, 2.5], endpoint=True)\n    [array([2, 3, 4, 4, 5]), array([1, 2, 3, 4, 5]), array([1, 1, 2, 2, 2])]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Draw a single-pixel thick line in n dimensions.\n\n    The line produced will be ndim-connected. That is, two subsequent\n    pixels in the line will be either direct or diagonal neighbours in\n    n dimensions.\n\n    Parameters\n    ----------\n    start : array-like, shape (N,)\n        The start coordinates of the line.\n    stop : array-like, shape (N,)\n        The end coordinates of the line.\n    endpoint : bool, optional\n        Whether to include the endpoint in the returned line. Defaults\n        to False, which allows for easy drawing of multi-point paths.\n    integer : bool, optional\n        Whether to round the coordinates to integer. If True (default),\n        the returned coordinates can be used to directly index into an\n        array. `False` could be used for e.g. vector drawing.\n\n    Returns\n    -------\n    coords : tuple of arrays\n        The coordinates of points on the line.\n\n    Examples\n    --------\n    >>> lin = line_nd((1, 1), (5, 2.5), endpoint=False)\n    >>> lin\n    [array([1, 2, 3, 4]), array([1, 1, 2, 2])]\n    >>> im = np.zeros((6, 5), dtype=int)\n    >>> im[lin] = 1\n    >>> im\n    array([[0, 0, 0, 0, 0],\n           [0, 1, 0, 0, 0],\n           [0, 1, 0, 0, 0],\n           [0, 0, 1, 0, 0],\n           [0, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0]])\n    >>> line_nd([2, 1, 1], [5, 5, 2.5], endpoint=True)\n    [array([2, 3, 4, 4, 5]), array([1, 2, 3, 4, 5]), array([1, 1, 2, 2, 2])]", "file": "skimage_draw_draw_nd.py", "function": "line_nd", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse(self, stream, media_type=None, parser_context=None):\n        \n        \n        \n\n        parser_context = parser_context or {}\n        request = parser_context['request']\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        meta = request.META\n        upload_handlers = request.upload_handlers\n        filename = self.get_filename(stream, media_type, parser_context)\n\n        content_type = meta.get('HTTP_CONTENT_TYPE', meta.get('CONTENT_TYPE', ''))\n        try:\n            content_length = int(meta.get('HTTP_CONTENT_LENGTH', meta.get('CONTENT_LENGTH', 0)))\n        except (ValueError, TypeError):\n            content_length = None\n\n        # See if the handler will want to take care of the parsing.\n        for handler in upload_handlers:\n            result = handler.handle_raw_input(None,\n                                              meta,\n                                              content_length,\n                                              None,\n                                              encoding)\n            if result is not None:\n                return DataAndFiles(None, {'file': result[1]})\n\n        possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n        chunk_size = min([2 ** 31 - 4] + possible_sizes)\n        chunks = ChunkIter(stream, chunk_size)\n        counters = [0] * len(upload_handlers)\n\n        for handler in upload_handlers:\n            try:\n                handler.new_file(None, filename, content_type, content_length, encoding)\n            except StopFutureHandlers:\n                break\n\n        for chunk in chunks:\n            for i, handler in enumerate(upload_handlers):\n                chunk_length = len(chunk)\n                chunk = handler.receive_data_chunk(chunk, counters[i])\n                counters[i] += chunk_length\n                if chunk is None:\n                    # If the chunk received by the handler is None, then don't continue.\n                    break\n\n        for i, handler in enumerate(upload_handlers):\n            file_obj = handler.file_complete(counters[i])\n            if file_obj:\n                return DataAndFiles(None, {'file': file_obj})\n        raise ParseError(\"FileUpload parse error - none of upload handlers can handle the stream\")```\n\nUpdated code:\n```python\ndef parse(self, stream, media_type=None, parser_context=None):\n        \n        \n        \n\n        parser_context = parser_context or {}\n        request = parser_context['request']\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        meta = request.META\n        upload_handlers = request.upload_handlers\n        filename = self.get_filename(stream, media_type, parser_context)\n\n        # Note that this code is extracted from Django's handling of\n        # file uploads in MultiPartParser.\n        content_type = meta.get('HTTP_CONTENT_TYPE',\n                                meta.get('CONTENT_TYPE', ''))\n        try:\n            content_length = int(meta.get('HTTP_CONTENT_LENGTH',\n                                          meta.get('CONTENT_LENGTH', 0)))\n        except (ValueError, TypeError):\n            content_length = None\n\n        # See if the handler will want to take care of the parsing.\n        for handler in upload_handlers:\n            result = handler.handle_raw_input(None,\n                                              meta,\n                                              content_length,\n                                              None,\n                                              encoding)\n            if result is not None:\n                return DataAndFiles(None, {'file': result[1]})\n\n        # This is the standard case.\n        possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n        chunk_size = min([2 ** 31 - 4] + possible_sizes)\n        chunks = ChunkIter(stream, chunk_size)\n        counters = [0] * len(upload_handlers)\n\n        for handler in upload_handlers:\n            try:\n                handler.new_file(None, filename, content_type,\n                                 content_length, encoding)\n            except StopFutureHandlers:\n                break\n\n        for chunk in chunks:\n            for i, handler in enumerate(upload_handlers):\n                chunk_length = len(chunk)\n                chunk = handler.receive_data_chunk(chunk, counters[i])\n                counters[i] += chunk_length\n                if chunk is None:\n                    break\n\n        for i, handler in enumerate(upload_handlers):\n            file_obj = handler.file_complete(counters[i])\n            if file_obj:\n                return DataAndFiles(None, {'file': file_obj})\n        raise ParseError(\"FileUpload parse error - \"\n                         \"none of upload handlers can handle the stream\")\n```\n\nOriginal documentation:\n```python\nReturns a DataAndFiles object.\n\n        `.data` will be None (we expect request body to be a file content).\n        `.files` will be a `QueryDict` containing one 'file' elemnt - a parsed file.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a DataAndFiles object.\n\n        `.data` will be None (we expect request body to be a file content).\n        `.files` will be a `QueryDict` containing one 'file' element.", "file": "rest_framework_parsers.py", "function": "FileUploadParser.parse", "project": "django-rest-framework", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef post_process(self, bboxlist):\n          \n        retval = list()\n        for i in range(len(bboxlist) // 2):\n            bboxlist[i * 2] = self.softmax(bboxlist[i * 2], axis=1)\n        for i in range(len(bboxlist) // 2):\n            ocls, oreg = bboxlist[i * 2], bboxlist[i * 2 + 1]\n            stride = 2 ** (i + 2)    # 4,8,16,32,64,128\n            poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))\n            for _, hindex, windex in poss:\n                axc, ayc = stride / 2 + windex * stride, stride / 2 + hindex * stride\n                score = ocls[0, 1, hindex, windex]\n                loc = np.ascontiguousarray(oreg[0, :, hindex, windex]).reshape((1, 4))\n                priors = np.array([[axc / 1.0, ayc / 1.0, stride * 4 / 1.0, stride * 4 / 1.0]])\n                variances = [0.1, 0.2]\n                box = self.decode(loc, priors, variances)\n                x_1, y_1, x_2, y_2 = box[0] * 1.0\n                retval.append([x_1, y_1, x_2, y_2, score])\n        retval = np.array(retval)\n        if len(retval) == 0:\n            retval = np.zeros((1, 5))\n        return retval```\n\nUpdated code:\n```python\ndef post_process(self, bboxlist):\n         \n        \n        retval = list()\n        for i in range(len(bboxlist) // 2):\n            bboxlist[i * 2] = self.softmax(bboxlist[i * 2], axis=1)\n        for i in range(len(bboxlist) // 2):\n            ocls, oreg = bboxlist[i * 2], bboxlist[i * 2 + 1]\n            stride = 2 ** (i + 2)    # 4,8,16,32,64,128\n            poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))\n            for _, hindex, windex in poss:\n                axc, ayc = stride / 2 + windex * stride, stride / 2 + hindex * stride\n                score = ocls[0, 1, hindex, windex]\n                loc = np.ascontiguousarray(oreg[0, :, hindex, windex]).reshape((1, 4))\n                priors = np.array([[axc / 1.0, ayc / 1.0, stride * 4 / 1.0, stride * 4 / 1.0]])\n                variances = [0.1, 0.2]\n                box = self.decode(loc, priors, variances)\n                x_1, y_1, x_2, y_2 = box[0] * 1.0\n                retval.append([x_1, y_1, x_2, y_2, score])\n        retval = np.array(retval)\n        if len(retval) == 0:\n            retval = np.zeros((1, 5))\n        return retval\n```\n\nOriginal documentation:\n```python\nPerform post processing on output\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform post processing on output\n            TODO: do this on the batch.", "file": "plugins_extract_detect_s3fd.py", "function": "S3fd.post_process", "project": "faceswap", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef instantaneous_frequency(phase_angle, time_axis=-2, use_unwrap=True):\n  \n  \n  if use_unwrap:  # can lead to loss of precision\n    phase_unwrapped = unwrap(phase_angle, axis=time_axis)\n    dphase = diff(phase_unwrapped, axis=time_axis)\n  else:\n    dphase = diff(phase_angle, axis=time_axis)\n\n  if not use_unwrap:\n    # note that 2 \"wheres\" below together run faster than a single mod-2pi expression\n    dphase = tf.where(dphase > np.pi, dphase-2*np.pi, dphase)  # keep dphase bounded\n    dphase = tf.where(dphase <-np.pi, dphase+2*np.pi, dphase)\n\n  # Add an initial phase to dphase\n  size = phase_angle.get_shape().as_list()\n  size[time_axis] = 1\n  begin = [0 for unused_s in size]\n  phase_slice = tf.slice(phase_angle, begin, size)\n  dphase = tf.concat([phase_slice, dphase], axis=time_axis) / np.pi\n  return dphase```\n\nUpdated code:\n```python\ndef instantaneous_frequency(phase_angle, time_axis=-2, use_unwrap=True):\n  \n  \n  if use_unwrap:\n    # Can lead to loss of precision.\n    phase_unwrapped = unwrap(phase_angle, axis=time_axis)\n    dphase = diff(phase_unwrapped, axis=time_axis)\n  else:\n    # Keep dphase bounded. N.B. runs faster than a single mod-2pi expression.\n    dphase = diff(phase_angle, axis=time_axis)\n    dphase = tf.where(dphase > np.pi, dphase - 2 * np.pi, dphase)\n    dphase = tf.where(dphase < -np.pi, dphase + 2 * np.pi, dphase)\n\n  # Add an initial phase to dphase.\n  size = phase_angle.get_shape().as_list()\n  size[time_axis] = 1\n  begin = [0 for unused_s in size]\n  phase_slice = tf.slice(phase_angle, begin, size)\n  dphase = tf.concat([phase_slice, dphase], axis=time_axis) / np.pi\n  return dphase\n```\n\nOriginal documentation:\n```python\nTransform a fft tensor from phase angle to instantaneous frequency.\n\n  (Unwrap and) Take the finite difference of the phase. Pad with initial phase to\n  keep the tensor the same size.\n  Args:\n    phase_angle: Tensor of angles in radians. [Batch, Time, Freqs]\n    time_axis: Axis over which to unwrap and take finite difference.\n\n  Returns:\n    dphase: Instantaneous frequency (derivative of phase). Same size as input.\n    \n  use_unwrap=True preserves original GANSynth behavior, whereas\n  use_unwrap=False will guard against loss of precision.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Transform a fft tensor from phase angle to instantaneous frequency.\n\n  Take the finite difference of the phase. Pad with initial phase to keep the\n  tensor the same size.\n  Args:\n    phase_angle: Tensor of angles in radians. [Batch, Time, Freqs]\n    time_axis: Axis over which to unwrap and take finite difference.\n    use_unwrap: True preserves original GANSynth behavior, whereas False will\n        guard against loss of precision.\n\n  Returns:\n    dphase: Instantaneous frequency (derivative of phase). Same size as input.", "file": "magenta_models_gansynth_lib_spectral_ops.py", "function": "instantaneous_frequency", "project": "magenta", "change_stats": {"code_changes": 15, "doc_changes": 11, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sw_index_daily_indicator(\n    index_code: str = \"801010\",\n    start_date: str = \"2019-12-01\",\n    end_date: str = \"2019-12-07\",\n    data_type: str = \"Day\",\n) -> pd.DataFrame:\n    \n    \n    \n    url = \"http://www.swsindex.com/excel.aspx?ctable=V_Report&where=%s\"\n    where_cond = (\n        \"swindexcode in ('%s') and BargainDate >= '%s' and BargainDate <= '%s' and type='%s'\"\n        % (index_code, start_date, end_date, data_type)\n    )\n    url = url % where_cond\n\n    response = requests.get(url).text\n    if response is None:\n        return None, \"\u83b7\u53d6\u6570\u636e\u5931\u8d25\"\n\n    soup = BeautifulSoup(response, \"html5lib\")\n    data = []\n    table = soup.findAll(\"table\")[0]\n    rows = table.findAll(\"tr\")\n    for row in rows:\n        cols = row.findAll(\"td\")\n        if len(cols) >= 14:\n            index_code = cols[0].text\n            index_name = cols[1].text\n            date = cols[2].text\n            close = cols[3].text\n            volume = cols[4].text\n            chg_pct = cols[5].text\n            turn_rate = cols[6].text\n            pe = cols[7].text\n            pb = cols[8].text\n            v_wap = cols[9].text\n            turnover_pct = cols[10].text\n            float_mv = cols[11].text\n            avg_float_mv = cols[12].text\n            dividend_yield_ratio = cols[13].text\n\n            data.append(\n                {\n                    \"index_code\": index_code,\n                    \"index_name\": index_name,\n                    \"date\": date,\n                    \"close\": close,\n                    \"volume\": volume,\n                    \"chg_pct\": chg_pct,\n                    \"turn_rate\": turn_rate,\n                    \"pe\": pe,\n                    \"pb\": pb,\n                    \"vwap\": v_wap,\n                    \"float_mv\": float_mv,\n                    \"avg_float_mv\": avg_float_mv,\n                    \"dividend_yield_ratio\": dividend_yield_ratio,\n                    \"turnover_pct\": turnover_pct,\n                }\n            )\n\n    df = pd.DataFrame(data)\n    if len(df) > 0:\n        df[\"date\"] = df[\"date\"].apply(\n            lambda x: datetime.datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\")\n        )\n    return df```\n\nUpdated code:\n```python\ndef sw_index_daily_indicator(\n    index_code: str = \"801010\",\n    start_date: str = \"2019-12-01\",\n    end_date: str = \"2019-12-07\",\n    data_type: str = \"Day\",\n) -> pd.DataFrame:\n    \n    \n    \n    url = \"http://www.swsindex.com/excel.aspx\"\n    params = {\n        \"ctable\": \"V_Report\",\n        \"where\": f\" swindexcode in ('{index_code}') and BargainDate >= '{start_date}' and BargainDate <= '{end_date}' and type='{data_type}'\",\n    }\n    r = requests.get(url, params=params)\n    soup = BeautifulSoup(r.text, \"html5lib\")\n    data = []\n    table = soup.findAll(\"table\")[0]\n    rows = table.findAll(\"tr\")\n    for row in rows:\n        cols = row.findAll(\"td\")\n        if len(cols) >= 14:\n            index_code = cols[0].text\n            index_name = cols[1].text\n            date = cols[2].text\n            close = cols[3].text\n            volume = cols[4].text\n            chg_pct = cols[5].text\n            turn_rate = cols[6].text\n            pe = cols[7].text\n            pb = cols[8].text\n            v_wap = cols[9].text\n            turnover_pct = cols[10].text\n            float_mv = cols[11].text\n            avg_float_mv = cols[12].text\n            dividend_yield_ratio = cols[13].text\n            data.append(\n                {\n                    \"index_code\": index_code,\n                    \"index_name\": index_name,\n                    \"date\": date,\n                    \"close\": close,\n                    \"volume\": volume,\n                    \"chg_pct\": chg_pct,\n                    \"turn_rate\": turn_rate,\n                    \"pe\": pe,\n                    \"pb\": pb,\n                    \"vwap\": v_wap,\n                    \"float_mv\": float_mv,\n                    \"avg_float_mv\": avg_float_mv,\n                    \"dividend_yield_ratio\": dividend_yield_ratio,\n                    \"turnover_pct\": turnover_pct,\n                }\n            )\n    temp_df = pd.DataFrame(data)\n    temp_df[\"date\"] = pd.to_datetime(temp_df[\"date\"]).dt.date\n    temp_df[\"close\"] = pd.to_numeric(temp_df[\"close\"])\n    temp_df[\"volume\"] = pd.to_numeric(temp_df[\"volume\"])\n    temp_df[\"chg_pct\"] = pd.to_numeric(temp_df[\"chg_pct\"])\n    temp_df[\"turn_rate\"] = pd.to_numeric(temp_df[\"turn_rate\"])\n    temp_df[\"pe\"] = pd.to_numeric(temp_df[\"pe\"])\n    temp_df[\"pb\"] = pd.to_numeric(temp_df[\"pb\"])\n    temp_df[\"vwap\"] = pd.to_numeric(temp_df[\"vwap\"])\n    temp_df[\"float_mv\"] = temp_df[\"float_mv\"].apply(lambda x: x.replace(\",\", \"\"))\n    temp_df[\"float_mv\"] = pd.to_numeric(\n        temp_df[\"float_mv\"],\n    )\n    temp_df[\"avg_float_mv\"] = temp_df[\"avg_float_mv\"].apply(lambda x: x.replace(\",\", \"\"))\n    temp_df[\"avg_float_mv\"] = pd.to_numeric(temp_df[\"avg_float_mv\"])\n    temp_df[\"dividend_yield_ratio\"] = pd.to_numeric(temp_df[\"dividend_yield_ratio\"])\n    temp_df[\"turnover_pct\"] = pd.to_numeric(temp_df[\"turnover_pct\"])\n    return temp_df\n```\n\nOriginal documentation:\n```python\n\u7533\u4e07\u4e00\u7ea7\u884c\u4e1a\u5386\u53f2\u884c\u60c5\u6307\u6807\n    http://www.swsindex.com/idx0200.aspx?columnid=8838&type=Day\n    :param index_code: \u7533\u4e07\u6307\u6570\n    :type index_code: str\n    :param start_date: \u5f00\u59cb\u65f6\u95f4\n    :type start_date: str\n    :param end_date: \u7ed3\u675f\u65f6\u95f4\n    :type end_date: str\n    :param data_type: \u9891\u7387\n    :type data_type: str\n    :return: \u7533\u4e07\u6307\u6570\u4e0d\u540c\u9891\u7387\u6570\u636e\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u7533\u4e07\u4e00\u7ea7\u884c\u4e1a\u5386\u53f2\u884c\u60c5\u6307\u6807\n    http://www.swsindex.com/idx0200.aspx?columnid=8838&type=Day\n    :param index_code: \u7533\u4e07\u6307\u6570\n    :type index_code: str\n    :param start_date: \u5f00\u59cb\u65f6\u95f4\n    :type start_date: str\n    :param end_date: \u7ed3\u675f\u65f6\u95f4\n    :type end_date: str\n    :param data_type: choice of {\"Day\": \u65e5\u62a5\u8868, \"Week\": \u5468\u62a5\u8868}\n    :type data_type: str\n    :return: \u7533\u4e07\u6307\u6570\u4e0d\u540c\u9891\u7387\u6570\u636e\n    :rtype: pandas.DataFrame", "file": "akshare_index_index_sw.py", "function": "sw_index_daily_indicator", "project": "akshare", "change_stats": {"code_changes": 47, "doc_changes": 4, "total_changes": 51}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef hvac_mode(self):\n        \n        \n        return HVAC_MODE_HEAT```\n\nUpdated code:\n```python\ndef hvac_mode(self) -> str:\n        \n        return HVAC_MODE_HEAT\n```\n\nOriginal documentation:\n```python\nReturn hvac operation ie. heat, cool mode.\n\n        Need to be one of HVAC_MODE_*.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return hvac operation ie. heat, cool mode.", "file": "homeassistant_components_velbus_climate.py", "function": "VelbusClimate.hvac_mode", "project": "core", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sample(self) -> tuple:\n        \n        \n        return tuple(space.sample() for space in self.spaces)```\n\nUpdated code:\n```python\ndef sample(self, mask: Optional[Tuple[Optional[np.ndarray]]] = None) -> tuple:\n        \n        \n        if mask is not None:\n            assert isinstance(\n                mask, tuple\n            ), f\"Expected type of mask is tuple, actual type: {type(mask)}\"\n            assert len(mask) == len(\n                self.spaces\n            ), f\"Expected length of mask is {len(self.spaces)}, actual length: {len(mask)}\"\n\n            return tuple(\n                space.sample(mask=sub_mask)\n                for space, sub_mask in zip(self.spaces, mask)\n            )\n\n        return tuple(space.sample() for space in self.spaces)\n```\n\nOriginal documentation:\n```python\nGenerates a single random sample inside this space.\n\n        This method draws independent samples from the subspaces.\n\n        Returns:\n            Tuple of the subspace's samples\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates a single random sample inside this space.\n\n        This method draws independent samples from the subspaces.\n\n        Args:\n            mask: An optional tuple of optional masks for each of the subspace's samples,\n                expects the same number of masks as spaces\n\n        Returns:\n            Tuple of the subspace's samples", "file": "gym_spaces_tuple.py", "function": "Tuple.sample", "project": "gym", "change_stats": {"code_changes": 17, "doc_changes": 6, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef subplot_tool(targetfig=None):\n    \n    \n    \n    if targetfig is None:\n        targetfig = gcf()\n    with rc_context({\"toolbar\": \"none\"}):  # No navbar for the toolfig.\n        # Use new_figure_manager() instead of figure() so that the figure\n        # doesn't get registered with pyplot.\n        manager = new_figure_manager(-1, (6, 3))\n    manager.set_window_title(\"Subplot configuration tool\")\n    tool_fig = manager.canvas.figure\n    tool_fig.subplots_adjust(top=0.9)\n    manager.show()\n    return SubplotTool(targetfig, tool_fig)```\n\nUpdated code:\n```python\ndef subplot_tool(targetfig=None):\n    \n    \n    \n    if targetfig is None:\n        targetfig = gcf()\n    tb = targetfig.canvas.manager.toolbar\n    if hasattr(tb, \"configure_subplots\"):  # toolbar2\n        return tb.configure_subplots()\n    elif hasattr(tb, \"trigger_tool\"):  # toolmanager\n        return tb.trigger_tool(\"subplots\")\n    else:\n        raise ValueError(\"subplot_tool can only be launched for figures with \"\n                         \"an associated toolbar\")\n```\n\nOriginal documentation:\n```python\nLaunch a subplot tool window for a figure.\n\n    A `matplotlib.widgets.SubplotTool` instance is returned. You must maintain\n    a reference to the instance to keep the associated callbacks alive.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Launch a subplot tool window for a figure.\n\n    Returns\n    -------\n    `matplotlib.widgets.SubplotTool`", "file": "lib_matplotlib_pyplot.py", "function": "subplot_tool", "project": "matplotlib", "change_stats": {"code_changes": 19, "doc_changes": 7, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _setup_run(self):\n        \n        with session_scope(session=self.get_session()) as session:\n            start = self.recording_start\n            end_incomplete_runs(session, start)\n            self.run_info = RecorderRuns(start=start, created=dt_util.utcnow())\n            session.add(self.run_info)\n            session.flush()\n            session.expunge(self.run_info)\n\n        self._open_event_session()```\n\nUpdated code:\n```python\ndef _setup_run(self):\n        \n        with session_scope(session=self.get_session()) as session:\n            start = self.recording_start\n            end_incomplete_runs(session, start)\n            self.run_info = RecorderRuns(start=start, created=dt_util.utcnow())\n            session.add(self.run_info)\n            session.flush()\n            session.expunge(self.run_info)\n            self._schedule_compile_missing_statistics(session)\n\n        self._open_event_session()\n```\n\nOriginal documentation:\n```python\nLog the start of the current run.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Log the start of the current run and schedule any needed jobs.", "file": "homeassistant_components_recorder___init__.py", "function": "Recorder._setup_run", "project": "core", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_config(self, config: TrainerConfigDict) -> None:\n        \n        \n        # Call super's validation method.\n        super().validate_config(config)\n\n        if isinstance(config[\"entropy_coeff\"], int):\n            config[\"entropy_coeff\"] = float(config[\"entropy_coeff\"])\n\n        if config[\"entropy_coeff\"] < 0.0:\n            raise DeprecationWarning(\"entropy_coeff must be >= 0.0\")\n\n        # SGD minibatch size must be smaller than train_batch_size (b/c\n        # we subsample a batch of `sgd_minibatch_size` from the train-batch for\n        # each `num_sgd_iter`).\n        # Note: Only check this if `train_batch_size` > 0 (DDPPO sets this\n        # to -1 to auto-calculate the actual batch size later).\n        if (\n            config[\"train_batch_size\"] > 0\n            and config[\"sgd_minibatch_size\"] > config[\"train_batch_size\"]\n        ):\n            raise ValueError(\n                \"`sgd_minibatch_size` ({}) must be <= \"\n                \"`train_batch_size` ({}).\".format(\n                    config[\"sgd_minibatch_size\"], config[\"train_batch_size\"]\n                )\n            )\n\n        # Check for mismatches between `train_batch_size` and\n        # `rollout_fragment_length` and auto-adjust `rollout_fragment_length`\n        # if necessary.\n        # Note: Only check this if `train_batch_size` > 0 (DDPPO sets this\n        # to -1 to auto-calculate the actual batch size later).\n        num_workers = config[\"num_workers\"] or 1\n        calculated_min_rollout_size = (\n            num_workers\n            * config[\"num_envs_per_worker\"]\n            * config[\"rollout_fragment_length\"]\n        )\n        if (\n            config[\"train_batch_size\"] > 0\n            and config[\"train_batch_size\"] % calculated_min_rollout_size != 0\n        ):\n            new_rollout_fragment_length = math.ceil(\n                config[\"train_batch_size\"]\n                / (num_workers * config[\"num_envs_per_worker\"])\n            )\n            logger.warning(\n                \"`train_batch_size` ({}) cannot be achieved with your other \"\n                \"settings (num_workers={} num_envs_per_worker={} \"\n                \"rollout_fragment_length={})! Auto-adjusting \"\n                \"`rollout_fragment_length` to {}.\".format(\n                    config[\"train_batch_size\"],\n                    config[\"num_workers\"],\n                    config[\"num_envs_per_worker\"],\n                    config[\"rollout_fragment_length\"],\n                    new_rollout_fragment_length,\n                )\n            )\n            config[\"rollout_fragment_length\"] = new_rollout_fragment_length\n\n        # Episodes may only be truncated (and passed into PPO's\n        # `postprocessing_fn`), iff generalized advantage estimation is used\n        # (value function estimate at end of truncated episode to estimate\n        # remaining value).\n        if config[\"batch_mode\"] == \"truncate_episodes\" and not config[\"use_gae\"]:\n            raise ValueError(\n                \"Episode truncation is not supported without a value \"\n                \"function (to estimate the return at the end of the truncated\"\n                \" trajectory). Consider setting \"\n                \"batch_mode=complete_episodes.\"\n            )\n\n        # Multi-agent mode and multi-GPU optimizer.\n        if config[\"multiagent\"][\"policies\"] and not config[\"simple_optimizer\"]:\n            logger.info(\n                \"In multi-agent mode, policies will be optimized sequentially\"\n                \" by the multi-GPU optimizer. Consider setting \"\n                \"simple_optimizer=True if this doesn't work for you.\"\n            )```\n\nUpdated code:\n```python\ndef validate_config(self, config: AlgorithmConfigDict) -> None:\n        \n        \n        # Call super's validation method.\n        super().validate_config(config)\n\n        if isinstance(config[\"entropy_coeff\"], int):\n            config[\"entropy_coeff\"] = float(config[\"entropy_coeff\"])\n\n        if config[\"entropy_coeff\"] < 0.0:\n            raise DeprecationWarning(\"entropy_coeff must be >= 0.0\")\n\n        # SGD minibatch size must be smaller than train_batch_size (b/c\n        # we subsample a batch of `sgd_minibatch_size` from the train-batch for\n        # each `num_sgd_iter`).\n        # Note: Only check this if `train_batch_size` > 0 (DDPPO sets this\n        # to -1 to auto-calculate the actual batch size later).\n        if (\n            config[\"train_batch_size\"] > 0\n            and config[\"sgd_minibatch_size\"] > config[\"train_batch_size\"]\n        ):\n            raise ValueError(\n                \"`sgd_minibatch_size` ({}) must be <= \"\n                \"`train_batch_size` ({}).\".format(\n                    config[\"sgd_minibatch_size\"], config[\"train_batch_size\"]\n                )\n            )\n\n        # Check for mismatches between `train_batch_size` and\n        # `rollout_fragment_length` and auto-adjust `rollout_fragment_length`\n        # if necessary.\n        # Note: Only check this if `train_batch_size` > 0 (DDPPO sets this\n        # to -1 to auto-calculate the actual batch size later).\n        num_workers = config[\"num_workers\"] or 1\n        calculated_min_rollout_size = (\n            num_workers\n            * config[\"num_envs_per_worker\"]\n            * config[\"rollout_fragment_length\"]\n        )\n        if (\n            config[\"train_batch_size\"] > 0\n            and config[\"train_batch_size\"] % calculated_min_rollout_size != 0\n        ):\n            new_rollout_fragment_length = math.ceil(\n                config[\"train_batch_size\"]\n                / (num_workers * config[\"num_envs_per_worker\"])\n            )\n            logger.warning(\n                \"`train_batch_size` ({}) cannot be achieved with your other \"\n                \"settings (num_workers={} num_envs_per_worker={} \"\n                \"rollout_fragment_length={})! Auto-adjusting \"\n                \"`rollout_fragment_length` to {}.\".format(\n                    config[\"train_batch_size\"],\n                    config[\"num_workers\"],\n                    config[\"num_envs_per_worker\"],\n                    config[\"rollout_fragment_length\"],\n                    new_rollout_fragment_length,\n                )\n            )\n            config[\"rollout_fragment_length\"] = new_rollout_fragment_length\n\n        # Episodes may only be truncated (and passed into PPO's\n        # `postprocessing_fn`), iff generalized advantage estimation is used\n        # (value function estimate at end of truncated episode to estimate\n        # remaining value).\n        if config[\"batch_mode\"] == \"truncate_episodes\" and not config[\"use_gae\"]:\n            raise ValueError(\n                \"Episode truncation is not supported without a value \"\n                \"function (to estimate the return at the end of the truncated\"\n                \" trajectory). Consider setting \"\n                \"batch_mode=complete_episodes.\"\n            )\n\n        # Multi-agent mode and multi-GPU optimizer.\n        if config[\"multiagent\"][\"policies\"] and not config[\"simple_optimizer\"]:\n            logger.info(\n                \"In multi-agent mode, policies will be optimized sequentially\"\n                \" by the multi-GPU optimizer. Consider setting \"\n                \"simple_optimizer=True if this doesn't work for you.\"\n            )\n```\n\nOriginal documentation:\n```python\nValidates the Trainer's config dict.\n\n        Args:\n            config: The Trainer's config to check.\n\n        Raises:\n            ValueError: In case something is wrong with the config.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Validates the Algorithm's config dict.\n\n        Args:\n            config: The Algorithm's config to check.\n\n        Raises:\n            ValueError: In case something is wrong with the config.", "file": "rllib_algorithms_ppo_ppo.py", "function": "PPO.validate_config", "project": "ray", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef initialize(self, version: str, build_data: dict[str, Any]) -> None:\n        \n        \n        \n        self._process_all_built_in_extras(version)\n        self._process_all_provider_extras(version)\n\n        # Adds all-core extras for the extras that are built-in and not devel\n        self.optional_dependencies[\"all-core\"] = sorted(\n            set([f\"apache-airflow[{extra}]\" for extra in CORE_EXTRAS.keys()])\n        )\n        # Adds \"apache-airflow[extra]\" for all extras that are not devel extras for wheel and editable builds\n        self.optional_dependencies[\"all\"] = [\n            f\"apache-airflow[{extra}]\" for extra in sorted(self.all_non_devel_extras)\n        ]\n        # Adds all devel extras for the extras that are built-in only for editable builds\n        if version != \"standard\":\n            self.optional_dependencies[\"devel-all\"] = [\n                f\"apache-airflow[{extra}]\" for extra in sorted(self.all_devel_extras)\n            ]\n        # This is special dependency that is used to install all possible\n        # 3rd-party dependencies for airflow for the CI image. It is exposed in the wheel package\n        # because we want to use for building the image cache from GitHub URL.\n        self.optional_dependencies[\"devel-ci\"] = sorted(self.all_devel_ci_dependencies)\n        self._dependencies = DEPENDENCIES\n\n        if version == \"standard\":\n            # Inject preinstalled providers into the dependencies for standard packages\n            for provider in PREINSTALLED_PROVIDERS:\n                self._dependencies.append(provider)\n            for not_ready_provider_dependency in PREINSTALLED_NOT_READY_DEPS:\n                self._dependencies.append(not_ready_provider_dependency)\n\n        # with hatchling, we can modify dependencies dynamically by modifying the build_data\n        build_data[\"dependencies\"] = self._dependencies\n\n        # unfortunately hatchling currently does not have a way to override optional_dependencies\n        # via build_data (or so it seem) so we need to modify internal _optional_dependencies\n        # field in core.metadata until this is possible\n        self.metadata.core._optional_dependencies = self.optional_dependencies```\n\nUpdated code:\n```python\ndef initialize(self, version: str, build_data: dict[str, Any]) -> None:\n        \n        \n        \n        self._process_all_built_in_extras(version)\n        self._process_all_provider_extras(version)\n\n        # Adds all-core extras for the extras that are built-in and not devel\n        self.optional_dependencies[\"all-core\"] = sorted(\n            set([f\"apache-airflow[{extra}]\" for extra in CORE_EXTRAS.keys()])\n        )\n        # Adds \"apache-airflow[extra]\" for all extras that are not devel extras for wheel and editable builds\n        self.optional_dependencies[\"all\"] = [\n            f\"apache-airflow[{extra}]\" for extra in sorted(self.all_non_devel_extras)\n        ]\n        # Adds all devel extras for the extras that are built-in only for editable builds\n        if version != \"standard\":\n            self.optional_dependencies[\"devel-all\"] = [\n                f\"apache-airflow[{extra}]\" for extra in sorted(self.all_devel_extras)\n            ]\n        # This is special dependency that is used to install all possible\n        # 3rd-party dependencies for airflow for the CI image. It is exposed in the wheel package\n        # because we want to use for building the image cache from GitHub URL.\n        self.optional_dependencies[\"devel-ci\"] = sorted(self.all_devel_ci_dependencies)\n        self._dependencies = DEPENDENCIES\n\n        if version == \"standard\":\n            # Inject preinstalled providers into the dependencies for standard packages\n            self._dependencies.extend(PREINSTALLED_PROVIDER_REQUIREMENTS)\n            self._dependencies.extend(PREINSTALLED_NOT_READY_PROVIDER_DEPS)\n        else:\n            self._dependencies.extend(ALL_PREINSTALLED_PROVIDER_DEPS)\n\n        # with hatchling, we can modify dependencies dynamically by modifying the build_data\n        build_data[\"dependencies\"] = self._dependencies\n\n        # unfortunately hatchling currently does not have a way to override optional_dependencies\n        # via build_data (or so it seem) so we need to modify internal _optional_dependencies\n        # field in core.metadata until this is possible\n        self.metadata.core._optional_dependencies = self.optional_dependencies\n```\n\nOriginal documentation:\n```python\nInitialize hook immediately before each build.\n\n        Any modifications to the build data will be seen by the build target.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize hook immediately before each build.\n\n        Any modifications to the build data will be seen by the build target.\n\n        :param version: \"standard\" or \"editable\" build.\n        :param build_data: build data dictionary.", "file": "hatch_build.py", "function": "CustomBuildHook.initialize", "project": "airflow", "change_stats": {"code_changes": 10, "doc_changes": 5, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _post_as_get(self, *args, **kwargs):\n        \n        \n        \n        if self.acme_version >= 2:\n            # We add an empty payload for POST-as-GET requests\n            new_args = args[:1] + (None,) + args[1:]\n            try:\n                return self._post(*new_args, **kwargs)\n            except messages.Error as error:\n                if error.code == 'malformed':\n                    logger.debug('Error during a POST-as-GET request, '\n                                 'your ACME CA server may not support it:\\n%s', error)\n                    logger.debug('Retrying request with GET.')\n                else:  # pragma: no cover\n                    raise\n\n        # If POST-as-GET is not supported yet, we use a GET instead.\n        return self.net.get(*args, **kwargs)```\n\nUpdated code:\n```python\ndef _post_as_get(self, *args, **kwargs):\n        \n        \n        \n        new_args = args[:1] + (None,) + args[1:]\n        return self._post(*new_args, **kwargs)\n```\n\nOriginal documentation:\n```python\nSend GET request using the POST-as-GET protocol if needed.\n        The request will be first issued using POST-as-GET for ACME v2. If the ACME CA servers do\n        not support this yet and return an error, request will be retried using GET.\n        For ACME v1, only GET request will be tried, as POST-as-GET is not supported.\n        :param args:\n        :param kwargs:\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Send GET request using the POST-as-GET protocol.\n        :param args:\n        :param kwargs:\n        :return:", "file": "acme_acme_client.py", "function": "ClientV2._post_as_get", "project": "certbot", "change_stats": {"code_changes": 19, "doc_changes": 7, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rgb2grey(rgb):\n    \n    \n    return _convert(grey_from_rgb, rgb)[..., 0]```\n\nUpdated code:\n```python\ndef rgb2grey(rgb):\n    \n    \n    return _convert(grey_from_rgb, rgb[:, :, :3])[..., 0]\n```\n\nOriginal documentation:\n```python\nCompute luminance of an RGB image.\n\n    Parameters\n    ----------\n    rgb : array_like\n        The image in RGB format, in a 3-D array of shape (.., .., 3).\n\n    Returns\n    -------\n    out : ndarray\n        The luminance image, a 2-D array.\n\n    Raises\n    ------\n    ValueError\n        If `rgb2grey` is not a 3-D array of shape (.., .., 3).\n\n    References\n    ----------\n    .. [1] http://www.poynton.com/PDFs/ColorFAQ.pdf\n\n    Notes\n    -----\n    The weights used in this conversion are calibrated for contemporary\n    CRT phosphors::\n\n        Y = 0.2125 R + 0.7154 G + 0.0721 B\n\n    Examples\n    --------\n    >>> import os\n    >>> from skimage import data_dir\n    >>> from skimage.io import imread\n    >>> from skimage.color import rgb2grey\n\n    >>> lena = imread(os.path.join(data_dir, 'lena.png'))\n    >>> lena_grey = rgb2grey(lena)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute luminance of an RGB image.\n\n    Parameters\n    ----------\n    rgb : array_like\n        The image in RGB format, in a 3-D array of shape (.., .., 3),\n        or in RGBA format with shape (.., .., 4).\n\n    Returns\n    -------\n    out : ndarray\n        The luminance image, a 2-D array.\n\n    Raises\n    ------\n    ValueError\n        If `rgb2grey` is not a 3-D array of shape (.., .., 3) or \n        (.., .., 4).\n\n    References\n    ----------\n    .. [1] http://www.poynton.com/PDFs/ColorFAQ.pdf\n\n    Notes\n    -----\n    The weights used in this conversion are calibrated for contemporary\n    CRT phosphors::\n\n        Y = 0.2125 R + 0.7154 G + 0.0721 B\n        \n    If there is an alpha channel present, it is ignored.\n\n    Examples\n    --------\n    >>> import os\n    >>> from skimage import data_dir\n    >>> from skimage.io import imread\n    >>> from skimage.color import rgb2grey\n\n    >>> lena = imread(os.path.join(data_dir, 'lena.png'))\n    >>> lena_grey = rgb2grey(lena)", "file": "skimage_color_colorconv.py", "function": "rgb2grey", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 10, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef graph_laplacian(graph, normed=False, return_diag=False):\n     \n    \n    if normed and (np.issubdtype(graph.dtype, np.int) or\n                   np.issubdtype(graph.dtype, np.uint)):\n        graph = graph.astype(np.float)\n    if sparse.isspmatrix(graph):\n        return _graph_laplacian_sparse(graph, normed=normed,\n                                       return_diag=return_diag)\n    else:\n        # We have a numpy array\n        return _graph_laplacian_dense(graph, normed=normed,\n                                      return_diag=return_diag)```\n\nUpdated code:\n```python\ndef graph_laplacian(csgraph, normed=False, return_diag=False):\n     \n    \n    if csgraph.ndim != 2 or csgraph.shape[0] != csgraph.shape[1]:\n        raise ValueError('csgraph must be a square matrix or array')\n\n    if normed and (np.issubdtype(csgraph.dtype, np.int)\n                   or np.issubdtype(csgraph.dtype, np.uint)):\n        csgraph = csgraph.astype(np.float)\n\n    if sparse.isspmatrix(csgraph):\n        return _laplacian_sparse(csgraph, normed=normed,\n                                 return_diag=return_diag)\n    else:\n        return _laplacian_dense(csgraph, normed=normed,\n                                return_diag=return_diag)\n```\n\nOriginal documentation:\n```python\nReturn the Laplacian of the given graph.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the Laplacian matrix of a directed graph.\n\n    For non-symmetric graphs the out-degree is used in the computation.\n\n    Parameters\n    ----------\n    csgraph : array_like or sparse matrix, 2 dimensions\n        compressed-sparse graph, with shape (N, N).\n    normed : bool, optional\n        If True, then compute normalized Laplacian.\n    return_diag : bool, optional\n        If True, then return diagonal as well as laplacian.\n\n    Returns\n    -------\n    lap : ndarray\n        The N x N laplacian matrix of graph.\n    diag : ndarray\n        The length-N diagonal of the laplacian matrix.\n        diag is returned only if return_diag is True.\n\n    Notes\n    -----\n    The Laplacian matrix of a graph is sometimes referred to as the\n    \"Kirchoff matrix\" or the \"admittance matrix\", and is useful in many\n    parts of spectral graph theory.  In particular, the eigen-decomposition\n    of the laplacian matrix can give insight into many properties of the graph.\n\n    For non-symmetric directed graphs, the laplacian is computed using the\n    out-degree of each node.\n\n    Examples\n    --------\n    >>> from scipy.sparse import csgraph\n    >>> G = np.arange(5) * np.arange(5)[:, np.newaxis]\n    >>> G\n    array([[ 0,  0,  0,  0,  0],\n           [ 0,  1,  2,  3,  4],\n           [ 0,  2,  4,  6,  8],\n           [ 0,  3,  6,  9, 12],\n           [ 0,  4,  8, 12, 16]])\n    >>> csgraph.laplacian(G, normed=False)\n    array([[  0,   0,   0,   0,   0],\n           [  0,   9,  -2,  -3,  -4],\n           [  0,  -2,  16,  -6,  -8],\n           [  0,  -3,  -6,  21, -12],\n           [  0,  -4,  -8, -12,  24]])", "file": "sklearn_utils_graph.py", "function": "graph_laplacian", "project": "scikit-learn", "change_stats": {"code_changes": 25, "doc_changes": 50, "total_changes": 75}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load(self):\n          \n        data = dict()\n        if not self.is_extract:\n            if not self.have_alignments_file:\n                return data\n            data = super().load()\n            return data\n\n        skip_existing = bool(hasattr(self.args, 'skip_existing')\n                             and self.args.skip_existing)\n        skip_faces = bool(hasattr(self.args, 'skip_faces')\n                          and self.args.skip_faces)\n\n        if not skip_existing and not skip_faces:\n            logger.debug(\"No skipping selected. Returning empty dictionary\")\n            return data\n\n        if not self.have_alignments_file and (skip_existing or skip_faces):\n            logger.warning(\"Skip Existing/Skip Faces selected, but no alignments file found!\")\n            return data\n\n        data = self.serializer.load(self.file)\n\n        if skip_faces:\n            # Remove items from alignments that have no faces so they will\n            # be re-detected\n            del_keys = [key for key, val in data.items() if not val]\n            logger.debug(\"Frames with no faces selected for redetection: %s\", len(del_keys))\n            for key in del_keys:\n                if key in data:\n                    logger.trace(\"Selected for redetection: '%s'\", key)\n                    del data[key]\n        return data```\n\nUpdated code:\n```python\ndef load(self):\n         \n        \n        data = dict()\n        if not self._is_extract:\n            if not self.have_alignments_file:\n                return data\n            data = super().load()\n            return data\n\n        skip_existing = hasattr(self._args, 'skip_existing') and self._args.skip_existing\n        skip_faces = hasattr(self._args, 'skip_faces') and self._args.skip_faces\n\n        if not skip_existing and not skip_faces:\n            logger.debug(\"No skipping selected. Returning empty dictionary\")\n            return data\n\n        if not self.have_alignments_file and (skip_existing or skip_faces):\n            logger.warning(\"Skip Existing/Skip Faces selected, but no alignments file found!\")\n            return data\n\n        data = self.serializer.load(self.file)\n\n        if skip_faces:\n            # Remove items from alignments that have no faces so they will\n            # be re-detected\n            del_keys = [key for key, val in data.items() if not val]\n            logger.debug(\"Frames with no faces selected for redetection: %s\", len(del_keys))\n            for key in del_keys:\n                if key in data:\n                    logger.trace(\"Selected for redetection: '%s'\", key)\n                    del data[key]\n        return data\n```\n\nOriginal documentation:\n```python\nOverride  parent loader to handle skip existing on extract\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Override the parent :func:`~lib.alignments.Alignments.load` to handle skip existing\n        frames and faces on extract.\n\n        If skip existing has been selected, existing alignments are loaded and returned to the\n        calling script.\n\n        Returns\n        -------\n        dict\n            Any alignments that have already been extracted if skip existing has been selected\n            otherwise an empty dictionary", "file": "scripts_fsmedia.py", "function": "Alignments.load", "project": "faceswap", "change_stats": {"code_changes": 13, "doc_changes": 14, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _put_object(\n        self,\n        data: Union[bytes, bytearray],\n        client_ref_id: bytes,\n        client_id: str,\n        context=None,\n    ):\n        \n        \n        try:\n            obj = loads_from_client(data, self)\n            with disable_client_hook():\n                objectref = ray.put(obj)\n        except Exception as e:\n            logger.exception(\"Put failed:\")\n            return ray_client_pb2.PutResponse(\n                id=b\"\", valid=False, error=cloudpickle.dumps(e)\n            )\n\n        self.object_refs[client_id][objectref.binary()] = objectref\n        if len(client_ref_id) > 0:\n            self.client_side_ref_map[client_id][client_ref_id] = objectref.binary()\n        logger.debug(\"put: %s\" % objectref)\n        return ray_client_pb2.PutResponse(id=objectref.binary(), valid=True)```\n\nUpdated code:\n```python\ndef _put_object(\n        self,\n        data: Union[bytes, bytearray],\n        client_ref_id: bytes,\n        client_id: str,\n        owner_id: bytes,\n        context=None,\n    ):\n        \n        \n        try:\n            obj = loads_from_client(data, self)\n\n            if owner_id:\n                owner = self.actor_refs[owner_id]\n            else:\n                owner = None\n            with disable_client_hook():\n                objectref = ray.put(obj, _owner=owner)\n        except Exception as e:\n            logger.exception(\"Put failed:\")\n            return ray_client_pb2.PutResponse(\n                id=b\"\", valid=False, error=cloudpickle.dumps(e)\n            )\n\n        self.object_refs[client_id][objectref.binary()] = objectref\n        if len(client_ref_id) > 0:\n            self.client_side_ref_map[client_id][client_ref_id] = objectref.binary()\n        logger.debug(\"put: %s\" % objectref)\n        return ray_client_pb2.PutResponse(id=objectref.binary(), valid=True)\n```\n\nOriginal documentation:\n```python\nPut an object in the cluster with ray.put() via gRPC.\n\n        Args:\n            data: Pickled data. Can either be bytearray if this is called\n              from the dataservicer, or bytes if called from PutObject.\n            client_ref_id: The id associated with this object on the client.\n            client_id: The client who owns this data, for tracking when to\n              delete this reference.\n            context: gRPC context.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Put an object in the cluster with ray.put() via gRPC.\n\n        Args:\n            data: Pickled data. Can either be bytearray if this is called\n              from the dataservicer, or bytes if called from PutObject.\n            client_ref_id: The id associated with this object on the client.\n            client_id: The client who owns this data, for tracking when to\n              delete this reference.\n            owner_id: The owner id of the object.\n            context: gRPC context.", "file": "python_ray_util_client_server_server.py", "function": "RayletServicer._put_object", "project": "ray", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_images(self):\n         \n        \n        logger.debug(\"Getting image paths\")\n        images = dict()\n        for side in (\"a\", \"b\"):\n            image_dir = getattr(self._args, \"input_{}\".format(side))\n            if not os.path.isdir(image_dir):\n                logger.error(\"Error: '%s' does not exist\", image_dir)\n                sys.exit(1)\n\n            images[side] = get_image_paths(image_dir)\n            if not images[side]:\n                logger.error(\"Error: '%s' contains no images\", image_dir)\n                sys.exit(1)\n\n        logger.info(\"Model A Directory: %s\", self._args.input_a)\n        logger.info(\"Model B Directory: %s\", self._args.input_b)\n        logger.debug(\"Got image paths: %s\", [(key, str(len(val)) + \" images\")\n                                             for key, val in images.items()])\n        self._validate_image_counts(images)\n        return images```\n\nUpdated code:\n```python\ndef _get_images(self):\n         \n        \n        logger.debug(\"Getting image paths\")\n        images = dict()\n        for side in (\"a\", \"b\"):\n            image_dir = getattr(self._args, \"input_{}\".format(side))\n            if not os.path.isdir(image_dir):\n                logger.error(\"Error: '%s' does not exist\", image_dir)\n                sys.exit(1)\n\n            images[side] = get_image_paths(image_dir, \".png\")\n            if not images[side]:\n                logger.error(\"Error: '%s' contains no images\", image_dir)\n                sys.exit(1)\n            # Validate the first image is a detected face\n            test_image = next(img for img in images[side])\n            meta = read_image_meta(test_image)\n            logger.debug(\"Test file: (filename: %s, metadata: %s)\", test_image, meta)\n            if \"itxt\" not in meta or \"alignments\" not in meta[\"itxt\"]:\n                logger.error(\"The input folder '%s' contains images that are not extracted faces.\",\n                             image_dir)\n                logger.error(\"You can only train a model on faces generated from Faceswap's \"\n                             \"extract process. Please check your sources and try again.\")\n                sys.exit(1)\n\n            logger.info(\"Model %s Directory: '%s' (%s images)\",\n                        side.upper(), image_dir, len(images[side]))\n        logger.debug(\"Got image paths: %s\", [(key, str(len(val)) + \" images\")\n                                             for key, val in images.items()])\n        self._validate_image_counts(images)\n        return images\n```\n\nOriginal documentation:\n```python\nCheck the image folders exist and contains images and obtain image paths.\n\n        Returns\n        -------\n        dict\n            The image paths for each side. The key is the side, the value is the list of paths\n            for that side.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check the image folders exist and contains valid extracted faces. Obtain image paths.\n\n        Returns\n        -------\n        dict\n            The image paths for each side. The key is the side, the value is the list of paths\n            for that side.", "file": "scripts_train.py", "function": "Train._get_images", "project": "faceswap", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_true_slices(l):\n    \n    \n    \n    return [isinstance(k, slice) and not is_null_slice(k) for k in l]```\n\nUpdated code:\n```python\ndef is_true_slices(line):\n    \n    \n    \n    return [isinstance(k, slice) and not is_null_slice(k) for k in line]\n```\n\nOriginal documentation:\n```python\nFind non-trivial slices in \"l\": return a list of booleans with same length.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Find non-trivial slices in \"line\": return a list of booleans with same length.", "file": "pandas_core_common.py", "function": "is_true_slices", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate(\n        self,\n        input_features: Optional[torch.Tensor] = None,\n        generation_config: Optional[GenerationConfig] = None,\n        logits_processor: Optional[LogitsProcessorList] = None,\n        stopping_criteria: Optional[StoppingCriteriaList] = None,\n        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n        synced_gpus: bool = False,\n        return_timestamps: Optional[bool] = None,\n        task: Optional[str] = None,\n        language: Optional[str] = None,\n        is_multilingual: Optional[bool] = None,\n        prompt_ids: Optional[torch.Tensor] = None,\n        condition_on_prev_tokens: Optional[bool] = None,\n        temperature: Optional[Union[float, Tuple[float, ...]]] = None,\n        compression_ratio_threshold: Optional[float] = None,\n        logprob_threshold: Optional[float] = None,\n        no_speech_threshold: Optional[float] = None,\n        num_segment_frames: Optional[int] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        time_precision: float = 0.02,\n        return_token_timestamps: Optional[bool] = None,\n        return_segments: bool = False,\n        return_dict_in_generate: Optional[bool] = None,\n        **kwargs,\n    ):\n        \n        \n\n        \n        # 0. deprecate old inputs\n        if \"inputs\" in kwargs:\n            input_features = kwargs.pop(\"inputs\")\n            warnings.warn(\n                \"The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\",\n                FutureWarning,\n            )\n        # 1. copy generation config\n        if generation_config is None:\n            generation_config = copy.deepcopy(self.generation_config)\n        else:\n            generation_config = copy.deepcopy(generation_config)\n\n        # 2. set global generate variables\n        input_stride = self.model.encoder.conv1.stride[0] * self.model.encoder.conv2.stride[0]\n        num_segment_frames = input_stride * self.config.max_source_positions\n        total_input_frames = self._retrieve_total_input_frames(\n            input_features=input_features, input_stride=input_stride, kwargs=kwargs\n        )\n        is_shortform = total_input_frames <= num_segment_frames\n\n        if is_shortform:\n            # warn user of ignored inputs\n            self._maybe_warn_unused_inputs(\n                condition_on_prev_tokens=condition_on_prev_tokens,\n                temperature=temperature,\n                compression_ratio_threshold=compression_ratio_threshold,\n                logprob_threshold=logprob_threshold,\n                no_speech_threshold=no_speech_threshold,\n                total_input_frames=total_input_frames,\n            )\n\n        # 3. Make sure generation config is correctly set\n        # Make sure the generation config is correctly set depending on whether timestamps are to be returned or not\n        self._set_return_outputs(\n            return_dict_in_generate=return_dict_in_generate,\n            return_token_timestamps=return_token_timestamps,\n            is_shortform=is_shortform,\n            logprob_threshold=logprob_threshold,\n            generation_config=generation_config,\n        )\n        self._set_return_timestamps(\n            return_timestamps=return_timestamps, is_shortform=is_shortform, generation_config=generation_config\n        )\n        self._set_language_and_task(\n            language=language, task=task, is_multilingual=is_multilingual, generation_config=generation_config\n        )\n        # pass self.config for backward compatibility\n        self._set_forced_decoder_ids(\n            task=task,\n            language=language,\n            prompt_ids=prompt_ids,\n            generation_config=generation_config,\n            config=self.config,\n            kwargs=kwargs,\n        )\n        self._set_token_ids(generation_config=generation_config, config=self.config, kwargs=kwargs)\n        self._set_num_frames(\n            return_token_timestamps=return_token_timestamps, generation_config=generation_config, kwargs=kwargs\n        )\n        self._set_thresholds_and_condition(\n            generation_config=generation_config,\n            logprob_threshold=logprob_threshold,\n            compression_ratio_threshold=compression_ratio_threshold,\n            no_speech_threshold=no_speech_threshold,\n            condition_on_prev_tokens=condition_on_prev_tokens,\n        )\n\n        # 4. Retrieve logits processors\n        logits_processor = self._retrieve_logit_processors(\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            no_speech_threshold=no_speech_threshold,\n            is_shortform=is_shortform,\n            num_beams=kwargs.get(\"num_beams\", 1),\n        )\n\n        # 5. If we're in shortform mode, simple generate the whole input at once and return the output\n        if is_shortform:\n            if temperature is not None:\n                kwargs[\"temperature\"] = temperature\n\n            outputs = super().generate(\n                input_features,\n                generation_config=generation_config,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                synced_gpus=synced_gpus,\n                **kwargs,\n            )\n\n            if generation_config.return_token_timestamps and hasattr(generation_config, \"alignment_heads\"):\n                outputs[\"token_timestamps\"] = self._extract_token_timestamps(\n                    outputs, generation_config.alignment_heads, num_frames=generation_config.num_frames\n                )\n\n            return outputs\n\n        # 6. Else we're in longform mode which is more complex.\n        # We need to chunk the audio input depending on when the model generates timestamp tokens\n\n        # 6.1 Set and retrieve global longform generation variables\n        self._set_condition_on_prev_tokens(\n            condition_on_prev_tokens=condition_on_prev_tokens, generation_config=generation_config\n        )\n\n        timestamp_begin = generation_config.no_timestamps_token_id + 1\n        temperatures = [temperature] if not isinstance(temperature, (list, tuple)) else temperature\n        temperature = temperatures[0]\n        batch_size = input_features.shape[0]\n\n        max_frames, seek = self._retrieve_max_frames_and_seek(\n            batch_size=batch_size, attention_mask=attention_mask, total_input_frames=total_input_frames\n        )\n        init_tokens = self._retrieve_init_tokens_from_forced_decoder_ids(generation_config=generation_config)\n\n        # 6.2 Preppare running variables, list for generation\n        cur_bsz = batch_size\n        current_segments = [[] for _ in range(batch_size)]\n        batch_idx_map = list(range(batch_size))\n        do_condition_on_prev_tokens = [condition_on_prev_tokens for _ in range(batch_size)]\n\n        # 6.2 Transcribe audio until we reach the end of all input audios\n        while (seek < max_frames).any():\n            # 6.3 NOTE: When in longform transcription mode and batch size > 1 we need to dynamically reduce the batch size during the loop\n            # in case one audio finished earlier than another one. Thus, we need to keep a table of \"previous-index-2-current-index\" in order\n            # to know which original audio is being decoded\n            # Set updated index map, duration of previously decoded chunks and number of max frames of current decoding chunk\n            input_features, cur_bsz, batch_idx_map = self._maybe_reduce_batch(\n                input_features=input_features,\n                seek=seek,\n                max_frames=max_frames,\n                cur_bsz=cur_bsz,\n                batch_idx_map=batch_idx_map,\n            )\n            time_offset = seek * time_precision / input_stride\n            seek_num_frames = (max_frames - seek).clamp(max=num_segment_frames)\n\n            # 6.4 cut out next 30s segment from input features\n            segment_input = self._get_input_segment(\n                input_features=input_features,\n                seek=seek,\n                seek_num_frames=seek_num_frames,\n                num_segment_frames=num_segment_frames,\n                cur_bsz=cur_bsz,\n                batch_idx_map=batch_idx_map,\n            )\n\n            # 6.5 prepare decoder input ids\n            suppress_tokens = _get_attr_from_logit_processors(\n                logits_processor, SuppressTokensLogitsProcessor, \"suppress_tokens\"\n            )\n            decoder_input_ids, kwargs = self._prepare_decoder_input_ids(\n                cur_bsz=cur_bsz,\n                init_tokens=init_tokens,\n                current_segments=current_segments,\n                batch_idx_map=batch_idx_map,\n                do_condition_on_prev_tokens=do_condition_on_prev_tokens,\n                generation_config=generation_config,\n                config=self.config,\n                device=segment_input.device,\n                suppress_tokens=suppress_tokens,\n                kwargs=kwargs,\n            )\n\n            # 6.6 set max new tokens or max length\n            kwargs = self._set_max_new_tokens_and_length(\n                config=self.config,\n                decoder_input_ids=decoder_input_ids,\n                generation_config=generation_config,\n                kwargs=kwargs,\n            )\n\n            # 6.7 Set current `begin_index` for all logit processors\n            for proc in logits_processor:\n                if hasattr(proc, \"set_begin_index\"):\n                    proc.set_begin_index(decoder_input_ids.shape[-1])\n\n            # 6.8 Run generate with fallback\n            seek_sequences, seek_outputs, should_skip, do_condition_on_prev_tokens = self.generate_with_fallback(\n                segment_input=segment_input,\n                decoder_input_ids=decoder_input_ids,\n                cur_bsz=cur_bsz,\n                batch_idx_map=batch_idx_map,\n                seek=seek,\n                num_segment_frames=num_segment_frames,\n                max_frames=max_frames,\n                temperatures=temperatures,\n                generation_config=generation_config,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                synced_gpus=synced_gpus,\n                return_token_timestamps=return_token_timestamps,\n                do_condition_on_prev_tokens=do_condition_on_prev_tokens,\n                kwargs=kwargs,\n            )\n\n            # 6.9 In every generated sequence, split by timestamp tokens and extract segments\n            for i, seek_sequence in enumerate(seek_sequences):\n                prev_i = batch_idx_map[i]\n\n                if should_skip[i]:\n                    seek[prev_i] += seek_num_frames[prev_i]\n                    continue\n\n                segments, segment_offset = self._retrieve_segment(\n                    seek_sequence=seek_sequence,\n                    seek_outputs=seek_outputs,\n                    time_offset=time_offset,\n                    timestamp_begin=timestamp_begin,\n                    seek_num_frames=seek_num_frames,\n                    time_precision=time_precision,\n                    input_stride=input_stride,\n                    prev_idx=prev_i,\n                    idx=i,\n                )\n\n                current_segments[prev_i] += segments\n                seek[prev_i] += segment_offset\n\n        # 7. Once all segments are added to the list of all segments, called `current_segments`, we extract the predicted\n        # output tokens from the list of dicts. If we use batch size > 1, we make sure to pad the output\n        sequences = _pad_to_max_length(current_segments, generation_config.pad_token_id, padding=\"right\")\n\n        # 8. If we return all segments, the predicted output sequences are put under `\"sequences\"`.\n        if return_segments:\n            return {\"sequences\": sequences, \"segments\": current_segments}\n\n        return sequences```\n\nUpdated code:\n```python\ndef generate(\n        self,\n        input_features: Optional[torch.Tensor] = None,\n        generation_config: Optional[GenerationConfig] = None,\n        logits_processor: Optional[LogitsProcessorList] = None,\n        stopping_criteria: Optional[StoppingCriteriaList] = None,\n        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n        synced_gpus: bool = False,\n        return_timestamps: Optional[bool] = None,\n        task: Optional[str] = None,\n        language: Optional[str] = None,\n        is_multilingual: Optional[bool] = None,\n        prompt_ids: Optional[torch.Tensor] = None,\n        prompt_condition_type: Optional[str] = None,  # first-segment, all-segments\n        condition_on_prev_tokens: Optional[bool] = None,\n        temperature: Optional[Union[float, Tuple[float, ...]]] = None,\n        compression_ratio_threshold: Optional[float] = None,\n        logprob_threshold: Optional[float] = None,\n        no_speech_threshold: Optional[float] = None,\n        num_segment_frames: Optional[int] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        time_precision: float = 0.02,\n        return_token_timestamps: Optional[bool] = None,\n        return_segments: bool = False,\n        return_dict_in_generate: Optional[bool] = None,\n        **kwargs,\n    ):\n        \n        \n\n        \n        # 0. deprecate old inputs\n        if \"inputs\" in kwargs:\n            input_features = kwargs.pop(\"inputs\")\n            warnings.warn(\n                \"The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\",\n                FutureWarning,\n            )\n        # 1. copy generation config\n        if generation_config is None:\n            generation_config = copy.deepcopy(self.generation_config)\n        else:\n            generation_config = copy.deepcopy(generation_config)\n\n        # 2. set global generate variables\n        input_stride = self.model.encoder.conv1.stride[0] * self.model.encoder.conv2.stride[0]\n        num_segment_frames = input_stride * self.config.max_source_positions\n        batch_size, total_input_frames = self._retrieve_total_input_frames(\n            input_features=input_features, input_stride=input_stride, kwargs=kwargs\n        )\n        is_shortform = total_input_frames <= num_segment_frames\n\n        if is_shortform:\n            # warn user of ignored inputs\n            self._maybe_warn_unused_inputs(\n                condition_on_prev_tokens=condition_on_prev_tokens,\n                temperature=temperature,\n                compression_ratio_threshold=compression_ratio_threshold,\n                logprob_threshold=logprob_threshold,\n                no_speech_threshold=no_speech_threshold,\n                total_input_frames=total_input_frames,\n            )\n\n        # 3. Make sure generation config is correctly set\n        # Make sure the generation config is correctly set depending on whether timestamps are to be returned or not\n        self._set_return_outputs(\n            return_dict_in_generate=return_dict_in_generate,\n            return_token_timestamps=return_token_timestamps,\n            is_shortform=is_shortform,\n            logprob_threshold=logprob_threshold,\n            generation_config=generation_config,\n        )\n        self._set_return_timestamps(\n            return_timestamps=return_timestamps, is_shortform=is_shortform, generation_config=generation_config\n        )\n        self._set_language_and_task(\n            language=language, task=task, is_multilingual=is_multilingual, generation_config=generation_config\n        )\n        self._set_token_ids(generation_config=generation_config, config=self.config, kwargs=kwargs)\n        self._set_num_frames(\n            return_token_timestamps=return_token_timestamps, generation_config=generation_config, kwargs=kwargs\n        )\n        self._set_thresholds_and_condition(\n            generation_config=generation_config,\n            logprob_threshold=logprob_threshold,\n            compression_ratio_threshold=compression_ratio_threshold,\n            no_speech_threshold=no_speech_threshold,\n            condition_on_prev_tokens=condition_on_prev_tokens,\n        )\n        self._set_prompt_condition_type(\n            generation_config=generation_config,\n            prompt_condition_type=prompt_condition_type,\n        )\n\n        # pass self.config for backward compatibility\n        init_tokens = self._retrieve_init_tokens(\n            input_features,\n            generation_config=generation_config,\n            config=self.config,\n            num_segment_frames=num_segment_frames,\n            kwargs=kwargs,\n        )\n        # TODO(Sanchit) - passing `decoder_input_ids` is deprecated. One should use `prompt_ids` instead\n        # This function should be be removed in v4.39\n        self._check_decoder_input_ids(\n            prompt_ids=prompt_ids, init_tokens=init_tokens, is_shortform=is_shortform, kwargs=kwargs\n        )\n\n        # 3. Retrieve logits processors\n        begin_index = len(init_tokens)\n        logits_processor = self._retrieve_logit_processors(\n            generation_config=generation_config,\n            logits_processor=logits_processor,\n            begin_index=begin_index,  # begin index is index of first generated decoder token\n            is_shortform=is_shortform,\n            num_beams=kwargs.get(\"num_beams\", 1),\n        )\n\n        # 5. If we're in shortform mode, simple generate the whole input at once and return the output\n        if is_shortform:\n            if temperature is not None:\n                kwargs[\"temperature\"] = temperature\n\n            decoder_input_ids = kwargs.pop(\"decoder_input_ids\", None)\n            if decoder_input_ids is None:\n                one_tensor = torch.ones((batch_size, 1), device=self.device, dtype=torch.long)\n                decoder_input_ids = torch.cat([t * one_tensor for t in init_tokens], dim=-1)\n\n            if prompt_ids is not None:\n                decoder_input_ids = torch.cat(\n                    [prompt_ids[None].repeat(decoder_input_ids.shape[0], 1), decoder_input_ids], dim=-1\n                )\n\n            if kwargs.get(\"max_new_tokens\", 0) + decoder_input_ids.shape[-1] > self.config.max_target_positions:\n                max_new_tokens = kwargs.get(\"max_new_tokens\", 0)\n                raise ValueError(\n                    f\"The length of `decoder_input_ids` equal `prompt_ids` plus special start tokens is {decoder_input_ids.shape[-1]}, and the `max_new_tokens` \"\n                    f\"is {max_new_tokens}. Thus, the combined length of \"\n                    f\"`decoder_input_ids` and `max_new_tokens` is: {max_new_tokens + decoder_input_ids.shape[-1]}. This exceeds the \"\n                    f\"`max_target_positions` of the Whisper model: {self.config.max_target_positions}. \"\n                    \"You should either reduce the length of your prompt, or reduce the value of `max_new_tokens`, \"\n                    f\"so that their combined length is less than {self.config.max_target_positions}.\"\n                )\n\n            outputs = super().generate(\n                input_features,\n                generation_config=generation_config,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                synced_gpus=synced_gpus,\n                decoder_input_ids=decoder_input_ids,\n                **kwargs,\n            )\n\n            if generation_config.return_token_timestamps and hasattr(generation_config, \"alignment_heads\"):\n                outputs[\"token_timestamps\"] = self._extract_token_timestamps(\n                    outputs, generation_config.alignment_heads, num_frames=generation_config.num_frames\n                )\n\n            return outputs\n\n        # 6. Else we're in longform mode which is more complex.\n        # We need to chunk the audio input depending on when the model generates timestamp tokens\n\n        # 6.1 Set and retrieve global longform generation variables\n        self._set_condition_on_prev_tokens(\n            condition_on_prev_tokens=condition_on_prev_tokens, generation_config=generation_config\n        )\n\n        timestamp_begin = generation_config.no_timestamps_token_id + 1\n        temperatures = [temperature] if not isinstance(temperature, (list, tuple)) else temperature\n        temperature = temperatures[0]\n        batch_size = input_features.shape[0]\n\n        max_frames, seek = self._retrieve_max_frames_and_seek(\n            batch_size=batch_size, attention_mask=attention_mask, total_input_frames=total_input_frames\n        )\n\n        # 6.2 Preppare running variables, list for generation\n        cur_bsz = batch_size\n        current_segments = self._prepare_segments(\n            prompt_ids=prompt_ids,\n            batch_size=batch_size,\n            generation_config=generation_config,\n        )\n\n        batch_idx_map = list(range(batch_size))\n        do_condition_on_prev_tokens = [condition_on_prev_tokens for _ in range(batch_size)]\n\n        # 6.2 Transcribe audio until we reach the end of all input audios\n        while (seek < max_frames).any():\n            # 6.3 NOTE: When in longform transcription mode and batch size > 1 we need to dynamically reduce the batch size during the loop\n            # in case one audio finished earlier than another one. Thus, we need to keep a table of \"previous-index-2-current-index\" in order\n            # to know which original audio is being decoded\n            # Set updated index map, duration of previously decoded chunks and number of max frames of current decoding chunk\n            input_features, cur_bsz, batch_idx_map = self._maybe_reduce_batch(\n                input_features=input_features,\n                seek=seek,\n                max_frames=max_frames,\n                cur_bsz=cur_bsz,\n                batch_idx_map=batch_idx_map,\n            )\n            time_offset = seek * time_precision / input_stride\n            seek_num_frames = (max_frames - seek).clamp(max=num_segment_frames)\n\n            # 6.4 cut out next 30s segment from input features\n            segment_input = self._get_input_segment(\n                input_features=input_features,\n                seek=seek,\n                seek_num_frames=seek_num_frames,\n                num_segment_frames=num_segment_frames,\n                cur_bsz=cur_bsz,\n                batch_idx_map=batch_idx_map,\n            )\n\n            # 6.5 prepare decoder input ids\n            suppress_tokens = _get_attr_from_logit_processors(\n                logits_processor, SuppressTokensLogitsProcessor, \"suppress_tokens\"\n            )\n            decoder_input_ids, kwargs = self._prepare_decoder_input_ids(\n                cur_bsz=cur_bsz,\n                init_tokens=init_tokens,\n                current_segments=current_segments,\n                batch_idx_map=batch_idx_map,\n                do_condition_on_prev_tokens=do_condition_on_prev_tokens,\n                prompt_ids=prompt_ids,\n                generation_config=generation_config,\n                config=self.config,\n                device=segment_input.device,\n                suppress_tokens=suppress_tokens,\n                kwargs=kwargs,\n            )\n\n            # 6.6 set max new tokens or max length\n            kwargs = self._set_max_new_tokens_and_length(\n                config=self.config,\n                decoder_input_ids=decoder_input_ids,\n                generation_config=generation_config,\n                kwargs=kwargs,\n            )\n\n            # 6.7 Set current `begin_index` for all logit processors\n            for proc in logits_processor:\n                if hasattr(proc, \"set_begin_index\"):\n                    proc.set_begin_index(decoder_input_ids.shape[-1])\n\n            # 6.8 Run generate with fallback\n            seek_sequences, seek_outputs, should_skip, do_condition_on_prev_tokens = self.generate_with_fallback(\n                segment_input=segment_input,\n                decoder_input_ids=decoder_input_ids,\n                cur_bsz=cur_bsz,\n                batch_idx_map=batch_idx_map,\n                seek=seek,\n                num_segment_frames=num_segment_frames,\n                max_frames=max_frames,\n                temperatures=temperatures,\n                generation_config=generation_config,\n                logits_processor=logits_processor,\n                stopping_criteria=stopping_criteria,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                synced_gpus=synced_gpus,\n                return_token_timestamps=return_token_timestamps,\n                do_condition_on_prev_tokens=do_condition_on_prev_tokens,\n                kwargs=kwargs,\n            )\n\n            # 6.9 In every generated sequence, split by timestamp tokens and extract segments\n            for i, seek_sequence in enumerate(seek_sequences):\n                prev_i = batch_idx_map[i]\n\n                if should_skip[i]:\n                    seek[prev_i] += seek_num_frames[prev_i]\n                    continue\n\n                segments, segment_offset = self._retrieve_segment(\n                    seek_sequence=seek_sequence,\n                    seek_outputs=seek_outputs,\n                    time_offset=time_offset,\n                    timestamp_begin=timestamp_begin,\n                    seek_num_frames=seek_num_frames,\n                    time_precision=time_precision,\n                    input_stride=input_stride,\n                    prev_idx=prev_i,\n                    idx=i,\n                )\n\n                current_segments[prev_i] += segments\n                seek[prev_i] += segment_offset\n\n        # 7. Once all segments are added to the list of all segments, called `current_segments`, we extract the predicted\n        # output tokens from the list of dicts. If we use batch size > 1, we make sure to pad the output\n        final_segments = (\n            [x[1:] for x in current_segments]\n            if (prompt_ids is not None and generation_config.prompt_condition_type == \"first-segment\")\n            else current_segments\n        )\n        sequences = _pad_to_max_length(final_segments, generation_config.pad_token_id, padding=\"right\")\n\n        # 8. If we return all segments, the predicted output sequences are put under `\"sequences\"`.\n        if return_segments:\n            return {\"sequences\": sequences, \"segments\": final_segments}\n\n        return sequences\n```\n\nOriginal documentation:\n```python\nTranscribes or translates log-mel input features to a sequence of auto-regressively generated token ids.\n\n        <Tip warning={true}>\n\n        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Parameters:\n            input_features (`torch.Tensor` of shape `(batch_size, feature_size, sequence_length)`, *optional*):\n                Float values of log-mel features extracted from the raw speech waveform. The raw speech waveform can be obtained by\n                loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n                the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n                [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n                tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`] for details.\n            generation_config (`~generation.GenerationConfig`, *optional*):\n                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n                passed to generate matching the attributes of `generation_config` will override them. If\n                `generation_config` is not provided, the default will be used, which had the following loading\n                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n                default values, whose documentation should be checked to parameterize generation.\n            logits_processor (`LogitsProcessorList`, *optional*):\n                Custom logits processors that complement the default logits processors built from arguments and\n                generation config. If a logit processor is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            stopping_criteria (`StoppingCriteriaList`, *optional*):\n                Custom stopping criteria that complement the default stopping criteria built from arguments and a\n                generation config. If a stopping criteria is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                Retrieval](https://arxiv.org/abs/2010.00904).\n            synced_gpus (`bool`, *optional*, defaults to `False`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            return_timestamps (`bool`, *optional*):\n                Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n            task (`str`, *optional*):\n                Task to use for generation, either \"translate\" or \"transcribe\". The `model.config.forced_decoder_ids`\n                will be updated accordingly.\n            language (`str`, *optional*):\n                Language token to use for generation, can be either in the form of `<|en|>`, `en` or `english`. You can\n                find all the possible language tokens in the `model.generation_config.lang_to_id` dictionary.\n            is_multilingual (`bool`, *optional*):\n                Whether or not the model is multilingual.\n            prompt_ids (`torch.Tensor`, *optional*):\n                Rank-1 tensor of token IDs created by passing text to [`~WhisperProcessor.get_prompt_ids`] that is\n                provided as a prompt to each chunk. This can be used to provide or \"prompt-engineer\" a context for\n                transcription, e.g. custom vocabularies or proper nouns to make it more likely to predict those words\n                correctly. It cannot be used in conjunction with `decoder_start_token_id` as it overwrites this value.\n            condition_on_prev_tokens (`bool`, *optional*):\n                Only relevant for long-form transcription. Whether to condition each segment on the previous segment.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            temperature (`float` or list of `float`, *optional*):\n                The temperature to be used for generation. Passing a single `float` value and `do_sample=True` activates\n                generation using sampling. For long-form transcription, temperature fallback can be activated by passing\n                a list of float values such as (0.0, 0.2, 0.4, 0.6, 0.8, 1.0). As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            compression_ratio_threshold (`float`, *optional*):\n                Only relevant for long-form transcription. If defined, the zlib compression rate of each segment will be computed. If the compression rate of\n                a segment is higher than `compression_ratio_threshold`, temperature fallback is activated: the generated segment is discarded and the generation is\n                repeated using a higher temperature. The intuition behind this feature is that segments with very high compression rates\n                suffer from a lot of repetition. The unwanted repetition can be reduced by injecting more randomness by increasing the temperature. If `compression_ratio_threshold` is defined\n                make sure that `temperature` is a list of values. A common value for `compression_ratio_threshold` is 1.35.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            logprob_threshold (`float`, *optional*):\n                Only relevant for long-form transcription. If defined, the average log-probability of each segment will be computed. If the log-probability of\n                a given segment is lower than `logprob_threshold`, temperature fallback is activated: the generated segment is discarded and the generation is\n                repeated using a higher temperature. The intuition behind this feature is that segments of low log-probability\n                can be improved by injecting more randomness by increasing the temperature. If `logprob_threshold` is defined\n                make sure that `temperature` is a list of values. A common value for `logprob_threshold` is -1.0.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            no_speech_threshold (`float`, *optional*):\n                Only relevant for long-form transcription. If defined, the \"no-speech\" token combined with the `logprob_threshold`\n                is used to determine whether a segment contains only silence. In this case, the transcription for this segment\n                is skipped.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            num_segment_frames (`int`, *optional*):\n                The number of frames a single segment is made of. If not defined, `num_segment_frames` defaults to the model's stride\n                times the maximum input length.\n            attention_mask (`torch.Tensor`, *optional*):\n                `attention_mask` needs to be passed when doing long-form transcription using a batch size > 1.\n            time_precision (`int`, *optional*, defaults to 0.02):\n                The duration of output token in seconds. *E.g.* 0.02 means that a generated token on average accounts\n                for 20 ms.\n            return_token_timestamps (`bool`, *optional*):\n                Whether to return token-level timestamps with the text. This can be used with or without the\n                `return_timestamps` option. To get word-level timestamps, use the tokenizer to group the tokens into\n                words.\n            return_segments (`bool`, *optional*, defaults to `False`):\n                Whether to additionally return a list of all segments. Note that this option can only be enabled\n                when doing long-form transcription.\n            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n                Whether or not to return a [`~utils.ModelOutput`] instead of just returning the generated tokens.\n                Note that when doing long-form transcription, `return_dict_in_generate` can only be enabled when\n                `return_segments` is set True. In this case the generation outputs of each segment is added to each\n                segment.\n            kwargs (`Dict[str, Any]`, *optional*):\n                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n\n        Return:\n            [`~utils.ModelOutput`] or `torch.LongTensor` or `Dict[str, Any]`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor` or a dict of segments when `return_segments=True`.\n\n                If the passed input is > 30 seconds / > 3000 mel input features and `return_segments=True` then a dictionary of generated sequence ids, called `sequences` and a list of each generated segment is returned.\n\n                else if the passed input is <= 30 seconds / >= 3000 mel input features, the possible [`~utils.ModelOutput`] types are:\n\n                    - [`~generation.GenerateEncoderDecoderOutput`],\n                    - [`~generation.GenerateBeamEncoderDecoderOutput`]\n\n                else only the generated output sequence ids are returned.\n\n        Example:\n\n        - *Longform transcription*: To transcribe or translate audios longer than 30 seconds, process the audio files without truncation and pass all mel features at once to generate.\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n        >>> from datasets import load_dataset, Audio\n\n        >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n        >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n        >>> model.cuda()\n\n        >>> # load audios > 30 seconds\n        >>> ds = load_dataset(\"distil-whisper/meanwhile\", \"default\")[\"test\"]\n        >>> # resample to 16kHz\n        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n        >>> # take first 8 audios and retrieve array\n        >>> audio = ds[:8][\"audio\"]\n        >>> audio = [x[\"array\"] for x in audio]\n\n        >>> # make sure to NOT truncate the input audio, to return the `attention_mask` and to pad to the longest audio\n        >>> inputs = processor(audio, return_tensors=\"pt\", truncation=False, padding=\"longest\", return_attention_mask=True, sampling_rate=16_000)\n        >>> inputs = inputs.to(\"cuda\", torch.float32)\n\n        >>> # transcribe audio to ids\n        >>> generated_ids = model.generate(**inputs)\n\n        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n        >>> transcription[0]\n        ' Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on a stained kid's place mat from a defunct dennies. set up a table inside a rusty cargo container down by the Wharf and challenged toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile!'\n        ```\n\n        - *Shortform transcription*: If passed mel input features are < 30 seconds, the whole audio will be transcribed with a single call to generate.\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n        >>> from datasets import load_dataset\n\n        >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n        >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n        >>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n        >>> input_features = inputs.input_features\n\n        >>> generated_ids = model.generate(inputs=input_features)\n\n        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        >>> transcription\n        ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n        ```\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Transcribes or translates log-mel input features to a sequence of auto-regressively generated token ids.\n\n        <Tip warning={true}>\n\n        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Parameters:\n            input_features (`torch.Tensor` of shape `(batch_size, feature_size, sequence_length)`, *optional*):\n                Float values of log-mel features extracted from the raw speech waveform. The raw speech waveform can be obtained by\n                loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n                the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n                [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n                tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`] for details.\n            generation_config (`~generation.GenerationConfig`, *optional*):\n                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n                passed to generate matching the attributes of `generation_config` will override them. If\n                `generation_config` is not provided, the default will be used, which had the following loading\n                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n                default values, whose documentation should be checked to parameterize generation.\n            logits_processor (`LogitsProcessorList`, *optional*):\n                Custom logits processors that complement the default logits processors built from arguments and\n                generation config. If a logit processor is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            stopping_criteria (`StoppingCriteriaList`, *optional*):\n                Custom stopping criteria that complement the default stopping criteria built from arguments and a\n                generation config. If a stopping criteria is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                Retrieval](https://arxiv.org/abs/2010.00904).\n            synced_gpus (`bool`, *optional*, defaults to `False`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            return_timestamps (`bool`, *optional*):\n                Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n            task (`str`, *optional*):\n                Task to use for generation, either \"translate\" or \"transcribe\". The `model.config.forced_decoder_ids`\n                will be updated accordingly.\n            language (`str`, *optional*):\n                Language token to use for generation, can be either in the form of `<|en|>`, `en` or `english`. You can\n                find all the possible language tokens in the `model.generation_config.lang_to_id` dictionary.\n            is_multilingual (`bool`, *optional*):\n                Whether or not the model is multilingual.\n            prompt_ids (`torch.Tensor`, *optional*):\n                Rank-1 tensor of token IDs created by passing text to [`~WhisperProcessor.get_prompt_ids`] that is\n                provided as a prompt to each chunk. This can be used to provide or \"prompt-engineer\" a context for\n                transcription, e.g. custom vocabularies or proper nouns to make it more likely to predict those words\n                correctly. It cannot be used in conjunction with `decoder_start_token_id` as it overwrites this value.\n            prompt_condition_type (`str`, *optional*):\n                Only relevant for long-form transcription. Condition type of `prompt_ids`. 'first-segment' means only the first segment is conditioned on `prompt_ids`. 'all-segments' means each segment is conditioned on `prompt_ids`. Make sure to enable `condition_on_prev_tokens` for 'all-segments'.\n                Defaults to 'first-segment'. For short-term transcription only 'first-segment' is possible.\n            condition_on_prev_tokens (`bool`, *optional*):\n                Only relevant for long-form transcription. Whether to condition each segment on the previous segment.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            temperature (`float` or list of `float`, *optional*):\n                The temperature to be used for generation. Passing a single `float` value and `do_sample=True` activates\n                generation using sampling. For long-form transcription, temperature fallback can be activated by passing\n                a list of float values such as (0.0, 0.2, 0.4, 0.6, 0.8, 1.0). As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            compression_ratio_threshold (`float`, *optional*):\n                Only relevant for long-form transcription. If defined, the zlib compression rate of each segment will be computed. If the compression rate of\n                a segment is higher than `compression_ratio_threshold`, temperature fallback is activated: the generated segment is discarded and the generation is\n                repeated using a higher temperature. The intuition behind this feature is that segments with very high compression rates\n                suffer from a lot of repetition. The unwanted repetition can be reduced by injecting more randomness by increasing the temperature. If `compression_ratio_threshold` is defined\n                make sure that `temperature` is a list of values. A common value for `compression_ratio_threshold` is 1.35.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            logprob_threshold (`float`, *optional*):\n                Only relevant for long-form transcription. If defined, the average log-probability of each segment will be computed. If the log-probability of\n                a given segment is lower than `logprob_threshold`, temperature fallback is activated: the generated segment is discarded and the generation is\n                repeated using a higher temperature. The intuition behind this feature is that segments of low log-probability\n                can be improved by injecting more randomness by increasing the temperature. If `logprob_threshold` is defined\n                make sure that `temperature` is a list of values. A common value for `logprob_threshold` is -1.0.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            no_speech_threshold (`float`, *optional*):\n                Only relevant for long-form transcription. If defined, the \"no-speech\" token combined with the `logprob_threshold`\n                is used to determine whether a segment contains only silence. In this case, the transcription for this segment\n                is skipped.\n                As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can help to improve\n                performance.\n            num_segment_frames (`int`, *optional*):\n                The number of frames a single segment is made of. If not defined, `num_segment_frames` defaults to the model's stride\n                times the maximum input length.\n            attention_mask (`torch.Tensor`, *optional*):\n                `attention_mask` needs to be passed when doing long-form transcription using a batch size > 1.\n            time_precision (`int`, *optional*, defaults to 0.02):\n                The duration of output token in seconds. *E.g.* 0.02 means that a generated token on average accounts\n                for 20 ms.\n            return_token_timestamps (`bool`, *optional*):\n                Whether to return token-level timestamps with the text. This can be used with or without the\n                `return_timestamps` option. To get word-level timestamps, use the tokenizer to group the tokens into\n                words.\n            return_segments (`bool`, *optional*, defaults to `False`):\n                Whether to additionally return a list of all segments. Note that this option can only be enabled\n                when doing long-form transcription.\n            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n                Whether or not to return a [`~utils.ModelOutput`] instead of just returning the generated tokens.\n                Note that when doing long-form transcription, `return_dict_in_generate` can only be enabled when\n                `return_segments` is set True. In this case the generation outputs of each segment is added to each\n                segment.\n            kwargs (`Dict[str, Any]`, *optional*):\n                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n\n        Return:\n            [`~utils.ModelOutput`] or `torch.LongTensor` or `Dict[str, Any]`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor` or a dict of segments when `return_segments=True`.\n\n                If the passed input is > 30 seconds / > 3000 mel input features and `return_segments=True` then a dictionary of generated sequence ids, called `sequences` and a list of each generated segment is returned.\n\n                else if the passed input is <= 30 seconds / >= 3000 mel input features, the possible [`~utils.ModelOutput`] types are:\n\n                    - [`~generation.GenerateEncoderDecoderOutput`],\n                    - [`~generation.GenerateBeamEncoderDecoderOutput`]\n\n                else only the generated output sequence ids are returned.\n\n        Example:\n\n        - *Longform transcription*: To transcribe or translate audios longer than 30 seconds, process the audio files without truncation and pass all mel features at once to generate.\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n        >>> from datasets import load_dataset, Audio\n\n        >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n        >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n        >>> model.cuda()\n\n        >>> # load audios > 30 seconds\n        >>> ds = load_dataset(\"distil-whisper/meanwhile\", \"default\")[\"test\"]\n        >>> # resample to 16kHz\n        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n        >>> # take first 8 audios and retrieve array\n        >>> audio = ds[:8][\"audio\"]\n        >>> audio = [x[\"array\"] for x in audio]\n\n        >>> # make sure to NOT truncate the input audio, to return the `attention_mask` and to pad to the longest audio\n        >>> inputs = processor(audio, return_tensors=\"pt\", truncation=False, padding=\"longest\", return_attention_mask=True, sampling_rate=16_000)\n        >>> inputs = inputs.to(\"cuda\", torch.float32)\n\n        >>> # transcribe audio to ids\n        >>> generated_ids = model.generate(**inputs)\n\n        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n        >>> transcription[0]\n        ' Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on a stained kid's place mat from a defunct dennies. set up a table inside a rusty cargo container down by the Wharf and challenged toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile!'\n        ```\n\n        - *Shortform transcription*: If passed mel input features are < 30 seconds, the whole audio will be transcribed with a single call to generate.\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n        >>> from datasets import load_dataset\n\n        >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n        >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n        >>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n        >>> input_features = inputs.input_features\n\n        >>> generated_ids = model.generate(inputs=input_features)\n\n        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        >>> transcription\n        ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n        ```", "file": "src_transformers_models_whisper_generation_whisper.py", "function": "WhisperGenerationMixin.generate", "project": "transformers", "change_stats": {"code_changes": 79, "doc_changes": 5, "total_changes": 84}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef subscribe_pipeline(callback: Callable[[Pipeline, Job], None], pipeline: Optional[Pipeline] = None):\n    \n    \n    return _PipelineManagerFactory._build_manager()._subscribe(callback, pipeline)```\n\nUpdated code:\n```python\ndef subscribe_pipeline(\n    callback: Callable[[Pipeline, Job], None], params: Optional[List[str]] = None, pipeline: Optional[Pipeline] = None\n):\n    \n    \n    return _PipelineManagerFactory._build_manager()._subscribe(callback, params, pipeline)\n```\n\nOriginal documentation:\n```python\nSubscribe a function to be called on job status change.\n\n    The subscription is applied to all jobs created for the execution of _pipeline_.\n\n    Parameters:\n        callback (Callable[[Pipeline^, Job^], None]): The callable function to be called on\n            status change.\n        pipeline (Optional[Pipeline^]): The pipeline to subscribe on. If None, the subscription\n            is actived for all pipelines.\n    Note:\n        Notifications are applied only for jobs created **after** this subscription.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Subscribe a function to be called on job status change.\n\n    The subscription is applied to all jobs created for the execution of _pipeline_.\n\n    Parameters:\n        callback (Callable[[Pipeline^, Job^], None]): The callable function to be called on\n            status change.\n        params (Optional[List[str]]): The parameters to be passed to the _callback_.\n        pipeline (Optional[Pipeline^]): The pipeline to subscribe on. If None, the subscription\n            is actived for all pipelines.\n    Note:\n        Notifications are applied only for jobs created **after** this subscription.", "file": "taipy_core_taipy.py", "function": "subscribe_pipeline", "project": "taipy", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef nginx_proxy_dns_resolver(domain_name):\n    \n    \n    \n    log = logging.getLogger('DNS')\n    log.debug(\"nginx_proxy_dns_resolver(%r)\" % domain_name)\n    if 'nginx-proxy' in domain_name:\n        nginxproxy_containers = docker_client.containers.list(filters={\"status\": \"running\", \"ancestor\": \"jwilder/nginx-proxy:test\"})\n        if len(nginxproxy_containers) == 0:\n            log.warn(\"no container found from image jwilder/nginx-proxy:test while resolving %r\", domain_name)\n            return\n        nginxproxy_container = nginxproxy_containers[0]\n        ip = container_ip(nginxproxy_container)\n        log.info(\"resolving domain name %r as IP address %s of nginx-proxy container %s\" % (domain_name, ip, nginxproxy_container.name))\n        return ip```\n\nUpdated code:\n```python\ndef nginx_proxy_dns_resolver(domain_name):\n    \n    \n    \n    log = logging.getLogger('DNS')\n    log.debug(\"nginx_proxy_dns_resolver(%r)\" % domain_name)\n    if 'nginx-proxy' in domain_name:\n        nginxproxy_containers = docker_client.containers.list(filters={\"status\": \"running\", \"ancestor\": \"nginxproxy/nginx-proxy:test\"})\n        if len(nginxproxy_containers) == 0:\n            log.warn(\"no container found from image nginxproxy/nginx-proxy:test while resolving %r\", domain_name)\n            return\n        nginxproxy_container = nginxproxy_containers[0]\n        ip = container_ip(nginxproxy_container)\n        log.info(\"resolving domain name %r as IP address %s of nginx-proxy container %s\" % (domain_name, ip, nginxproxy_container.name))\n        return ip\n```\n\nOriginal documentation:\n```python\nif \"nginx-proxy\" if found in host, return the ip address of the docker container\n    issued from the docker image jwilder/nginx-proxy:test.\n\n    :return: IP or None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "if \"nginx-proxy\" if found in host, return the ip address of the docker container\n    issued from the docker image nginxproxy/nginx-proxy:test.\n\n    :return: IP or None", "file": "test_conftest.py", "function": "nginx_proxy_dns_resolver", "project": "nginx-proxy", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_timestamp(self, freq='D', how='start', copy=True):\n        \n        \n        \n        new_values = self.values\n        if copy:\n            new_values = new_values.copy()\n\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return Series(new_values, index=new_index, name=self.name)```\n\nUpdated code:\n```python\ndef to_timestamp(self, freq=None, how='start', copy=True):\n        \n        \n        \n        new_values = self.values\n        if copy:\n            new_values = new_values.copy()\n\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return Series(new_values, index=new_index, name=self.name)\n```\n\nOriginal documentation:\n```python\nCast to datetimeindex of timestamps, at *beginning* of period\n\n        Parameters\n        ----------\n        freq : string, default 'D'\n            Desired frequency\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end\n\n        Returns\n        -------\n        ts : TimeSeries with DatetimeIndex\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Cast to datetimeindex of timestamps, at *beginning* of period\n\n        Parameters\n        ----------\n        freq : string, default frequency of PeriodIndex\n            Desired frequency\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end\n\n        Returns\n        -------\n        ts : TimeSeries with DatetimeIndex", "file": "pandas_core_series.py", "function": "TimeSeries.to_timestamp", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _set_callbacks(self):\n         \n        \n        self.vars[\"refresh_graph\"].trace(\"w\", self._update_current_session)\n        self.vars[\"is_training\"].trace(\"w\", self._remove_current_session)\n        self.vars[\"analysis_folder\"].trace(\"w\", self._populate_from_folder)```\n\nUpdated code:\n```python\ndef _set_callbacks(self):\n         \n        \n        self.vars[\"refresh_graph\"].trace(\"w\", self._update_current_session)\n        self.vars[\"analysis_folder\"].trace(\"w\", self._populate_from_folder)\n```\n\nOriginal documentation:\n```python\nAdds callbacks to update the analysis summary statistics and add them to :attr:`vars`\n\n        Training graph refresh - Updates the stats for the current training session when the graph\n        has been updated.\n\n        When training is commenced - Removes the currently displayed session.\n\n        When the analysis folder has been populated - Updates the stats from that folder.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adds callbacks to update the analysis summary statistics and add them to :attr:`vars`\n\n        Training graph refresh - Updates the stats for the current training session when the graph\n        has been updated.\n\n        When the analysis folder has been populated - Updates the stats from that folder.", "file": "lib_gui_display_analysis.py", "function": "Analysis._set_callbacks", "project": "faceswap", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _gen_pair_whitelist(self, base_currency: str, key: str = 'BaseVolume') -> List[str]:\n        \n        \n        \n        summaries = sorted(\n            (s for s in exchange.get_market_summaries() if\n             s['MarketName'].startswith(base_currency)),\n            key=lambda s: s.get(key) or 0.0,\n            reverse=True\n        )\n\n        return [s['MarketName'].replace('-', '_') for s in summaries]```\n\nUpdated code:\n```python\ndef _gen_pair_whitelist(self, base_currency: str, key: str = 'quoteVolume') -> List[str]:\n        \n        \n        \n\n        if not exchange.exchange_has('fetchTickers'):\n            raise OperationalException(\n                'Exchange does not support dynamic whitelist.'\n                'Please edit your config and restart the bot'\n            )\n\n        tickers = exchange.get_tickers()\n        # check length so that we make sure that '/' is actually in the string\n        tickers = [v for k, v in tickers.items()\n                   if len(k.split('/')) == 2 and k.split('/')[1] == base_currency]\n\n        sorted_tickers = sorted(tickers, reverse=True, key=lambda t: t[key])\n        pairs = [s['symbol'] for s in sorted_tickers]\n        return pairs\n```\n\nOriginal documentation:\n```python\nUpdates the whitelist with with a dynamically generated list\n        :param base_currency: base currency as str\n        :param key: sort key (defaults to 'BaseVolume')\n        :return: List of pairs\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Updates the whitelist with with a dynamically generated list\n        :param base_currency: base currency as str\n        :param key: sort key (defaults to 'quoteVolume')\n        :return: List of pairs", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot._gen_pair_whitelist", "project": "freqtrade", "change_stats": {"code_changes": 25, "doc_changes": 4, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_gpu(self, device=None):\n        # type: (tp.Optional[types.CudaDeviceSpec]) -> 'Link'\n        \n\n        \n        cuda.check_cuda_available()\n        return self._to_device(\n            cuda._get_device_or_current(device),\n            skip_between_cupy_devices=True)```\n\nUpdated code:\n```python\ndef to_gpu(self, device=None, sticky=None):\n        # type: (tp.Optional[types.CudaDeviceSpec], tp.Optional[bool]) -> 'Link' # NOQA\n        \n\n        \n        cuda.check_cuda_available()\n        return self._to_device(\n            cuda._get_device_or_current(device),\n            skip_between_cupy_devices=sticky)\n```\n\nOriginal documentation:\n```python\nCopies parameter variables and persistent values to GPU.\n\n        This method does not handle non-registered attributes. If some of such\n        attributes must be copied to GPU, the link implementation must\n        override :meth:`Link.to_device` to do so.\n\n        Args:\n            device: Target device specifier. If omitted, the current device is\n                used.\n\n        Returns: self\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Copies parameter variables and persistent values to GPU.\n\n        This method does not handle non-registered attributes. If some of such\n        attributes must be copied to GPU, the link implementation must\n        override :meth:`Link.to_device` to do so.\n\n        Args:\n            device: Target device specifier. If omitted, the current device is\n                used.\n            sticky (bool): If ``False``, the link will be transferred to the\n                specified GPU device even if the Link is already on another\n                GPU. If ``True``, ``to_gpu`` does nothing if the Link is\n                already on GPU. If omitted, ``True`` is used as the default\n                in Chainer v5. Note that the default is planned to be changed\n                to ``False`` in the future release (possibly in Chainer v6).\n\n        Returns: self", "file": "chainer_link.py", "function": "Link.to_gpu", "project": "chainer", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef profile():\n    \n\n    \n    profiler.start()\n    try:\n        yield\n    finally:\n        profiler.stop()```\n\nUpdated code:\n```python\ndef profile():\n    \n    \n    warnings.warn(\n        'cupy.cuda.profile has been deprecated since CuPy v10 '\n        'and will be removed in the future. Use cupyx.profiler.profile '\n        'instead.')\n\n    profiler.start()\n    try:\n        yield\n    finally:\n        profiler.stop()\n```\n\nOriginal documentation:\n```python\nEnable CUDA profiling during with statement.\n\n    This function enables profiling on entering a with statement, and disables\n    profiling on leaving the statement.\n\n    >>> with cupy.cuda.profile():\n    ...    # do something you want to measure\n    ...    pass\n\n    .. note::\n        When starting ``nvprof`` from the command line, manually setting\n        ``--profile-from-start off`` may be required for the desired behavior.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Enable CUDA profiling during with statement.\n\n    This function enables profiling on entering a with statement, and disables\n    profiling on leaving the statement.\n\n    >>> with cupy.cuda.profile():\n    ...    # do something you want to measure\n    ...    pass\n\n    .. note::\n        When starting ``nvprof`` from the command line, manually setting\n        ``--profile-from-start off`` may be required for the desired behavior.\n\n    .. warning:: This context manager is deprecated. Please use\n        :class:`cupyx.profiler.profile` instead.", "file": "cupy_cuda___init__.py", "function": "profile", "project": "cupy", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef read(self, where=None):\n          \n\n        self.read_axes(where)\n\n        index  = self.index_axes[0].values\n        column = self.index_axes[1].values\n\n        major = Factor.from_array(index)\n        minor = Factor.from_array(column)\n\n        J, K = len(major.levels), len(minor.levels)\n        key = major.labels * K + minor.labels\n\n        panels = []\n        if len(unique(key)) == len(key):\n            sorter, _ = algos.groupsort_indexer(com._ensure_int64(key), J * K)\n            sorter = com._ensure_platform_int(sorter)\n\n            # create the panels\n            for c in self.values_axes:\n\n                # the data need to be sorted\n                sorted_values = c.data.take(sorter, axis=0)\n                major_labels = major.labels.take(sorter)\n                minor_labels = minor.labels.take(sorter)\n                items        = Index(c.values)\n\n                block = block2d_to_block3d(sorted_values, items, (J, K),\n                                           major_labels, minor_labels)\n\n                mgr = BlockManager([block], [items, major.levels, minor.levels])\n                panels.append(Panel(mgr))\n\n        else:\n            if not self._quiet:  # pragma: no cover\n                print ('Duplicate entries in table, taking most recently '\n                       'appended')\n\n            # reconstruct\n            long_index = MultiIndex.from_arrays([index, column])\n\n            panels = []\n            for c in self.values_axes:\n                lp = DataFrame(c.data, index=long_index, columns=c.values)\n\n                # need a better algorithm\n                tuple_index = long_index._tuple_index\n\n                unique_tuples = lib.fast_unique(tuple_index)\n                unique_tuples = _asarray_tuplesafe(unique_tuples)\n\n                indexer = match(unique_tuples, tuple_index)\n                indexer = com._ensure_platform_int(indexer)\n\n                new_index = long_index.take(indexer)\n                new_values = lp.values.take(indexer, axis=0)\n\n                lp = DataFrame(new_values, index=new_index, columns=lp.columns)\n                panels.append(lp.to_panel())\n\n        # append the panels\n        wp = concat(panels, axis = 0, verify_integrity = True)\n\n        # reorder by any non_index_axes\n        for axis,labels in self.non_index_axes:\n            wp = wp.reindex_axis(labels,axis=axis,copy=False)\n\n        if self.selection.filter:\n            new_minor = sorted(set(wp.minor_axis) & self.selection.filter)\n            wp = wp.reindex(minor=new_minor, copy = False)\n\n        return wp```\n\nUpdated code:\n```python\ndef read(self, where=None):\n          \n\n        self.read_axes(where)\n\n        indicies = [ i.values for i in self.index_axes ]\n        factors  = [ Factor.from_array(i) for i in indicies ]\n        levels   = [ f.levels for f in factors ]\n        N        = [ len(f.levels) for f in factors ]\n        labels   = [ f.labels for f in factors ]\n\n        # compute the key\n        key      = factor_indexer(N[1:], labels)\n\n        objs = []\n        if len(unique(key)) == len(key):\n\n            sorter, _ = algos.groupsort_indexer(com._ensure_int64(key), np.prod(N))\n            sorter = com._ensure_platform_int(sorter)\n\n            # create the objs\n            for c in self.values_axes:\n\n                # the data need to be sorted\n                sorted_values = c.data.take(sorter, axis=0)\n                \n                take_labels   = [ l.take(sorter) for l in labels ]\n                items         = Index(c.values)\n\n                block = block2d_to_blocknd(sorted_values, items, tuple(N), take_labels)\n                mgr = BlockManager([block], [items] + levels)\n                objs.append(self.obj_type(mgr))\n\n        else:\n            if not self._quiet:  # pragma: no cover\n                print ('Duplicate entries in table, taking most recently '\n                       'appended')\n\n            # reconstruct\n            long_index = MultiIndex.from_arrays(indicies)\n\n            for c in self.values_axes:\n                lp = DataFrame(c.data, index=long_index, columns=c.values)\n\n                # need a better algorithm\n                tuple_index = long_index._tuple_index\n\n                unique_tuples = lib.fast_unique(tuple_index)\n                unique_tuples = _asarray_tuplesafe(unique_tuples)\n\n                indexer = match(unique_tuples, tuple_index)\n                indexer = com._ensure_platform_int(indexer)\n\n                new_index = long_index.take(indexer)\n                new_values = lp.values.take(indexer, axis=0)\n\n                lp = DataFrame(new_values, index=new_index, columns=lp.columns)\n                objs.append(lp.to_panel())\n\n        # create the composite object\n        wp = concat(objs, axis = 0, verify_integrity = True)\n\n        # reorder by any non_index_axes\n        for axis,labels in self.non_index_axes:\n            wp = wp.reindex_axis(labels,axis=axis,copy=False)\n\n        if self.selection.filter:\n            new_minor = sorted(set(wp.minor_axis) & self.selection.filter)\n            wp = wp.reindex(minor=new_minor, copy = False)\n\n        return wp\n```\n\nOriginal documentation:\n```python\nwe have 2 indexable columns, with an arbitrary number of data axes\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "we have n indexable columns, with an arbitrary number of data axes", "file": "pandas_io_pytables.py", "function": "LegacyTable.read", "project": "pandas", "change_stats": {"code_changes": 46, "doc_changes": 4, "total_changes": 50}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _validate_parse_dates_presence(self, columns: Sequence[Hashable]) -> Iterable:\n        \n        \n\n        \n        cols_needed: Iterable\n        if is_dict_like(self.parse_dates):\n            cols_needed = itertools.chain(*self.parse_dates.values())\n        elif is_list_like(self.parse_dates):\n            # a column in parse_dates could be represented\n            # ColReference = Union[int, str]\n            # DateGroups = List[ColReference]\n            # ParseDates = Union[DateGroups, List[DateGroups],\n            #     Dict[ColReference, DateGroups]]\n            cols_needed = itertools.chain.from_iterable(\n                col if is_list_like(col) and not isinstance(col, tuple) else [col]\n                for col in self.parse_dates\n            )\n        else:\n            cols_needed = []\n\n        cols_needed = list(cols_needed)\n\n        # get only columns that are references using names (str), not by index\n        missing_cols = \", \".join(\n            sorted(\n                {\n                    col\n                    for col in cols_needed\n                    if isinstance(col, str) and col not in columns\n                }\n            )\n        )\n        if missing_cols:\n            raise ValueError(\n                f\"Missing column provided to 'parse_dates': '{missing_cols}'\"\n            )\n        # Convert positions to actual column names\n        return [\n            col if (isinstance(col, str) or col in columns) else columns[col]\n            for col in cols_needed\n        ]```\n\nUpdated code:\n```python\ndef _validate_parse_dates_presence(self, columns: Sequence[Hashable]) -> set:\n        \n        \n\n        \n        if not isinstance(self.parse_dates, list):\n            return set()\n\n        # get only columns that are references using names (str), not by index\n        missing_cols = \", \".join(\n            sorted(\n                {\n                    col\n                    for col in self.parse_dates\n                    if isinstance(col, str) and col not in columns\n                }\n            )\n        )\n        if missing_cols:\n            raise ValueError(\n                f\"Missing column provided to 'parse_dates': '{missing_cols}'\"\n            )\n        # Convert positions to actual column names\n        return {\n            col if (isinstance(col, str) or col in columns) else columns[col]\n            for col in self.parse_dates\n        }\n```\n\nOriginal documentation:\n```python\nCheck if parse_dates are in columns.\n\n        If user has provided names for parse_dates, check if those columns\n        are available.\n\n        Parameters\n        ----------\n        columns : list\n            List of names of the dataframe.\n\n        Returns\n        -------\n        The names of the columns which will get parsed later if a dict or list\n        is given as specification.\n\n        Raises\n        ------\n        ValueError\n            If column to parse_date is not in dataframe.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if parse_dates are in columns.\n\n        If user has provided names for parse_dates, check if those columns\n        are available.\n\n        Parameters\n        ----------\n        columns : list\n            List of names of the dataframe.\n\n        Returns\n        -------\n        The names of the columns which will get parsed later if a list\n        is given as specification.\n\n        Raises\n        ------\n        ValueError\n            If column to parse_date is not in dataframe.", "file": "pandas_io_parsers_base_parser.py", "function": "ParserBase._validate_parse_dates_presence", "project": "pandas", "change_stats": {"code_changes": 31, "doc_changes": 4, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_sql(\n    frame,\n    name: str,\n    con,\n    schema: str | None = None,\n    if_exists: str = \"fail\",\n    index: bool = True,\n    index_label=None,\n    chunksize: int | None = None,\n    dtype: DtypeArg | None = None,\n    method: str | None = None,\n    engine: str = \"auto\",\n    **engine_kwargs,\n) -> None:\n    \n    \n    \n    if if_exists not in (\"fail\", \"replace\", \"append\"):\n        raise ValueError(f\"'{if_exists}' is not valid for if_exists\")\n\n    pandas_sql = pandasSQL_builder(con, schema=schema)\n\n    if isinstance(frame, Series):\n        frame = frame.to_frame()\n    elif not isinstance(frame, DataFrame):\n        raise NotImplementedError(\n            \"'frame' argument should be either a Series or a DataFrame\"\n        )\n\n    pandas_sql.to_sql(\n        frame,\n        name,\n        if_exists=if_exists,\n        index=index,\n        index_label=index_label,\n        schema=schema,\n        chunksize=chunksize,\n        dtype=dtype,\n        method=method,\n        engine=engine,\n        **engine_kwargs,\n    )```\n\nUpdated code:\n```python\ndef to_sql(\n    frame,\n    name: str,\n    con,\n    schema: str | None = None,\n    if_exists: str = \"fail\",\n    index: bool = True,\n    index_label=None,\n    chunksize: int | None = None,\n    dtype: DtypeArg | None = None,\n    method: str | None = None,\n    engine: str = \"auto\",\n    **engine_kwargs,\n) -> int | None:\n    \n    \n      # noqa:E501\n    if if_exists not in (\"fail\", \"replace\", \"append\"):\n        raise ValueError(f\"'{if_exists}' is not valid for if_exists\")\n\n    pandas_sql = pandasSQL_builder(con, schema=schema)\n\n    if isinstance(frame, Series):\n        frame = frame.to_frame()\n    elif not isinstance(frame, DataFrame):\n        raise NotImplementedError(\n            \"'frame' argument should be either a Series or a DataFrame\"\n        )\n\n    return pandas_sql.to_sql(\n        frame,\n        name,\n        if_exists=if_exists,\n        index=index,\n        index_label=index_label,\n        schema=schema,\n        chunksize=chunksize,\n        dtype=dtype,\n        method=method,\n        engine=engine,\n        **engine_kwargs,\n    )\n```\n\nOriginal documentation:\n```python\nWrite records stored in a DataFrame to a SQL database.\n\n    Parameters\n    ----------\n    frame : DataFrame, Series\n    name : str\n        Name of SQL table.\n    con : SQLAlchemy connectable(engine/connection) or database string URI\n        or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    schema : str, optional\n        Name of SQL schema in database to write to (if database flavor\n        supports this). If None, use default schema (default).\n    if_exists : {'fail', 'replace', 'append'}, default 'fail'\n        - fail: If table exists, do nothing.\n        - replace: If table exists, drop it, recreate it, and insert data.\n        - append: If table exists, insert data. Create if does not exist.\n    index : bool, default True\n        Write DataFrame index as a column.\n    index_label : str or sequence, optional\n        Column label for index column(s). If None is given (default) and\n        `index` is True, then the index names are used.\n        A sequence should be given if the DataFrame uses MultiIndex.\n    chunksize : int, optional\n        Specify the number of rows in each batch to be written at a time.\n        By default, all rows will be written at once.\n    dtype : dict or scalar, optional\n        Specifying the datatype for columns. If a dictionary is used, the\n        keys should be the column names and the values should be the\n        SQLAlchemy types or strings for the sqlite3 fallback mode. If a\n        scalar is provided, it will be applied to all columns.\n    method : {None, 'multi', callable}, optional\n        Controls the SQL insertion clause used:\n\n        - None : Uses standard SQL ``INSERT`` clause (one per row).\n        - 'multi': Pass multiple values in a single ``INSERT`` clause.\n        - callable with signature ``(pd_table, conn, keys, data_iter)``.\n\n        Details and a sample callable implementation can be found in the\n        section :ref:`insert method <io.sql.method>`.\n    engine : {'auto', 'sqlalchemy'}, default 'auto'\n        SQL engine library to use. If 'auto', then the option\n        ``io.sql.engine`` is used. The default ``io.sql.engine``\n        behavior is 'sqlalchemy'\n\n        .. versionadded:: 1.3.0\n\n    **engine_kwargs\n        Any additional kwargs are passed to the engine.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write records stored in a DataFrame to a SQL database.\n\n    Parameters\n    ----------\n    frame : DataFrame, Series\n    name : str\n        Name of SQL table.\n    con : SQLAlchemy connectable(engine/connection) or database string URI\n        or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    schema : str, optional\n        Name of SQL schema in database to write to (if database flavor\n        supports this). If None, use default schema (default).\n    if_exists : {'fail', 'replace', 'append'}, default 'fail'\n        - fail: If table exists, do nothing.\n        - replace: If table exists, drop it, recreate it, and insert data.\n        - append: If table exists, insert data. Create if does not exist.\n    index : bool, default True\n        Write DataFrame index as a column.\n    index_label : str or sequence, optional\n        Column label for index column(s). If None is given (default) and\n        `index` is True, then the index names are used.\n        A sequence should be given if the DataFrame uses MultiIndex.\n    chunksize : int, optional\n        Specify the number of rows in each batch to be written at a time.\n        By default, all rows will be written at once.\n    dtype : dict or scalar, optional\n        Specifying the datatype for columns. If a dictionary is used, the\n        keys should be the column names and the values should be the\n        SQLAlchemy types or strings for the sqlite3 fallback mode. If a\n        scalar is provided, it will be applied to all columns.\n    method : {None, 'multi', callable}, optional\n        Controls the SQL insertion clause used:\n\n        - None : Uses standard SQL ``INSERT`` clause (one per row).\n        - ``'multi'``: Pass multiple values in a single ``INSERT`` clause.\n        - callable with signature ``(pd_table, conn, keys, data_iter) -> int | None``.\n\n        Details and a sample callable implementation can be found in the\n        section :ref:`insert method <io.sql.method>`.\n    engine : {'auto', 'sqlalchemy'}, default 'auto'\n        SQL engine library to use. If 'auto', then the option\n        ``io.sql.engine`` is used. The default ``io.sql.engine``\n        behavior is 'sqlalchemy'\n\n        .. versionadded:: 1.3.0\n\n    **engine_kwargs\n        Any additional kwargs are passed to the engine.\n\n    Returns\n    -------\n    None or int\n        Number of rows affected by to_sql. None is returned if the callable\n        passed into ``method`` does not return the number of rows.\n\n        .. versionadded:: 1.4.0\n\n    Notes\n    -----\n    The returned rows affected is the sum of the ``rowcount`` attribute of ``sqlite3.Cursor``\n    or SQLAlchemy connectable. The returned value may not reflect the exact number of written\n    rows as stipulated in the\n    `sqlite3 <https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount>`__ or\n    `SQLAlchemy <https://docs.sqlalchemy.org/en/14/core/connections.html#sqlalchemy.engine.BaseCursorResult.rowcount>`__", "file": "pandas_io_sql.py", "function": "to_sql", "project": "pandas", "change_stats": {"code_changes": 8, "doc_changes": 22, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _mode_deprecations(mode):\n     \n    if mode.lower() == 'nearest':\n        warnings.warn(skimage_deprecation(\n            \"Mode 'nearest' has been renamed 'edge'. Mode 'nearest' will be \"\n            \"removed in a future release.\"))\n        mode = 'edge'\n    return mode```\n\nUpdated code:\n```python\ndef _mode_deprecations(mode):\n    \n    if mode.lower() == 'nearest':\n        warnings.warn(skimage_deprecation(\n            \"Mode 'nearest' has been renamed to 'edge'. Mode 'nearest' will be \"\n            \"removed in a future release.\"))\n        mode = 'edge'\n    return mode\n```\n\nOriginal documentation:\n```python\nto be used by functions to update deprecated mode names in\n    `skimage._shared.interpolation.pyx`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Used to update deprecated mode names in\n    `skimage._shared.interpolation.pyx`.", "file": "skimage__shared_utils.py", "function": "_mode_deprecations", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef insert(self, query: ast.Select):\n        \n        \n\n        if self.handler.connection_data.get(\"api_key\", None) is None:\n            raise ValueError(\n                \"Need an authenticated connection in order to insert a GitHub issue\"\n            )\n\n        self.handler.connect()\n\n        columns = [col.name for col in query.columns]\n\n        supported_columns = {\"title\", \"body\", \"assignees\", \"milestone\", \"labels\"}\n\n        if not set(columns).issubset(supported_columns):\n            unsupported_columns = set(columns).difference(supported_columns)\n            raise ValueError(\n                \"Unsupported columns for GitHub issue insert: \"\n                + \", \".join(unsupported_columns)\n            )\n\n        for a_row in query.values:\n            insert_kwargs = {}\n            a_value = dict(zip(columns, a_row))\n\n            if a_value.get(\"title\", None) is None:\n                raise ValueError(\"Title parameter is required to insert a GitHub issue\")\n\n            if a_value.get(\"body\", None):\n                insert_kwargs[\"body\"] = a_value[\"body\"]\n\n            if a_value.get(\"assignees\", None):\n                insert_kwargs[\"assignees\"] = []\n                for an_assignee in a_value[\"assignees\"].split(\",\"):\n                    an_assignee = an_assignee.replace(\" \", \"\")\n                    try:\n                        github_user = self.handler.connection.get_user(an_assignee)\n                    except Exception as e:\n                        raise ValueError(\n                            f'Encountered an exception looking up assignee \"{an_assignee}\" in GitHub: '\n                            f\"{type(e).__name__} - {e}\"\n                        )\n\n                    insert_kwargs[\"assignees\"].append(github_user)\n\n            if a_value.get(\"milestone\", None):\n                current_milestones = self.handler.connection.get_repo(\n                    self.handler.repository\n                ).get_milestones()\n\n                found_existing_milestone = False\n                for a_milestone in current_milestones:\n                    if a_milestone.title == a_value[\"milestone\"]:\n                        insert_kwargs[\"milestone\"] = a_milestone\n                        found_existing_milestone = True\n                        break\n\n                if not found_existing_milestone:\n                    logger.debug(\n                        f\"Milestone \\\"{a_value['milestone']}\\\" not found, creating it\"\n                    )\n                    insert_kwargs[\"milestone\"] = self.handler.connection.get_repo(\n                        self.handler.repository\n                    ).create_milestone(a_value[\"milestone\"])\n                else:\n                    logger.debug(f\"Milestone \\\"{a_value['milestone']}\\\" already exists\")\n\n            if a_value.get(\"labels\", None):\n                insert_kwargs[\"labels\"] = []\n\n                inserted_labels = []\n                for a_label in a_value[\"labels\"].split(\",\"):\n                    a_label = a_label.replace(\" \", \"\")\n                    inserted_labels.append(a_label)\n\n                existing_labels = self.handler.connection.get_repo(\n                    self.handler.repository\n                ).get_labels()\n\n                existing_labels_set = set([label.name for label in existing_labels])\n\n                if not set(inserted_labels).issubset(existing_labels_set):\n                    new_inserted_labels = set(inserted_labels).difference(\n                        existing_labels_set\n                    )\n                    logger.debug(\n                        \"Inserting new labels: \" + \", \".join(new_inserted_labels)\n                    )\n                    for a_new_label in new_inserted_labels:\n                        self.handler.connection.get_repo(\n                            self.handler.repository\n                        ).create_label(a_new_label, \"000000\")\n\n                for a_label in existing_labels:\n                    if a_label.name in inserted_labels:\n                        insert_kwargs[\"labels\"].append(a_label)\n\n            try:\n                self.handler.connection.get_repo(self.handler.repository).create_issue(\n                    a_value[\"title\"], **insert_kwargs\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Encountered an exception creating an issue in GitHub: \"\n                    f\"{type(e).__name__} - {e}\"\n                )```\n\nUpdated code:\n```python\ndef insert(self, query: ast.Insert):\n        \n        \n\n        if self.handler.connection_data.get(\"api_key\", None) is None:\n            raise ValueError(\n                \"Need an authenticated connection in order to insert a GitHub issue\"\n            )\n\n        self.handler.connect()\n\n        columns = [col.name for col in query.columns]\n\n        supported_columns = {\"title\", \"body\", \"assignees\", \"milestone\", \"labels\"}\n\n        if not set(columns).issubset(supported_columns):\n            unsupported_columns = set(columns).difference(supported_columns)\n            raise ValueError(\n                \"Unsupported columns for GitHub issue insert: \"\n                + \", \".join(unsupported_columns)\n            )\n\n        for a_row in query.values:\n            insert_kwargs = {}\n            a_value = dict(zip(columns, a_row))\n\n            if a_value.get(\"title\", None) is None:\n                raise ValueError(\"Title parameter is required to insert a GitHub issue\")\n\n            if a_value.get(\"body\", None):\n                insert_kwargs[\"body\"] = a_value[\"body\"]\n\n            if a_value.get(\"assignees\", None):\n                insert_kwargs[\"assignees\"] = []\n                for an_assignee in a_value[\"assignees\"].split(\",\"):\n                    an_assignee = an_assignee.replace(\" \", \"\")\n                    try:\n                        github_user = self.handler.connection.get_user(an_assignee)\n                    except Exception as e:\n                        raise ValueError(\n                            f'Encountered an exception looking up assignee \"{an_assignee}\" in GitHub: '\n                            f\"{type(e).__name__} - {e}\"\n                        )\n\n                    insert_kwargs[\"assignees\"].append(github_user)\n\n            if a_value.get(\"milestone\", None):\n                current_milestones = self.handler.connection.get_repo(\n                    self.handler.repository\n                ).get_milestones()\n\n                found_existing_milestone = False\n                for a_milestone in current_milestones:\n                    if a_milestone.title == a_value[\"milestone\"]:\n                        insert_kwargs[\"milestone\"] = a_milestone\n                        found_existing_milestone = True\n                        break\n\n                if not found_existing_milestone:\n                    logger.debug(\n                        f\"Milestone \\\"{a_value['milestone']}\\\" not found, creating it\"\n                    )\n                    insert_kwargs[\"milestone\"] = self.handler.connection.get_repo(\n                        self.handler.repository\n                    ).create_milestone(a_value[\"milestone\"])\n                else:\n                    logger.debug(f\"Milestone \\\"{a_value['milestone']}\\\" already exists\")\n\n            if a_value.get(\"labels\", None):\n                insert_kwargs[\"labels\"] = []\n\n                inserted_labels = []\n                for a_label in a_value[\"labels\"].split(\",\"):\n                    a_label = a_label.replace(\" \", \"\")\n                    inserted_labels.append(a_label)\n\n                existing_labels = self.handler.connection.get_repo(\n                    self.handler.repository\n                ).get_labels()\n\n                existing_labels_set = set([label.name for label in existing_labels])\n\n                if not set(inserted_labels).issubset(existing_labels_set):\n                    new_inserted_labels = set(inserted_labels).difference(\n                        existing_labels_set\n                    )\n                    logger.debug(\n                        \"Inserting new labels: \" + \", \".join(new_inserted_labels)\n                    )\n                    for a_new_label in new_inserted_labels:\n                        self.handler.connection.get_repo(\n                            self.handler.repository\n                        ).create_label(a_new_label, \"000000\")\n\n                for a_label in existing_labels:\n                    if a_label.name in inserted_labels:\n                        insert_kwargs[\"labels\"].append(a_label)\n\n            try:\n                self.handler.connection.get_repo(self.handler.repository).create_issue(\n                    a_value[\"title\"], **insert_kwargs\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Encountered an exception creating an issue in GitHub: \"\n                    f\"{type(e).__name__} - {e}\"\n                )\n```\n\nOriginal documentation:\n```python\nInserts data into the GitHub \"Create an issue\" API\n\n        Parameters\n        ----------\n        query : ast.Select\n           Given SQL SELECT query\n\n        Raises\n        ------\n        ValueError\n            If the query contains an unsupported condition\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inserts data into the GitHub \"Create an issue\" API\n\n        Parameters\n        ----------\n        query : ast.Insert\n           Given SQL INSERT query\n\n        Raises\n        ------\n        ValueError\n            If the query contains an unsupported condition", "file": "mindsdb_integrations_handlers_github_handler_github_tables.py", "function": "GithubIssuesTable.insert", "project": "mindsdb", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef count_params(self):\n        \n\n        \n\n        size = 0\n        for name, param in self.namedparams():\n            if param.array is None:\n                warnings.warn(\n                    '{} has not been initialized, so the resulting size will '\n                    'not include the number of parameters in it.'.format(name))\n                continue\n            size += param.size\n        return size```\n\nUpdated code:\n```python\ndef count_params(self):\n        \n\n        \n\n        size = 0\n        for name, param in self.namedparams():\n            if param.array is None:\n                warnings.warn(\n                    'Parameter \\'{}\\' has not been initialized, so the '\n                    'resulting count will not include the number of parameters'\n                    ' in it.'.format(name))\n                continue\n            size += param.size\n        return size\n```\n\nOriginal documentation:\n```python\nCount the total size of parameters.\n\n        If the link containts uninitialized parameters, this raises warnings.\n\n        Returns:\n            The total size of parameters (int)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Counts the total number of parameters.\n\n        This method counts the total number of scalar values included in all\n        the :class:`~chainer.Parameters` held by this link and its descendants.\n\n        If the link containts uninitialized parameters, this method raises a\n        warning.\n\n        Returns:\n            The total size of parameters (int)", "file": "chainer_link.py", "function": "Link.count_params", "project": "chainer", "change_stats": {"code_changes": 7, "doc_changes": 10, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_leverage(self, leverage: Int, symbol: Str = None, params={}):\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' setLeverage() requires a symbol argument')\n        side = self.safe_string_upper(params, 'side')\n        self.check_required_argument('setLeverage', side, 'side', ['LONG', 'SHORT'])\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'symbol': market['id'],\n            'side': side,\n            'leverage': leverage,\n        }\n        #\n        #    {\n        #        \"code\": 0,\n        #        \"msg\": \"\",\n        #        \"data\": {\n        #            \"leverage\": 6,\n        #            \"symbol\": \"BTC-USDT\"\n        #        }\n        #    }\n        #\n        return self.swapV2PrivatePostTradeLeverage(self.extend(request, params))```\n\nUpdated code:\n```python\ndef set_leverage(self, leverage: Int, symbol: Str = None, params={}):\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' setLeverage() requires a symbol argument')\n        side = self.safe_string_upper(params, 'side')\n        self.check_required_argument('setLeverage', side, 'side', ['LONG', 'SHORT', 'BOTH'])\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'symbol': market['id'],\n            'side': side,\n            'leverage': leverage,\n        }\n        #\n        #    {\n        #        \"code\": 0,\n        #        \"msg\": \"\",\n        #        \"data\": {\n        #            \"leverage\": 6,\n        #            \"symbol\": \"BTC-USDT\"\n        #        }\n        #    }\n        #\n        return self.swapV2PrivatePostTradeLeverage(self.extend(request, params))\n```\n\nOriginal documentation:\n```python\nset the level of leverage for a market\n        :see: https://bingx-api.github.io/docs/#/swapV2/trade-api.html#Switch%20Leverage\n        :param float leverage: the rate of leverage\n        :param str symbol: unified market symbol\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :returns dict: response from the exchange\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "set the level of leverage for a market\n        :see: https://bingx-api.github.io/docs/#/swapV2/trade-api.html#Switch%20Leverage\n        :param float leverage: the rate of leverage\n        :param str symbol: unified market symbol\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.side]: hedged: ['long' or 'short']. one way: ['both']\n        :returns dict: response from the exchange", "file": "python_ccxt_bingx.py", "function": "bingx.set_leverage", "project": "ccxt", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef futures_zh_spot(\n    subscribe_list: str = \"nf_IF1912,nf_TF1912,nf_IH1912,nf_IC1912\",\n    market: str = \"CF\",\n    adjust: bool = False,\n) -> pd.DataFrame:\n    \n    \n    \n    url = f\"https://hq.sinajs.cn/rn={round(time.time() * 1000)}&list={subscribe_list}\"\n    res = requests.get(url)\n    data_df = pd.DataFrame(\n        [\n            item.strip().split(\"=\")[1].split(\",\")\n            for item in res.text.split(\";\")\n            if item.strip() != \"\"\n        ]\n    )\n    data_df.iloc[:, 0] = data_df.iloc[:, 0].str.replace('\"', \"\")\n    data_df.iloc[:, -1] = data_df.iloc[:, -1].str.replace('\"', \"\")\n    if adjust:\n        contract_name_list = [item.split(\"_\")[1] for item in subscribe_list.split(\",\")]\n        contract_min_list = []\n        contract_exchange_list = []\n        for contract_name in contract_name_list:\n            # print(contract_name)\n            # contract_name = 'AP2101'\n            temp_df = futures_contract_detail(symbol=contract_name)\n            exchange_name = temp_df[temp_df[\"item\"] == \"\u4e0a\u5e02\u4ea4\u6613\u6240\"][\"value\"].values[0]\n            contract_exchange_list.append(exchange_name)\n            contract_min = temp_df[temp_df[\"item\"] == \"\u6700\u5c0f\u53d8\u52a8\u4ef7\u4f4d\"][\"value\"].values[0]\n            contract_min_list.append(contract_min)\n        if market == \"CF\":\n            data_df.columns = [\n                \"symbol\",\n                \"time\",\n                \"open\",\n                \"high\",\n                \"low\",\n                \"last_close\",\n                \"bid_price\",\n                \"ask_price\",\n                \"current_price\",\n                \"avg_price\",\n                \"last_settle_price\",\n                \"buy_vol\",\n                \"sell_vol\",\n                \"hold\",\n                \"volume\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"bid_price\",\n                    \"ask_price\",\n                    \"buy_vol\",\n                    \"sell_vol\",\n                    \"hold\",\n                    \"volume\",\n                    \"avg_price\",\n                    \"last_close\",\n                    \"last_settle_price\",\n                ]\n            ]\n            data_df[\"exchange\"] = contract_exchange_list\n            data_df[\"contract\"] = contract_name_list\n            data_df[\"contract_min_change\"] = contract_min_list\n            return data_df\n        else:\n            data_df.columns = [\n                \"open\",\n                \"high\",\n                \"low\",\n                \"current_price\",\n                \"volume\",\n                \"amount\",\n                \"hold\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\" \"_\",\n                \"time\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"symbol\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"hold\",\n                    \"volume\",\n                    \"amount\",\n                ]\n            ]\n            data_df[\"exchange\"] = contract_exchange_list\n            data_df[\"contract\"] = contract_name_list\n            data_df[\"contract_min_change\"] = contract_min_list\n            return data_df\n    else:\n        if market == \"CF\":\n            data_df.columns = [\n                \"symbol\",\n                \"time\",\n                \"open\",\n                \"high\",\n                \"low\",\n                \"last_close\",\n                \"bid_price\",\n                \"ask_price\",\n                \"current_price\",\n                \"avg_price\",\n                \"last_settle_price\",\n                \"buy_vol\",\n                \"sell_vol\",\n                \"hold\",\n                \"volume\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"bid_price\",\n                    \"ask_price\",\n                    \"buy_vol\",\n                    \"sell_vol\",\n                    \"hold\",\n                    \"volume\",\n                    \"avg_price\",\n                    \"last_close\",\n                    \"last_settle_price\",\n                ]\n            ]\n            return data_df\n        else:\n            data_df.columns = [\n                \"open\",\n                \"high\",\n                \"low\",\n                \"current_price\",\n                \"volume\",\n                \"amount\",\n                \"hold\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\" \"_\",\n                \"time\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"symbol\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"hold\",\n                    \"volume\",\n                    \"amount\",\n                ]\n            ]\n            return data_df```\n\nUpdated code:\n```python\ndef futures_zh_spot(\n    symbol: str = 'V2205, P2205, B2201, M2205',\n    market: str = \"CF\",\n    adjust: str = '0',\n) -> pd.DataFrame:\n    \n    \n    \n    subscribe_list = ','.join(['nf_' + item.strip() for item in symbol.split(',')])\n    url = f\"https://hq.sinajs.cn/rn={round(time.time() * 1000)}&list={subscribe_list}\"\n    r = requests.get(url)\n    data_df = pd.DataFrame(\n        [\n            item.strip().split(\"=\")[1].split(\",\")\n            for item in r.text.split(\";\")\n            if item.strip() != \"\"\n        ]\n    )\n    data_df.iloc[:, 0] = data_df.iloc[:, 0].str.replace('\"', \"\")\n    data_df.iloc[:, -1] = data_df.iloc[:, -1].str.replace('\"', \"\")\n    if adjust == '1':\n        contract_name_list = [item.split(\"_\")[1] for item in subscribe_list.split(\",\")]\n        contract_min_list = []\n        contract_exchange_list = []\n        for contract_name in contract_name_list:\n            # print(contract_name)\n            # contract_name = 'AP2101'\n            temp_df = futures_contract_detail(symbol=contract_name)\n            exchange_name = temp_df[temp_df[\"item\"] == \"\u4e0a\u5e02\u4ea4\u6613\u6240\"][\"value\"].values[0]\n            contract_exchange_list.append(exchange_name)\n            contract_min = temp_df[temp_df[\"item\"] == \"\u6700\u5c0f\u53d8\u52a8\u4ef7\u4f4d\"][\"value\"].values[0]\n            contract_min_list.append(contract_min)\n        if market == \"CF\":\n            data_df.columns = [\n                \"symbol\",\n                \"time\",\n                \"open\",\n                \"high\",\n                \"low\",\n                \"last_close\",\n                \"bid_price\",\n                \"ask_price\",\n                \"current_price\",\n                \"avg_price\",\n                \"last_settle_price\",\n                \"buy_vol\",\n                \"sell_vol\",\n                \"hold\",\n                \"volume\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"bid_price\",\n                    \"ask_price\",\n                    \"buy_vol\",\n                    \"sell_vol\",\n                    \"hold\",\n                    \"volume\",\n                    \"avg_price\",\n                    \"last_close\",\n                    \"last_settle_price\",\n                ]\n            ]\n            data_df[\"exchange\"] = contract_exchange_list\n            data_df[\"contract\"] = contract_name_list\n            data_df[\"contract_min_change\"] = contract_min_list\n\n            data_df['open'] = pd.to_numeric(data_df['open'])\n            data_df['high'] = pd.to_numeric(data_df['high'])\n            data_df['low'] = pd.to_numeric(data_df['low'])\n            data_df['current_price'] = pd.to_numeric(data_df['current_price'])\n            data_df['bid_price'] = pd.to_numeric(data_df['bid_price'])\n            data_df['ask_price'] = pd.to_numeric(data_df['ask_price'])\n            data_df['buy_vol'] = pd.to_numeric(data_df['buy_vol'])\n            data_df['sell_vol'] = pd.to_numeric(data_df['sell_vol'])\n            data_df['hold'] = pd.to_numeric(data_df['hold'])\n            data_df['volume'] = pd.to_numeric(data_df['volume'])\n            data_df['avg_price'] = pd.to_numeric(data_df['avg_price'])\n            data_df['last_close'] = pd.to_numeric(data_df['last_close'])\n            data_df['last_settle_price'] = pd.to_numeric(data_df['last_settle_price'])\n\n            data_df.dropna(subset=['current_price'], inplace=True)\n            return data_df\n        else:\n            data_df.columns = [\n                \"open\",\n                \"high\",\n                \"low\",\n                \"current_price\",\n                \"volume\",\n                \"amount\",\n                \"hold\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\" \"_\",\n                \"time\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"symbol\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"hold\",\n                    \"volume\",\n                    \"amount\",\n                ]\n            ]\n            data_df[\"exchange\"] = contract_exchange_list\n            data_df[\"contract\"] = contract_name_list\n            data_df[\"contract_min_change\"] = contract_min_list\n\n            data_df['open'] = pd.to_numeric(data_df['open'])\n            data_df['high'] = pd.to_numeric(data_df['high'])\n            data_df['low'] = pd.to_numeric(data_df['low'])\n            data_df['current_price'] = pd.to_numeric(data_df['current_price'])\n            data_df['hold'] = pd.to_numeric(data_df['hold'])\n            data_df['volume'] = pd.to_numeric(data_df['volume'])\n            data_df['amount'] = pd.to_numeric(data_df['amount'])\n\n            data_df.dropna(subset=['current_price'], inplace=True)\n            return data_df\n    else:\n        if market == \"CF\":\n            data_df.columns = [\n                \"symbol\",\n                \"time\",\n                \"open\",\n                \"high\",\n                \"low\",\n                \"last_close\",\n                \"bid_price\",\n                \"ask_price\",\n                \"current_price\",\n                \"avg_price\",\n                \"last_settle_price\",\n                \"buy_vol\",\n                \"sell_vol\",\n                \"hold\",\n                \"volume\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"bid_price\",\n                    \"ask_price\",\n                    \"buy_vol\",\n                    \"sell_vol\",\n                    \"hold\",\n                    \"volume\",\n                    \"avg_price\",\n                    \"last_close\",\n                    \"last_settle_price\",\n                ]\n            ]\n\n            data_df['open'] = pd.to_numeric(data_df['open'])\n            data_df['high'] = pd.to_numeric(data_df['high'])\n            data_df['low'] = pd.to_numeric(data_df['low'])\n            data_df['current_price'] = pd.to_numeric(data_df['current_price'])\n            data_df['bid_price'] = pd.to_numeric(data_df['bid_price'])\n            data_df['ask_price'] = pd.to_numeric(data_df['ask_price'])\n            data_df['buy_vol'] = pd.to_numeric(data_df['buy_vol'])\n            data_df['sell_vol'] = pd.to_numeric(data_df['sell_vol'])\n            data_df['hold'] = pd.to_numeric(data_df['hold'])\n            data_df['volume'] = pd.to_numeric(data_df['volume'])\n            data_df['avg_price'] = pd.to_numeric(data_df['avg_price'])\n            data_df['last_close'] = pd.to_numeric(data_df['last_close'])\n            data_df['last_settle_price'] = pd.to_numeric(data_df['last_settle_price'])\n\n            data_df.dropna(subset=['current_price'], inplace=True)\n            return data_df\n        else:\n            data_df.columns = [\n                \"open\",\n                \"high\",\n                \"low\",\n                \"current_price\",\n                \"volume\",\n                \"amount\",\n                \"hold\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\" \"_\",\n                \"time\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"_\",\n                \"symbol\",\n            ]\n            data_df = data_df[\n                [\n                    \"symbol\",\n                    \"time\",\n                    \"open\",\n                    \"high\",\n                    \"low\",\n                    \"current_price\",\n                    \"hold\",\n                    \"volume\",\n                    \"amount\",\n                ]\n            ]\n            data_df['open'] = pd.to_numeric(data_df['open'])\n            data_df['high'] = pd.to_numeric(data_df['high'])\n            data_df['low'] = pd.to_numeric(data_df['low'])\n            data_df['current_price'] = pd.to_numeric(data_df['current_price'])\n            data_df['hold'] = pd.to_numeric(data_df['hold'])\n            data_df['volume'] = pd.to_numeric(data_df['volume'])\n            data_df['amount'] = pd.to_numeric(data_df['amount'])\n\n            data_df.dropna(subset=['current_price'], inplace=True)\n            return data_df\n```\n\nOriginal documentation:\n```python\n\u671f\u8d27\u7684\u5b9e\u65f6\u884c\u60c5\u6570\u636e\n    :param subscribe_list: \u884c\u60c5\u7684\u5b57\u7b26\u4e32\u7ec4\u5408\n    :type subscribe_list: str\n    :param market: CF \u4e3a\u5546\u54c1\u671f\u8d27\n    :type market: str\n    :param adjust: True or False\n    :type adjust: bool\n    :return: \u671f\u8d27\u7684\u5b9e\u65f6\u884c\u60c5\u6570\u636e\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u671f\u8d27\u7684\u5b9e\u65f6\u884c\u60c5\u6570\u636e\n    http://vip.stock.finance.sina.com.cn/quotes_service/view/qihuohangqing.html#titlePos_1\n    :param symbol: \u5408\u7ea6\u540d\u79f0\u7684\u5b57\u7b26\u4e32\u7ec4\u5408\n    :type symbol: str\n    :param market: CF \u4e3a\u5546\u54c1\u671f\u8d27\n    :type market: str\n    :param adjust: '1' or '0'; \u5b57\u7b26\u4e32\u7684 0 \u6216 1\n    :type adjust: str\n    :return: \u671f\u8d27\u7684\u5b9e\u65f6\u884c\u60c5\u6570\u636e\n    :rtype: pandas.DataFrame", "file": "akshare_futures_futures_zh_sina.py", "function": "futures_zh_spot", "project": "akshare", "change_stats": {"code_changes": 64, "doc_changes": 11, "total_changes": 75}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef safe_run_module(self, mod_name, where):\n        \n        \n        try:\n            where.update(\n                runpy.run_module(str(mod_name), run_name=\"__main__\",\n                                 alter_sys=True)\n                )\n        except:\n            self.showtraceback()\n            warn('Unknown failure executing module: <%s>' % mod_name)```\n\nUpdated code:\n```python\ndef safe_run_module(self, mod_name, where):\n        \n        \n        try:\n            try:\n                where.update(\n                    runpy.run_module(str(mod_name), run_name=\"__main__\",\n                                     alter_sys=True)\n                    )\n            except SystemExit as status:\n                if status.code:\n                    raise\n        except:\n            self.showtraceback()\n            warn('Unknown failure executing module: <%s>' % mod_name)\n```\n\nOriginal documentation:\n```python\nA safe version of runpy.run_module().\n\n        This version will never throw an exception, but instead print\n        helpful error messages to the screen.\n\n        Parameters\n        ----------\n        mod_name : string\n            The name of the module to be executed.\n        where : dict\n            The globals namespace.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A safe version of runpy.run_module().\n\n        This version will never throw an exception, but instead print\n        helpful error messages to the screen.\n\n        `SystemExit` exceptions with status code 0 or None are ignored.\n\n        Parameters\n        ----------\n        mod_name : string\n            The name of the module to be executed.\n        where : dict\n            The globals namespace.", "file": "IPython_core_interactiveshell.py", "function": "InteractiveShell.safe_run_module", "project": "ipython", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef htmlsafe_dumps(obj, **kwargs):\n    \n    \n    return dumps(obj, **kwargs).translate(_htmlsafe_map)```\n\nUpdated code:\n```python\ndef htmlsafe_dumps(obj, **kwargs):\n    \n    \n    return _jinja_htmlsafe_dumps(obj, dumps=dumps, **kwargs)\n```\n\nOriginal documentation:\n```python\nSerialize an object to a string of JSON, replacing HTML-unsafe\n    characters with Unicode escapes. Otherwise behaves the same as\n    :func:`dumps`.\n\n    This is available in templates as the ``|tojson`` filter, which will\n    also mark the result with ``|safe``.\n\n    The returned string is safe to render in HTML documents and\n    ``<script>`` tags. The exception is in HTML attributes that are\n    double quoted; either use single quotes or the ``|forceescape``\n    filter.\n\n    .. versionchanged:: 0.10\n        Single quotes are escaped, making this safe to use in HTML,\n        ``<script>`` tags, and single-quoted attributes without further\n        escaping.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Serialize an object to a string of JSON with :func:`dumps`, then\n    replace HTML-unsafe characters with Unicode escapes and mark the\n    result safe with :class:`~markupsafe.Markup`.\n\n    This is available in templates as the ``|tojson`` filter.\n\n    The returned string is safe to render in HTML documents and\n    ``<script>`` tags. The exception is in HTML attributes that are\n    double quoted; either use single quotes or the ``|forceescape``\n    filter.\n\n    .. versionchanged:: 2.0\n        Uses :func:`jinja2.utils.htmlsafe_json_dumps`. The returned\n        value is marked safe by wrapping in :class:`~markupsafe.Markup`.\n\n    .. versionchanged:: 0.10\n        Single quotes are escaped, making this safe to use in HTML,\n        ``<script>`` tags, and single-quoted attributes without further\n        escaping.", "file": "src_flask_json___init__.py", "function": "htmlsafe_dumps", "project": "flask", "change_stats": {"code_changes": 4, "doc_changes": 15, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef preset_modes(self):\n        \n        \n        return [PRESET_NONE, PRESET_BOOST]```\n\nUpdated code:\n```python\ndef preset_modes(self):\n        \n        presets = [PRESET_NONE, PRESET_BOOST]\n        presets.extend(self._device_profile_names)\n        return presets\n```\n\nOriginal documentation:\n```python\nReturn a list of available preset modes.\n\n        Requires SUPPORT_PRESET_MODE.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a list of available preset modes incl profiles.", "file": "homeassistant_components_homematicip_cloud_climate.py", "function": "HomematicipHeatingGroup.preset_modes", "project": "core", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef render_speed(self, task: \"Task\") -> Text:\n        \n        speed = task.finished_speed or task.speed\n        if speed is None:\n            return Text(\"\", style=\"progress.percentage\")\n        unit, suffix = filesize.pick_unit_and_suffix(\n            int(speed),\n            [\"\", \"\u00d710\u00b3\", \"\u00d710\u2076\", \"\u00d710\u2079\", \"\u00d710\u00b9\u00b2\"],\n            1000,\n        )\n        data_speed = speed / unit\n        return Text(f\"{data_speed:.1f}{suffix} it/s\", style=\"progress.percentage\")```\n\nUpdated code:\n```python\ndef render_speed(cls, speed: float | None) -> Text:\n        \n        \n        if speed is None:\n            return Text(\"\", style=\"progress.percentage\")\n        unit, suffix = filesize.pick_unit_and_suffix(\n            int(speed),\n            [\"\", \"\u00d710\u00b3\", \"\u00d710\u2076\", \"\u00d710\u2079\", \"\u00d710\u00b9\u00b2\"],\n            1000,\n        )\n        data_speed = speed / unit\n        return Text(f\"{data_speed:.1f}{suffix} it/s\", style=\"progress.percentage\")\n```\n\nOriginal documentation:\n```python\nShow data transfer speed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Render the speed in iterations per second.\n\n        Args:\n            task (Task): A Task object.\n\n        Returns:\n            Text: Text object containing the task speed.", "file": "rich_progress.py", "function": "TaskProgressColumn.render_speed", "project": "rich", "change_stats": {"code_changes": 6, "doc_changes": 10, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, **kwargs):\n        \n        super().__init__(**kwargs)\n        self._cluster = list(kwargs['in_clusters'].values())[0]```\n\nUpdated code:\n```python\ndef __init__(self, unique_id, zha_device, listeners, **kwargs):\n        \n        super().__init__(unique_id, zha_device, listeners, **kwargs)\n        sensor_type = kwargs.get(SENSOR_TYPE, GENERIC)\n        self._unit = UNIT_REGISTRY.get(sensor_type)\n        self._formatter_function = FORMATTER_FUNC_REGISTRY.get(\n            sensor_type,\n            pass_through_formatter\n        )\n        self._force_update = FORCE_UPDATE_REGISTRY.get(\n            sensor_type,\n            False\n        )\n        self._should_poll = POLLING_REGISTRY.get(\n            sensor_type,\n            False\n        )\n        self._listener = self.cluster_listeners.get(\n            LISTENER_REGISTRY.get(sensor_type, LISTENER_ATTRIBUTE)\n        )\n```\n\nOriginal documentation:\n```python\nInit ZHA Sensor instance.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Init this sensor.", "file": "homeassistant_components_zha_sensor.py", "function": "Sensor.__init__", "project": "core", "change_stats": {"code_changes": 24, "doc_changes": 4, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _GetMSBuildExternalBuilderTargets(spec):\n  \n  \n  build_cmd = _BuildCommandLineForRuleRaw(\n      spec, spec['msvs_external_builder_build_cmd'],\n      False, False, False, False)\n  build_target = ['Target', {'Name': 'Build'}]\n  build_target.append(['Exec', {'Command': build_cmd}])\n\n  clean_cmd = _BuildCommandLineForRuleRaw(\n      spec, spec['msvs_external_builder_clean_cmd'],\n      False, False, False, False)\n  clean_target = ['Target', {'Name': 'Clean'}]\n  clean_target.append(['Exec', {'Command': clean_cmd}])\n\n  return [build_target, clean_target]```\n\nUpdated code:\n```python\ndef _GetMSBuildExternalBuilderTargets(spec):\n  \n  \n  build_cmd = _BuildCommandLineForRuleRaw(\n      spec, spec['msvs_external_builder_build_cmd'],\n      False, False, False, False)\n  build_target = ['Target', {'Name': 'Build'}]\n  build_target.append(['Exec', {'Command': build_cmd}])\n\n  clean_cmd = _BuildCommandLineForRuleRaw(\n      spec, spec['msvs_external_builder_clean_cmd'],\n      False, False, False, False)\n  clean_target = ['Target', {'Name': 'Clean'}]\n  clean_target.append(['Exec', {'Command': clean_cmd}])\n\n  targets = [build_target, clean_target]\n\n  if spec.get('msvs_external_builder_clcompile_cmd'):\n    clcompile_cmd = _BuildCommandLineForRuleRaw(\n        spec, spec['msvs_external_builder_clcompile_cmd'],\n        False, False, False, False)\n    clcompile_target = ['Target', {'Name': 'ClCompile'}]\n    clcompile_target.append(['Exec', {'Command': clcompile_cmd}])\n    targets.append(clcompile_target)\n\n  return targets\n```\n\nOriginal documentation:\n```python\nReturn a list of MSBuild targets for external builders.\n\n  Right now, only \"Build\" and \"Clean\" targets are generated.\n\n  Arguments:\n    spec: The gyp target spec.\n  Returns:\n    List of MSBuild 'Target' specs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a list of MSBuild targets for external builders.\n\n  The \"Build\" and \"Clean\" targets are always generated.  If the spec contains\n  'msvs_external_builder_clcompile_cmd', then the \"ClCompile\" target will also\n  be generated, to support building selected C/C++ files.\n\n  Arguments:\n    spec: The gyp target spec.\n  Returns:\n    List of MSBuild 'Target' specs.", "file": "gyp_pylib_gyp_generator_msvs.py", "function": "_GetMSBuildExternalBuilderTargets", "project": "node-gyp", "change_stats": {"code_changes": 14, "doc_changes": 6, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _add_solids(self, X, Y, C):\n        \n        \n        \n        if C.shape[0] == Y.shape[0]:\n            # trim the last one to be compatible with old behavior.\n            C = C[:-1]\n        if self.orientation == 'vertical':\n            args = (X, Y, C)\n        else:\n            args = (np.transpose(Y), np.transpose(X), np.transpose(C))\n        kw = dict(cmap=self.cmap,\n                  norm=self.norm,\n                  alpha=self.alpha,\n                  edgecolors='None')\n        _log.debug('Setting pcolormesh')\n        col = self.ax.pcolormesh(*args, **kw, shading='flat')\n        # self.add_observer(col) # We should observe, not be observed...\n\n        if self.solids is not None:\n            self.solids.remove()\n        self.solids = col\n        if self.dividers is not None:\n            self.dividers.remove()\n            self.dividers = None\n        if self.drawedges:\n            linewidths = (0.5 * mpl.rcParams['axes.linewidth'],)\n            self.dividers = collections.LineCollection(\n                    self._edges(X, Y),\n                    colors=(mpl.rcParams['axes.edgecolor'],),\n                    linewidths=linewidths)\n            self.ax.add_collection(self.dividers)\n        elif len(self._y) >= self.n_rasterize:\n            self.solids.set_rasterized(True)```\n\nUpdated code:\n```python\ndef _add_solids(self, X, Y, C):\n        \n        # Cleanup previously set artists.\n        if self.solids is not None:\n            self.solids.remove()\n        for solid in self.solids_patches:\n            solid.remove()\n        # Add new artist(s), based on mappable type.  Use individual patches if\n        # hatching is needed, pcolormesh otherwise.\n        mappable = getattr(self, 'mappable', None)\n        if (isinstance(mappable, contour.ContourSet)\n                and any(hatch is not None for hatch in mappable.hatches)):\n            self._add_solids_patches(X, Y, C, mappable)\n        else:\n            self._add_solids_pcolormesh(X, Y, C)\n        self.dividers.set_segments(\n            np.dstack([X, Y])[1:-1] if self.drawedges else [])\n```\n\nOriginal documentation:\n```python\nDraw the colors using `~.axes.Axes.pcolormesh`;\n        optionally add separators.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Draw the colors; optionally add separators.", "file": "lib_matplotlib_colorbar.py", "function": "ColorbarBase._add_solids", "project": "matplotlib", "change_stats": {"code_changes": 45, "doc_changes": 5, "total_changes": 50}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _sub_labels(self, axis, subs=()):\n        \"\"\n        fmt = axis.get_minor_formatter()\n        minor_tlocs = axis.get_minorticklocs()\n        fmt.set_locs(minor_tlocs)\n        coefs = minor_tlocs / 10**(np.floor(np.log10(minor_tlocs)))\n        label_expected = [round(c) in subs for c in coefs]\n        label_test = [fmt(x) != '' for x in minor_tlocs]\n        assert label_test == label_expected```\n\nUpdated code:\n```python\ndef _sub_labels(self, axis, subs=()):\n        \n        fmt = axis.get_minor_formatter()\n        minor_tlocs = axis.get_minorticklocs()\n        fmt.set_locs(minor_tlocs)\n        coefs = minor_tlocs / 10**(np.floor(np.log10(minor_tlocs)))\n        label_expected = [round(c) in subs for c in coefs]\n        label_test = [fmt(x) != '' for x in minor_tlocs]\n        assert label_test == label_expected\n```\n\nOriginal documentation:\n```python\nTest whether locator marks subs to be labeled\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test whether locator marks subs to be labeled.", "file": "lib_matplotlib_tests_test_ticker.py", "function": "TestLogFormatter._sub_labels", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef null_point_align(self, mobject):\n        \n        \n        \n        if self.submobjects:\n            mobject.push_self_into_submobjects()\n        else:\n            self.points = np.array([mobject.points[0]])\n        return self```\n\nUpdated code:\n```python\ndef null_point_align(self, mobject):\n        \n        \n        \n        for m1, m2 in (self, mobject), (mobject, self):\n            if m1.has_no_points() and m2.has_points():\n                m2.push_self_into_submobjects()\n        return self\n```\n\nOriginal documentation:\n```python\nIf self has no points, but needs to align\n        with mobject, which has points\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "If a mobject with points is being aligned to\n        one without, treat both as groups, and push\n        the one with points into its own submobjects\n        list.", "file": "manimlib_mobject_mobject.py", "function": "Mobject.null_point_align", "project": "manim", "change_stats": {"code_changes": 9, "doc_changes": 8, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fuse_conv_bn_relu(conv, bn, relu):\n    \n    \n    assert(conv.training == bn.training == relu.training),\\\n        \"Conv and BN both must be in the same mode (train or eval).\"\n\n    if conv.training:\n        return torch_fused.ConvBnReLU2d(conv, bn, relu)\n    else:\n        return torch_fused.ConvReLU2d(\n            torch.nn.utils.fusion.fuse_conv_bn_eval(conv, bn), relu)```\n\nUpdated code:\n```python\ndef fuse_conv_bn_relu(conv, bn, relu):\n    \n    \n    assert(conv.training == bn.training == relu.training),\\\n        \"Conv and BN both must be in the same mode (train or eval).\"\n    is_3d = isinstance(conv, torch.nn.Conv3d)\n    if conv.training:\n        assert conv.bias is None, 'Only support fusing Conv that does not have bias'\n        assert bn.num_features == conv.out_channels, 'Output channel of Conv must match num_features of BatchNorm'\n        assert bn.affine, 'Only support fusing BatchNorm with affine set to True'\n        assert bn.track_running_stats, 'Only support fusing BatchNorm with tracking_running_stats set to True'\n\n        return torch_fused.ConvBnReLU3d(conv, bn, relu) if is_3d \\\n            else torch_fused.ConvBnReLU2d(conv, bn, relu)\n    else:\n        return torch_fused.ConvReLU3d(\n            torch.nn.utils.fusion.fuse_conv_bn_eval(conv, bn), relu) if is_3d \\\n            else torch_fused.ConvReLU2d(\n                torch.nn.utils.fusion.fuse_conv_bn_eval(conv, bn), relu)\n```\n\nOriginal documentation:\n```python\nr\"\"\"Given the conv and bn modules, fuses them and returns the fused module\n\n    Args:\n        conv: Module instance of type conv2d\n        bn: Spatial BN instance that needs to be fused with the conv\n\n    Examples::\n\n        >>> m1 = nn.Conv2d(10, 20, 3)\n        >>> b1 = nn.BatchNorm2d(20)\n        >>> m2 = fuse_conv_bn(m1, b1)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Given the conv and bn modules, fuses them and returns the fused module\n\n    Args:\n        conv: Module instance of type conv2d/conv3d\n        bn: Spatial BN instance that needs to be fused with the conv\n\n    Examples::\n\n        >>> m1 = nn.Conv2d(10, 20, 3)\n        >>> b1 = nn.BatchNorm2d(20)\n        >>> m2 = fuse_conv_bn(m1, b1)", "file": "torch_quantization_fuse_modules.py", "function": "fuse_conv_bn_relu", "project": "pytorch", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _restore_volume(self, message=None):\n        \n            \n        \n        if self.current:\n            LOG.debug('restoring volume')\n            self.volume_is_low = False\n            time.sleep(2)\n            if not self.volume_is_low:\n                self.current.restore_volume()```\n\nUpdated code:\n```python\ndef _restore_volume(self, _=None):\n        \n        current = self.current\n        if current:\n            LOG.debug('restoring volume')\n            self.volume_is_low = False\n            current.restore_volume()\n```\n\nOriginal documentation:\n```python\nIs triggered when mycroft is done speaking and restores the volume\n\n            Args:\n                message: message bus message, not used but required\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Triggered when mycroft is done speaking and restores the volume.", "file": "mycroft_audio_audioservice.py", "function": "AudioService._restore_volume", "project": "mycroft-core", "change_stats": {"code_changes": 13, "doc_changes": 7, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(self):\n    \n    sleeper = concurrency.Sleeper()\n    while True:\n      now = time.time()\n      tick_number = max(0, int((now - self._start_time) // self._period) + 1)\n      tick_time = tick_number * self._period + self._start_time\n\n      if self._stop_time is not None and self._stop_time < tick_time:\n        break\n\n      sleeper.sleep_until(tick_time)\n\n      metric_position = tick_number % len(self._pitches)\n      self._outport.send(\n          mido.Message(\n              type='note_on',\n              note=self._pitches[metric_position],\n              channel=_METRONOME_CHANNEL,\n              velocity=self._velocity))\n\n      sleeper.sleep(self._duration)\n\n      self._outport.send(\n          mido.Message(\n              type='note_off',\n              note=self._pitches[metric_position],\n              channel=_METRONOME_CHANNEL))```\n\nUpdated code:\n```python\ndef run(self):\n    \n    sleeper = concurrency.Sleeper()\n    while True:\n      now = time.time()\n      tick_number = max(0, int((now - self._start_time) // self._period) + 1)\n      tick_time = tick_number * self._period + self._start_time\n\n      if self._stop_time is not None and self._stop_time < tick_time:\n        break\n\n      sleeper.sleep_until(tick_time)\n\n      metric_position = tick_number % len(self._messages)\n      tick_message = self._messages[metric_position]\n\n      if tick_message is None:\n        continue\n\n      tick_message.channel = self._channel\n      self._outport.send(tick_message)\n\n      if tick_message.type == 'note_on':\n        sleeper.sleep(self._duration)\n        end_tick_message = mido.Message(\n            'note_off', note=tick_message.note, channel=self._channel)\n        self._outport.send(end_tick_message)\n```\n\nOriginal documentation:\n```python\nOutputs metronome tone on the qpm interval until stop signal received.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sends message on the qpm interval until stop signal received.", "file": "magenta_interfaces_midi_midi_hub.py", "function": "Metronome.run", "project": "magenta", "change_stats": {"code_changes": 27, "doc_changes": 4, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _formatparam(param, value=None, quote=True):\n    \n    \n    if value is not None and len(value) > 0:\n        # A tuple is used for RFC 2231 encoded parameter values where items\n        # are (charset, language, value).  charset is a string, not a Charset\n        # instance.\n        if isinstance(value, tuple):\n            # Encode as per RFC 2231\n            param += '*'\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\n        # BAW: Please check this.  I think that if quote is set it should\n        # force quoting even if not necessary.\n        if quote or tspecials.search(value):\n            return '%s=\"%s\"' % (param, utils.quote(value))\n        else:\n            return '%s=%s' % (param, value)\n    else:\n        return param```\n\nUpdated code:\n```python\ndef _formatparam(param, value=None, quote=True):\n    \n    \n    if value is not None and len(value) > 0:\n        # A tuple is used for RFC 2231 encoded parameter values where items\n        # are (charset, language, value).  charset is a string, not a Charset\n        # instance.  RFC 2231 encoded values are never quoted, per RFC.\n        if isinstance(value, tuple):\n            # Encode as per RFC 2231\n            param += '*'\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\n            return '%s=%s' % (param, value)\n        else:\n            try:\n                value.encode('ascii')\n            except UnicodeEncodeError:\n                param += '*'\n                value = utils.encode_rfc2231(value, 'utf-8', '')\n                return '%s=%s' % (param, value)\n        # BAW: Please check this.  I think that if quote is set it should\n        # force quoting even if not necessary.\n        if quote or tspecials.search(value):\n            return '%s=\"%s\"' % (param, utils.quote(value))\n        else:\n            return '%s=%s' % (param, value)\n    else:\n        return param\n```\n\nOriginal documentation:\n```python\nConvenience function to format and return a key=value pair.\n\n    This will quote the value if needed or if quote is true.  If value is a\n    three tuple (charset, language, value), it will be encoded according\n    to RFC2231 rules.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convenience function to format and return a key=value pair.\n\n    This will quote the value if needed or if quote is true.  If value is a\n    three tuple (charset, language, value), it will be encoded according\n    to RFC2231 rules.  If it contains non-ascii characters it will likewise\n    be encoded according to RFC2231 rules, using the utf-8 charset and\n    a null language.", "file": "Lib_email_message.py", "function": "_formatparam", "project": "cpython", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_config_file(self):\n          \n        dirname = os.path.dirname(sys.modules[self.__module__].__file__)\n        folder, fname = os.path.split(dirname)\n        retval = os.path.join(os.path.dirname(folder), \"config\", \"{}.ini\".format(fname))\n        logger.debug(\"Config File location: '%s'\", retval)\n        return retval```\n\nUpdated code:\n```python\ndef get_config_file(self, configfile):\n          \n        if configfile is not None:\n            if not os.path.isfile(configfile):\n                err = \"Config file does not exist at: {}\".format(configfile)\n                logger.error(err)\n                raise ValueError(err)\n            return configfile\n        dirname = os.path.dirname(sys.modules[self.__module__].__file__)\n        folder, fname = os.path.split(dirname)\n        retval = os.path.join(os.path.dirname(folder), \"config\", \"{}.ini\".format(fname))\n        logger.debug(\"Config File location: '%s'\", retval)\n        return retval\n```\n\nOriginal documentation:\n```python\nReturn the config file from the calling folder\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the config file from the calling folder or the provided file", "file": "lib_config.py", "function": "FaceswapConfig.get_config_file", "project": "faceswap", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef os_finish_daily_mission(self, question=True, rescan=None):\n        \n        \n        \n        logger.hr('OS finish daily mission', level=1)\n        while 1:\n            result = self.os_get_next_mission()\n            if not result:\n                break\n\n            if result != 'pinned_at_archive_zone':\n                # The name of archive zone is \"archive zone\", which is not an existing zone.\n                # After archive zone, it go back to previous zone automatically.\n                self.zone_init()\n            if result == 'already_at_mission_zone':\n                self.globe_goto(self.zone, refresh=True)\n            self.fleet_set(self.config.OpsiFleet_Fleet)\n            self.os_order_execute(\n                recon_scan=False,\n                submarine_call=self.config.OpsiFleet_Submarine and result != 'pinned_at_archive_zone')\n            self.run_auto_search(question, rescan)\n            self.handle_after_auto_search()\n            self.config.check_task_switch()\n\n        return True```\n\nUpdated code:\n```python\ndef os_finish_daily_mission(self, question=True, rescan=None):\n        \n        \n        \n        logger.hr('OS finish daily mission', level=1)\n        count = 0\n        while 1:\n            result = self.os_get_next_mission()\n            if not result:\n                break\n\n            if result != 'pinned_at_archive_zone':\n                # The name of archive zone is \"archive zone\", which is not an existing zone.\n                # After archive zone, it go back to previous zone automatically.\n                self.zone_init()\n            if result == 'already_at_mission_zone':\n                self.globe_goto(self.zone, refresh=True)\n            self.fleet_set(self.config.OpsiFleet_Fleet)\n            self.os_order_execute(\n                recon_scan=False,\n                submarine_call=self.config.OpsiFleet_Submarine and result != 'pinned_at_archive_zone')\n            self.run_auto_search(question, rescan)\n            self.handle_after_auto_search()\n            count += 1\n            self.config.check_task_switch()\n\n        return count\n```\n\nOriginal documentation:\n```python\nFinish all daily mission in Operation Siren.\n        Suggest to run os_port_daily to accept missions first.\n\n        Args:\n            question (bool): refer to run_auto_search\n            rescan (None, bool): refer to run_auto_search\n\n        Returns:\n            bool: True if all finished.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Finish all daily mission in Operation Siren.\n        Suggest to run os_port_daily to accept missions first.\n\n        Args:\n            question (bool): refer to run_auto_search\n            rescan (None, bool): refer to run_auto_search\n\n        Returns:\n            int: Number of missions finished", "file": "module_os_operation_siren.py", "function": "OperationSiren.os_finish_daily_mission", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _compute_loss(\n        self,\n        speech_mix,\n        speech_lengths,\n        speech_ref,\n        dereverb_speech_ref=None,\n        noise_ref=None,\n        cal_loss=True,\n    ):\n        \n        \n        feature_mix, flens = self.encoder(speech_mix, speech_lengths)\n        feature_pre, flens, others = self.separator(feature_mix, flens)\n\n        if self.loss_type not in [\"si_snr\", \"ci_sdr\"]:\n            spectrum_mix = feature_mix\n            spectrum_pre = feature_pre\n            # predict separated speech and masks\n            if self.stft_consistency:\n                # pseudo STFT -> time-domain -> STFT (compute loss)\n                tmp_t_domain = [\n                    self.decoder(sp, speech_lengths)[0] for sp in spectrum_pre\n                ]\n                spectrum_pre = [\n                    self.encoder(sp, speech_lengths)[0] for sp in tmp_t_domain\n                ]\n\n            if not cal_loss:\n                loss, perm = None, None\n                return loss, spectrum_pre, others, flens, perm\n\n            # prepare reference speech and reference spectrum\n            speech_ref = torch.unbind(speech_ref, dim=1)\n            # List[ComplexTensor(Batch, T, F)] or List[ComplexTensor(Batch, T, C, F)]\n            spectrum_ref = [self.encoder(sr, speech_lengths)[0] for sr in speech_ref]\n\n            # compute TF masking loss\n            if self.loss_type == \"magnitude\":\n                # compute loss on magnitude spectrum\n                assert spectrum_pre is not None\n                magnitude_pre = [abs(ps + 1e-15) for ps in spectrum_pre]\n                if spectrum_ref[0].dim() > magnitude_pre[0].dim():\n                    # only select one channel as the reference\n                    magnitude_ref = [\n                        abs(sr[..., self.ref_channel, :]) for sr in spectrum_ref\n                    ]\n                else:\n                    magnitude_ref = [abs(sr) for sr in spectrum_ref]\n\n                tf_loss, perm = self._permutation_loss(\n                    magnitude_ref, magnitude_pre, self.tf_mse_loss\n                )\n            elif self.loss_type.startswith(\"spectrum\"):\n                # compute loss on complex spectrum\n                if self.loss_type == \"spectrum\":\n                    loss_func = self.tf_mse_loss\n                elif self.loss_type == \"spectrum_log\":\n                    loss_func = self.tf_log_mse_loss\n                else:\n                    raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n                assert spectrum_pre is not None\n                if spectrum_ref[0].dim() > spectrum_pre[0].dim():\n                    # only select one channel as the reference\n                    spectrum_ref = [sr[..., self.ref_channel, :] for sr in spectrum_ref]\n\n                tf_loss, perm = self._permutation_loss(\n                    spectrum_ref, spectrum_pre, loss_func\n                )\n            elif self.loss_type.startswith(\"mask\"):\n                if self.loss_type == \"mask_mse\":\n                    loss_func = self.tf_mse_loss\n                else:\n                    raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n                assert others is not None\n                mask_pre_ = [\n                    others[\"mask_spk{}\".format(spk + 1)] for spk in range(self.num_spk)\n                ]\n\n                # prepare ideal masks\n                mask_ref = self._create_mask_label(\n                    spectrum_mix, spectrum_ref, mask_type=self.mask_type\n                )\n\n                # compute TF masking loss\n                tf_loss, perm = self._permutation_loss(mask_ref, mask_pre_, loss_func)\n\n                if \"mask_dereverb1\" in others:\n                    if dereverb_speech_ref is None:\n                        raise ValueError(\n                            \"No dereverberated reference for training!\\n\"\n                            'Please specify \"--use_dereverb_ref true\" in run.sh'\n                        )\n\n                    mask_wpe_pre = [\n                        others[\"mask_dereverb{}\".format(spk + 1)]\n                        for spk in range(self.num_spk)\n                        if \"mask_dereverb{}\".format(spk + 1) in others\n                    ]\n                    assert len(mask_wpe_pre) == dereverb_speech_ref.size(1), (\n                        len(mask_wpe_pre),\n                        dereverb_speech_ref.size(1),\n                    )\n                    dereverb_speech_ref = torch.unbind(dereverb_speech_ref, dim=1)\n                    dereverb_spectrum_ref = [\n                        self.encoder(dr, speech_lengths)[0]\n                        for dr in dereverb_speech_ref\n                    ]\n                    dereverb_mask_ref = self._create_mask_label(\n                        spectrum_mix, dereverb_spectrum_ref, mask_type=self.mask_type\n                    )\n\n                    tf_dereverb_loss, perm_d = self._permutation_loss(\n                        dereverb_mask_ref, mask_wpe_pre, loss_func\n                    )\n                    tf_loss = tf_loss + tf_dereverb_loss\n\n                if \"mask_noise1\" in others:\n                    if noise_ref is None:\n                        raise ValueError(\n                            \"No noise reference for training!\\n\"\n                            'Please specify \"--use_noise_ref true\" in run.sh'\n                        )\n\n                    noise_ref = torch.unbind(noise_ref, dim=1)\n                    noise_spectrum_ref = [\n                        self.encoder(nr, speech_lengths)[0] for nr in noise_ref\n                    ]\n                    noise_mask_ref = self._create_mask_label(\n                        spectrum_mix, noise_spectrum_ref, mask_type=self.mask_type\n                    )\n\n                    mask_noise_pre = [\n                        others[\"mask_noise{}\".format(n + 1)]\n                        for n in range(self.num_noise_type)\n                    ]\n                    tf_noise_loss, perm_n = self._permutation_loss(\n                        noise_mask_ref, mask_noise_pre, loss_func\n                    )\n                    tf_loss = tf_loss + tf_noise_loss\n            else:\n                raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n            loss = tf_loss\n            return loss, spectrum_pre, others, flens, perm\n\n        else:\n            speech_pre = [self.decoder(ps, speech_lengths)[0] for ps in feature_pre]\n            if not cal_loss:\n                loss, perm = None, None\n                return loss, speech_pre, None, speech_lengths, perm\n\n            # speech_pre: list[(batch, sample)]\n            assert speech_pre[0].dim() == 2, speech_pre[0].dim()\n\n            if speech_ref.dim() == 4:\n                # For si_snr loss of multi-channel input,\n                # only select one channel as the reference\n                speech_ref = speech_ref[..., self.ref_channel]\n            speech_ref = torch.unbind(speech_ref, dim=1)\n\n            if self.loss_type == \"si_snr\":\n                # compute si-snr loss\n                loss, perm = self._permutation_loss(\n                    speech_ref, speech_pre, self.si_snr_loss_zeromean\n                )\n            elif self.loss_type == \"ci_sdr\":\n                # compute ci-snr loss\n                loss, perm = self._permutation_loss(\n                    speech_ref, speech_pre, self.ci_sdr_loss\n                )\n            else:\n                raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n            return loss, speech_pre, None, speech_lengths, perm```\n\nUpdated code:\n```python\ndef _compute_loss(\n        self,\n        speech_mix,\n        speech_lengths,\n        speech_ref,\n        dereverb_speech_ref=None,\n        noise_ref=None,\n        cal_loss=True,\n    ):\n        \n        \n        feature_mix, flens = self.encoder(speech_mix, speech_lengths)\n        feature_pre, flens, others = self.separator(feature_mix, flens)\n\n        if self.loss_type not in [\"si_snr\", \"ci_sdr\"]:\n            spectrum_mix = feature_mix\n            spectrum_pre = feature_pre\n            # predict separated speech and masks\n            if self.stft_consistency:\n                # pseudo STFT -> time-domain -> STFT (compute loss)\n                tmp_t_domain = [\n                    self.decoder(sp, speech_lengths)[0] for sp in spectrum_pre\n                ]\n                spectrum_pre = [\n                    self.encoder(sp, speech_lengths)[0] for sp in tmp_t_domain\n                ]\n                pass\n\n            if spectrum_pre is not None and not isinstance(\n                spectrum_pre[0], ComplexTensor\n            ):\n                spectrum_pre = [\n                    ComplexTensor(*torch.unbind(sp, dim=-1)) for sp in spectrum_pre\n                ]\n\n            if not cal_loss:\n                loss, perm = None, None\n                return loss, spectrum_pre, others, flens, perm\n\n            # prepare reference speech and reference spectrum\n            speech_ref = torch.unbind(speech_ref, dim=1)\n            # List[ComplexTensor(Batch, T, F)] or List[ComplexTensor(Batch, T, C, F)]\n            spectrum_ref = [self.encoder(sr, speech_lengths)[0] for sr in speech_ref]\n\n            # compute TF masking loss\n            if self.loss_type == \"magnitude\":\n                # compute loss on magnitude spectrum\n                assert spectrum_pre is not None\n                magnitude_pre = [abs(ps + 1e-15) for ps in spectrum_pre]\n                if spectrum_ref[0].dim() > magnitude_pre[0].dim():\n                    # only select one channel as the reference\n                    magnitude_ref = [\n                        abs(sr[..., self.ref_channel, :]) for sr in spectrum_ref\n                    ]\n                else:\n                    magnitude_ref = [abs(sr) for sr in spectrum_ref]\n\n                tf_loss, perm = self._permutation_loss(\n                    magnitude_ref, magnitude_pre, self.tf_mse_loss\n                )\n            elif self.loss_type.startswith(\"spectrum\"):\n                # compute loss on complex spectrum\n                if self.loss_type == \"spectrum\":\n                    loss_func = self.tf_mse_loss\n                elif self.loss_type == \"spectrum_log\":\n                    loss_func = self.tf_log_mse_loss\n                else:\n                    raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n                assert spectrum_pre is not None\n                if spectrum_ref[0].dim() > spectrum_pre[0].dim():\n                    # only select one channel as the reference\n                    spectrum_ref = [sr[..., self.ref_channel, :] for sr in spectrum_ref]\n\n                tf_loss, perm = self._permutation_loss(\n                    spectrum_ref, spectrum_pre, loss_func\n                )\n            elif self.loss_type.startswith(\"mask\"):\n                if self.loss_type == \"mask_mse\":\n                    loss_func = self.tf_mse_loss\n                else:\n                    raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n                assert others is not None\n                mask_pre_ = [\n                    others[\"mask_spk{}\".format(spk + 1)] for spk in range(self.num_spk)\n                ]\n\n                # prepare ideal masks\n                mask_ref = self._create_mask_label(\n                    spectrum_mix, spectrum_ref, mask_type=self.mask_type\n                )\n\n                # compute TF masking loss\n                tf_loss, perm = self._permutation_loss(mask_ref, mask_pre_, loss_func)\n\n                if \"mask_dereverb1\" in others:\n                    if dereverb_speech_ref is None:\n                        raise ValueError(\n                            \"No dereverberated reference for training!\\n\"\n                            'Please specify \"--use_dereverb_ref true\" in run.sh'\n                        )\n\n                    mask_wpe_pre = [\n                        others[\"mask_dereverb{}\".format(spk + 1)]\n                        for spk in range(self.num_spk)\n                        if \"mask_dereverb{}\".format(spk + 1) in others\n                    ]\n                    assert len(mask_wpe_pre) == dereverb_speech_ref.size(1), (\n                        len(mask_wpe_pre),\n                        dereverb_speech_ref.size(1),\n                    )\n                    dereverb_speech_ref = torch.unbind(dereverb_speech_ref, dim=1)\n                    dereverb_spectrum_ref = [\n                        self.encoder(dr, speech_lengths)[0]\n                        for dr in dereverb_speech_ref\n                    ]\n                    dereverb_mask_ref = self._create_mask_label(\n                        spectrum_mix, dereverb_spectrum_ref, mask_type=self.mask_type\n                    )\n\n                    tf_dereverb_loss, perm_d = self._permutation_loss(\n                        dereverb_mask_ref, mask_wpe_pre, loss_func\n                    )\n                    tf_loss = tf_loss + tf_dereverb_loss\n\n                if \"mask_noise1\" in others:\n                    if noise_ref is None:\n                        raise ValueError(\n                            \"No noise reference for training!\\n\"\n                            'Please specify \"--use_noise_ref true\" in run.sh'\n                        )\n\n                    noise_ref = torch.unbind(noise_ref, dim=1)\n                    noise_spectrum_ref = [\n                        self.encoder(nr, speech_lengths)[0] for nr in noise_ref\n                    ]\n                    noise_mask_ref = self._create_mask_label(\n                        spectrum_mix, noise_spectrum_ref, mask_type=self.mask_type\n                    )\n\n                    mask_noise_pre = [\n                        others[\"mask_noise{}\".format(n + 1)]\n                        for n in range(self.num_noise_type)\n                    ]\n                    tf_noise_loss, perm_n = self._permutation_loss(\n                        noise_mask_ref, mask_noise_pre, loss_func\n                    )\n                    tf_loss = tf_loss + tf_noise_loss\n            else:\n                raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n            loss = tf_loss\n            return loss, spectrum_pre, others, flens, perm\n\n        else:\n            speech_pre = [self.decoder(ps, speech_lengths)[0] for ps in feature_pre]\n            if not cal_loss:\n                loss, perm = None, None\n                return loss, speech_pre, None, speech_lengths, perm\n\n            # speech_pre: list[(batch, sample)]\n            assert speech_pre[0].dim() == 2, speech_pre[0].dim()\n\n            if speech_ref.dim() == 4:\n                # For si_snr loss of multi-channel input,\n                # only select one channel as the reference\n                speech_ref = speech_ref[..., self.ref_channel]\n            speech_ref = torch.unbind(speech_ref, dim=1)\n\n            if self.loss_type == \"si_snr\":\n                # compute si-snr loss\n                loss, perm = self._permutation_loss(\n                    speech_ref, speech_pre, self.si_snr_loss_zeromean\n                )\n            elif self.loss_type == \"ci_sdr\":\n                # compute ci-snr loss\n                loss, perm = self._permutation_loss(\n                    speech_ref, speech_pre, self.ci_sdr_loss\n                )\n            else:\n                raise ValueError(\"Unsupported loss type: %s\" % self.loss_type)\n\n            return loss, speech_pre, None, speech_lengths, perm\n```\n\nOriginal documentation:\n```python\nCompute loss according to self.loss_type.\n\n        Args:\n            speech_mix: (Batch, samples) or (Batch, samples, channels)\n            speech_lengths: (Batch,), default None for chunk interator,\n                            because the chunk-iterator does not have the\n                            speech_lengths returned. see in\n                            espnet2/iterators/chunk_iter_factory.py\n            speech_ref: (Batch, num_speaker, samples)\n                        or (Batch, num_speaker, samples, channels)\n            dereverb_speech_ref: (Batch, N, samples)\n                        or (Batch, num_speaker, samples, channels)\n            noise_ref: (Batch, num_noise_type, samples)\n                        or (Batch, num_speaker, samples, channels)\n            cal_loss: whether to calculate enh loss, defualt is True\n\n        Returns:\n            loss: (torch.Tensor) speech enhancement loss\n            speech_pre: (List[torch.Tensor] or List[ComplexTensor])\n                        enhanced speech or spectrum(s)\n            others: (OrderedDict) estimated masks or None\n            output_lengths: (Batch,)\n            perm: () best permutation\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute loss according to self.loss_type.\n\n        Args:\n            speech_mix: (Batch, samples) or (Batch, samples, channels)\n            speech_lengths: (Batch,), default None for chunk interator,\n                            because the chunk-iterator does not have the\n                            speech_lengths returned. see in\n                            espnet2/iterators/chunk_iter_factory.py\n            speech_ref: (Batch, num_speaker, samples)\n                        or (Batch, num_speaker, samples, channels)\n            dereverb_speech_ref: (Batch, N, samples)\n                        or (Batch, num_speaker, samples, channels)\n            noise_ref: (Batch, num_noise_type, samples)\n                        or (Batch, num_speaker, samples, channels)\n            cal_loss: whether to calculate enh loss, default is True\n\n        Returns:\n            loss: (torch.Tensor) speech enhancement loss\n            speech_pre: (List[torch.Tensor] or List[ComplexTensor])\n                        enhanced speech or spectrum(s)\n            others: (OrderedDict) estimated masks or None\n            output_lengths: (Batch,)\n            perm: () best permutation", "file": "espnet2_enh_espnet_model.py", "function": "ESPnetEnhancementModel._compute_loss", "project": "espnet", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef clear(self):\n        \n        self.state = None\n        self._lambda = None\n        self.expElogbeta = None```\n\nUpdated code:\n```python\ndef clear(self):\n        \n        self.state = None\n        self.Elogbeta = None\n```\n\nOriginal documentation:\n```python\nClear model state (free up some memory).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Clear model state (free up some memory). Used in the distributed algo.", "file": "gensim_models_ldamodel.py", "function": "LdaModel.clear", "project": "gensim", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_currencies(self, params={}):\n        \n        \n        \n        if not self.check_required_credentials(False):\n            return None\n        response = self.privateGetAssetV3PrivateCoinInfoQuery(params)\n        #\n        #    {\n        #        \"retCode\": \"0\",\n        #        \"retMsg\": \"OK\",\n        #        \"result\": {\n        #            \"rows\": [\n        #                {\n        #                    \"name\": \"MATIC\",\n        #                    \"coin\": \"MATIC\",\n        #                    \"remainAmount\": \"1652850\",\n        #                    \"chains\": [\n        #                        {\n        #                            \"chainType\": \"MATIC\",\n        #                            \"confirmation\": \"128\",\n        #                            \"withdrawFee\": \"0.1\",\n        #                            \"depositMin\": \"0\",\n        #                            \"withdrawMin\": \"0.1\",\n        #                            \"chain\": \"MATIC\",\n        #                            \"chainDeposit\": \"1\",\n        #                            \"chainWithdraw\": \"1\",\n        #                            \"minAccuracy\": \"8\"\n        #                        },\n        #                        {\n        #                            \"chainType\": \"ERC20\",\n        #                            \"confirmation\": \"12\",\n        #                            \"withdrawFee\": \"10\",\n        #                            \"depositMin\": \"0\",\n        #                            \"withdrawMin\": \"20\",\n        #                            \"chain\": \"ETH\",\n        #                            \"chainDeposit\": \"1\",\n        #                            \"chainWithdraw\": \"1\",\n        #                            \"minAccuracy\": \"8\"\n        #                        },\n        #                        {\n        #                            \"chainType\": \"BSC(BEP20)\",\n        #                            \"confirmation\": \"15\",\n        #                            \"withdrawFee\": \"1\",\n        #                            \"depositMin\": \"0\",\n        #                            \"withdrawMin\": \"1\",\n        #                            \"chain\": \"BSC\",\n        #                            \"chainDeposit\": \"1\",\n        #                            \"chainWithdraw\": \"1\",\n        #                            \"minAccuracy\": \"8\"\n        #                        }\n        #                    ]\n        #                },\n        #            ]\n        #        },\n        #        \"retExtInfo\": null,\n        #        \"time\": \"1666728888775\"\n        #    }\n        #\n        data = self.safe_value(response, 'result', [])\n        rows = self.safe_value(data, 'rows', [])\n        result = {}\n        for i in range(0, len(rows)):\n            currency = rows[i]\n            currencyId = self.safe_string(currency, 'coin')\n            code = self.safe_currency_code(currencyId)\n            name = self.safe_string(currency, 'name')\n            chains = self.safe_value(currency, 'chains', [])\n            networks = {}\n            minPrecision = None\n            for j in range(0, len(chains)):\n                chain = chains[j]\n                networkId = self.safe_string(chain, 'chain')\n                networkCode = self.network_id_to_code(networkId)\n                precision = self.parse_number(self.parse_precision(self.safe_string(chain, 'minAccuracy')))\n                minPrecision = precision if (minPrecision is None) else min(minPrecision, precision)\n                depositAllowed = self.safe_integer(chain, 'chainDeposit') == 1\n                withdrawAllowed = self.safe_integer(chain, 'chainWithdraw') == 1\n                networks[networkCode] = {\n                    'info': chain,\n                    'id': networkId,\n                    'network': networkCode,\n                    'active': None,\n                    'deposit': depositAllowed,\n                    'withdraw': withdrawAllowed,\n                    'fee': self.safe_number(chain, 'withdrawFee'),\n                    'precision': precision,\n                    'limits': {\n                        'withdraw': {\n                            'min': self.safe_number(chain, 'withdrawMin'),\n                            'max': None,\n                        },\n                        'deposit': {\n                            'min': self.safe_number(chain, 'depositMin'),\n                            'max': None,\n                        },\n                    },\n                }\n            result[code] = {\n                'info': currency,\n                'code': code,\n                'id': currencyId,\n                'name': name,\n                'active': None,\n                'deposit': None,\n                'withdraw': None,\n                'fee': None,\n                'precision': minPrecision,\n                'limits': {\n                    'amount': {\n                        'min': None,\n                        'max': None,\n                    },\n                },\n                'networks': networks,\n            }\n        return result```\n\nUpdated code:\n```python\ndef fetch_currencies(self, params={}):\n        \n        \n        \n        if not self.check_required_credentials(False):\n            return None\n        response = self.privateGetV5AssetCoinQueryInfo(params)\n        #\n        #     {\n        #         \"retCode\": 0,\n        #         \"retMsg\": \"\",\n        #         \"result\": {\n        #             \"rows\": [\n        #                 {\n        #                     \"name\": \"BTC\",\n        #                     \"coin\": \"BTC\",\n        #                     \"remainAmount\": \"150\",\n        #                     \"chains\": [\n        #                         {\n        #                             \"chainType\": \"BTC\",\n        #                             \"confirmation\": \"10000\",\n        #                             \"withdrawFee\": \"0.0005\",\n        #                             \"depositMin\": \"0.0005\",\n        #                             \"withdrawMin\": \"0.001\",\n        #                             \"chain\": \"BTC\",\n        #                             \"chainDeposit\": \"1\",\n        #                             \"chainWithdraw\": \"1\",\n        #                             \"minAccuracy\": \"8\"\n        #                         }\n        #                     ]\n        #                 }\n        #             ]\n        #         },\n        #         \"retExtInfo\": {},\n        #         \"time\": 1672194582264\n        #     }\n        #\n        data = self.safe_value(response, 'result', [])\n        rows = self.safe_value(data, 'rows', [])\n        result = {}\n        for i in range(0, len(rows)):\n            currency = rows[i]\n            currencyId = self.safe_string(currency, 'coin')\n            code = self.safe_currency_code(currencyId)\n            name = self.safe_string(currency, 'name')\n            chains = self.safe_value(currency, 'chains', [])\n            networks = {}\n            minPrecision = None\n            for j in range(0, len(chains)):\n                chain = chains[j]\n                networkId = self.safe_string(chain, 'chain')\n                networkCode = self.network_id_to_code(networkId)\n                precision = self.parse_number(self.parse_precision(self.safe_string(chain, 'minAccuracy')))\n                minPrecision = precision if (minPrecision is None) else min(minPrecision, precision)\n                depositAllowed = self.safe_integer(chain, 'chainDeposit') == 1\n                withdrawAllowed = self.safe_integer(chain, 'chainWithdraw') == 1\n                networks[networkCode] = {\n                    'info': chain,\n                    'id': networkId,\n                    'network': networkCode,\n                    'active': None,\n                    'deposit': depositAllowed,\n                    'withdraw': withdrawAllowed,\n                    'fee': self.safe_number(chain, 'withdrawFee'),\n                    'precision': precision,\n                    'limits': {\n                        'withdraw': {\n                            'min': self.safe_number(chain, 'withdrawMin'),\n                            'max': None,\n                        },\n                        'deposit': {\n                            'min': self.safe_number(chain, 'depositMin'),\n                            'max': None,\n                        },\n                    },\n                }\n            result[code] = {\n                'info': currency,\n                'code': code,\n                'id': currencyId,\n                'name': name,\n                'active': None,\n                'deposit': None,\n                'withdraw': None,\n                'fee': None,\n                'precision': minPrecision,\n                'limits': {\n                    'amount': {\n                        'min': None,\n                        'max': None,\n                    },\n                },\n                'networks': networks,\n            }\n        return result\n```\n\nOriginal documentation:\n```python\nfetches all available currencies on an exchange\n        :param dict params: extra parameters specific to the bybit api endpoint\n        :returns dict: an associative dictionary of currencies\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetches all available currencies on an exchange\n        see https://bybit-exchange.github.io/docs/v5/asset/coin-info\n        :param dict params: extra parameters specific to the bybit api endpoint\n        :returns dict: an associative dictionary of currencies", "file": "python_ccxt_bybit.py", "function": "bybit.fetch_currencies", "project": "ccxt", "change_stats": {"code_changes": 82, "doc_changes": 3, "total_changes": 85}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef register(self, event, function):\n        \n        \n        if not callable(function):\n            raise TypeError('Need a callable, got %r' % function)\n        self.callbacks[event].append(_adapt_function(event, function))```\n\nUpdated code:\n```python\ndef register(self, event, function):\n        \n        \n        if not callable(function):\n            raise TypeError('Need a callable, got %r' % function)\n        callback_proto = available_events.get(event)\n        self.callbacks[event].append(callback_proto.adapt(function))\n```\n\nOriginal documentation:\n```python\nRegister a new event callback\n        \n        Parameters\n        ----------\n        event : str\n          The event for which to register this callback.\n        function : callable\n          A function to be called on the given event. It should take the same\n          parameters as the appropriate callback prototype.\n        \n        Raises\n        ------\n        TypeError\n          If ``function`` is not callable.\n        KeyError\n          If ``event`` is not one of the known events.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Register a new event callback.\n        \n        Parameters\n        ----------\n        event : str\n          The event for which to register this callback.\n        function : callable\n          A function to be called on the given event. It should take the same\n          parameters as the appropriate callback prototype.\n        \n        Raises\n        ------\n        TypeError\n          If ``function`` is not callable.\n        KeyError\n          If ``event`` is not one of the known events.", "file": "IPython_core_events.py", "function": "EventManager.register", "project": "ipython", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef transcribe_file(self, path):\n        \n        \n        waveform = self.load_audio(path)\n        # Fake a batch:\n        batch = waveform.unsqueeze(0)\n        rel_length = torch.tensor([1.0])\n        predicted_words, predicted_tokens = self.transcribe_batch(\n            batch, rel_length\n        )\n        return \" \".join(predicted_words[0])```\n\nUpdated code:\n```python\ndef transcribe_file(\n        self,\n        path: str,\n        task: Optional[str] = None,\n        initial_prompt: Optional[str] = None,\n        logprob_threshold: Optional[float] = -1.0,\n        no_speech_threshold=0.6,\n        condition_on_previous_text: bool = False,\n        verbose: bool = False,\n        use_torchaudio_streaming: bool = False,\n        chunk_size: Optional[int] = 30,\n        **kwargs,\n    ) -> List[ASRWhisperSegment]:\n        \n        \n        results = []\n        for whisper_segment in self.transcribe_file_streaming(\n            path,\n            task=task,\n            initial_prompt=initial_prompt,\n            logprob_threshold=logprob_threshold,\n            no_speech_threshold=no_speech_threshold,\n            condition_on_previous_text=condition_on_previous_text,\n            verbose=verbose,\n            use_torchaudio_streaming=use_torchaudio_streaming,\n            chunk_size=chunk_size,\n            **kwargs,\n        ):\n            results.append(whisper_segment)\n            if verbose:\n                pred = (\n                    whisper_segment.words\n                    if task != \"lang_id\"\n                    else whisper_segment.lang_id\n                )\n                print(\n                    f\"[{whisper_segment.start}s --> {whisper_segment.end}s] {pred}\"\n                )\n        return results\n```\n\nOriginal documentation:\n```python\nTranscribes the given audiofile into a sequence of words.\n\n        Arguments\n        ---------\n        path : str\n            Path to audio file which to transcribe.\n\n        Returns\n        -------\n        str\n            The audiofile transcription produced by this ASR system.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run the Whisper model using the specified task on the given audio file and return the ``ASRWhisperSegment`` objects\n        for each segment.\n\n        This method supports the following tasks: ``transcribe``, ``translate``, and ``lang_id``.\n        It can process an input audio file longer than 30 seconds by splitting it into chunk_size-second segments.\n\n        Arguments\n        ---------\n        path : str\n            URI/path to the audio to transcribe. When\n            ``use_torchaudio_streaming`` is ``False``, uses SB fetching to allow\n            fetching from HF or a local file. When ``True``, resolves the URI\n            through ffmpeg, as documented in\n            :class:`torchaudio.io.StreamReader`.\n        task : Optional[str]\n            The task to perform. If None, the default task is the one passed in the Whisper model.\n            It can be one of the following: ``transcribe``, ``translate``, ``lang_id``.\n        initial_prompt : Optional[str]\n            The initial prompt to condition the model on.\n        logprob_threshold : Optional[float]\n            The log probability threshold to continue decoding the current segment.\n        no_speech_threshold : float\n            The threshold to skip decoding segment if the no_speech_prob is higher than this value.\n        condition_on_previous_text : bool\n            If True, the model will be condition on the last 224 tokens.\n        verbose : bool\n            If True, print the details of each segment.\n        use_torchaudio_streaming : bool\n            Whether the audio file can be loaded in a streaming fashion. If not,\n            transcription is still performed through chunks of audio, but the\n            entire audio file is fetched and loaded at once.\n            This skips the usual fetching method and instead resolves the URI\n            using torchaudio (via ffmpeg).\n        chunk_size : Optional[int]\n            The size of the chunks to split the audio into. The default\n            chunk size is 30 seconds which corresponds to the maximal length\n            that the model can process in one go.\n        **kwargs : dict\n            Arguments forwarded to ``load_audio``\n\n        Returns\n        -------\n        results : list\n            A list of ``WhisperASRChunk`` objects, each containing the task result.", "file": "speechbrain_inference_ASR.py", "function": "WhisperASR.transcribe_file", "project": "speechbrain", "change_stats": {"code_changes": 48, "doc_changes": 43, "total_changes": 91}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_dpapi_secret_valid(self, dpapiSecretID):\n        \n        \n        \n        cur = self.conn.cursor()\n        cur.execute('SELECT * FROM dpapi_secrets WHERE id=? LIMIT 1', [dpapiSecretID])\n        results = cur.fetchall()\n        cur.close()\n\n        logging.debug('is_dpapi_secret_valid(groupID={}) => {}'.format(dpapiSecretID, True if len(results) else False))\n        return len(results) > 0```\n\nUpdated code:\n```python\ndef is_dpapi_secret_valid(self, dpapi_secret_id):\n        \n        \n        \n        q = select(self.DpapiSecrets).filter(\n            func.lower(self.DpapiSecrets.c.id) == dpapi_secret_id\n        )\n        results = asyncio.run(self.conn.execute(q)).first()\n        valid = True if results is not None else False\n        logging.debug(f\"is_dpapi_secret_valid(groupID={dpapi_secret_id}) => {valid}\")\n        return valid\n```\n\nOriginal documentation:\n```python\nCheck if this group ID is valid.\n        :dpapiSecretID is a primary id\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if this group ID is valid.\n        :dpapi_secret_id is a primary id", "file": "cme_protocols_smb_database.py", "function": "database.is_dpapi_secret_valid", "project": "CrackMapExec", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_deploy_single_package():\n     \n    \n    c = TestClient()\n    c.save({\"conanfile.py\": GenConanfile(\"pkg\", \"1.0\").with_package_file(\"include/hi.h\", \"hi\"),\n            \"consumer/conanfile.txt\": \"[requires]\\npkg/1.0\"})\n    c.run(\"create .\")\n\n    # if we deploy one --requires, we get that package\n    c.run(\"install  --requires=pkg/1.0 --deploy=direct_deploy --output-folder=output\")\n    header = c.load(\"output/pkg/include/hi.h\")\n    assert \"hi\" in header\n\n    # If we deploy a local conanfile.txt, we get deployed its direct dependencies\n    c.run(\"install consumer/conanfile.txt --deploy=direct_deploy --output-folder=output2\")\n    header = c.load(\"output2/pkg/include/hi.h\")\n    assert \"hi\" in header```\n\nUpdated code:\n```python\ndef test_deploy_single_package():\n     \n    \n    c = TestClient()\n    c.save({\"conanfile.py\": GenConanfile(\"pkg\", \"1.0\").with_package_file(\"include/hi.h\", \"hi\"),\n            \"consumer/conanfile.txt\": \"[requires]\\npkg/1.0\"})\n    c.run(\"create .\")\n\n    # if we deploy one --requires, we get that package\n    c.run(\"install  --requires=pkg/1.0 --deploy=direct_deploy --output-folder=output\")\n    header = c.load(\"output/direct_deploy/pkg/include/hi.h\")\n    assert \"hi\" in header\n\n    # If we deploy a local conanfile.txt, we get deployed its direct dependencies\n    c.run(\"install consumer/conanfile.txt --deploy=direct_deploy --output-folder=output2\")\n    header = c.load(\"output2/direct_deploy/pkg/include/hi.h\")\n    assert \"hi\" in header\n```\n\nOriginal documentation:\n```python\nLets try a deploy that executes on a single package reference\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Let's try a deploy that executes on a single package reference", "file": "conans_test_functional_command_test_install_deploy.py", "function": "test_deploy_single_package", "project": "conan", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_worker(self, worker_id, worker_type, worker_info):\n        \n        \n        worker_data = gcs_utils.WorkerTableData()\n        worker_data.is_alive = True\n        worker_data.worker_address.worker_id = worker_id\n        worker_data.worker_type = worker_type\n        for k, v in worker_info.items():\n            worker_data.worker_info[k] = bytes(v, encoding=\"utf-8\")\n        return self.global_state_accessor.add_worker_info(\n            worker_data.SerializeToString()\n        )```\n\nUpdated code:\n```python\ndef add_worker(self, worker_id, worker_type, worker_info):\n        \n        \n        worker_data = gcs_pb2.WorkerTableData()\n        worker_data.is_alive = True\n        worker_data.worker_address.worker_id = worker_id\n        worker_data.worker_type = worker_type\n        for k, v in worker_info.items():\n            worker_data.worker_info[k] = bytes(v, encoding=\"utf-8\")\n        return self.global_state_accessor.add_worker_info(\n            worker_data.SerializeToString()\n        )\n```\n\nOriginal documentation:\n```python\nAdd a worker to the cluster.\n\n        Args:\n            worker_id: ID of this worker. Type is bytes.\n            worker_type: Type of this worker. Value is gcs_utils.DRIVER or\n                gcs_utils.WORKER.\n            worker_info: Info of this worker. Type is dict{str: str}.\n\n        Returns:\n             Is operation success\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add a worker to the cluster.\n\n        Args:\n            worker_id: ID of this worker. Type is bytes.\n            worker_type: Type of this worker. Value is common_pb2.DRIVER or\n                common_pb2.WORKER.\n            worker_info: Info of this worker. Type is dict{str: str}.\n\n        Returns:\n             Is operation success", "file": "python_ray__private_state.py", "function": "GlobalState.add_worker", "project": "ray", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef evaluate_spearman(self, embedding):\n        \n\n        \n        predicted_scores = []\n        expected_scores = []\n        skipped = 0\n        count = 0\n        vocab_trie = self.create_vocab_trie(embedding)\n        for (word_1, word_2), expected_score in self.scores.items():\n            try:\n                predicted_score = self.score_function(embedding, vocab_trie, word_1, word_2)\n            except ValueError:\n                skipped += 1\n                continue\n            count += 1\n            predicted_scores.append(predicted_score)\n            expected_scores.append(expected_score)\n        print('Skipped pairs: %d out of %d' % (skipped, len(self.scores)))\n        spearman = spearmanr(expected_scores, predicted_scores)\n        return spearman.correlation```\n\nUpdated code:\n```python\ndef evaluate_spearman(self, embedding):\n        \n\n        \n        predicted_scores = []\n        expected_scores = []\n        skipped = 0\n        count = 0\n        vocab_trie = self.create_vocab_trie(embedding)\n        for (word_1, word_2), expected_score in self.scores.items():\n            try:\n                predicted_score = self.score_function(embedding, vocab_trie, word_1, word_2)\n            except ValueError:\n                skipped += 1\n                continue\n            count += 1\n            predicted_scores.append(predicted_score)\n            expected_scores.append(expected_score)\n        logger.info('skipped pairs: %d out of %d' % (skipped, len(self.scores)))\n        spearman = spearmanr(expected_scores, predicted_scores)\n        return spearman.correlation\n```\n\nOriginal documentation:\n```python\nEvaluate spearman scores for lexical entailment for given embedding.\n\n        Parameters\n        ----------\n        embedding : PoincareKeyedVectors instance\n            Embedding for which evaluation is to be done.\n\n        Returns\n        -------\n        float\n            Spearman correlation score for the task for input embedding.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Evaluate spearman scores for lexical entailment for given embedding.\n\n        Parameters\n        ----------\n        embedding : :class:`~gensim.models.poincare.PoincareKeyedVectors`\n            Embedding for which evaluation is to be done.\n\n        Returns\n        -------\n        float\n            Spearman correlation score for the task for input embedding.", "file": "gensim_models_poincare.py", "function": "LexicalEntailmentEvaluation.evaluate_spearman", "project": "gensim", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_last_dependency(self):\n        \n        \n        \n        # Load graph\n        loader = MigrationLoader(connection)\n        # Make state\n        before = self.make_project_state([])\n        after = self.make_project_state([self.book_migrations_fk])\n        after.real_apps = [\"migrations\"]\n        autodetector = MigrationAutodetector(before, after)\n        changes = autodetector._detect_changes(graph=loader.graph)\n        # Right number of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"Book\")\n        # Right dependencies?\n        self.assertEqual(changes['otherapp'][0].dependencies, [(\"migrations\", \"__latest__\")])```\n\nUpdated code:\n```python\ndef test_last_dependency(self):\n        \n        \n        \n        # Load graph\n        loader = MigrationLoader(connection)\n        # Make state\n        before = self.make_project_state([])\n        after = self.make_project_state([self.book_migrations_fk])\n        after.real_apps = [\"migrations\"]\n        autodetector = MigrationAutodetector(before, after)\n        changes = autodetector._detect_changes(graph=loader.graph)\n        # Right number of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"Book\")\n        # Right dependencies?\n        self.assertEqual(changes['otherapp'][0].dependencies, [(\"migrations\", \"0002_second\")])\n```\n\nOriginal documentation:\n```python\nTests that a dependency to an app with existing migrations uses __latest__.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tests that a dependency to an app with existing migrations uses the\n        last migration of that app.", "file": "tests_migrations_test_autodetector.py", "function": "AutodetectorTests.test_last_dependency", "project": "django", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef vat_id(self) -> str:\n        \n        \n        \n        return self.bothify(self.random_element(self.vat_id_formats))```\n\nUpdated code:\n```python\ndef vat_id(self) -> str:\n\n        vat_id_random_section = (\n            '#######'\n        )\n\n        vat_id_possible_initial_numbers = (\n            '0',\n            '1'\n        )\n        \n        \n        \n        generated_initial_number = self.random_element(vat_id_possible_initial_numbers)\n        vat_without_check = self.bothify(generated_initial_number + vat_id_random_section)\n        vat_as_int = int(vat_without_check)\n        vat_check = 97 - (vat_as_int % 97)\n        vat_check_str = f\"{vat_check:0>2}\"\n\n        return \"BE\" + vat_without_check + vat_check_str\n```\n\nOriginal documentation:\n```python\nhttp://ec.europa.eu/taxation_customs/vies/faq.html#item_11\n        :return: A random Belgian VAT ID\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "http://ec.europa.eu/taxation_customs/vies/faq.html#item_11\n        https://en.wikipedia.org/wiki/VAT_identification_number\n        :return: A random Belgian VAT ID starting with 0 or 1 and has a correct checksum with a modulo 97 check", "file": "faker_providers_ssn_nl_BE___init__.py", "function": "Provider.vat_id", "project": "faker", "change_stats": {"code_changes": 19, "doc_changes": 5, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef metrics(self) -> _METRICS:\n        \n        on_step = not self._epoch_end_reached\n        assert self.trainer._results is not None\n        return self.trainer._results.metrics(on_step)```\n\nUpdated code:\n```python\ndef metrics(self) -> _METRICS:\n        \n        on_step = self._first_loop_iter is not None\n        assert self.trainer._results is not None\n        return self.trainer._results.metrics(on_step)\n```\n\nOriginal documentation:\n```python\nThis function returns either batch or epoch metrics depending on ``_epoch_end_reached``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This function returns either batch or epoch metrics.", "file": "src_lightning_pytorch_trainer_connectors_logger_connector_logger_connector.py", "function": "_LoggerConnector.metrics", "project": "pytorch-lightning", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef futures_foreign_commodity_realtime(subscribe_list: list) -> pd.DataFrame:\n    \n    \n    \n    payload = \"&list=\" + \",\".join([\"hf_\" + item for item in subscribe_list])\n    prefix = f\"rn={round(time.time() * 1000)}\"\n    url = \"http://hq.sinajs.cn/\" + prefix + payload\n    r = requests.get(url)\n    data_text = r.text\n    data_df = pd.DataFrame(\n        [\n            item.strip().split(\"=\")[1].split(\",\")\n            for item in data_text.split(\";\")\n            if item.strip() != \"\"\n        ]\n    )\n    data_df.iloc[:, 0] = data_df.iloc[:, 0].str.replace('\"', \"\")\n    data_df.iloc[:, -1] = data_df.iloc[:, -1].str.replace('\"', \"\")\n    data_df.columns = [\n        \"current_price\",\n        \"-\",\n        \"bid\",\n        \"ask\",\n        \"high\",\n        \"low\",\n        \"time\",\n        \"last_settle_price\",\n        \"open\",\n        \"hold\",\n        \"-\",\n        \"-\",\n        \"date\",\n        \"symbol\",\n        \"current_price_rmb\",\n    ]\n    data_df[\"symbol\"] = [\n        \"NYBOT-\u68c9\u82b1\",\n        \"LME\u954d3\u4e2a\u6708\",\n        \"LME\u94c53\u4e2a\u6708\",\n        \"LME\u95213\u4e2a\u6708\",\n        \"LME\u950c3\u4e2a\u6708\",\n        \"LME\u94dd3\u4e2a\u6708\",\n        \"LME\u94dc3\u4e2a\u6708\",\n        \"CBOT-\u9ec4\u8c46\",\n        \"CBOT-\u5c0f\u9ea6\",\n        \"CBOT-\u7389\u7c73\",\n        \"CBOT-\u9ec4\u8c46\u6cb9\",\n        \"CBOT-\u9ec4\u8c46\u7c89\",\n        \"\u65e5\u672c\u6a61\u80f6\",\n        \"COMEX\u94dc\",\n        \"NYMEX\u5929\u7136\u6c14\",\n        \"NYMEX\u539f\u6cb9\",\n        \"COMEX\u767d\u94f6\",\n        \"COMEX\u9ec4\u91d1\",\n        \"CME-\u7626\u8089\u732a\",\n        \"\u5e03\u4f26\u7279\u539f\u6cb9\",\n        \"\u4f26\u6566\u91d1\",\n        \"\u4f26\u6566\u94f6\",\n        \"\u4f26\u6566\u94c2\u91d1\",\n        \"\u4f26\u6566\u94af\u91d1\",\n        \"\u9a6c\u68d5\u6cb9\",\n        \"\u6b27\u6d32\u78b3\u6392\u653e\",\n    ]\n    data_df = data_df[\n        [\n            \"symbol\",\n            \"current_price\",\n            \"current_price_rmb\",\n            \"bid\",\n            \"ask\",\n            \"high\",\n            \"low\",\n            \"time\",\n            \"last_settle_price\",\n            \"open\",\n            \"hold\",\n            \"date\",\n        ]\n    ]\n    data_df.columns = [\n        \"\u540d\u79f0\",\n        \"\u6700\u65b0\u4ef7\",\n        \"\u4eba\u6c11\u5e01\u62a5\u4ef7\",\n        \"\u4e70\u4ef7\",\n        \"\u5356\u4ef7\",\n        \"\u6700\u9ad8\u4ef7\",\n        \"\u6700\u4f4e\u4ef7\",\n        \"\u884c\u60c5\u65f6\u95f4\",\n        \"\u6628\u65e5\u7ed3\u7b97\u4ef7\",\n        \"\u5f00\u76d8\u4ef7\",\n        \"\u6301\u4ed3\u91cf\",\n        \"\u65e5\u671f\",\n    ]\n    data_df.dropna(how=\"all\", inplace=True)\n    data_df[\"\u6700\u65b0\u4ef7\"] = pd.to_numeric(data_df[\"\u6700\u65b0\u4ef7\"])\n    data_df[\"\u4eba\u6c11\u5e01\u62a5\u4ef7\"] = pd.to_numeric(data_df[\"\u4eba\u6c11\u5e01\u62a5\u4ef7\"])\n    data_df[\"\u4e70\u4ef7\"] = pd.to_numeric(data_df[\"\u4e70\u4ef7\"])\n    data_df[\"\u5356\u4ef7\"] = pd.to_numeric(data_df[\"\u5356\u4ef7\"])\n    data_df[\"\u6700\u9ad8\u4ef7\"] = pd.to_numeric(data_df[\"\u6700\u9ad8\u4ef7\"])\n    data_df[\"\u6700\u4f4e\u4ef7\"] = pd.to_numeric(data_df[\"\u6700\u4f4e\u4ef7\"])\n    data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"] = pd.to_numeric(data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"])\n    data_df[\"\u5f00\u76d8\u4ef7\"] = pd.to_numeric(data_df[\"\u5f00\u76d8\u4ef7\"])\n    data_df[\"\u6301\u4ed3\u91cf\"] = pd.to_numeric(data_df[\"\u6301\u4ed3\u91cf\"])\n    data_df[\"\u6da8\u8dcc\u989d\"] = data_df[\"\u6700\u65b0\u4ef7\"] - data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"]\n    data_df[\"\u6da8\u8dcc\u5e45\"] = (data_df[\"\u6700\u65b0\u4ef7\"] - data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"]) / data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"] * 100\n    data_df = data_df[\n        [\n            \"\u540d\u79f0\",\n            \"\u6700\u65b0\u4ef7\",\n            \"\u4eba\u6c11\u5e01\u62a5\u4ef7\",\n            \"\u6da8\u8dcc\u989d\",\n            \"\u6da8\u8dcc\u5e45\",\n            \"\u5f00\u76d8\u4ef7\",\n            \"\u6700\u9ad8\u4ef7\",\n            \"\u6700\u4f4e\u4ef7\",\n            \"\u6628\u65e5\u7ed3\u7b97\u4ef7\",\n            \"\u6301\u4ed3\u91cf\",\n            \"\u4e70\u4ef7\",\n            \"\u5356\u4ef7\",\n            \"\u884c\u60c5\u65f6\u95f4\",\n            \"\u65e5\u671f\",\n        ]\n    ]\n\n    # \u83b7\u53d6\u8f6c\u6362\u6bd4\u4f8b\u6570\u636e\n    url = \"https://finance.sina.com.cn/money/future/hf.html\"\n    r = requests.get(url)\n    r.encoding = \"gb2312\"\n    soup = BeautifulSoup(r.text, \"lxml\")\n    data_text = soup.find_all(\"script\", attrs={\"type\": \"text/javascript\"})[\n        -2\n    ].string.strip()\n    raw_text = data_text[data_text.find(\"oHF_1 = \") : data_text.find(\"oHF_2\")]\n    need_text = raw_text[raw_text.find(\"{\") : raw_text.rfind(\"}\") + 1]\n    data_json = demjson.decode(need_text)\n    price_mul = pd.DataFrame(\n        [\n            [item[0] for item in data_json.values()],\n            [item[1][0] for item in data_json.values()],\n        ]\n    ).T\n    price_mul.columns = [\"symbol\", \"price\"]\n\n    # \u83b7\u53d6\u6c47\u7387\u6570\u636e\n    url = \"https://hq.sinajs.cn/?list=USDCNY\"\n    r = requests.get(url)\n    data_text = r.text\n    usd_rmb = float(\n        data_text[data_text.find('\"') + 1 : data_text.find(\",\u7f8e\u5143\u4eba\u6c11\u5e01\")].split(\",\")[-1]\n    )\n\n    # \u8ba1\u7b97\u4eba\u6c11\u5e01\u62a5\u4ef7\n    data_df[\"\u4eba\u6c11\u5e01\u62a5\u4ef7\"] = data_df[\"\u6700\u65b0\u4ef7\"] * price_mul[\"price\"] * usd_rmb\n\n    data_df.dropna(thresh=4, inplace=True)\n    return data_df```\n\nUpdated code:\n```python\ndef futures_foreign_commodity_realtime(subscribe_list: list) -> pd.DataFrame:\n    \n    \n    \n    payload = \"?list=\" + \",\".join([\"hf_\" + item for item in subscribe_list])\n    url = \"http://hq.sinajs.cn/\" + payload\n    r = requests.get(url)\n    data_text = r.text\n    data_df = pd.DataFrame(\n        [\n            item.strip().split(\"=\")[1].split(\",\")\n            for item in data_text.split(\";\")\n            if item.strip() != \"\"\n        ]\n    )\n    data_df.iloc[:, 0] = data_df.iloc[:, 0].str.replace('\"', \"\")\n    data_df.iloc[:, -1] = data_df.iloc[:, -1].str.replace('\"', \"\")\n    data_df.columns = [\n        \"current_price\",\n        \"-\",\n        \"bid\",\n        \"ask\",\n        \"high\",\n        \"low\",\n        \"time\",\n        \"last_settle_price\",\n        \"open\",\n        \"hold\",\n        \"-\",\n        \"-\",\n        \"date\",\n        \"symbol\",\n        \"current_price_rmb\",\n    ]\n    temp_symbol_code_df = futures_hq_subscribe_exchange_symbol()\n    temp_symbol_code_dict = dict(zip(temp_symbol_code_df['code'], temp_symbol_code_df['symbol']))\n    data_df[\"symbol\"] = [temp_symbol_code_dict[subscribe] for subscribe in subscribe_list]\n    data_df = data_df[\n        [\n            \"symbol\",\n            \"current_price\",\n            \"current_price_rmb\",\n            \"bid\",\n            \"ask\",\n            \"high\",\n            \"low\",\n            \"time\",\n            \"last_settle_price\",\n            \"open\",\n            \"hold\",\n            \"date\",\n        ]\n    ]\n    data_df.columns = [\n        \"\u540d\u79f0\",\n        \"\u6700\u65b0\u4ef7\",\n        \"\u4eba\u6c11\u5e01\u62a5\u4ef7\",\n        \"\u4e70\u4ef7\",\n        \"\u5356\u4ef7\",\n        \"\u6700\u9ad8\u4ef7\",\n        \"\u6700\u4f4e\u4ef7\",\n        \"\u884c\u60c5\u65f6\u95f4\",\n        \"\u6628\u65e5\u7ed3\u7b97\u4ef7\",\n        \"\u5f00\u76d8\u4ef7\",\n        \"\u6301\u4ed3\u91cf\",\n        \"\u65e5\u671f\",\n    ]\n    data_df.dropna(how=\"all\", inplace=True)\n    data_df[\"\u6700\u65b0\u4ef7\"] = pd.to_numeric(data_df[\"\u6700\u65b0\u4ef7\"])\n    data_df[\"\u4eba\u6c11\u5e01\u62a5\u4ef7\"] = pd.to_numeric(data_df[\"\u4eba\u6c11\u5e01\u62a5\u4ef7\"])\n    data_df[\"\u4e70\u4ef7\"] = pd.to_numeric(data_df[\"\u4e70\u4ef7\"])\n    data_df[\"\u5356\u4ef7\"] = pd.to_numeric(data_df[\"\u5356\u4ef7\"])\n    data_df[\"\u6700\u9ad8\u4ef7\"] = pd.to_numeric(data_df[\"\u6700\u9ad8\u4ef7\"])\n    data_df[\"\u6700\u4f4e\u4ef7\"] = pd.to_numeric(data_df[\"\u6700\u4f4e\u4ef7\"])\n    data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"] = pd.to_numeric(data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"])\n    data_df[\"\u5f00\u76d8\u4ef7\"] = pd.to_numeric(data_df[\"\u5f00\u76d8\u4ef7\"])\n    data_df[\"\u6301\u4ed3\u91cf\"] = pd.to_numeric(data_df[\"\u6301\u4ed3\u91cf\"])\n    data_df[\"\u6da8\u8dcc\u989d\"] = data_df[\"\u6700\u65b0\u4ef7\"] - data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"]\n    data_df[\"\u6da8\u8dcc\u5e45\"] = (data_df[\"\u6700\u65b0\u4ef7\"] - data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"]) / data_df[\"\u6628\u65e5\u7ed3\u7b97\u4ef7\"] * 100\n    data_df = data_df[\n        [\n            \"\u540d\u79f0\",\n            \"\u6700\u65b0\u4ef7\",\n            \"\u4eba\u6c11\u5e01\u62a5\u4ef7\",\n            \"\u6da8\u8dcc\u989d\",\n            \"\u6da8\u8dcc\u5e45\",\n            \"\u5f00\u76d8\u4ef7\",\n            \"\u6700\u9ad8\u4ef7\",\n            \"\u6700\u4f4e\u4ef7\",\n            \"\u6628\u65e5\u7ed3\u7b97\u4ef7\",\n            \"\u6301\u4ed3\u91cf\",\n            \"\u4e70\u4ef7\",\n            \"\u5356\u4ef7\",\n            \"\u884c\u60c5\u65f6\u95f4\",\n            \"\u65e5\u671f\",\n        ]\n    ]\n\n    # \u83b7\u53d6\u8f6c\u6362\u6bd4\u4f8b\u6570\u636e\n    url = \"https://finance.sina.com.cn/money/future/hf.html\"\n    r = requests.get(url)\n    r.encoding = \"gb2312\"\n    soup = BeautifulSoup(r.text, \"lxml\")\n    data_text = soup.find_all(\"script\", attrs={\"type\": \"text/javascript\"})[\n        -2\n    ].string.strip()\n    raw_text = data_text[data_text.find(\"oHF_1 = \"): data_text.find(\"oHF_2\")]\n    need_text = raw_text[raw_text.find(\"{\"): raw_text.rfind(\"}\") + 1]\n    data_json = demjson.decode(need_text)\n    price_mul = pd.DataFrame(\n        [\n            [item[0] for item in data_json.values()],\n            [item[1][0] for item in data_json.values()],\n        ]\n    ).T\n    price_mul.columns = [\"symbol\", \"price\"]\n\n    # \u83b7\u53d6\u6c47\u7387\u6570\u636e\n    url = \"https://hq.sinajs.cn/?list=USDCNY\"\n    r = requests.get(url)\n    data_text = r.text\n    usd_rmb = float(\n        data_text[data_text.find('\"') + 1: data_text.find(\",\u7f8e\u5143\u4eba\u6c11\u5e01\")].split(\",\")[-1]\n    )\n\n    # \u8ba1\u7b97\u4eba\u6c11\u5e01\u62a5\u4ef7\n    data_df[\"\u4eba\u6c11\u5e01\u62a5\u4ef7\"] = data_df[\"\u6700\u65b0\u4ef7\"] * price_mul[\"price\"] * usd_rmb\n    data_df.dropna(thresh=4, inplace=True)\n    return data_df\n```\n\nOriginal documentation:\n```python\n\u65b0\u6d6a-\u5916\u76d8\u671f\u8d27-\u884c\u60c5\u6570\u636e\n    https://finance.sina.com.cn/money/future/hf.html\n    :param subscribe_list: \u901a\u8fc7\u8c03\u7528 ak.hq_subscribe_exchange_symbol \u51fd\u6570\u6765\u83b7\u53d6\n    :type subscribe_list: list\n    :return: \u884c\u60c5\u6570\u636e\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u65b0\u6d6a-\u5916\u76d8\u671f\u8d27-\u884c\u60c5\u6570\u636e\n    https://finance.sina.com.cn/money/future/hf.html\n    :param subscribe_list: \u901a\u8fc7\u8c03\u7528 ak.futures_hq_subscribe_exchange_symbol \u51fd\u6570\u6765\u83b7\u53d6\n    :type subscribe_list: list\n    :return: \u884c\u60c5\u6570\u636e\n    :rtype: pandas.DataFrame", "file": "akshare_futures_futures_hq_sina.py", "function": "futures_foreign_commodity_realtime", "project": "akshare", "change_stats": {"code_changes": 45, "doc_changes": 4, "total_changes": 49}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef populate_any_indicators(\n        self, metadata, pair, df, tf, informative=None, coin=\"\", set_generalized_indicators=False\n    ):\n        \n        \n        \n\n        with self.freqai.lock:\n            if informative is None:\n                informative = self.dp.get_pair_dataframe(pair, tf)\n\n            # first loop is automatically duplicating indicators for time periods\n            for t in self.freqai_info[\"feature_parameters\"][\"indicator_periods_candles\"]:\n\n                t = int(t)\n                informative[f\"%-{coin}rsi-period_{t}\"] = ta.RSI(informative, timeperiod=t)\n                informative[f\"%-{coin}mfi-period_{t}\"] = ta.MFI(informative, timeperiod=t)\n                informative[f\"%-{coin}adx-period_{t}\"] = ta.ADX(informative, window=t)\n                informative[f\"{coin}20sma-period_{t}\"] = ta.SMA(informative, timeperiod=t)\n                informative[f\"{coin}21ema-period_{t}\"] = ta.EMA(informative, timeperiod=t)\n                informative[f\"%-{coin}close_over_20sma-period_{t}\"] = (\n                    informative[\"close\"] / informative[f\"{coin}20sma-period_{t}\"]\n                )\n\n                informative[f\"%-{coin}mfi-period_{t}\"] = ta.MFI(informative, timeperiod=t)\n\n                bollinger = qtpylib.bollinger_bands(\n                    qtpylib.typical_price(informative), window=t, stds=2.2\n                )\n                informative[f\"{coin}bb_lowerband-period_{t}\"] = bollinger[\"lower\"]\n                informative[f\"{coin}bb_middleband-period_{t}\"] = bollinger[\"mid\"]\n                informative[f\"{coin}bb_upperband-period_{t}\"] = bollinger[\"upper\"]\n\n                informative[f\"%-{coin}bb_width-period_{t}\"] = (\n                    informative[f\"{coin}bb_upperband-period_{t}\"]\n                    - informative[f\"{coin}bb_lowerband-period_{t}\"]\n                ) / informative[f\"{coin}bb_middleband-period_{t}\"]\n                informative[f\"%-{coin}close-bb_lower-period_{t}\"] = (\n                    informative[\"close\"] / informative[f\"{coin}bb_lowerband-period_{t}\"]\n                )\n\n                informative[f\"%-{coin}roc-period_{t}\"] = ta.ROC(informative, timeperiod=t)\n\n                informative[f\"%-{coin}relative_volume-period_{t}\"] = (\n                    informative[\"volume\"] / informative[\"volume\"].rolling(t).mean()\n                )\n\n            informative[f\"%-{coin}pct-change\"] = informative[\"close\"].pct_change()\n            informative[f\"%-{coin}raw_volume\"] = informative[\"volume\"]\n            informative[f\"%-{coin}raw_price\"] = informative[\"close\"]\n\n            indicators = [col for col in informative if col.startswith(\"%\")]\n            # This loop duplicates and shifts all indicators to add a sense of recency to data\n            for n in range(self.freqai_info[\"feature_parameters\"][\"include_shifted_candles\"] + 1):\n                if n == 0:\n                    continue\n                informative_shift = informative[indicators].shift(n)\n                informative_shift = informative_shift.add_suffix(\"_shift-\" + str(n))\n                informative = pd.concat((informative, informative_shift), axis=1)\n\n            df = merge_informative_pair(df, informative, self.config[\"timeframe\"], tf, ffill=True)\n            skip_columns = [\n                (s + \"_\" + tf) for s in [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n            ]\n            df = df.drop(columns=skip_columns)\n\n            # Add generalized indicators here (because in live, it will call this\n            # function to populate indicators during training). Notice how we ensure not to\n            # add them multiple times\n            if set_generalized_indicators:\n                df[\"%-day_of_week\"] = (df[\"date\"].dt.dayofweek + 1) / 7\n                df[\"%-hour_of_day\"] = (df[\"date\"].dt.hour + 1) / 25\n\n                # user adds targets here by prepending them with &- (see convention below)\n                # If user wishes to use multiple targets, a multioutput prediction model\n                # needs to be used such as templates/CatboostPredictionMultiModel.py\n                df[\"&-s_close\"] = (\n                    df[\"close\"]\n                    .shift(-self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                    .rolling(self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                    .mean()\n                    / df[\"close\"]\n                    - 1\n                )\n\n        return df```\n\nUpdated code:\n```python\ndef populate_any_indicators(\n        self, pair, df, tf, informative=None, set_generalized_indicators=False\n    ):\n        \n        \n        \n\n        coin = pair.split('/')[0]\n\n        with self.freqai.lock:\n            if informative is None:\n                informative = self.dp.get_pair_dataframe(pair, tf)\n\n            # first loop is automatically duplicating indicators for time periods\n            for t in self.freqai_info[\"feature_parameters\"][\"indicator_periods_candles\"]:\n\n                t = int(t)\n                informative[f\"%-{coin}rsi-period_{t}\"] = ta.RSI(informative, timeperiod=t)\n                informative[f\"%-{coin}mfi-period_{t}\"] = ta.MFI(informative, timeperiod=t)\n                informative[f\"%-{coin}adx-period_{t}\"] = ta.ADX(informative, window=t)\n                informative[f\"{coin}sma-period_{t}\"] = ta.SMA(informative, timeperiod=t)\n                informative[f\"{coin}ema-period_{t}\"] = ta.EMA(informative, timeperiod=t)\n\n                informative[f\"%-{coin}mfi-period_{t}\"] = ta.MFI(informative, timeperiod=t)\n\n                bollinger = qtpylib.bollinger_bands(\n                    qtpylib.typical_price(informative), window=t, stds=2.2\n                )\n                informative[f\"{coin}bb_lowerband-period_{t}\"] = bollinger[\"lower\"]\n                informative[f\"{coin}bb_middleband-period_{t}\"] = bollinger[\"mid\"]\n                informative[f\"{coin}bb_upperband-period_{t}\"] = bollinger[\"upper\"]\n\n                informative[f\"%-{coin}bb_width-period_{t}\"] = (\n                    informative[f\"{coin}bb_upperband-period_{t}\"]\n                    - informative[f\"{coin}bb_lowerband-period_{t}\"]\n                ) / informative[f\"{coin}bb_middleband-period_{t}\"]\n                informative[f\"%-{coin}close-bb_lower-period_{t}\"] = (\n                    informative[\"close\"] / informative[f\"{coin}bb_lowerband-period_{t}\"]\n                )\n\n                informative[f\"%-{coin}roc-period_{t}\"] = ta.ROC(informative, timeperiod=t)\n\n                informative[f\"%-{coin}relative_volume-period_{t}\"] = (\n                    informative[\"volume\"] / informative[\"volume\"].rolling(t).mean()\n                )\n\n            informative[f\"%-{coin}pct-change\"] = informative[\"close\"].pct_change()\n            informative[f\"%-{coin}raw_volume\"] = informative[\"volume\"]\n            informative[f\"%-{coin}raw_price\"] = informative[\"close\"]\n\n            indicators = [col for col in informative if col.startswith(\"%\")]\n            # This loop duplicates and shifts all indicators to add a sense of recency to data\n            for n in range(self.freqai_info[\"feature_parameters\"][\"include_shifted_candles\"] + 1):\n                if n == 0:\n                    continue\n                informative_shift = informative[indicators].shift(n)\n                informative_shift = informative_shift.add_suffix(\"_shift-\" + str(n))\n                informative = pd.concat((informative, informative_shift), axis=1)\n\n            df = merge_informative_pair(df, informative, self.config[\"timeframe\"], tf, ffill=True)\n            skip_columns = [\n                (s + \"_\" + tf) for s in [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n            ]\n            df = df.drop(columns=skip_columns)\n\n            # Add generalized indicators here (because in live, it will call this\n            # function to populate indicators during training). Notice how we ensure not to\n            # add them multiple times\n            if set_generalized_indicators:\n                df[\"%-day_of_week\"] = (df[\"date\"].dt.dayofweek + 1) / 7\n                df[\"%-hour_of_day\"] = (df[\"date\"].dt.hour + 1) / 25\n\n                # user adds targets here by prepending them with &- (see convention below)\n                df[\"&-s_close\"] = (\n                    df[\"close\"]\n                    .shift(-self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                    .rolling(self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                    .mean()\n                    / df[\"close\"]\n                    - 1\n                )\n\n                # If user wishes to use multiple targets, they can add more by\n                # appending more columns with '&'. User should keep in mind that multi targets\n                # requires a multioutput prediction model such as\n                # templates/CatboostPredictionMultiModel.py,\n\n                # df[\"&-s_range\"] = (\n                #     df[\"close\"]\n                #     .shift(-self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                #     .rolling(self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                #     .max()\n                #     -\n                #     df[\"close\"]\n                #     .shift(-self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                #     .rolling(self.freqai_info[\"feature_parameters\"][\"label_period_candles\"])\n                #     .min()\n                # )\n\n        return df\n```\n\nOriginal documentation:\n```python\nFunction designed to automatically generate, name and merge features\n        from user indicated timeframes in the configuration file. User controls the indicators\n        passed to the training/prediction by prepending indicators with `'%-' + coin `\n        (see convention below). I.e. user should not prepend any supporting metrics\n        (e.g. bb_lowerband below) with % unless they explicitly want to pass that metric to the\n        model.\n        :param pair: pair to be used as informative\n        :param df: strategy dataframe which will receive merges from informatives\n        :param tf: timeframe of the dataframe which will modify the feature names\n        :param informative: the dataframe associated with the informative pair\n        :param coin: the name of the coin which will modify the feature names.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Function designed to automatically generate, name and merge features\n        from user indicated timeframes in the configuration file. User controls the indicators\n        passed to the training/prediction by prepending indicators with `'%-' + coin `\n        (see convention below). I.e. user should not prepend any supporting metrics\n        (e.g. bb_lowerband below) with % unless they explicitly want to pass that metric to the\n        model.\n        :param pair: pair to be used as informative\n        :param df: strategy dataframe which will receive merges from informatives\n        :param tf: timeframe of the dataframe which will modify the feature names\n        :param informative: the dataframe associated with the informative pair", "file": "freqtrade_templates_FreqaiExampleStrategy.py", "function": "FreqaiExampleStrategy.populate_any_indicators", "project": "freqtrade", "change_stats": {"code_changes": 32, "doc_changes": 3, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef base64(self, format=\"PNG\"):\n\t\t\n\t\t\n\t\t\n\t\tself.calculate()\n\t\tfp = StringIO()\n\t\tself.image.encoderinfo = {}\n\t\tself.image.encoderconfig = ()\n\n\t\tif format.upper() not in Image.SAVE:\n\t\t\tImage.init()\n\t\tsave_handler = Image.SAVE[format.upper()]\n\t\ttry:\n\t\t\tsave_handler(self.image, fp, \"\")\n\t\tfinally:\n\t\t\tfp.seek(0)\n\t\t\treturn f\"data:image/png;base64,{base64.b64encode(fp.read())}\"```\n\nUpdated code:\n```python\ndef base64(self, format=\"PNG\"):\n\t\t\n\t\t\n\t\t\n\t\tself.calculate()\n\n\t\tself.image.encoderinfo = {}\n\t\tself.image.encoderconfig = ()\n\n\t\tbuff = BytesIO()\n\t\tself.image.save(buff, format=format.upper())\n\n\t\treturn f\"data:image/png;base64,{b64encode(buff.getvalue()).decode()}\"\n```\n\nOriginal documentation:\n```python\nusage:  i = Identicon('xx')\n\t\t                print(i.base64())\n\t\treturn: this image's base64 code\n\t\tcreated by: liuzheng712\n\t\tbug report: https://github.com/liuzheng712/identicons/issues\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the identicon's base64\n\n\t\tCreated by: liuzheng712\n\t\tBug report: https://github.com/liuzheng712/identicons/issues", "file": "frappe_utils_identicon.py", "function": "Identicon.base64", "project": "frappe", "change_stats": {"code_changes": 16, "doc_changes": 11, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _create_kinesis_mock_server(self, account_id: str) -> KinesisMockServer:\n        \n        \n        \n        port = get_free_tcp_port()\n        kinesismock_package.install()\n        kinesis_mock_js_path = Path(kinesismock_package.get_installer().get_executable_path())\n\n        # kinesis-mock stores state in json files <account_id>.json, so we can dump everything into `kinesis/`\n        persist_path = os.path.join(config.dirs.data, \"kinesis\")\n        mkdir(persist_path)\n        if config.KINESIS_MOCK_LOG_LEVEL:\n            log_level = config.KINESIS_MOCK_LOG_LEVEL.upper()\n        elif config.LS_LOG:\n            if config.LS_LOG == \"warning\":\n                log_level = \"WARN\"\n            else:\n                log_level = config.LS_LOG.upper()\n        else:\n            log_level = \"INFO\"\n        latency = config.KINESIS_LATENCY + \"ms\"\n        initialize_streams = (\n            config.KINESIS_INITIALIZE_STREAMS if config.KINESIS_INITIALIZE_STREAMS else None\n        )\n\n        server = KinesisMockServer(\n            port=port,\n            js_path=kinesis_mock_js_path,\n            log_level=log_level,\n            latency=latency,\n            initialize_streams=initialize_streams,\n            data_dir=persist_path,\n            account_id=account_id,\n        )\n        return server```\n\nUpdated code:\n```python\ndef _create_kinesis_mock_server(self, account_id: str) -> KinesisMockServer:\n        \n        \n        \n        port = get_free_tcp_port()\n        kinesismock_package.install()\n        kinesis_mock_js_path = Path(kinesismock_package.get_installer().get_executable_path())\n\n        # kinesis-mock stores state in json files <account_id>.json, so we can dump everything into `kinesis/`\n        persist_path = os.path.join(config.dirs.data, \"kinesis\")\n        mkdir(persist_path)\n        if config.KINESIS_MOCK_LOG_LEVEL:\n            log_level = config.KINESIS_MOCK_LOG_LEVEL.upper()\n        elif config.LS_LOG:\n            if config.LS_LOG == \"warning\":\n                log_level = \"WARN\"\n            else:\n                log_level = config.LS_LOG.upper()\n        else:\n            log_level = \"INFO\"\n        latency = config.KINESIS_LATENCY + \"ms\"\n\n        server = KinesisMockServer(\n            port=port,\n            js_path=kinesis_mock_js_path,\n            log_level=log_level,\n            latency=latency,\n            data_dir=persist_path,\n            account_id=account_id,\n        )\n        return server\n```\n\nOriginal documentation:\n```python\nCreates a new Kinesis Mock server instance. Installs Kinesis Mock on the host first if necessary.\n        Introspects on the host config to determine server configuration:\n        config.dirs.data -> if set, the server runs with persistence using the path to store data\n        config.LS_LOG -> configure kinesis mock log level (defaults to INFO)\n        config.KINESIS_LATENCY -> configure stream latency (in milliseconds)\n        config.KINESIS_INITIALIZE_STREAMS -> Initialize the given streams on startup\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates a new Kinesis Mock server instance. Installs Kinesis Mock on the host first if necessary.\n        Introspects on the host config to determine server configuration:\n        config.dirs.data -> if set, the server runs with persistence using the path to store data\n        config.LS_LOG -> configure kinesis mock log level (defaults to INFO)\n        config.KINESIS_LATENCY -> configure stream latency (in milliseconds)", "file": "localstack_services_kinesis_kinesis_mock_server.py", "function": "KinesisServerManager._create_kinesis_mock_server", "project": "localstack", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _update_ticks(self, renderer):\n        \n        \n        \n\n        interval = self.get_view_interval()\n        tick_tups = list(self.iter_ticks())  # iter_ticks calls the locator\n        if self._smart_bounds and tick_tups:\n            # handle inverted limits\n            view_low, view_high = sorted(interval)\n            data_low, data_high = sorted(self.get_data_interval())\n            locs = np.sort([ti[1] for ti in tick_tups])\n            if data_low <= view_low:\n                # data extends beyond view, take view as limit\n                ilow = view_low\n            else:\n                # data stops within view, take best tick\n                good_locs = locs[locs <= data_low]\n                if len(good_locs):\n                    # last tick prior or equal to first data point\n                    ilow = good_locs[-1]\n                else:\n                    # No ticks (why not?), take first tick\n                    ilow = locs[0]\n            if data_high >= view_high:\n                # data extends beyond view, take view as limit\n                ihigh = view_high\n            else:\n                # data stops within view, take best tick\n                good_locs = locs[locs >= data_high]\n                if len(good_locs):\n                    # first tick after or equal to last data point\n                    ihigh = good_locs[0]\n                else:\n                    # No ticks (why not?), take last tick\n                    ihigh = locs[-1]\n            tick_tups = [ti for ti in tick_tups if ilow <= ti[1] <= ihigh]\n\n        if interval[1] <= interval[0]:\n            interval = interval[1], interval[0]\n        inter = self.get_transform().transform(interval)\n\n        ticks_to_draw = []\n        for tick, loc, label in tick_tups:\n            # draw each tick if it is in interval.   Note the transform\n            # to pixel space to take care of log transforms etc.\n            # interval_contains has a floating point tolerance.\n            if tick is None:\n                continue\n            # NB: always update labels and position to avoid issues like #9397\n            tick.update_position(loc)\n            tick.set_label1(label)\n            tick.set_label2(label)\n            try:\n                loct = self.get_transform().transform(loc)\n            except AssertionError:\n                # transforms.transform doesn't allow masked values but\n                # some scales might make them, so we need this try/except.\n                loct = None\n                continue\n            if not mtransforms._interval_contains_close(inter, loct):\n                continue\n            ticks_to_draw.append(tick)\n\n        return ticks_to_draw```\n\nUpdated code:\n```python\ndef _update_ticks(self):\n        \n        \n        \n\n        major_locs = self.major.locator()\n        major_ticks = self.get_major_ticks(len(major_locs))\n        self.major.formatter.set_locs(major_locs)\n        major_labels = self.major.formatter.format_ticks(major_locs)\n        for tick, loc, label in zip(major_ticks, major_locs, major_labels):\n            tick.update_position(loc)\n            tick.set_label1(label)\n            tick.set_label2(label)\n        minor_locs = self.minor.locator()\n        minor_ticks = self.get_minor_ticks(len(minor_locs))\n        self.minor.formatter.set_locs(minor_locs)\n        minor_labels = self.minor.formatter.format_ticks(minor_locs)\n        for tick, loc, label in zip(minor_ticks, minor_locs, minor_labels):\n            tick.update_position(loc)\n            tick.set_label1(label)\n            tick.set_label2(label)\n        ticks = [*major_ticks, *minor_ticks]\n\n        view_low, view_high = self.get_view_interval()\n        if view_low > view_high:\n            view_low, view_high = view_high, view_low\n\n        if self._smart_bounds and ticks:\n            # handle inverted limits\n            data_low, data_high = sorted(self.get_data_interval())\n            locs = np.sort([tick.get_loc() for tick in ticks])\n            if data_low <= view_low:\n                # data extends beyond view, take view as limit\n                ilow = view_low\n            else:\n                # data stops within view, take best tick\n                good_locs = locs[locs <= data_low]\n                if len(good_locs):\n                    # last tick prior or equal to first data point\n                    ilow = good_locs[-1]\n                else:\n                    # No ticks (why not?), take first tick\n                    ilow = locs[0]\n            if data_high >= view_high:\n                # data extends beyond view, take view as limit\n                ihigh = view_high\n            else:\n                # data stops within view, take best tick\n                good_locs = locs[locs >= data_high]\n                if len(good_locs):\n                    # first tick after or equal to last data point\n                    ihigh = good_locs[0]\n                else:\n                    # No ticks (why not?), take last tick\n                    ihigh = locs[-1]\n            ticks = [tick for tick in ticks if ilow <= tick.get_loc() <= ihigh]\n\n        interval_t = self.get_transform().transform([view_low, view_high])\n\n        ticks_to_draw = []\n        for tick in ticks:\n            try:\n                loc_t = self.get_transform().transform(tick.get_loc())\n            except AssertionError:\n                # transforms.transform doesn't allow masked values but\n                # some scales might make them, so we need this try/except.\n                pass\n            else:\n                if mtransforms._interval_contains_close(interval_t, loc_t):\n                    ticks_to_draw.append(tick)\n\n        return ticks_to_draw\n```\n\nOriginal documentation:\n```python\nUpdate ticks (position and labels) using the current data\n        interval of the axes. Returns a list of ticks that will be\n        drawn.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update ticks (position and labels) using the current data interval of\n        the axes.  Return the list of ticks that will be drawn.", "file": "lib_matplotlib_axis.py", "function": "Axis._update_ticks", "project": "matplotlib", "change_stats": {"code_changes": 61, "doc_changes": 7, "total_changes": 68}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _apply_file_unidiff(patched_file, child_files, parent_file_layers):\n    \n    patched_file_path = Path(patched_file.path)\n    if patched_file.is_added_file:\n        if patched_file_path in child_files:\n            assert child_files[patched_file_path] is None\n        assert len(patched_file) == 1 # Should be only one hunk\n        assert patched_file[0].removed == 0\n        assert patched_file[0].target_start == 1\n        child_files[patched_file_path] = [x.value for x in patched_file[0]]\n    elif patched_file.is_removed_file:\n        child_files[patched_file_path] = None\n    else: # Patching an existing file\n        assert patched_file.is_modified_file\n        if patched_file_path not in child_files:\n            child_files[patched_file_path] = parent_file_layers[patched_file_path].copy()\n        _modify_file_lines(patched_file, child_files[patched_file_path])```\n\nUpdated code:\n```python\ndef _apply_file_unidiff(patched_file, files_under_test):\n    \n    patched_file_path = Path(patched_file.path)\n    if patched_file.is_added_file:\n        if patched_file_path in files_under_test:\n            assert files_under_test[patched_file_path] is None\n        assert len(patched_file) == 1 # Should be only one hunk\n        assert patched_file[0].removed == 0\n        assert patched_file[0].target_start == 1\n        files_under_test[patched_file_path] = [x.value for x in patched_file[0]]\n    elif patched_file.is_removed_file:\n        files_under_test[patched_file_path] = None\n    else: # Patching an existing file\n        assert patched_file.is_modified_file\n        _modify_file_lines(patched_file, files_under_test[patched_file_path])\n```\n\nOriginal documentation:\n```python\nApplies the unidiff.PatchedFile to the files at the current file layer\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Applies the unidiff.PatchedFile to the source files under testing", "file": "devutils_validate_patches.py", "function": "_apply_file_unidiff", "project": "ungoogled-chromium", "change_stats": {"code_changes": 16, "doc_changes": 4, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_pca_initialization_not_compatible_with_precomputed_kernel():\n    \n    tsne = TSNE(affinity=\"precomputed\", init=\"pca\")\n    assert_raises_regexp(ValueError, \"The parameter init=\\\"pca\\\" cannot be \"\n                         \"used with affinity=\\\"precomputed\\\".\",\n                         tsne.fit_transform, np.array([[0.0], [1.0]]))```\n\nUpdated code:\n```python\ndef test_pca_initialization_not_compatible_with_precomputed_kernel():\n    \n    tsne = TSNE(metric=\"precomputed\", init=\"pca\")\n    assert_raises_regexp(ValueError, \"The parameter init=\\\"pca\\\" cannot be \"\n                         \"used with metric=\\\"precomputed\\\".\",\n                         tsne.fit_transform, np.array([[0.0], [1.0]]))\n```\n\nOriginal documentation:\n```python\nPrecomputed affinity matrices must be square matrices.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Precomputed distance matrices must be square matrices.", "file": "sklearn_manifold_tests_test_t_sne.py", "function": "test_pca_initialization_not_compatible_with_precomputed_kernel", "project": "scikit-learn", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef batch_normalization(x, gamma, beta, **kwargs):\n    \n\n      # NOQA\n\n    argument.check_unexpected_kwargs(\n        kwargs, train='train argument is not supported anymore. '\n        'Use chainer.using_config')\n    eps, running_mean, running_var, decay = argument.parse_kwargs(\n        kwargs, ('eps', 2e-5), ('running_mean', None),\n        ('running_var', None), ('decay', 0.9))\n\n    return BatchNormalization(eps, running_mean, running_var, decay).apply(\n        (x, gamma, beta))[0]```\n\nUpdated code:\n```python\ndef batch_normalization(x, gamma, beta, **kwargs):\n    \n\n      # NOQA\n\n    argument.check_unexpected_kwargs(\n        kwargs, train='train argument is not supported anymore. '\n        'Use chainer.using_config')\n    eps, running_mean, running_var, decay, axis = argument.parse_kwargs(\n        kwargs, ('eps', 2e-5), ('running_mean', None),\n        ('running_var', None), ('decay', 0.9), ('axis', None))\n\n    return BatchNormalization(eps, running_mean, running_var, decay,\n                              axis).apply((x, gamma, beta))[0]\n```\n\nOriginal documentation:\n```python\nbatch_normalization(x, gamma, beta, eps=2e-5, running_mean=None, running_var=None, decay=0.9)\n\n    Batch normalization function.\n\n    It takes the input variable ``x`` and two parameter variables ``gamma`` and\n    ``beta``. The parameter variables must both have the same dimensionality,\n    which is referred to as the channel shape. This channel shape corresponds\n    to the dimensions in the input which are not averaged over. Since the\n    first dimension of the input corresponds to the batch size, the second\n    dimension of `x` will correspond to the first dimension of the channel\n    shape, the third dimension of `x` will correspond to the second channel\n    dimension (if it exists) and so on. Therefore, the dimensionality of the\n    input must be at least one plus the number of channel dimensions. The\n    total effective \"batch size\" will then be considered to be the product of\n    all dimensions in `x` except for the channel dimensions.\n\n    As an example, if the input is four dimensional and the parameter\n    variables are one dimensional, then it is assumed that the first\n    dimension of the input is the batch size, the second dimension is the\n    channel size, and the remaining two dimensions are considered\n    to be spatial dimensions that will be averaged over along with the\n    batch size in the batch normalization computations. That is,\n    the total batch size will be considered to be the product of all\n    input dimensions except the second dimension.\n\n    Note: If this function is called, it will not be possible to access the\n    updated running mean and variance statistics, because they are members\n    of the function object, which cannot be accessed by the caller.\n    If it is desired to access the updated running statistics, it is necessary\n    to get a new instance of the function object, call the object, and then\n    access the running_mean and/or running_var attributes. See the\n    corresponding Link class for an example of how to do this.\n\n    .. warning::\n\n       ``train`` argument is not supported anymore since v2.\n       Instead, use ``chainer.using_config('train', train)``.\n       See :func:`chainer.using_config`.\n\n    Args:\n        x (Variable): Input variable.\n        gamma (Variable): Scaling parameter of normalized data.\n        beta (Variable): Shifting parameter of scaled normalized data.\n        eps (float): Epsilon value for numerical stability.\n        running_mean (numpy.ndarray or cupy.ndarray):\n            Running average of the mean. This is a\n            running average of the mean over several mini-batches using\n            the decay parameter. If ``None``, the running average is not\n            computed. If this is ``None``, then ``runnng_var`` must also\n            be ``None``.\n        running_var (numpy.ndarray or cupy.ndarray):\n            Running average of the variance. This is a\n            running average of the variance over several mini-batches using\n            the decay parameter. If ``None``, the running average is not\n            computed. If this is ``None``, then ``running_mean`` must also\n            be ``None``.\n        decay (float): Decay rate of moving average. It is used during\n            training.\n\n    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n\n    .. seealso:: :class:`links.BatchNormalization`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "batch_normalization(x, gamma, beta, eps=2e-5, running_mean=None, running_var=None, decay=0.9)\n\n    Batch normalization function.\n\n    It takes the input variable ``x`` and two parameter variables ``gamma`` and\n    ``beta``. The parameter variables must both have the same dimensionality,\n    which is referred to as the channel shape. This channel shape corresponds\n    to the dimensions in the input which are not averaged over. Since the\n    first dimension of the input corresponds to the batch size, the second\n    dimension of `x` will correspond to the first dimension of the channel\n    shape, the third dimension of `x` will correspond to the second channel\n    dimension (if it exists) and so on. Therefore, the dimensionality of the\n    input must be at least one plus the number of channel dimensions. The\n    total effective \"batch size\" will then be considered to be the product of\n    all dimensions in `x` except for the channel dimensions.\n\n    As an example, if the input is four dimensional and the parameter\n    variables are one dimensional, then it is assumed that the first\n    dimension of the input is the batch size, the second dimension is the\n    channel size, and the remaining two dimensions are considered\n    to be spatial dimensions that will be averaged over along with the\n    batch size in the batch normalization computations. That is,\n    the total batch size will be considered to be the product of all\n    input dimensions except the second dimension.\n\n    Note: If this function is called, it will not be possible to access the\n    updated running mean and variance statistics, because they are members\n    of the function object, which cannot be accessed by the caller.\n    If it is desired to access the updated running statistics, it is necessary\n    to get a new instance of the function object, call the object, and then\n    access the running_mean and/or running_var attributes. See the\n    corresponding Link class for an example of how to do this.\n\n    .. warning::\n\n       ``train`` argument is not supported anymore since v2.\n       Instead, use ``chainer.using_config('train', train)``.\n       See :func:`chainer.using_config`.\n\n    Args:\n        x (Variable): Input variable.\n        gamma (Variable): Scaling parameter of normalized data.\n        beta (Variable): Shifting parameter of scaled normalized data.\n        eps (float): Epsilon value for numerical stability.\n        running_mean (numpy.ndarray or cupy.ndarray):\n            Running average of the mean. This is a\n            running average of the mean over several mini-batches using\n            the decay parameter. If ``None``, the running average is not\n            computed. If this is ``None``, then ``runnng_var`` must also\n            be ``None``.\n        running_var (numpy.ndarray or cupy.ndarray):\n            Running average of the variance. This is a\n            running average of the variance over several mini-batches using\n            the decay parameter. If ``None``, the running average is not\n            computed. If this is ``None``, then ``running_mean`` must also\n            be ``None``.\n        decay (float): Decay rate of moving average. It is used during\n            training.\n        axis (int or tuple of int): Axis over which normalization is\n            performed. When axis is ``None``, it is determined from input\n            dimensions. For example, if ``x.ndim`` is 4, axis becomes (0, 2, 3)\n            and normalization is performed over 0th, 2nd and 3rd axis of input.\n            If it is 2, axis becomes (0) and normalization is performed\n            over 0th axis of input. When a tuple of int is given to this\n            option, numbers in the tuple must be being sorted in ascending\n            order. For example, (0, 2) is OK, but (2, 0) is not.\n\n    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n\n    .. seealso:: :class:`links.BatchNormalization`", "file": "chainer_", "function": "batch_normalization", "project": "chainer", "change_stats": {"code_changes": 10, "doc_changes": 10, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef walk_modules(path):\n    \n    \n    mod = __import__(path, {}, {}, [''])\n    if hasattr(mod, '__path__'):\n        for _, path, _ in walk_packages(mod.__path__, mod.__name__ + '.'):\n            yield __import__(path, {}, {}, [''])\n    yield mod```\n\nUpdated code:\n```python\ndef walk_modules(path, load=False):\n    \n    \n\n    mods = []\n    mod = __import__(path, {}, {}, [''])\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = __import__(fullpath, {}, {}, [''])\n                mods.append(submod)\n    return mods\n```\n\nOriginal documentation:\n```python\nLoads a module and all its submodules given its absolute path and\n    returns them.\n\n    path ie: 'scrapy.contrib.downloadermiddelware.redirect'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads a module and all its submodules from a the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')", "file": "scrapy_utils_misc.py", "function": "walk_modules", "project": "scrapy", "change_stats": {"code_changes": 18, "doc_changes": 9, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_sell_rate(self, pair: str, refresh: bool) -> float:\n        \n        \n        \n        if not refresh:\n            rate = self._sell_rate_cache.get(pair)\n            # Check if cache has been invalidated\n            if rate:\n                logger.info(f\"Using cached sell rate for {pair}.\")\n                return rate\n\n        config_ask_strategy = self.config.get('ask_strategy', {})\n        if config_ask_strategy.get('use_order_book', False):\n            # This code is only used for notifications, selling uses the generator directly\n            logger.debug('Using order book to get sell rate')\n            rate = next(self._order_book_gen(pair, f\"{config_ask_strategy['price_side']}s\"))\n\n        else:\n            rate = self.exchange.fetch_ticker(pair)[config_ask_strategy['price_side']]\n        self._sell_rate_cache[pair] = rate\n        return rate```\n\nUpdated code:\n```python\ndef get_sell_rate(self, pair: str, refresh: bool) -> float:\n        \n        \n        \n        if not refresh:\n            rate = self._sell_rate_cache.get(pair)\n            # Check if cache has been invalidated\n            if rate:\n                logger.info(f\"Using cached sell rate for {pair}.\")\n                return rate\n\n        ask_strategy = self.config.get('ask_strategy', {})\n        if ask_strategy.get('use_order_book', False):\n            # This code is only used for notifications, selling uses the generator directly\n            logger.info(\n                f\"Getting price from order book {ask_strategy['price_side'].capitalize()} side.\"\n            )\n            rate = next(self._order_book_gen(pair, f\"{ask_strategy['price_side']}s\"))\n\n        else:\n            rate = self.exchange.fetch_ticker(pair)[ask_strategy['price_side']]\n        self._sell_rate_cache[pair] = rate\n        return rate\n```\n\nOriginal documentation:\n```python\nGet sell rate - either using get-ticker bid or first bid based on orderbook\n        The orderbook portion is only used for rpc messaging, which would otherwise fail\n        for BitMex (has no bid/ask in fetch_ticker)\n        or remain static in any other case since it's not updating.\n        :param pair: Pair to get rate for\n        :param refresh: allow cached data\n        :return: Bid rate\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get sell rate - either using ticker bid or first bid based on orderbook\n        The orderbook portion is only used for rpc messaging, which would otherwise fail\n        for BitMex (has no bid/ask in fetch_ticker)\n        or remain static in any other case since it's not updating.\n        :param pair: Pair to get rate for\n        :param refresh: allow cached data\n        :return: Bid rate", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.get_sell_rate", "project": "freqtrade", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fib_binet(n: int) -> list[int]:\n    \n    \n    \n    if n < 0:\n        raise Exception(\"n is negative\")\n    if n >= 1475:\n        raise Exception(\"n is too large\")\n    sqrt_5 = sqrt(5)\n    phi = (1 + sqrt_5) / 2\n    return [round(phi**i / sqrt_5) for i in range(n + 1)]```\n\nUpdated code:\n```python\ndef fib_binet(n: int) -> list[int]:\n    \n    \n    \n    if n < 0:\n        raise ValueError(\"n is negative\")\n    if n >= 1475:\n        raise ValueError(\"n is too large\")\n    sqrt_5 = sqrt(5)\n    phi = (1 + sqrt_5) / 2\n    return [round(phi**i / sqrt_5) for i in range(n + 1)]\n```\n\nOriginal documentation:\n```python\nCalculates the first n (0-indexed) Fibonacci numbers using a simplified form\n    of Binet's formula:\n    https://en.m.wikipedia.org/wiki/Fibonacci_number#Computation_by_rounding\n\n    NOTE 1: this function diverges from fib_iterative at around n = 71, likely\n    due to compounding floating-point arithmetic errors\n\n    NOTE 2: this function doesn't accept n >= 1475 because it overflows\n    thereafter due to the size limitations of Python floats\n    >>> fib_binet(0)\n    [0]\n    >>> fib_binet(1)\n    [0, 1]\n    >>> fib_binet(5)\n    [0, 1, 1, 2, 3, 5]\n    >>> fib_binet(10)\n    [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n    >>> fib_binet(-1)\n    Traceback (most recent call last):\n        ...\n    Exception: n is negative\n    >>> fib_binet(1475)\n    Traceback (most recent call last):\n        ...\n    Exception: n is too large\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculates the first n (0-indexed) Fibonacci numbers using a simplified form\n    of Binet's formula:\n    https://en.m.wikipedia.org/wiki/Fibonacci_number#Computation_by_rounding\n\n    NOTE 1: this function diverges from fib_iterative at around n = 71, likely\n    due to compounding floating-point arithmetic errors\n\n    NOTE 2: this function doesn't accept n >= 1475 because it overflows\n    thereafter due to the size limitations of Python floats\n    >>> fib_binet(0)\n    [0]\n    >>> fib_binet(1)\n    [0, 1]\n    >>> fib_binet(5)\n    [0, 1, 1, 2, 3, 5]\n    >>> fib_binet(10)\n    [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n    >>> fib_binet(-1)\n    Traceback (most recent call last):\n        ...\n    ValueError: n is negative\n    >>> fib_binet(1475)\n    Traceback (most recent call last):\n        ...\n    ValueError: n is too large", "file": "maths_fibonacci.py", "function": "fib_binet", "project": "Python", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _wait_for_workers_report_after_ramp_up(self) -> float:\n        \n        \n        \n        locust_wait_for_workers_report_after_ramp_up = os.getenv(\"LOCUST_WAIT_FOR_WORKERS_REPORT_AFTER_RAMP_UP\")\n        if locust_wait_for_workers_report_after_ramp_up is None:\n            return 0.1\n\n        match = re.search(\n            r\"^(?P<coeff>(\\d+)|(\\d+\\.\\d+))[ ]*\\*[ ]*WORKER_REPORT_INTERVAL$\",\n            locust_wait_for_workers_report_after_ramp_up,\n        )\n        if match is None:\n            assert float(locust_wait_for_workers_report_after_ramp_up) >= 0\n            return float(locust_wait_for_workers_report_after_ramp_up)\n        else:\n            return float(match.group(\"coeff\")) * WORKER_REPORT_INTERVAL```\n\nUpdated code:\n```python\ndef _wait_for_workers_report_after_ramp_up(self) -> float:\n        \n        \n        \n        locust_wait_for_workers_report_after_ramp_up = os.getenv(\"LOCUST_WAIT_FOR_WORKERS_REPORT_AFTER_RAMP_UP\")\n        if locust_wait_for_workers_report_after_ramp_up is None:\n            return 1.0\n\n        match = re.search(\n            r\"^(?P<coeff>(\\d+)|(\\d+\\.\\d+))[ ]*\\*[ ]*WORKER_REPORT_INTERVAL$\",\n            locust_wait_for_workers_report_after_ramp_up,\n        )\n        if match is None:\n            assert float(locust_wait_for_workers_report_after_ramp_up) >= 0\n            return float(locust_wait_for_workers_report_after_ramp_up)\n        else:\n            return float(match.group(\"coeff\")) * WORKER_REPORT_INTERVAL\n```\n\nOriginal documentation:\n```python\nThe amount of time to wait after a ramp-up in order for all the workers to report their state\n        to the master. If not supplied by the user, it is 100ms by default. If the supplied value is a number,\n        it is taken as-is. If the supplied value is a pattern like \"some_number * WORKER_REPORT_INTERVAL\",\n        the value will be \"some_number * WORKER_REPORT_INTERVAL\". The most sensible value would be something\n        like \"1.25 * WORKER_REPORT_INTERVAL\". However, some users might find it too high, so it is left\n        to a really small value of 100ms by default.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "The amount of time to wait after a ramp-up in order for all the workers to report their state\n        to the master. If not supplied by the user, it is 1000ms by default. If the supplied value is a number,\n        it is taken as-is. If the supplied value is a pattern like \"some_number * WORKER_REPORT_INTERVAL\",\n        the value will be \"some_number * WORKER_REPORT_INTERVAL\". The most sensible value would be something\n        like \"1.25 * WORKER_REPORT_INTERVAL\". However, some users might find it too high, so it is left\n        to a relatively small value of 1000ms by default.", "file": "locust_runners.py", "function": "MasterRunner._wait_for_workers_report_after_ramp_up", "project": "locust", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef strip_sensitive_data_from_sentry_event(\n    event: Dict[Text, Any], _unused_hint: Optional[Dict[Text, Any]] = None\n) -> Dict[Text, Any]:\n    \n    \n    # removes any paths from stack traces (avoids e.g. sending\n    # a users home directory name if package is installed there)\n    for value in event.get(\"exception\", {}).get(\"values\", []):\n        for frame in value.get(\"stacktrace\", {}).get(\"frames\", []):\n            frame[\"abs_path\"] = \"\"\n    return event```\n\nUpdated code:\n```python\ndef strip_sensitive_data_from_sentry_event(\n    event: Dict[Text, Any], _unused_hint: Optional[Dict[Text, Any]] = None\n) -> Optional[Dict[Text, Any]]:\n    \n    \n    # removes any paths from stack traces (avoids e.g. sending\n    # a users home directory name if package is installed there)\n    for value in event.get(\"exception\", {}).get(\"values\", []):\n        for frame in value.get(\"stacktrace\", {}).get(\"frames\", []):\n            frame[\"abs_path\"] = \"\"\n\n            if \"site-packages\" in frame[\"filename\"]:\n                # drop site-packages and following slash / backslash\n                relative_name = frame[\"filename\"].split(\"site-packages\")[-1][1:]\n                frame[\"filename\"] = os.path.join(\"site-packages\", relative_name)\n            elif \"dist-packages\" in frame[\"filename\"]:\n                # drop dist-packages and following slash / backslash\n                relative_name = frame[\"filename\"].split(\"dist-packages\")[-1][1:]\n                frame[\"filename\"] = os.path.join(\"dist-packages\", relative_name)\n            elif os.path.isabs(frame[\"filename\"]):\n                # if the file path is absolute, we'll drop the whole event as this is\n                # very likely custom code. needs to happen after cleaning as\n                # site-packages / dist-packages paths are also absolute, but fine.\n                return None\n    return event\n```\n\nOriginal documentation:\n```python\nRemove any sensitive data from the event (e.g. path names).\n\n    Args:\n        event: event to be logged to sentry\n        _unused_hint: some hinting information sent alongside of the event\n\n    Returns:\n        the event without any sensitive / PII data.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Remove any sensitive data from the event (e.g. path names).\n\n    Args:\n        event: event to be logged to sentry\n        _unused_hint: some hinting information sent alongside of the event\n\n    Returns:\n        the event without any sensitive / PII data or `None` if the event should\n        be discarded.", "file": "rasa_telemetry.py", "function": "strip_sensitive_data_from_sentry_event", "project": "rasa", "change_stats": {"code_changes": 18, "doc_changes": 5, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef Time2Internaldate(date_time):\n\n    \n    \n\n    if isinstance(date_time, (int, float)):\n        tt = time.localtime(date_time)\n    elif isinstance(date_time, (tuple, time.struct_time)):\n        tt = date_time\n    elif isinstance(date_time, str) and (date_time[0],date_time[-1]) == ('\"','\"'):\n        return date_time        # Assume in correct format\n    else:\n        raise ValueError(\"date_time not of a known type\")\n\n    dt = time.strftime(\"%d-%b-%Y %H:%M:%S\", tt)\n    if dt[0] == '0':\n        dt = ' ' + dt[1:]\n    if time.daylight and tt[-1]:\n        zone = -time.altzone\n    else:\n        zone = -time.timezone\n    return '\"' + dt + \" %+03d%02d\" % divmod(zone//60, 60) + '\"'```\n\nUpdated code:\n```python\ndef Time2Internaldate(date_time):\n\n    \n    \n    if isinstance(date_time, (int, float)):\n        dt = datetime.fromtimestamp(date_time,\n                                    timezone.utc).astimezone()\n    elif isinstance(date_time, tuple):\n        try:\n            gmtoff = date_time.tm_gmtoff\n        except AttributeError:\n            if time.daylight:\n                dst = date_time[8]\n                if dst == -1:\n                    dst = time.localtime(time.mktime(date_time))[8]\n                gmtoff = -(time.timezone, time.altzone)[dst]\n            else:\n                gmtoff = -time.timezone\n        delta = timedelta(seconds=gmtoff)\n        dt = datetime(*date_time[:6], tzinfo=timezone(delta))\n    elif isinstance(date_time, datetime):\n        if date_time.tzinfo is None:\n            raise ValueError(\"date_time must be aware\")\n        dt = date_time\n    elif isinstance(date_time, str) and (date_time[0],date_time[-1]) == ('\"','\"'):\n        return date_time        # Assume in correct format\n    else:\n        raise ValueError(\"date_time not of a known type\")\n    fmt = '\"%d-{}-%Y %H:%M:%S %z\"'.format(Months[dt.month])\n    return dt.strftime(fmt)\n```\n\nOriginal documentation:\n```python\nConvert date_time to IMAP4 INTERNALDATE representation.\n\n    Return string in form: '\"DD-Mmm-YYYY HH:MM:SS +HHMM\"'.  The\n    date_time argument can be a number (int or float) representing\n    seconds since epoch (as returned by time.time()), a 9-tuple\n    representing local time (as returned by time.localtime()), or a\n    double-quoted string.  In the last case, it is assumed to already\n    be in the correct format.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert date_time to IMAP4 INTERNALDATE representation.\n\n    Return string in form: '\"DD-Mmm-YYYY HH:MM:SS +HHMM\"'.  The\n    date_time argument can be a number (int or float) representing\n    seconds since epoch (as returned by time.time()), a 9-tuple\n    representing local time, an instance of time.struct_time (as\n    returned by time.localtime()), an aware datetime instance or a\n    double-quoted string.  In the last case, it is assumed to already\n    be in the correct format.", "file": "Lib_imaplib.py", "function": "Time2Internaldate", "project": "cpython", "change_stats": {"code_changes": 36, "doc_changes": 5, "total_changes": 41}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef bulk_create(self, objs, batch_size=None):\n        \n        \n        \n        # So this case is fun. When you bulk insert you don't get the primary\n        # keys back (if it's an autoincrement), so you can't insert into the\n        # child tables which references this. There are two workarounds, 1)\n        # this could be implemented if you didn't have an autoincrement pk,\n        # and 2) you could do it by doing O(n) normal inserts into the parent\n        # tables to get the primary keys back, and then doing a single bulk\n        # insert into the childmost table. Some databases might allow doing\n        # this by using RETURNING clause for the insert query. We're punting\n        # on these for now because they are relatively rare cases.\n        assert batch_size is None or batch_size > 0\n        # Check that the parents share the same concrete model with the our\n        # model to detect the inheritance pattern ConcreteGrandParent ->\n        # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n        # would not identify that case as involving multiple tables.\n        for parent in self.model._meta.get_parent_list():\n            if parent._meta.concrete_model is not self.model._meta.concrete_model:\n                raise ValueError(\"Can't bulk create a multi-table inherited model\")\n        if not objs:\n            return objs\n        self._for_write = True\n        connection = connections[self.db]\n        fields = self.model._meta.concrete_fields\n        objs = list(objs)\n        self._populate_pk_values(objs)\n        with transaction.atomic(using=self.db, savepoint=False):\n            if (connection.features.can_combine_inserts_with_and_without_auto_increment_pk\n                    and self.model._meta.has_auto_field):\n                self._batched_insert(objs, fields, batch_size)\n            else:\n                objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n                if objs_with_pk:\n                    self._batched_insert(objs_with_pk, fields, batch_size)\n                if objs_without_pk:\n                    fields = [f for f in fields if not isinstance(f, AutoField)]\n                    self._batched_insert(objs_without_pk, fields, batch_size)\n\n        return objs```\n\nUpdated code:\n```python\ndef bulk_create(self, objs, batch_size=None):\n        \n        \n        \n        # When you bulk insert you don't get the primary keys back (if it's an\n        # autoincrement, except if can_return_ids_from_bulk_insert=True), so\n        # you can't insert into the child tables which references this. There\n        # are two workarounds:\n        # 1) This could be implemented if you didn't have an autoincrement pk\n        # 2) You could do it by doing O(n) normal inserts into the parent\n        #    tables to get the primary keys back and then doing a single bulk\n        #    insert into the childmost table.\n        # We currently set the primary keys on the objects when using\n        # PostgreSQL via the RETURNING ID clause. It should be possible for\n        # Oracle as well, but the semantics for  extracting the primary keys is\n        # trickier so it's not done yet.\n        assert batch_size is None or batch_size > 0\n        # Check that the parents share the same concrete model with the our\n        # model to detect the inheritance pattern ConcreteGrandParent ->\n        # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n        # would not identify that case as involving multiple tables.\n        for parent in self.model._meta.get_parent_list():\n            if parent._meta.concrete_model is not self.model._meta.concrete_model:\n                raise ValueError(\"Can't bulk create a multi-table inherited model\")\n        if not objs:\n            return objs\n        self._for_write = True\n        connection = connections[self.db]\n        fields = self.model._meta.concrete_fields\n        objs = list(objs)\n        self._populate_pk_values(objs)\n        with transaction.atomic(using=self.db, savepoint=False):\n            if (connection.features.can_combine_inserts_with_and_without_auto_increment_pk\n                    and self.model._meta.has_auto_field):\n                self._batched_insert(objs, fields, batch_size)\n            else:\n                objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n                if objs_with_pk:\n                    self._batched_insert(objs_with_pk, fields, batch_size)\n                if objs_without_pk:\n                    fields = [f for f in fields if not isinstance(f, AutoField)]\n                    ids = self._batched_insert(objs_without_pk, fields, batch_size)\n                    if connection.features.can_return_ids_from_bulk_insert:\n                        assert len(ids) == len(objs_without_pk)\n                    for i in range(len(ids)):\n                        objs_without_pk[i].pk = ids[i]\n\n        return objs\n```\n\nOriginal documentation:\n```python\nInserts each of the instances into the database. This does *not* call\n        save() on each of the instances, does not send any pre/post save\n        signals, and does not set the primary key attribute if it is an\n        autoincrement field. Multi-table models are not supported.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inserts each of the instances into the database. This does *not* call\n        save() on each of the instances, does not send any pre/post save\n        signals, and does not set the primary key attribute if it is an\n        autoincrement field (except if features.can_return_ids_from_bulk_insert=True).\n        Multi-table models are not supported.", "file": "django_db_models_query.py", "function": "QuerySet.bulk_create", "project": "django", "change_stats": {"code_changes": 29, "doc_changes": 5, "total_changes": 34}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef estimate(self, src, dst, order=2):\n        \n\n        \n        xs = src[:, 0]\n        ys = src[:, 1]\n        xd = dst[:, 0]\n        yd = dst[:, 1]\n        rows = src.shape[0]\n\n        # number of unknown polynomial coefficients\n        order = safe_as_int(order)\n        u = (order + 1) * (order + 2)\n\n        A = np.zeros((rows * 2, u + 1))\n        pidx = 0\n        for j in range(order + 1):\n            for i in range(j + 1):\n                A[:rows, pidx] = xs ** (j - i) * ys ** i\n                A[rows:, pidx + u // 2] = xs ** (j - i) * ys ** i\n                pidx += 1\n\n        A[:rows, -1] = xd\n        A[rows:, -1] = yd\n\n        _, _, V = np.linalg.svd(A)\n\n        # solution is right singular vector that corresponds to smallest\n        # singular value\n        params = - V[-1, :-1] / V[-1, -1]\n\n        self.params = params.reshape((2, u // 2))\n\n        return True```\n\nUpdated code:\n```python\ndef estimate(self, src, dst, order=2, weights=None):\n        \n\n        \n        xs = src[:, 0]\n        ys = src[:, 1]\n        xd = dst[:, 0]\n        yd = dst[:, 1]\n        rows = src.shape[0]\n\n        # number of unknown polynomial coefficients\n        order = safe_as_int(order)\n        u = (order + 1) * (order + 2)\n\n        A = np.zeros((rows * 2, u + 1))\n        pidx = 0\n        for j in range(order + 1):\n            for i in range(j + 1):\n                A[:rows, pidx] = xs ** (j - i) * ys ** i\n                A[rows:, pidx + u // 2] = xs ** (j - i) * ys ** i\n                pidx += 1\n\n        A[:rows, -1] = xd\n        A[rows:, -1] = yd\n\n        # Get the vectors that correspond to singular values, also applying\n        # the weighting if provided\n        if weights is None:\n            _, _, V = np.linalg.svd(A)\n        else:\n            W = np.diag(np.tile(np.sqrt(weights / np.max(weights)), 2))\n            _, _, V = np.linalg.svd(W @ A)\n\n        # solution is right singular vector that corresponds to smallest\n        # singular value\n        params = - V[-1, :-1] / V[-1, -1]\n\n        self.params = params.reshape((2, u // 2))\n\n        return True\n```\n\nOriginal documentation:\n```python\nEstimate the transformation from a set of corresponding points.\n\n        You can determine the over-, well- and under-determined parameters\n        with the total least-squares method.\n\n        Number of source and destination coordinates must match.\n\n        The transformation is defined as::\n\n            X = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i ))\n            Y = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i ))\n\n        These equations can be transformed to the following form::\n\n            0 = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i )) - X\n            0 = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i )) - Y\n\n        which exist for each set of corresponding points, so we have a set of\n        N * 2 equations. The coefficients appear linearly so we can write\n        A x = 0, where::\n\n            A   = [[1 x y x**2 x*y y**2 ... 0 ...             0 -X]\n                   [0 ...                 0 1 x y x**2 x*y y**2 -Y]\n                    ...\n                    ...\n                  ]\n            x.T = [a00 a10 a11 a20 a21 a22 ... ann\n                   b00 b10 b11 b20 b21 b22 ... bnn c3]\n\n        In case of total least-squares the solution of this homogeneous system\n        of equations is the right singular vector of A which corresponds to the\n        smallest singular value normed by the coefficient c3.\n\n        Parameters\n        ----------\n        src : (N, 2) array\n            Source coordinates.\n        dst : (N, 2) array\n            Destination coordinates.\n        order : int, optional\n            Polynomial order (number of coefficients is order + 1).\n\n        Returns\n        -------\n        success : bool\n            True, if model estimation succeeds.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Estimate the transformation from a set of corresponding points.\n\n        You can determine the over-, well- and under-determined parameters\n        with the total least-squares method.\n\n        Number of source and destination coordinates must match.\n\n        The transformation is defined as::\n\n            X = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i ))\n            Y = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i ))\n\n        These equations can be transformed to the following form::\n\n            0 = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i )) - X\n            0 = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i )) - Y\n\n        which exist for each set of corresponding points, so we have a set of\n        N * 2 equations. The coefficients appear linearly so we can write\n        A x = 0, where::\n\n            A   = [[1 x y x**2 x*y y**2 ... 0 ...             0 -X]\n                   [0 ...                 0 1 x y x**2 x*y y**2 -Y]\n                    ...\n                    ...\n                  ]\n            x.T = [a00 a10 a11 a20 a21 a22 ... ann\n                   b00 b10 b11 b20 b21 b22 ... bnn c3]\n\n        In case of total least-squares the solution of this homogeneous system\n        of equations is the right singular vector of A which corresponds to the\n        smallest singular value normed by the coefficient c3.\n\n        Weights can be applied to each pair of corresponding points to\n        indicate, particularly in an overdetermined system, if point pairs have\n        higher or lower confidence or uncertainties associated with them. From\n        the matrix treatment of least squares problems, these weight values are\n        normalised, square-rooted, then built into a diagonal matrix, by which\n        A is multiplied.\n\n        Parameters\n        ----------\n        src : (N, 2) array\n            Source coordinates.\n        dst : (N, 2) array\n            Destination coordinates.\n        order : int, optional\n            Polynomial order (number of coefficients is order + 1).\n        weights : (N,) array, optional\n            Relative weight values for each pair of points.\n\n        Returns\n        -------\n        success : bool\n            True, if model estimation succeeds.", "file": "skimage_transform__geometric.py", "function": "PolynomialTransform.estimate", "project": "scikit-image", "change_stats": {"code_changes": 12, "doc_changes": 11, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef upload_with_dedup(self, file_name, extension=None):\n        \n        \n        \n\n        # This construction of remote_path is critical to preventing duplicate\n        # uploads of same object. Uploader will check if the file exists in S3\n        # and re-upload only if necessary. So the template points to same file\n        # in multiple places, this will upload only once\n        filemd5 = self.file_checksum(file_name)\n        remote_path = filemd5\n        if extension:\n            remote_path = remote_path + \".\" + extension\n\n        return self.upload(file_name, remote_path)```\n\nUpdated code:\n```python\ndef upload_with_dedup(self, file_name, extension=None, precomputed_md5=None):\n        \n        \n        \n\n        # This construction of remote_path is critical to preventing duplicate\n        # uploads of same object. Uploader will check if the file exists in S3\n        # and re-upload only if necessary. So the template points to same file\n        # in multiple places, this will upload only once\n        filemd5 = precomputed_md5 or file_checksum(file_name)\n        remote_path = filemd5\n        if extension:\n            remote_path = remote_path + \".\" + extension\n\n        return self.upload(file_name, remote_path)\n```\n\nOriginal documentation:\n```python\nMakes and returns name of the S3 object based on the file's MD5 sum\n\n        :param file_name: file to upload\n        :param extension: String of file extension to append to the object\n        :return: S3 URL of the uploaded object\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Makes and returns name of the S3 object based on the file's MD5 sum\n\n        :param file_name: file to upload\n        :param extension: String of file extension to append to the object\n        :param precomputed_md5: Specified md5 hash for the file to be uploaded.\n        :return: S3 URL of the uploaded object", "file": "samcli_lib_package_s3_uploader.py", "function": "S3Uploader.upload_with_dedup", "project": "aws-sam-cli", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef custom_sell(self, pair: str, trade: Trade, current_time: datetime, current_rate: float,\n                    current_profit: float, **kwargs) -> bool:\n        \n        \n        \n        return False```\n\nUpdated code:\n```python\ndef custom_sell(self, pair: str, trade: Trade, current_time: datetime, current_rate: float,\n                    current_profit: float, **kwargs) -> Optional[Union[str, bool]]:\n        \n        \n        \n        return None\n```\n\nOriginal documentation:\n```python\nCustom sell signal logic indicating that specified position should be sold. Returning True\n        from this method is equal to setting sell signal on a candle at specified time.\n        This method is not called when sell signal is set.\n\n        This method should be overridden to create sell signals that depend on trade parameters. For\n        example you could implement a stoploss relative to candle when trade was opened, or a custom\n        1:2 risk-reward ROI.\n\n        :param pair: Pair that's currently analyzed\n        :param trade: trade object.\n        :param current_time: datetime object, containing the current datetime\n        :param current_rate: Rate, calculated based on pricing settings in ask_strategy.\n        :param current_profit: Current profit (as ratio), calculated based on current_rate.\n        :param **kwargs: Ensure to keep this here so updates to this won't break your strategy.\n        :return bool: Whether trade should exit now.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Custom sell signal logic indicating that specified position should be sold. Returning a\n        string or True from this method is equal to setting sell signal on a candle at specified\n        time. This method is not called when sell signal is set.\n\n        This method should be overridden to create sell signals that depend on trade parameters. For\n        example you could implement a stoploss relative to candle when trade was opened, or a custom\n        1:2 risk-reward ROI.\n\n        Custom sell reason max length is 64. Exceeding this limit will raise OperationalException.\n\n        :param pair: Pair that's currently analyzed\n        :param trade: trade object.\n        :param current_time: datetime object, containing the current datetime\n        :param current_rate: Rate, calculated based on pricing settings in ask_strategy.\n        :param current_profit: Current profit (as ratio), calculated based on current_rate.\n        :param **kwargs: Ensure to keep this here so updates to this won't break your strategy.\n        :return: To execute sell, return a string with custom sell reason or True. Otherwise return\n        None or False.", "file": "freqtrade_strategy_interface.py", "function": "IStrategy.custom_sell", "project": "freqtrade", "change_stats": {"code_changes": 6, "doc_changes": 13, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef project_manage(request, project_slug):\n    \n    \n    \n    project = get_object_or_404(request.user.projects.live(), slug=project_slug)\n    return object_list(\n        request,\n        queryset=project.files.live(),\n        extra_context={'project': project},\n        page=int(request.GET.get('page', 1)),\n        template_object_name='file',\n        template_name='projects/project_manage.html',\n    )```\n\nUpdated code:\n```python\ndef project_manage(request, project_slug):\n    \n    \n    \n    return HttpResponseRedirect(reverse('projects_detail', args=[project_slug]))\n```\n\nOriginal documentation:\n```python\nThe management view for a project, where you will have links to edit\n    the projects' configuration, edit the files associated with that\n    project, etc.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "The management view for a project, where you will have links to edit\n    the projects' configuration, edit the files associated with that\n    project, etc.\n\n    Now redirects to the normal /projects/<slug> view.", "file": "readthedocs_projects_views_private.py", "function": "project_manage", "project": "readthedocs.org", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_csv(\n        self,\n        path_or_buf: Union[PathLike, BinaryIO],\n        batch_size: Optional[int] = None,\n        num_proc: Optional[int] = None,\n        index: bool = False,\n        **to_csv_kwargs,\n    ) -> int:\n        \n        \n        # Dynamic import to avoid circular dependency\n        from .io.csv import CsvDatasetWriter\n\n        return CsvDatasetWriter(\n            self, path_or_buf, batch_size=batch_size, num_proc=num_proc, index=index, **to_csv_kwargs\n        ).write()```\n\nUpdated code:\n```python\ndef to_csv(\n        self,\n        path_or_buf: Union[PathLike, BinaryIO],\n        batch_size: Optional[int] = None,\n        num_proc: Optional[int] = None,\n        **to_csv_kwargs,\n    ) -> int:\n        \n        \n        # Dynamic import to avoid circular dependency\n        from .io.csv import CsvDatasetWriter\n\n        return CsvDatasetWriter(self, path_or_buf, batch_size=batch_size, num_proc=num_proc, **to_csv_kwargs).write()\n```\n\nOriginal documentation:\n```python\nExports the dataset to csv\n\n        Args:\n            path_or_buf (`PathLike` or `FileOrBuffer`):\n                Either a path to a file or a BinaryIO.\n            batch_size (`int`, *optional*):\n                Size of the batch to load in memory and write at once.\n                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n            num_proc (`int`, *optional*):\n                Number of processes for multiprocessing. By default it doesn't\n                use multiprocessing. `batch_size` in this case defaults to\n                `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n                value if you have sufficient compute power.\n            index (`bool`, default `False`): Write row names (index).\n\n                <Changed version=\"2.10.0\">\n\n                Now, `index` defaults to `False`.\n\n                If you would like to write the index, set it to `True` and also set a name for the index column by\n                passing `index_label`.\n\n                </Changed>\n\n            **to_csv_kwargs (additional keyword arguments):\n                Parameters to pass to pandas's `pandas.DataFrame.to_csv`.\n\n        Returns:\n            `int`: The number of characters or bytes written.\n\n        Example:\n\n        ```py\n        >>> ds.to_csv(\"path/to/dataset/directory\")\n        ```\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Exports the dataset to csv\n\n        Args:\n            path_or_buf (`PathLike` or `FileOrBuffer`):\n                Either a path to a file or a BinaryIO.\n            batch_size (`int`, *optional*):\n                Size of the batch to load in memory and write at once.\n                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n            num_proc (`int`, *optional*):\n                Number of processes for multiprocessing. By default it doesn't\n                use multiprocessing. `batch_size` in this case defaults to\n                `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n                value if you have sufficient compute power.\n            **to_csv_kwargs (additional keyword arguments):\n                Parameters to pass to pandas's [`pandas.DataFrame.to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).\n\n                <Changed version=\"2.10.0\">\n\n                Now, `index` defaults to `False` if not specified.\n\n                If you would like to write the index, pass `index=True` and also set a name for the index column by\n                passing `index_label`.\n\n                </Changed>\n\n        Returns:\n            `int`: The number of characters or bytes written.\n\n        Example:\n\n        ```py\n        >>> ds.to_csv(\"path/to/dataset/directory\")\n        ```", "file": "src_datasets_arrow_dataset.py", "function": "Dataset.to_csv", "project": "datasets", "change_stats": {"code_changes": 7, "doc_changes": 12, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef preprocess(\n        self,\n        images: ImageInput,\n        do_resize: bool = None,\n        size: Dict[str, int] = None,\n        resample=None,\n        do_center_crop: bool = None,\n        crop_size: Dict[str, int] = None,\n        do_rescale: bool = None,\n        rescale_factor: float = None,\n        rescale_offset: bool = None,\n        do_normalize: bool = None,\n        image_mean: Optional[Union[float, List[float]]] = None,\n        image_std: Optional[Union[float, List[float]]] = None,\n        include_top: bool = None,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        data_format: ChannelDimension = ChannelDimension.FIRST,\n        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n        **kwargs,\n    ) -> PIL.Image.Image:\n        \n        \n        \n        do_resize = do_resize if do_resize is not None else self.do_resize\n        resample = resample if resample is not None else self.resample\n        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n        rescale_offset = rescale_offset if rescale_offset is not None else self.rescale_offset\n        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n        image_mean = image_mean if image_mean is not None else self.image_mean\n        image_std = image_std if image_std is not None else self.image_std\n        include_top = include_top if include_top is not None else self.include_top\n\n        size = size if size is not None else self.size\n        size = get_size_dict(size)\n        crop_size = crop_size if crop_size is not None else self.crop_size\n        crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n\n        images = make_list_of_images(images)\n\n        if not valid_images(images):\n            raise ValueError(\n                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n            )\n\n        if do_resize and size is None or resample is None:\n            raise ValueError(\"Size and resample must be specified if do_resize is True.\")\n\n        if do_center_crop and crop_size is None:\n            raise ValueError(\"Crop size must be specified if do_center_crop is True.\")\n\n        if do_rescale and rescale_factor is None:\n            raise ValueError(\"Rescale factor must be specified if do_rescale is True.\")\n\n        if do_normalize and (image_mean is None or image_std is None):\n            raise ValueError(\"Image mean and std must be specified if do_normalize is True.\")\n\n        # All transformations expect numpy arrays.\n        images = [to_numpy_array(image) for image in images]\n\n        if input_data_format is None:\n            # We assume that all images have the same channel dimension format.\n            input_data_format = infer_channel_dimension_format(images[0])\n\n        if do_resize:\n            images = [\n                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n                for image in images\n            ]\n\n        if do_center_crop:\n            images = [\n                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n            ]\n\n        if do_rescale:\n            images = [\n                self.rescale(\n                    image=image, scale=rescale_factor, offset=rescale_offset, input_data_format=input_data_format\n                )\n                for image in images\n            ]\n\n        if do_normalize:\n            images = [\n                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n                for image in images\n            ]\n\n        if include_top:\n            images = [\n                self.normalize(image=image, mean=0, std=image_std, input_data_format=input_data_format)\n                for image in images\n            ]\n\n        images = [\n            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n        ]\n\n        data = {\"pixel_values\": images}\n        return BatchFeature(data=data, tensor_type=return_tensors)```\n\nUpdated code:\n```python\ndef preprocess(\n        self,\n        images: ImageInput,\n        do_resize: bool = None,\n        size: Dict[str, int] = None,\n        resample=None,\n        do_center_crop: bool = None,\n        crop_size: Dict[str, int] = None,\n        do_rescale: bool = None,\n        rescale_factor: float = None,\n        rescale_offset: bool = None,\n        do_normalize: bool = None,\n        image_mean: Optional[Union[float, List[float]]] = None,\n        image_std: Optional[Union[float, List[float]]] = None,\n        include_top: bool = None,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        data_format: ChannelDimension = ChannelDimension.FIRST,\n        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n        **kwargs,\n    ) -> PIL.Image.Image:\n        \n        \n        \n        do_resize = do_resize if do_resize is not None else self.do_resize\n        resample = resample if resample is not None else self.resample\n        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n        rescale_offset = rescale_offset if rescale_offset is not None else self.rescale_offset\n        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n        image_mean = image_mean if image_mean is not None else self.image_mean\n        image_std = image_std if image_std is not None else self.image_std\n        include_top = include_top if include_top is not None else self.include_top\n\n        size = size if size is not None else self.size\n        size = get_size_dict(size)\n        crop_size = crop_size if crop_size is not None else self.crop_size\n        crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n\n        images = make_list_of_images(images)\n\n        if not valid_images(images):\n            raise ValueError(\n                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n            )\n\n        if do_resize and size is None or resample is None:\n            raise ValueError(\"Size and resample must be specified if do_resize is True.\")\n\n        if do_center_crop and crop_size is None:\n            raise ValueError(\"Crop size must be specified if do_center_crop is True.\")\n\n        if do_rescale and rescale_factor is None:\n            raise ValueError(\"Rescale factor must be specified if do_rescale is True.\")\n\n        if do_normalize and (image_mean is None or image_std is None):\n            raise ValueError(\"Image mean and std must be specified if do_normalize is True.\")\n\n        # All transformations expect numpy arrays.\n        images = [to_numpy_array(image) for image in images]\n\n        if is_scaled_image(images[0]) and do_rescale:\n            logger.warning_once(\n                \"It looks like you are trying to rescale already rescaled images. If the input\"\n                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n            )\n\n        if input_data_format is None:\n            # We assume that all images have the same channel dimension format.\n            input_data_format = infer_channel_dimension_format(images[0])\n\n        if do_resize:\n            images = [\n                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n                for image in images\n            ]\n\n        if do_center_crop:\n            images = [\n                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n            ]\n\n        if do_rescale:\n            images = [\n                self.rescale(\n                    image=image, scale=rescale_factor, offset=rescale_offset, input_data_format=input_data_format\n                )\n                for image in images\n            ]\n\n        if do_normalize:\n            images = [\n                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n                for image in images\n            ]\n\n        if include_top:\n            images = [\n                self.normalize(image=image, mean=0, std=image_std, input_data_format=input_data_format)\n                for image in images\n            ]\n\n        images = [\n            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n        ]\n\n        data = {\"pixel_values\": images}\n        return BatchFeature(data=data, tensor_type=return_tensors)\n```\n\nOriginal documentation:\n```python\nPreprocess an image or batch of images.\n\n        Args:\n            images (`ImageInput`):\n                Image to preprocess.\n            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                Whether to resize the image.\n            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n                Size of the image after `resize`.\n            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n                PILImageResampling filter to use if resizing the image Only has an effect if `do_resize` is set to\n                `True`.\n            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                Whether to center crop the image.\n            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                Size of the image after center crop. If one edge the image is smaller than `crop_size`, it will be\n                padded with zeros and then cropped\n            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n                Whether to rescale the image values between [0 - 1].\n            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n            rescale_offset (`bool`, *optional*, defaults to `self.rescale_offset`):\n                Whether to rescale the image between [-scale_range, scale_range] instead of [0, scale_range].\n            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                Whether to normalize the image.\n            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n                Image mean.\n            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                Image standard deviation.\n            include_top (`bool`, *optional*, defaults to `self.include_top`):\n                Rescales the image again for image classification if set to True.\n            return_tensors (`str` or `TensorType`, *optional*):\n                The type of tensors to return. Can be one of:\n                    - `None`: Return a list of `np.ndarray`.\n                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n                The channel dimension format for the output image. Can be one of:\n                    - `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                    - `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n            input_data_format (`ChannelDimension` or `str`, *optional*):\n                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n                from the input image. Can be one of:\n                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Preprocess an image or batch of images.\n\n        Args:\n            images (`ImageInput`):\n                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                Whether to resize the image.\n            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n                Size of the image after `resize`.\n            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n                PILImageResampling filter to use if resizing the image Only has an effect if `do_resize` is set to\n                `True`.\n            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                Whether to center crop the image.\n            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                Size of the image after center crop. If one edge the image is smaller than `crop_size`, it will be\n                padded with zeros and then cropped\n            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n                Whether to rescale the image values between [0 - 1].\n            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n            rescale_offset (`bool`, *optional*, defaults to `self.rescale_offset`):\n                Whether to rescale the image between [-scale_range, scale_range] instead of [0, scale_range].\n            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                Whether to normalize the image.\n            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n                Image mean.\n            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                Image standard deviation.\n            include_top (`bool`, *optional*, defaults to `self.include_top`):\n                Rescales the image again for image classification if set to True.\n            return_tensors (`str` or `TensorType`, *optional*):\n                The type of tensors to return. Can be one of:\n                    - `None`: Return a list of `np.ndarray`.\n                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n                The channel dimension format for the output image. Can be one of:\n                    - `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                    - `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n            input_data_format (`ChannelDimension` or `str`, *optional*):\n                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n                from the input image. Can be one of:\n                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.", "file": "src_transformers_models_efficientnet_image_processing_efficientnet.py", "function": "EfficientNetImageProcessor.preprocess", "project": "transformers", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_property(data):\n        \n        \n        \n        if isinstance(data, dict):\n            return data[server.server]\n        else:\n            return data```\n\nUpdated code:\n```python\ndef parse_property(data, s=None):\n        \n        \n        \n        if s is None:\n            s = server.server\n        if isinstance(data, dict):\n            return data[s]\n        else:\n            return data\n```\n\nOriginal documentation:\n```python\nParse properties of Button or Template object input.\n        Such as `area`, `color` and `button`.\n\n        Args:\n            data: Dict or str\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parse properties of Button or Template object input.\n        Such as `area`, `color` and `button`.\n\n        Args:\n            data: Dict or str\n            s (str): Load from given a server or load from global attribute `server.server`", "file": "module_base_resource.py", "function": "Resource.parse_property", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update_registration(self, regr):\n        \n\n        \n        response = self.net.post(\n            regr.uri, messages.UpdateRegistration(**dict(regr.body)))\n\n        # TODO: Boulder returns httplib.ACCEPTED\n        #assert response.status_code == httplib.OK\n\n        # TODO: Boulder does not set Location or Link on update\n        # (c.f. acme-spec #94)\n\n        updated_regr = self._regr_from_response(\n            response, uri=regr.uri, new_authzr_uri=regr.new_authzr_uri,\n            terms_of_service=regr.terms_of_service)\n        if updated_regr != regr:\n            raise errors.UnexpectedUpdate(regr)\n        return updated_regr```\n\nUpdated code:\n```python\ndef update_registration(self, regr, update=None):\n        \n\n        \n        update = regr.body if update is None else update\n        updated_regr = self._send_recv_regr(\n            regr, body=messages.UpdateRegistration(**dict(update)))\n        if updated_regr != regr:\n            raise errors.UnexpectedUpdate(regr)\n        return updated_regr\n```\n\nOriginal documentation:\n```python\nUpdate registration.\n\n        :pram regr: Registration Resource.\n        :type regr: `.RegistrationResource`\n\n        :returns: Updated Registration Resource.\n        :rtype: `.RegistrationResource`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update registration.\n\n        :param messages.RegistrationResource regr: Registration Resource.\n        :param messages.Registration update: Updated body of the\n            resource. If not provided, body will be taken from `regr`.\n\n        :returns: Updated Registration Resource.\n        :rtype: `.RegistrationResource`", "file": "acme_acme_client.py", "function": "Client.update_registration", "project": "certbot", "change_stats": {"code_changes": 19, "doc_changes": 7, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _crash_handler(log_format):\n     \n    \n    log_crash = logging.StreamHandler(_debug_buffer)\n    log_crash.setFormatter(log_format)\n    log_crash.setLevel(logging.DEBUG)\n    return log_crash```\n\nUpdated code:\n```python\ndef _crash_handler(log_format):\n     \n    \n    log_crash = logging.StreamHandler(_DEBUG_BUFFER)\n    log_crash.setFormatter(log_format)\n    log_crash.setLevel(logging.DEBUG)\n    return log_crash\n```\n\nOriginal documentation:\n```python\nAdd a handler that stores the last 100 debug lines to :attr:'_debug_buffer' for use in\n    crash reports.\n\n    Parameters\n    ----------\n    log_format: :class:`FaceswapFormatter:\n        The formatting to store log messages as\n\n    Returns\n    -------\n    :class:`logging.StreamHandler`\n        The crash log handler\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add a handler that stores the last 100 debug lines to :attr:'_DEBUG_BUFFER' for use in\n    crash reports.\n\n    Parameters\n    ----------\n    log_format: :class:`FaceswapFormatter:\n        The formatting to store log messages as\n\n    Returns\n    -------\n    :class:`logging.StreamHandler`\n        The crash log handler", "file": "lib_logger.py", "function": "_crash_handler", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef decimate(x, q, n=None, axis=-1, zero_phase=True):\n    \n    \n    \n\n    x = cupy.asarray(x)\n\n    if isinstance(n, (list, cupy.ndarray)):\n        b = cupy.asarray(n)\n    else:\n        if n is None:\n            half_len = 10 * q  # reasonable cutoff for our sinc-like function\n            n = 2 * half_len\n\n        b = firwin(n + 1, 1.0 / q, window=\"hamming\")\n\n    sl = [slice(None)] * x.ndim\n\n    if zero_phase:\n        y = resample_poly(x, 1, q, axis=axis, window=b)\n    else:\n        # upfirdn is generally faster than lfilter by a factor equal to the\n        # downsampling factor, since it only calculates the needed outputs\n        n_out = x.shape[axis] // q + bool(x.shape[axis] % q)\n        y = upfirdn(b, x, 1, q, axis)\n        sl[axis] = slice(None, n_out, None)\n\n    return y[tuple(sl)]```\n\nUpdated code:\n```python\ndef decimate(x, q, n=None, ftype='iir', axis=-1, zero_phase=True):\n    \n    \n    \n\n    x = cupy.asarray(x)\n    q = operator.index(q)\n\n    if n is not None:\n        n = operator.index(n)\n\n    result_type = x.dtype\n    if not cupy.issubdtype(result_type, cupy.inexact) \\\n       or result_type.type == cupy.float16:\n        # upcast integers and float16 to float64\n        result_type = cupy.float64\n\n    if ftype == 'fir':\n        if n is None:\n            half_len = 10 * q  # reasonable cutoff for our sinc-like function\n            n = 2 * half_len\n        b, a = firwin(n+1, 1. / q, window='hamming'), 1.\n        b = cupy.asarray(b, dtype=result_type)\n        a = cupy.asarray(a, dtype=result_type)\n    elif ftype == 'iir':\n        iir_use_sos = True\n        if n is None:\n            n = 8\n        sos = cheby1(n, 0.05, 0.8 / q, output='sos')\n        sos = cupy.asarray(sos, dtype=result_type)\n    elif isinstance(ftype, dlti):\n        system = ftype._as_zpk()\n        if system.poles.shape[0] == 0:\n            # FIR\n            system = ftype._as_tf()\n            b, a = system.num, system.den\n            ftype = 'fir'\n        elif (any(cupy.iscomplex(system.poles))\n              or any(cupy.iscomplex(system.poles))\n              or cupy.iscomplex(system.gain)):\n            # sosfilt & sosfiltfilt don't handle complex coeffs\n            iir_use_sos = False\n            system = ftype._as_tf()\n            b, a = system.num, system.den\n        else:\n            iir_use_sos = True\n            sos = zpk2sos(system.zeros, system.poles, system.gain)\n            sos = cupy.asarray(sos, dtype=result_type)\n    else:\n        raise ValueError('invalid ftype')\n\n    sl = [slice(None)] * x.ndim\n\n    if ftype == 'fir':\n        b = b / a\n        if zero_phase:\n            y = resample_poly(x, 1, q, axis=axis, window=b)\n        else:\n            # upfirdn is generally faster than lfilter by a factor equal to the\n            # downsampling factor, since it only calculates the needed outputs\n            n_out = x.shape[axis] // q + bool(x.shape[axis] % q)\n            y = upfirdn(b, x, up=1, down=q, axis=axis)\n            sl[axis] = slice(None, n_out, None)\n\n    else:  # IIR case\n        if zero_phase:\n            if iir_use_sos:\n                y = sosfiltfilt(sos, x, axis=axis)\n            else:\n                y = filtfilt(b, a, x, axis=axis)\n        else:\n            if iir_use_sos:\n                y = sosfilt(sos, x, axis=axis)\n            else:\n                y = lfilter(b, a, x, axis=axis)\n\n        sl[axis] = slice(None, None, q)\n\n    return y[tuple(sl)]\n```\n\nOriginal documentation:\n```python\nDownsample the signal after applying an anti-aliasing filter.\n\n    Parameters\n    ----------\n    x : array_like\n        The signal to be downsampled, as an N-dimensional array.\n    q : int\n        The downsampling factor.\n    n : int or array_like, optional\n        The order of the filter (1 less than the length for FIR) to calculate,\n        or the FIR filter coefficients to employ. Defaults to calculating the\n        coefficients for 20 times the downsampling factor.\n    axis : int, optional\n        The axis along which to decimate.\n    zero_phase : bool, optional\n        Prevent shifting the outputs back by the filter's\n        group delay when using an FIR filter. The default value of ``True`` is\n        recommended, since a phase shift is generally not desired.\n\n    Returns\n    -------\n    y : ndarray\n        The down-sampled signal.\n\n    See Also\n    --------\n    resample : Resample up or down using the FFT method.\n    resample_poly : Resample using polyphase filtering and an FIR filter.\n\n    Notes\n    -----\n    Only FIR filter types are currently supported in cuSignal.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Downsample the signal after applying an anti-aliasing filter.\n\n    By default, an order 8 Chebyshev type I filter is used. A 30 point FIR\n    filter with Hamming window is used if `ftype` is 'fir'.\n\n    Parameters\n    ----------\n    x : array_like\n        The signal to be downsampled, as an N-dimensional array.\n    q : int\n        The downsampling factor. When using IIR downsampling, it is recommended\n        to call `decimate` multiple times for downsampling factors higher than\n        13.\n    n : int, optional\n        The order of the filter (1 less than the length for 'fir'). Defaults to\n        8 for 'iir' and 20 times the downsampling factor for 'fir'.\n    ftype : str {'iir', 'fir'} or ``dlti`` instance, optional\n        If 'iir' or 'fir', specifies the type of lowpass filter. If an instance\n        of an `dlti` object, uses that object to filter before downsampling.\n    axis : int, optional\n        The axis along which to decimate.\n    zero_phase : bool, optional\n        Prevent phase shift by filtering with `filtfilt` instead of `lfilter`\n        when using an IIR filter, and shifting the outputs back by the filter's\n        group delay when using an FIR filter. The default value of ``True`` is\n        recommended, since a phase shift is generally not desired.\n\n    Returns\n    -------\n    y : ndarray\n        The down-sampled signal.\n\n    See Also\n    --------\n    resample : Resample up or down using the FFT method.\n    resample_poly : Resample using polyphase filtering and an FIR filter.", "file": "cupyx_scipy_signal__resample.py", "function": "decimate", "project": "cupy", "change_stats": {"code_changes": 81, "doc_changes": 26, "total_changes": 107}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_before_task_publish_handler_consume(self):\n        \n        \n        \n        self.util_call_before_task_publish_handler(headers_to_use=self.HEADERS_CONSUME)\n\n        task = PaperlessTask.objects.get()\n        self.assertIsNotNone(task)\n        self.assertEqual(self.HEADERS_CONSUME[\"id\"], task.task_id)\n        self.assertListEqual([\"/consume/hello-999.pdf\"], task.task_args)\n        self.assertDictEqual({\"override_tag_ids\": None}, task.task_kwargs)\n        self.assertEqual(\"hello-999.pdf\", task.task_file_name)\n        self.assertEqual(\"documents.tasks.consume_file\", task.task_name)\n        self.assertEqual(celery.states.PENDING, task.status)```\n\nUpdated code:\n```python\ndef test_before_task_publish_handler_consume(self):\n        \n        \n        \n        self.util_call_before_task_publish_handler(\n            headers_to_use=self.HEADERS_CONSUME,\n            body_to_use=self.BODY_CONSUME,\n        )\n\n        task = PaperlessTask.objects.get()\n        self.assertIsNotNone(task)\n        self.assertEqual(self.HEADERS_CONSUME[\"id\"], task.task_id)\n        self.assertEqual(\"hello-999.pdf\", task.task_file_name)\n        self.assertEqual(\"documents.tasks.consume_file\", task.task_name)\n        self.assertEqual(celery.states.PENDING, task.status)\n```\n\nOriginal documentation:\n```python\nGIVEN:\n            - A celery task completed with an exception\n        WHEN:\n            - API call is made to get tasks\n        THEN:\n            - The returned result is the exception info\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "GIVEN:\n            - A celery task is started via the consume folder\n        WHEN:\n            - Task before publish handler is called\n        THEN:\n            - The task is created and marked as pending", "file": "src_documents_tests_test_task_signals.py", "function": "TestTaskSignalHandler.test_before_task_publish_handler_consume", "project": "paperless-ngx", "change_stats": {"code_changes": 9, "doc_changes": 8, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_on(self):\n        \n        return self.current_binary_state```\n\nUpdated code:\n```python\ndef is_on(self) -> bool | None:\n        \n        \n        props = self.fibaro_device.properties\n        if self.current_binary_state:\n            return True\n        if \"brightness\" in props and props.brightness != \"0\":\n            return True\n        if \"currentProgram\" in props and props.currentProgram != \"0\":\n            return True\n        if \"currentProgramID\" in props and props.currentProgramID != \"0\":\n            return True\n\n        return False\n```\n\nOriginal documentation:\n```python\nReturn true if device is on.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return true if device is on.\n\n        Dimmable and RGB lights can be on based on different\n        properties, so we need to check here several values.", "file": "homeassistant_components_fibaro_light.py", "function": "FibaroLight.is_on", "project": "core", "change_stats": {"code_changes": 17, "doc_changes": 5, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef verify_liking(browser, max, min, logger):\n      \n    try:\n        likes_count = browser.execute_script(\n            \"return window._sharedData.entry_data.\"\n            \"PostPage[0].graphql.shortcode_media.edge_media_preview_like\"\n            \".count\")\n\n    except WebDriverException:\n        try:\n            browser.execute_script(\"location.reload()\")\n            update_activity(browser, state=None)\n\n            likes_count = browser.execute_script(\n                \"return window._sharedData.entry_data.\"\n                \"PostPage[0].graphql.shortcode_media.edge_media_preview_like\"\n                \".count\")\n\n        except WebDriverException:\n            try:\n                likes_count = (browser.find_element_by_css_selector(\n                    \"section._1w76c._nlmjy > div > a > span\").text)\n\n                if likes_count:\n                    likes_count = format_number(likes_count)\n                else:\n                    logger.info(\n                        \"Failed to check likes' count  ~empty string\\n\")\n                    return True\n\n            except NoSuchElementException:\n                logger.info(\"Failed to check likes' count\\n\")\n                return True\n\n    if max is not None and likes_count > max:\n        logger.info(\n            \"Not liked this post! ~more likes exist off maximum limit at \"\n            \"{}\".format(likes_count))\n        return False\n    elif min is not None and likes_count < min:\n        logger.info(\n            \"Not liked this post! ~less likes exist off minumum limit \"\n            \"at {}\".format(likes_count)\n        )\n        return False\n\n    return True```\n\nUpdated code:\n```python\ndef verify_liking(browser, maximum, minimum, logger):\n      \n    try:\n        likes_count = browser.execute_script(\n            \"return window._sharedData.entry_data.\"\n            \"PostPage[0].graphql.shortcode_media.edge_media_preview_like\"\n            \".count\")\n\n    except WebDriverException:\n        try:\n            browser.execute_script(\"location.reload()\")\n            update_activity(browser, state=None)\n\n            likes_count = browser.execute_script(\n                \"return window._sharedData.entry_data.\"\n                \"PostPage[0].graphql.shortcode_media.edge_media_preview_like\"\n                \".count\")\n\n        except WebDriverException:\n            try:\n                likes_count = (browser.find_element_by_css_selector(\n                    \"section._1w76c._nlmjy > div > a > span\").text)\n\n                if likes_count:\n                    likes_count = format_number(likes_count)\n                else:\n                    logger.info(\n                        \"Failed to check likes' count  ~empty string\\n\")\n                    return True\n\n            except NoSuchElementException:\n                logger.info(\"Failed to check likes' count\\n\")\n                return True\n\n    if maximum is not None and likes_count > maximum:\n        logger.info(\n            \"Not liked this post! ~more likes exist off maximum limit at \"\n            \"{}\".format(likes_count))\n        return False\n    elif min is not None and likes_count < minimum:\n        logger.info(\n            \"Not liked this post! ~less likes exist off minumum limit \"\n            \"at {}\".format(likes_count)\n        )\n        return False\n\n    return True\n```\n\nOriginal documentation:\n```python\nGet the amount of existing existing likes and compare it against max\n    & min values defined by user\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the amount of existing existing likes and compare it against maximum\n    & minimum values defined by user", "file": "instapy_like_util.py", "function": "verify_liking", "project": "InstaPy", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pprint_columns(columns, width, margin, format_string, format_args, columns_dict, color=\"yellow\"):\n    \n\n    \n    \n    for columns_text in zip_longest(*wrapped_text_generator(columns, width, margin), fillvalue=\"\"):\n        counter = count()\n        # Generate columnar data that correspond to the column names and update them.\n        for k, _ in columns_dict.items():\n            columns_dict[k] = columns_text[next(counter)]\n\n        click.secho(format_string.format(*format_args, **columns_dict), fg=color)```\n\nUpdated code:\n```python\ndef pprint_columns(columns, width, margin, format_string, format_args, columns_dict, color=\"yellow\", **textwrap_kwargs):\n    \n\n    \n    \n    for columns_text in zip_longest(*wrapped_text_generator(columns, width, margin, **textwrap_kwargs), fillvalue=\"\"):\n        counter = count()\n        # Generate columnar data that correspond to the column names and update them.\n        for k, _ in columns_dict.items():\n            columns_dict[k] = columns_text[next(counter)]\n\n        click.secho(format_string.format(*format_args, **columns_dict), fg=color)\n```\n\nOriginal documentation:\n```python\nPrint columns based on list of columnar text, associated formatting string and associated format arguments.\n\n    :param columns: list of columnnar text that go into columns as specified by the format_string\n    :param width: width of the text to be wrapped\n    :param margin: margin to be reduced from width for cleaner UX\n    :param format_string: A format string that has both width and text specifiers set.\n    :param format_args: list of offset specifiers\n    :param columns_dict: arguments dictionary that have dummy values per column\n    :param color: color supplied for rows within the table.\n    :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Print columns based on list of columnar text, associated formatting string and associated format arguments.\n\n    :param columns: list of columnnar text that go into columns as specified by the format_string\n    :param width: width of the text to be wrapped\n    :param margin: margin to be reduced from width for cleaner UX\n    :param format_string: A format string that has both width and text specifiers set.\n    :param format_args: list of offset specifiers\n    :param columns_dict: arguments dictionary that have dummy values per column\n    :param color: color supplied for rows within the table.\n    :param textwrap_kwargs: keyword arguments that are passed to textwrap.wrap\n    :return:", "file": "samcli_commands__utils_table_print.py", "function": "pprint_columns", "project": "aws-sam-cli", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _request_subtitle(self, sub_lang, sub_name, video_id, format):\n        \n        \n        \n        self.report_video_subtitles_request(video_id, sub_lang, format)\n        params = compat_urllib_parse.urlencode({\n            'lang': sub_lang,\n            'name': sub_name,\n            'v': video_id,\n            'fmt': format,\n        })\n        url = 'http://www.youtube.com/api/timedtext?' + params\n        try:\n            sub = compat_urllib_request.urlopen(url).read().decode('utf-8')\n        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n            self._downloader.report_warning(u'unable to download video subtitles for %s: %s' % (sub_lang, compat_str(err)))\n            return\n        if not sub:\n            self._downloader.report_warning(u'Did not fetch video subtitles')\n            return\n        return sub```\n\nUpdated code:\n```python\ndef _request_subtitle(self, sub_lang, sub_name, video_id, format):\n        \n        \n        \n        self.report_video_subtitles_request(video_id, sub_lang, format)\n        params = compat_urllib_parse.urlencode({\n            'lang': sub_lang,\n            'name': sub_name,\n            'v': video_id,\n            'fmt': format,\n        })\n        url = 'http://www.youtube.com/api/timedtext?' + params\n        try:\n            sub = compat_urllib_request.urlopen(url).read().decode('utf-8')\n        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n            return (u'unable to download video subtitles: %s' % compat_str(err), None, None)\n        if not sub:\n            return (u'Did not fetch video subtitles', None, None)\n        return (None, sub_lang, sub)\n```\n\nOriginal documentation:\n```python\nReturn the subtitle as a string or None if they are not found\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return tuple:\n        (error_message, sub_lang, sub)", "file": "youtube_dl_extractor_youtube.py", "function": "YoutubeIE._request_subtitle", "project": "youtube-dl", "change_stats": {"code_changes": 10, "doc_changes": 5, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef resize_to_range(image,\n                    label=None,\n                    min_size=None,\n                    max_size=None,\n                    factor=None,\n                    align_corners=True,\n                    label_layout_is_chw=False,\n                    scope=None,\n                    method=tf.image.ResizeMethod.BILINEAR):\n  \n  \n  with tf.name_scope(scope, 'resize_to_range', [image]):\n    new_tensor_list = []\n    min_size = tf.cast(min_size, tf.float32)\n    if max_size is not None:\n      max_size = tf.cast(max_size, tf.float32)\n      # Modify the max_size to be a multiple of factor plus 1 and make sure the\n      # max dimension after resizing is no larger than max_size.\n      if factor is not None:\n        max_size = (max_size + (factor - (max_size - 1) % factor) % factor\n                    - factor)\n\n    [orig_height, orig_width, _] = resolve_shape(image, rank=3)\n    orig_height = tf.cast(orig_height, tf.float32)\n    orig_width = tf.cast(orig_width, tf.float32)\n    orig_min_size = tf.minimum(orig_height, orig_width)\n\n    # Calculate the larger of the possible sizes\n    large_scale_factor = min_size / orig_min_size\n    large_height = tf.to_int32(tf.ceil(orig_height * large_scale_factor))\n    large_width = tf.to_int32(tf.ceil(orig_width * large_scale_factor))\n    large_size = tf.stack([large_height, large_width])\n\n    new_size = large_size\n    if max_size is not None:\n      # Calculate the smaller of the possible sizes, use that if the larger\n      # is too big.\n      orig_max_size = tf.maximum(orig_height, orig_width)\n      small_scale_factor = max_size / orig_max_size\n      small_height = tf.to_int32(tf.ceil(orig_height * small_scale_factor))\n      small_width = tf.to_int32(tf.ceil(orig_width * small_scale_factor))\n      small_size = tf.stack([small_height, small_width])\n      new_size = tf.cond(\n          tf.cast(tf.reduce_max(large_size), tf.float32) > max_size,\n          lambda: small_size,\n          lambda: large_size)\n    # Ensure that both output sides are multiples of factor plus one.\n    if factor is not None:\n      new_size += (factor - (new_size - 1) % factor) % factor\n    new_tensor_list.append(tf.image.resize_images(\n        image, new_size, method=method, align_corners=align_corners))\n    if label is not None:\n      if label_layout_is_chw:\n        # Input label has shape [channel, height, width].\n        resized_label = tf.expand_dims(label, 3)\n        resized_label = tf.image.resize_nearest_neighbor(\n            resized_label, new_size, align_corners=align_corners)\n        resized_label = tf.squeeze(resized_label, 3)\n      else:\n        # Input label has shape [height, width, channel].\n        resized_label = tf.image.resize_images(\n            label, new_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n            align_corners=align_corners)\n      new_tensor_list.append(resized_label)\n    else:\n      new_tensor_list.append(None)\n    return new_tensor_list```\n\nUpdated code:\n```python\ndef resize_to_range(image,\n                    label=None,\n                    min_size=None,\n                    max_size=None,\n                    factor=None,\n                    keep_aspect_ratio=True,\n                    align_corners=True,\n                    label_layout_is_chw=False,\n                    scope=None,\n                    method=tf.image.ResizeMethod.BILINEAR):\n  \n  \n  with tf.name_scope(scope, 'resize_to_range', [image]):\n    new_tensor_list = []\n    min_size = tf.cast(min_size, tf.float32)\n    if max_size is not None:\n      max_size = tf.cast(max_size, tf.float32)\n      # Modify the max_size to be a multiple of factor plus 1 and make sure the\n      # max dimension after resizing is no larger than max_size.\n      if factor is not None:\n        max_size = (max_size - (max_size - 1) % factor)\n\n    [orig_height, orig_width, _] = resolve_shape(image, rank=3)\n    orig_height = tf.cast(orig_height, tf.float32)\n    orig_width = tf.cast(orig_width, tf.float32)\n    orig_min_size = tf.minimum(orig_height, orig_width)\n\n    # Calculate the larger of the possible sizes\n    large_scale_factor = min_size / orig_min_size\n    large_height = tf.cast(tf.floor(orig_height * large_scale_factor), tf.int32)\n    large_width = tf.cast(tf.floor(orig_width * large_scale_factor), tf.int32)\n    large_size = tf.stack([large_height, large_width])\n\n    new_size = large_size\n    if max_size is not None:\n      # Calculate the smaller of the possible sizes, use that if the larger\n      # is too big.\n      orig_max_size = tf.maximum(orig_height, orig_width)\n      small_scale_factor = max_size / orig_max_size\n      small_height = tf.cast(\n          tf.floor(orig_height * small_scale_factor), tf.int32)\n      small_width = tf.cast(tf.floor(orig_width * small_scale_factor), tf.int32)\n      small_size = tf.stack([small_height, small_width])\n      new_size = tf.cond(\n          tf.cast(tf.reduce_max(large_size), tf.float32) > max_size,\n          lambda: small_size,\n          lambda: large_size)\n    # Ensure that both output sides are multiples of factor plus one.\n    if factor is not None:\n      new_size += (factor - (new_size - 1) % factor) % factor\n    if not keep_aspect_ratio:\n      # If not keep the aspect ratio, we resize everything to max_size, allowing\n      # us to do pre-processing without extra padding.\n      new_size = [tf.reduce_max(new_size), tf.reduce_max(new_size)]\n    new_tensor_list.append(tf.image.resize(\n        image, new_size, method=method, align_corners=align_corners))\n    if label is not None:\n      if label_layout_is_chw:\n        # Input label has shape [channel, height, width].\n        resized_label = tf.expand_dims(label, 3)\n        resized_label = tf.image.resize(\n            resized_label,\n            new_size,\n            method=get_label_resize_method(label),\n            align_corners=align_corners)\n        resized_label = tf.squeeze(resized_label, 3)\n      else:\n        # Input label has shape [height, width, channel].\n        resized_label = tf.image.resize(\n            label,\n            new_size,\n            method=get_label_resize_method(label),\n            align_corners=align_corners)\n      new_tensor_list.append(resized_label)\n    else:\n      new_tensor_list.append(None)\n    return new_tensor_list\n```\n\nOriginal documentation:\n```python\nResizes image or label so their sides are within the provided range.\n\n  The output size can be described by two cases:\n  1. If the image can be rescaled so its minimum size is equal to min_size\n     without the other side exceeding max_size, then do so.\n  2. Otherwise, resize so the largest side is equal to max_size.\n\n  An integer in `range(factor)` is added to the computed sides so that the\n  final dimensions are multiples of `factor` plus one.\n\n  Args:\n    image: A 3D tensor of shape [height, width, channels].\n    label: (optional) A 3D tensor of shape [height, width, channels] (default)\n      or [channels, height, width] when label_layout_is_chw = True.\n    min_size: (scalar) desired size of the smaller image side.\n    max_size: (scalar) maximum allowed size of the larger image side. Note\n      that the output dimension is no larger than max_size and may be slightly\n      smaller than min_size when factor is not None.\n    factor: Make output size multiple of factor plus one.\n    align_corners: If True, exactly align all 4 corners of input and output.\n    label_layout_is_chw: If true, the label has shape [channel, height, width].\n      We support this case because for some instance segmentation dataset, the\n      instance segmentation is saved as [num_instances, height, width].\n    scope: Optional name scope.\n    method: Image resize method. Defaults to tf.image.ResizeMethod.BILINEAR.\n\n  Returns:\n    A 3-D tensor of shape [new_height, new_width, channels], where the image\n    has been resized (with the specified method) so that\n    min(new_height, new_width) == ceil(min_size) or\n    max(new_height, new_width) == ceil(max_size).\n\n  Raises:\n    ValueError: If the image is not a 3D tensor.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Resizes image or label so their sides are within the provided range.\n\n  The output size can be described by two cases:\n  1. If the image can be rescaled so its minimum size is equal to min_size\n     without the other side exceeding max_size, then do so.\n  2. Otherwise, resize so the largest side is equal to max_size.\n\n  An integer in `range(factor)` is added to the computed sides so that the\n  final dimensions are multiples of `factor` plus one.\n\n  Args:\n    image: A 3D tensor of shape [height, width, channels].\n    label: (optional) A 3D tensor of shape [height, width, channels] (default)\n      or [channels, height, width] when label_layout_is_chw = True.\n    min_size: (scalar) desired size of the smaller image side.\n    max_size: (scalar) maximum allowed size of the larger image side. Note\n      that the output dimension is no larger than max_size and may be slightly\n      smaller than max_size when factor is not None.\n    factor: Make output size multiple of factor plus one.\n    keep_aspect_ratio: Boolean, keep aspect ratio or not. If True, the input\n      will be resized while keeping the original aspect ratio. If False, the\n      input will be resized to [max_resize_value, max_resize_value] without\n      keeping the original aspect ratio.\n    align_corners: If True, exactly align all 4 corners of input and output.\n    label_layout_is_chw: If true, the label has shape [channel, height, width].\n      We support this case because for some instance segmentation dataset, the\n      instance segmentation is saved as [num_instances, height, width].\n    scope: Optional name scope.\n    method: Image resize method. Defaults to tf.image.ResizeMethod.BILINEAR.\n\n  Returns:\n    A 3-D tensor of shape [new_height, new_width, channels], where the image\n    has been resized (with the specified method) so that\n    min(new_height, new_width) == ceil(min_size) or\n    max(new_height, new_width) == ceil(max_size).\n\n  Raises:\n    ValueError: If the image is not a 3D tensor.", "file": "research_deeplab_core_preprocess_utils.py", "function": "resize_to_range", "project": "models", "change_stats": {"code_changes": 34, "doc_changes": 8, "total_changes": 42}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,shell,namespace=None,global_namespace=None,\n                 omit__names=0,alias_table=None):\n         \n\n        Completer.__init__(self,namespace,global_namespace)\n\n        self.magic_escape = ESC_MAGIC\n        self.readline = readline\n        delims = self.readline.get_completer_delims()\n        delims = delims.replace(self.magic_escape,'')\n        self.readline.set_completer_delims(delims)\n        self.get_line_buffer = self.readline.get_line_buffer\n        self.get_endidx = self.readline.get_endidx\n        self.omit__names = omit__names\n        self.merge_completions = shell.readline_merge_completions\n        self.shell = shell.shell\n        if alias_table is None:\n            alias_table = {}\n        self.alias_table = alias_table\n        # Regexp to split filenames with spaces in them\n        self.space_name_re = re.compile(r'([^\\\\] )')\n        # Hold a local ref. to glob.glob for speed\n        self.glob = glob.glob\n\n        # Determine if we are running on 'dumb' terminals, like (X)Emacs\n        # buffers, to avoid completion problems.\n        term = os.environ.get('TERM','xterm')\n        self.dumb_terminal = term in ['dumb','emacs']\n        \n        # Special handling of backslashes needed in win32 platforms\n        if sys.platform == \"win32\":\n            self.clean_glob = self._clean_glob_win32\n        else:\n            self.clean_glob = self._clean_glob\n\n        # All active matcher routines for completion\n        self.matchers = [self.python_matches,\n                         self.file_matches,\n                         self.magic_matches,\n                         self.alias_matches,\n                         self.python_func_kw_matches]```\n\nUpdated code:\n```python\ndef __init__(self, shell, namespace=None, global_namespace=None,\n                 omit__names=0, alias_table=None, use_readline=True):\n        \n\n        Completer.__init__(self,namespace,global_namespace)\n\n        self.magic_escape = ESC_MAGIC\n\n        # Readline-dependent code\n        self.use_readline = use_readline\n        if use_readline:\n            import IPython.utils.rlineimpl as readline\n            self.readline = readline\n            delims = self.readline.get_completer_delims()\n            delims = delims.replace(self.magic_escape,'')\n            self.readline.set_completer_delims(delims)\n            self.get_line_buffer = self.readline.get_line_buffer\n            self.get_endidx = self.readline.get_endidx\n        # /end readline-dependent code\n\n        # List where completion matches will be stored\n        self.matches = []\n        self.omit__names = omit__names\n        self.merge_completions = shell.readline_merge_completions\n        self.shell = shell.shell\n        if alias_table is None:\n            alias_table = {}\n        self.alias_table = alias_table\n        # Regexp to split filenames with spaces in them\n        self.space_name_re = re.compile(r'([^\\\\] )')\n        # Hold a local ref. to glob.glob for speed\n        self.glob = glob.glob\n\n        # Determine if we are running on 'dumb' terminals, like (X)Emacs\n        # buffers, to avoid completion problems.\n        term = os.environ.get('TERM','xterm')\n        self.dumb_terminal = term in ['dumb','emacs']\n        \n        # Special handling of backslashes needed in win32 platforms\n        if sys.platform == \"win32\":\n            self.clean_glob = self._clean_glob_win32\n        else:\n            self.clean_glob = self._clean_glob\n\n        # All active matcher routines for completion\n        self.matchers = [self.python_matches,\n                         self.file_matches,\n                         self.magic_matches,\n                         self.alias_matches,\n                         self.python_func_kw_matches,\n                         ]\n```\n\nOriginal documentation:\n```python\nIPCompleter() -> completer\n\n        Return a completer object suitable for use by the readline library\n        via readline.set_completer().\n\n        Inputs:\n\n        - shell: a pointer to the ipython shell itself.  This is needed\n        because this completer knows about magic functions, and those can\n        only be accessed via the ipython instance.\n\n        - namespace: an optional dict where completions are performed.\n\n        - global_namespace: secondary optional dict for completions, to\n        handle cases (such as IPython embedded inside functions) where\n        both Python scopes are visible.\n\n        - The optional omit__names parameter sets the completer to omit the\n        'magic' names (__magicname__) for python objects unless the text\n        to be completed explicitly starts with one or more underscores.\n\n        - If alias_table is supplied, it should be a dictionary of aliases\n        to complete.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "IPCompleter() -> completer\n\n        Return a completer object suitable for use by the readline library\n        via readline.set_completer().\n\n        Inputs:\n\n        - shell: a pointer to the ipython shell itself.  This is needed\n        because this completer knows about magic functions, and those can\n        only be accessed via the ipython instance.\n\n        - namespace: an optional dict where completions are performed.\n\n        - global_namespace: secondary optional dict for completions, to\n        handle cases (such as IPython embedded inside functions) where\n        both Python scopes are visible.\n\n        - The optional omit__names parameter sets the completer to omit the\n        'magic' names (__magicname__) for python objects unless the text\n        to be completed explicitly starts with one or more underscores.\n\n        - If alias_table is supplied, it should be a dictionary of aliases\n        to complete.\n\n        use_readline : bool, optional\n          If true, use the readline library.  This completer can still function\n          without readline, though in that case callers must provide some extra\n          information on each call about the current line.", "file": "IPython_core_completer.py", "function": "IPCompleter.__init__", "project": "ipython", "change_stats": {"code_changes": 32, "doc_changes": 7, "total_changes": 39}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _str_extract_noexpand(arr, pat, flags=0):\n    \n    \n    \n    from pandas import array as pd_array\n\n    regex = re.compile(pat, flags=flags)\n    groups_or_na = _groups_or_na_fun(regex)\n    result_dtype = _result_dtype(arr)\n\n    result = np.array([groups_or_na(val)[0] for val in np.asarray(arr)], dtype=object)\n    # not dispatching, so we have to reconstruct here.\n    result = pd_array(result, dtype=result_dtype)\n    return result```\n\nUpdated code:\n```python\ndef _str_extract_noexpand(arr: ArrayLike, pat: str, flags=0):\n    \n    \n    \n    regex = re.compile(pat, flags=flags)\n    groups_or_na = _groups_or_na_fun(regex)\n\n    result = np.array([groups_or_na(val)[0] for val in np.asarray(arr)], dtype=object)\n    return result\n```\n\nOriginal documentation:\n```python\nFind groups in each string in the Series/Index using passed regular expression.\n\n    This function is called from str_extract(expand=False) when there is a single group\n    in the regex.\n\n    Returns\n    -------\n    np.ndarray\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Find groups in each string in the array using passed regular expression.\n\n    This function is called from str_extract(expand=False) when there is a single group\n    in the regex.\n\n    Returns\n    -------\n    np.ndarray", "file": "pandas_core_strings_accessor.py", "function": "_str_extract_noexpand", "project": "pandas", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef call(self, inputs, state):\n        \n        \n        state = tf.expand_dims(state[0], 2)\n        transition_scores = state + self._transition_params\n        new_state = inputs + tf.reduce_max(transition_scores, [1])\n        backpointers = tf.argmax(transition_scores, 1)\n        backpointers = tf.cast(backpointers, tf.float32)\n        return tf.concat([backpointers, new_state], axis=1), new_state```\n\nUpdated code:\n```python\ndef call(\n        self, inputs: TensorLike, state: TensorLike\n    ) -> Tuple[tf.Tensor, tf.Tensor]:\n        \n        \n        state = tf.expand_dims(state[0], 2)\n        transition_scores = state + self._transition_params\n        new_state = inputs + tf.reduce_max(transition_scores, [1])\n        backpointers = tf.argmax(transition_scores, 1)\n        backpointers = tf.cast(backpointers, tf.float32)\n        scores = tf.reduce_max(tf.nn.softmax(transition_scores, axis=1), [1])\n        return tf.concat([backpointers, scores], axis=1), new_state\n```\n\nOriginal documentation:\n```python\nBuild the CrfDecodeForwardRnnCell.\n\n        Args:\n          inputs: A [batch_size, num_tags] matrix of unary potentials.\n          state: A [batch_size, num_tags] matrix containing the previous step's\n                score values.\n\n        Returns:\n          backpointers: A [batch_size, num_tags] matrix of backpointers.\n          new_state: A [batch_size, num_tags] matrix of new score values.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Build the CrfDecodeForwardRnnCell.\n\n        Args:\n          inputs: A [batch_size, num_tags] matrix of unary potentials.\n          state: A [batch_size, num_tags] matrix containing the previous step's\n                score values.\n\n        Returns:\n          output: A [batch_size, num_tags * 2] matrix of backpointers and scores.\n          new_state: A [batch_size, num_tags] matrix of new score values.", "file": "rasa_utils_tensorflow_crf.py", "function": "CrfDecodeForwardRnnCell.call", "project": "rasa", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_sparsity_levels(pruner, precision=32):\n        \n        return [round(group['sparsity_level'], precision) for group in pruner.groups]```\n\nUpdated code:\n```python\ndef _get_sparsity_levels(sparsifier, precision=32):\n        \n        return [round(group['sparsity_level'], precision) for group in sparsifier.groups]\n```\n\nOriginal documentation:\n```python\nr\"\"\"Gets the current levels of sparsity in a pruner.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Gets the current levels of sparsity in a sparsifier.", "file": "test_ao_sparsity_test_scheduler.py", "function": "TestCubicScheduler._get_sparsity_levels", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef render_str(self, text: str) -> \"Text\":\n        \n\n        \n        if self._markup:\n            return markup.render(text)\n\n        return markup.render_text(text)```\n\nUpdated code:\n```python\ndef render_str(self, text: str, style: Union[str, Style] = \"\") -> \"Text\":\n        \n\n        \n        if self._markup:\n            return markup.render(text, style=style)\n\n        from .text import Text\n\n        return Text(text, style=style)\n```\n\nOriginal documentation:\n```python\nConvert a string to a Text instance.\n\n        Args:\n            text (str): Text to render.\n\n        Returns:\n            ConsoleRenderable: Renderable object.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert a string to a Text instance.\n\n        Args:\n            text (str): Text to render.\n            style (Union[str, Style], optional): Style to apply to rendered text.\n        Returns:\n            ConsoleRenderable: Renderable object.", "file": "rich_console.py", "function": "Console.render_str", "project": "rich", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef push_line(self, line):\n        \n        \n\n        # autoindent management should be done here, and not in the\n        # interactive loop, since that one is only seen by keyboard input.  We\n        # need this done correctly even for code run via runlines (which uses\n        # push).\n\n        #print 'push line: <%s>' % line  # dbg\n        self.buffer.append(line)\n        full_source = '\\n'.join(self.buffer)\n        more = self.runsource(full_source, self.filename)\n        if not more:\n            self.history_manager.store_inputs('\\n'.join(self.buffer_raw),\n                                              full_source)\n            self.resetbuffer()\n            self.execution_count += 1\n        return more```\n\nUpdated code:\n```python\ndef push_line(self, line):\n        \n        \n\n        # autoindent management should be done here, and not in the\n        # interactive loop, since that one is only seen by keyboard input.  We\n        # need this done correctly even for code run via runlines (which uses\n        # push).\n\n        #print 'push line: <%s>' % line  # dbg\n        self.buffer.append(line)\n        full_source = '\\n'.join(self.buffer)\n        more = self.run_source(full_source, self.filename)\n        if not more:\n            self.history_manager.store_inputs('\\n'.join(self.buffer_raw),\n                                              full_source)\n            self.reset_buffer()\n            self.execution_count += 1\n        return more\n```\n\nOriginal documentation:\n```python\nPush a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource()).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's run_source() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as run_source()).", "file": "IPython_core_interactiveshell.py", "function": "InteractiveShell.push_line", "project": "ipython", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef retrieve_entity_from(entity_id: Union[str, int], events: List[Dict], actions: List[Dict]) -> Optional[Dict]:\n    \n    \n    \n    for event in events:\n        if event.get(\"id\") == entity_id:\n            return event\n    for action in actions:\n        if action.get(\"id\") == entity_id:\n            return action\n    return None```\n\nUpdated code:\n```python\ndef retrieve_entity_from(entity_id: str, events: List[Dict], actions: List[Dict]) -> Optional[Dict]:\n    \n    \n    \n    for event in events:\n        if event.get(\"id\") == entity_id:\n            return event\n    for action in actions:\n        if str(action.get(\"id\")) == entity_id:\n            return action\n    return None\n```\n\nOriginal documentation:\n```python\nRetrieves the entity from the events and actions.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieves the entity from the events and actions.\n\n    NOTE: entity_id here is considered always to be a string. event ids are\n    strings, and action ids are ints. Elsewhere we get the `entity_id` from a\n    get request, from which we do not get type information, and we do not\n    require the entity type to be provided. A more complete solution might be to\n    require entity type information, but to resolve the issue we cast the action\n    id to a string, such that we can get equality.\n\n    This doesn't preclude ths issue that an event name could be a string that is\n    also a valid number however, but this should be an unlikely occurance.", "file": "posthog_api_utils.py", "function": "retrieve_entity_from", "project": "posthog", "change_stats": {"code_changes": 6, "doc_changes": 12, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef local_maxima(image, selem=None, connectivity=None, indices=False,\n                 allow_borders=True):\n    \n    \n    image = np.asarray(image, order=\"C\")\n    if image.size == 0:\n        # Return early for empty input\n        if indices:\n            # Make sure that output is a tuple of 1 empty array per dimension\n            return np.nonzero(image)\n        else:\n            return np.zeros(image.shape, dtype=np.uint8)\n\n    if allow_borders:\n        # Ensure that local maxima are always at least one smaller sample away\n        # from the image border\n        image = _fast_pad(image, image.min())\n\n    # Array of flags used to store the state of each pixel during evaluation.\n    # See _extrema_cy.pyx for their meaning\n    flags = np.zeros(image.shape, dtype=np.uint8)\n    _set_edge_values_inplace(flags, value=3)\n\n    if any(s < 3 for s in image.shape):\n        # Warn and skip if any dimension is smaller than 3\n        # -> no maxima can exist & structuring element can't be applied\n        warn(\n            \"maxima can't exist for an image with any dimension smaller 3 \"\n            \"if borders aren't allowed\",\n            stacklevel=3\n        )\n    else:\n        selem = _resolve_neighborhood(selem, connectivity, image.ndim)\n        neighbor_offsets = _offsets_to_raveled_neighbors(\n            image.shape, selem, center=((1,) * image.ndim)\n        )\n\n        try:\n            _local_maxima(image.ravel(), flags.ravel(), neighbor_offsets)\n        except TypeError:\n            if image.dtype == np.float16:\n                # Provide the user with clearer error message\n                raise TypeError(\"dtype of `image` is float16 which is not \"\n                                \"supported, try upcasting to float32\")\n            else:\n                raise  # Otherwise raise original message\n\n    if allow_borders:\n        # Revert padding performed at the beginning of the function\n        flags = crop(flags, 1)\n    else:\n        # No padding was performed but set edge values back to 0\n        _set_edge_values_inplace(flags, value=0)\n\n    if indices:\n        return np.nonzero(flags)\n    else:\n        return flags```\n\nUpdated code:\n```python\ndef local_maxima(image, selem=None, connectivity=None, indices=False,\n                 allow_borders=True):\n    \n    \n    image = np.asarray(image, order=\"C\")\n    if image.size == 0:\n        # Return early for empty input\n        if indices:\n            # Make sure that output is a tuple of 1 empty array per dimension\n            return np.nonzero(image)\n        else:\n            return np.zeros(image.shape, dtype=np.bool)\n\n    if allow_borders:\n        # Ensure that local maxima are always at least one smaller sample away\n        # from the image border\n        image = _fast_pad(image, image.min())\n\n    # Array of flags used to store the state of each pixel during evaluation.\n    # See _extrema_cy.pyx for their meaning\n    flags = np.zeros(image.shape, dtype=np.uint8)\n    _set_edge_values_inplace(flags, value=3)\n\n    if any(s < 3 for s in image.shape):\n        # Warn and skip if any dimension is smaller than 3\n        # -> no maxima can exist & structuring element can't be applied\n        warn(\n            \"maxima can't exist for an image with any dimension smaller 3 \"\n            \"if borders aren't allowed\",\n            stacklevel=3\n        )\n    else:\n        selem = _resolve_neighborhood(selem, connectivity, image.ndim)\n        neighbor_offsets = _offsets_to_raveled_neighbors(\n            image.shape, selem, center=((1,) * image.ndim)\n        )\n\n        try:\n            _local_maxima(image.ravel(), flags.ravel(), neighbor_offsets)\n        except TypeError:\n            if image.dtype == np.float16:\n                # Provide the user with clearer error message\n                raise TypeError(\"dtype of `image` is float16 which is not \"\n                                \"supported, try upcasting to float32\")\n            else:\n                raise  # Otherwise raise original message\n\n    if allow_borders:\n        # Revert padding performed at the beginning of the function\n        flags = crop(flags, 1)\n    else:\n        # No padding was performed but set edge values back to 0\n        _set_edge_values_inplace(flags, value=0)\n\n    if indices:\n        return np.nonzero(flags)\n    else:\n        return flags.view(np.bool)\n```\n\nOriginal documentation:\n```python\nFind local maxima of n-dimensional array.\n\n    The local maxima are defined as connected sets of pixels with equal gray\n    level (plateaus) strictly greater than the gray levels of all pixels in the\n    neighborhood.\n\n    Parameters\n    ----------\n    image : ndarray\n        An n-dimensional array.\n    selem : ndarray, optional\n        A structuring element used to determine the neighborhood of each\n        evaluated pixel. It must contain only 1's and 0's, have the same number\n        of dimensions as `image`. If not given, all adjacent pixels are\n        considered as part of the neighborhood.\n    connectivity : int, optional\n        A number used to determine the neighborhood of each evaluated pixel.\n        Adjacent pixels whose squared distance from the center is larger or\n        equal to `connectivity` are considered neighbors. Ignored if\n        `selem` is not None.\n    indices : bool, optional\n        If True, the output will be a tuple of one-dimensional arrays\n        representing the indices of local maxima in each dimension. If False,\n        the output will be an array of 0's and 1's with the same shape as\n        `image`.\n    allow_borders : bool, optional\n        If true, plateaus that touch the image border are valid maxima.\n\n    Returns\n    -------\n    maxima : ndarray or tuple[ndarray]\n        If `indices` is false, an array with the same shape as `image` is\n        returned with 1's indicating the position of local maxima\n        (0 otherwise). If `indices` is true, a tuple of one-dimensional arrays\n        containing the coordinates (indices) of all found maxima.\n\n    Warns\n    -----\n    UserWarning\n        If `allow_borders` is false and any dimension of the given `image` is\n        shorter than 3 samples, maxima can't exist and a warning is shown.\n\n    See Also\n    --------\n    skimage.morphology.local_minima\n    skimage.morphology.h_maxima\n    skimage.morphology.h_minima\n\n    Notes\n    -----\n    This function operates on the following ideas:\n\n    1. Make a first pass over the image's last dimension and flag candidates\n       for local maxima by comparing pixels in only one direction.\n       If the pixels aren't connected in the last dimension all pixels are\n       flagged as candidates instead.\n\n    For each candidate:\n\n    2. Perform a flood-fill to find all connected pixels that have the same\n       gray value and are part of the plateau.\n    3. Consider the connected neighborhood of a plateau: if no bordering sample\n       has a higher gray level, mark the plateau as a definite local maximum.\n\n    Examples\n    --------\n    >>> from skimage.morphology import local_maxima\n    >>> image = np.zeros((4, 7), dtype=int)\n    >>> image[1:3, 1:3] = 1\n    >>> image[3, 0] = 1\n    >>> image[1:3, 4:6] = 2\n    >>> image[3, 6] = 3\n    >>> image\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 1, 0, 2, 2, 0],\n           [0, 1, 1, 0, 2, 2, 0],\n           [1, 0, 0, 0, 0, 0, 3]])\n\n    Find local maxima by comparing to all neighboring pixels (maximal\n    connectivity):\n\n    >>> local_maxima(image)\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 1, 0, 0, 0, 0],\n           [0, 1, 1, 0, 0, 0, 0],\n           [1, 0, 0, 0, 0, 0, 1]], dtype=uint8)\n    >>> local_maxima(image, indices=True)\n    (array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 0, 6]))\n\n    Find local maxima without comparing to diagonal pixels (connectivity 1):\n\n    >>> local_maxima(image, connectivity=1)\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 1, 0, 1, 1, 0],\n           [0, 1, 1, 0, 1, 1, 0],\n           [1, 0, 0, 0, 0, 0, 1]], dtype=uint8)\n\n    and exclude maxima that border the image edge:\n\n    >>> local_maxima(image, connectivity=1, allow_borders=False)\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 1, 0, 1, 1, 0],\n           [0, 1, 1, 0, 1, 1, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Find local maxima of n-dimensional array.\n\n    The local maxima are defined as connected sets of pixels with equal gray\n    level (plateaus) strictly greater than the gray levels of all pixels in the\n    neighborhood.\n\n    Parameters\n    ----------\n    image : ndarray\n        An n-dimensional array.\n    selem : ndarray, optional\n        A structuring element used to determine the neighborhood of each\n        evaluated pixel (``True`` denotes a connected pixel). It must be a\n        boolean array and have the same number of dimensions as `image`. If\n        neither `selem` nor `connectivity` are given, all adjacent pixels are\n        considered as part of the neighborhood.\n    connectivity : int, optional\n        A number used to determine the neighborhood of each evaluated pixel.\n        Adjacent pixels whose squared distance from the center is larger or\n        equal to `connectivity` are considered neighbors. Ignored if\n        `selem` is not None.\n    indices : bool, optional\n        If True, the output will be a tuple of one-dimensional arrays\n        representing the indices of local maxima in each dimension. If False,\n        the output will be a boolean array with the same shape as `image`.\n    allow_borders : bool, optional\n        If true, plateaus that touch the image border are valid maxima.\n\n    Returns\n    -------\n    maxima : ndarray or tuple[ndarray]\n        If `indices` is false, a boolean array with the same shape as `image`\n        is returned with ``True`` indicating the position of local maxima\n        (``False`` otherwise). If `indices` is true, a tuple of one-dimensional\n        arrays containing the coordinates (indices) of all found maxima.\n\n    Warns\n    -----\n    UserWarning\n        If `allow_borders` is false and any dimension of the given `image` is\n        shorter than 3 samples, maxima can't exist and a warning is shown.\n\n    See Also\n    --------\n    skimage.morphology.local_minima\n    skimage.morphology.h_maxima\n    skimage.morphology.h_minima\n\n    Notes\n    -----\n    This function operates on the following ideas:\n\n    1. Make a first pass over the image's last dimension and flag candidates\n       for local maxima by comparing pixels in only one direction.\n       If the pixels aren't connected in the last dimension all pixels are\n       flagged as candidates instead.\n\n    For each candidate:\n\n    2. Perform a flood-fill to find all connected pixels that have the same\n       gray value and are part of the plateau.\n    3. Consider the connected neighborhood of a plateau: if no bordering sample\n       has a higher gray level, mark the plateau as a definite local maximum.\n\n    Examples\n    --------\n    >>> from skimage.morphology import local_maxima\n    >>> image = np.zeros((4, 7), dtype=int)\n    >>> image[1:3, 1:3] = 1\n    >>> image[3, 0] = 1\n    >>> image[1:3, 4:6] = 2\n    >>> image[3, 6] = 3\n    >>> image\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 1, 0, 2, 2, 0],\n           [0, 1, 1, 0, 2, 2, 0],\n           [1, 0, 0, 0, 0, 0, 3]])\n\n    Find local maxima by comparing to all neighboring pixels (maximal\n    connectivity):\n\n    >>> local_maxima(image)\n    array([[False, False, False, False, False, False, False],\n           [False,  True,  True, False, False, False, False],\n           [False,  True,  True, False, False, False, False],\n           [ True, False, False, False, False, False,  True]], dtype=bool)\n    >>> local_maxima(image, indices=True)\n    (array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 0, 6]))\n\n    Find local maxima without comparing to diagonal pixels (connectivity 1):\n\n    >>> local_maxima(image, connectivity=1)\n    array([[False, False, False, False, False, False, False],\n           [False,  True,  True, False,  True,  True, False],\n           [False,  True,  True, False,  True,  True, False],\n           [ True, False, False, False, False, False,  True]], dtype=bool)\n\n    and exclude maxima that border the image edge:\n\n    >>> local_maxima(image, connectivity=1, allow_borders=False)\n    array([[False, False, False, False, False, False, False],\n           [False,  True,  True, False,  True,  True, False],\n           [False,  True,  True, False,  True,  True, False],\n           [False, False, False, False, False, False, False]], dtype=bool)", "file": "skimage_morphology_extrema.py", "function": "local_maxima", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 42, "total_changes": 48}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef search_for_installed_packages(\n        self,\n        specification: PackageSpecification,\n    ) -> list[Package]:\n        \n        \n        \n        if not self._installed_packages:\n            return []\n\n        logger.debug(\n            \"Falling back to installed packages to discover metadata for <c2>%s</>\",\n            specification.complete_name,\n        )\n        packages = [\n            package\n            for package in self._installed_packages\n            if package.provides(specification)\n        ]\n        logger.debug(\n            \"Found <c2>%d</> compatible packages for <c2>%s</>\",\n            len(packages),\n            specification.complete_name,\n        )\n        return packages```\n\nUpdated code:\n```python\ndef search_for_installed_packages(\n        self,\n        dependency: Dependency,\n    ) -> list[Package]:\n        \n        \n        \n        if not self._installed_packages:\n            return []\n\n        logger.debug(\n            \"Falling back to installed packages to discover metadata for <c2>%s</>\",\n            dependency.complete_name,\n        )\n        packages = [\n            package\n            for package in self._installed_packages\n            if package.satisfies(dependency, ignore_source_type=True)\n        ]\n        logger.debug(\n            \"Found <c2>%d</> compatible packages for <c2>%s</>\",\n            len(packages),\n            dependency.complete_name,\n        )\n        return packages\n```\n\nOriginal documentation:\n```python\nSearch for installed packages, when available, that provides the given\n        specification.\n\n        This is useful when dealing with packages that are under development, not\n        published on package sources and/or only available via system installations.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Search for installed packages, when available, that satisfy the given\n        dependency.\n\n        This is useful when dealing with packages that are under development, not\n        published on package sources and/or only available via system installations.", "file": "src_poetry_puzzle_provider.py", "function": "Provider.search_for_installed_packages", "project": "poetry", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _kmeans_single_elkan(X, sample_weight, n_clusters, max_iter=300,\n                         init='k-means++', verbose=False, x_squared_norms=None,\n                         random_state=None, tol=1e-4, n_threads=1):\n    \n    \n    random_state = check_random_state(random_state)\n    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n    # init\n    centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n                              x_squared_norms=x_squared_norms)\n\n    if verbose:\n        print('Initialization complete')\n\n    n_samples = X.shape[0]\n\n    centers_new = np.zeros_like(centers)\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n    labels = np.full(n_samples, -1, dtype=np.int32)\n    center_half_distances = euclidean_distances(centers) / 2\n    distance_next_center = np.partition(np.asarray(center_half_distances),\n                                        kth=1, axis=0)[1]\n    upper_bounds = np.zeros(n_samples, dtype=X.dtype)\n    lower_bounds = np.zeros((n_samples, n_clusters), dtype=X.dtype)\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n\n    if sp.issparse(X):\n        init_bounds = init_bounds_sparse\n        elkan_iter = elkan_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        init_bounds = init_bounds_dense\n        elkan_iter = elkan_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    init_bounds(X, centers, center_half_distances,\n                labels, upper_bounds, lower_bounds)\n\n    for i in range(max_iter):\n        elkan_iter(X, sample_weight, centers, centers_new,\n                   weight_in_clusters, center_half_distances,\n                   distance_next_center, upper_bounds, lower_bounds,\n                   labels, center_shift, n_threads)\n\n        # compute new pairwise distances between centers and closest other\n        # center of each center for next iterations\n        center_half_distances = euclidean_distances(centers_new) / 2\n        distance_next_center = np.partition(\n            np.asarray(center_half_distances), kth=1, axis=0)[1]\n\n        if verbose:\n            inertia = _inertia(X, sample_weight, centers, labels)\n            print(\"Iteration {0}, inertia {1}\" .format(i, inertia))\n\n        center_shift_tot = (center_shift**2).sum()\n        if center_shift_tot <= tol:\n            if verbose:\n                print(\"Converged at iteration {0}: \"\n                      \"center shift {1} within tolerance {2}\"\n                      .format(i, center_shift_tot, tol))\n            break\n\n        centers, centers_new = centers_new, centers\n\n    if center_shift_tot > 0:\n        # rerun E-step so that predicted labels match cluster centers\n        elkan_iter(X, sample_weight, centers, centers, weight_in_clusters,\n                   center_half_distances, distance_next_center,\n                   upper_bounds, lower_bounds, labels, center_shift,\n                   n_threads, update_centers=False)\n\n    inertia = _inertia(X, sample_weight, centers, labels)\n\n    return labels, inertia, centers, i + 1```\n\nUpdated code:\n```python\ndef _kmeans_single_elkan(X, sample_weight, centers_init, max_iter=300,\n                         verbose=False, x_squared_norms=None, tol=1e-4,\n                         n_threads=1):\n    \n    \n    n_samples = X.shape[0]\n    n_clusters = centers_init.shape[0]\n\n    # Buffers to avoid new allocations at each iteration.\n    centers = centers_init\n    centers_new = np.zeros_like(centers)\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n    labels = np.full(n_samples, -1, dtype=np.int32)\n    center_half_distances = euclidean_distances(centers) / 2\n    distance_next_center = np.partition(np.asarray(center_half_distances),\n                                        kth=1, axis=0)[1]\n    upper_bounds = np.zeros(n_samples, dtype=X.dtype)\n    lower_bounds = np.zeros((n_samples, n_clusters), dtype=X.dtype)\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n\n    if sp.issparse(X):\n        init_bounds = init_bounds_sparse\n        elkan_iter = elkan_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        init_bounds = init_bounds_dense\n        elkan_iter = elkan_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    init_bounds(X, centers, center_half_distances,\n                labels, upper_bounds, lower_bounds)\n\n    for i in range(max_iter):\n        elkan_iter(X, sample_weight, centers, centers_new,\n                   weight_in_clusters, center_half_distances,\n                   distance_next_center, upper_bounds, lower_bounds,\n                   labels, center_shift, n_threads)\n\n        # compute new pairwise distances between centers and closest other\n        # center of each center for next iterations\n        center_half_distances = euclidean_distances(centers_new) / 2\n        distance_next_center = np.partition(\n            np.asarray(center_half_distances), kth=1, axis=0)[1]\n\n        if verbose:\n            inertia = _inertia(X, sample_weight, centers, labels)\n            print(f\"Iteration {i}, inertia {inertia}\")\n\n        centers, centers_new = centers_new, centers\n\n        center_shift_tot = (center_shift**2).sum()\n        if center_shift_tot <= tol:\n            if verbose:\n                print(f\"Converged at iteration {i}: center shift \"\n                      f\"{center_shift_tot} within tolerance {tol}.\")\n            break\n\n    if center_shift_tot > 0:\n        # rerun E-step so that predicted labels match cluster centers\n        elkan_iter(X, sample_weight, centers, centers, weight_in_clusters,\n                   center_half_distances, distance_next_center,\n                   upper_bounds, lower_bounds, labels, center_shift,\n                   n_threads, update_centers=False)\n\n    inertia = _inertia(X, sample_weight, centers, labels)\n\n    return labels, inertia, centers, i + 1\n```\n\nOriginal documentation:\n```python\nA single run of k-means elkan, assumes preparation completed prior.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. If sparse matrix, must be in CSR format.\n\n    sample_weight : array-like of shape (n_samples,)\n        The weights for each observation in X.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    init : {'k-means++', 'random', ndarray, callable}, default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    verbose : bool, default=False\n        Verbosity mode\n\n    x_squared_norms : array-like, default=None\n        Precomputed x_squared_norms.\n\n    random_state : int, RandomState instance, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n        It's not advised to set `tol=0` since convergence might never be\n        declared due to rounding errors. Use a very small number instead.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    n_iter : int\n        Number of iterations run.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A single run of k-means elkan, assumes preparation completed prior.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. If sparse matrix, must be in CSR format.\n\n    sample_weight : array-like of shape (n_samples,)\n        The weights for each observation in X.\n\n    centers_init : ndarray of shape (n_clusters, n_features)\n        The initial centers.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode\n\n    x_squared_norms : array-like, default=None\n        Precomputed x_squared_norms.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n        It's not advised to set `tol=0` since convergence might never be\n        declared due to rounding errors. Use a very small number instead.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    n_iter : int\n        Number of iterations run.", "file": "sklearn_cluster__kmeans.py", "function": "_kmeans_single_elkan", "project": "scikit-learn", "change_stats": {"code_changes": 34, "doc_changes": 28, "total_changes": 62}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _generate_normals(self, polygons):\n        \n        \n\n        \n        if isinstance(polygons, np.ndarray):\n            # optimization: polygons all have the same number of points, so can\n            # vectorize\n            n = polygons.shape[-2]\n            i1, i2, i3 = 0, n//3, 2*n//3\n            v1 = polygons[..., i1, :] - polygons[..., i2, :]\n            v2 = polygons[..., i2, :] - polygons[..., i3, :]\n        else:\n            # The subtraction doesn't vectorize because polygons is jagged.\n            v1 = np.empty((len(polygons), 3))\n            v2 = np.empty((len(polygons), 3))\n            for poly_i, ps in enumerate(polygons):\n                n = len(ps)\n                i1, i2, i3 = 0, n//3, 2*n//3\n                v1[poly_i, :] = ps[i1, :] - ps[i2, :]\n                v2[poly_i, :] = ps[i2, :] - ps[i3, :]\n        return np.cross(v1, v2)```\n\nUpdated code:\n```python\ndef _generate_normals(self, polygons):\n        '''\n        \n        '''\n\n        normals = []\n        for verts in polygons:\n            v1 = np.array(verts[0]) - np.array(verts[1])\n            v2 = np.array(verts[2]) - np.array(verts[0])\n            normals.append(np.cross(v1, v2))\n        return normals\n```\n\nOriginal documentation:\n```python\nTakes a list of polygons and return an array of their normals.\n\n        Normals point towards the viewer for a face with its vertices in\n        counterclockwise order, following the right hand rule.\n\n        Uses three points equally spaced around the polygon.\n        This normal of course might not make sense for polygons with more than\n        three points not lying in a plane, but it's a plausible and fast\n        approximation.\n\n        Parameters\n        ----------\n        polygons: list of (M_i, 3) array_like, or (..., M, 3) array_like\n            A sequence of polygons to compute normals for, which can have\n            varying numbers of vertices. If the polygons all have the same\n            number of vertices and array is passed, then the operation will\n            be vectorized.\n\n        Returns\n        -------\n        normals: (..., 3) array_like\n            A normal vector estimated for the polygon.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate normals for polygons by using the first three points.\n        This normal of course might not make sense for polygons with\n        more than three points not lying in a plane.", "file": "lib_mpl_toolkits_mplot3d_axes3d.py", "function": "Axes3D._generate_normals", "project": "matplotlib", "change_stats": {"code_changes": 29, "doc_changes": 27, "total_changes": 56}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef reload(\n        self,\n        devices: Union[str, List[str]],\n        on_done: CallbackFnType = None,\n        on_error: CallbackFnType = None,\n        on_always: CallbackFnType = None,\n        **kwargs\n    ):\n        \n        \n\n        if isinstance(devices, str):\n            devices = [devices]\n        kwargs['devices'] = devices\n\n        self.mode = RequestType.CONTROL\n        return run_async(\n            self._get_results,\n            [],\n            on_done,\n            on_error,\n            on_always,\n            command='RELOAD',\n            **kwargs\n        )```\n\nUpdated code:\n```python\ndef reload(\n        self,\n        targets: Union[str, List[str]],\n        on_done: CallbackFnType = None,\n        on_error: CallbackFnType = None,\n        on_always: CallbackFnType = None,\n        **kwargs\n    ):\n        \n        \n\n        if isinstance(targets, str):\n            targets = [targets]\n        kwargs['targets'] = targets\n\n        self.mode = RequestType.CONTROL\n        return run_async(\n            self._get_results,\n            [],\n            on_done,\n            on_error,\n            on_always,\n            command='RELOAD',\n            **kwargs\n        )\n```\n\nOriginal documentation:\n```python\nSend 'reload' request to the Flow.\n\n        :param devices: the regex string or list of regex strings to match the pea/pod names.\n        :param on_done: the function to be called when the :class:`Request` object is resolved.\n        :param on_error: the function to be called when the :class:`Request` object is rejected.\n        :param on_always: the function to be called when the :class:`Request` object is  is either resolved or rejected.\n        :param kwargs: additional parameters\n        :return: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Send 'reload' request to the Flow.\n\n        :param targets: the regex string or list of regex strings to match the pea/pod names.\n        :param on_done: the function to be called when the :class:`Request` object is resolved.\n        :param on_error: the function to be called when the :class:`Request` object is rejected.\n        :param on_always: the function to be called when the :class:`Request` object is  is either resolved or rejected.\n        :param kwargs: additional parameters\n        :return: None", "file": "jina_clients___init__.py", "function": "Client.reload", "project": "jina", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef inject_fake_data(\n        self, tmpdir: str, config: Dict[str, Any]\n    ) -> Union[int, Dict[str, Any], Tuple[Sequence[Any], Union[int, Dict[str, Any]]]]:\n        \n        \n        raise NotImplementedError(\"You need to provide fake data in order for the tests to run.\")```\n\nUpdated code:\n```python\ndef inject_fake_data(self, tmpdir: str, config: Dict[str, Any]) -> Union[int, Dict[str, Any]]:\n        \n        \n        raise NotImplementedError(\"You need to provide fake data in order for the tests to run.\")\n```\n\nOriginal documentation:\n```python\nInject fake data for dataset into a temporary directory.\n\n        Args:\n            tmpdir (str): Path to a temporary directory. For most cases this acts as root directory for the dataset\n                to be created and in turn also for the fake data injected here.\n            config (Dict[str, Any]): Configuration that will be used to create the dataset.\n\n        Needs to return one of the following:\n\n            1. (int): Number of examples in the dataset to be created,\n            2. (Dict[str, Any]): Additional information about the injected fake data. Must contain the field\n                ``\"num_examples\"`` that corresponds to the number of examples in the dataset to be created, or\n            3. (Tuple[Sequence[Any], Union[int, Dict[str, Any]]]): Additional required parameters that are passed to\n                the dataset constructor. The second element corresponds to cases 1. and 2.\n\n        If no ``args`` is returned (case 1. and 2.), the ``tmp_dir`` is passed as first parameter to the dataset\n        constructor. In most cases this corresponds to ``root``. If the dataset has more parameters without default\n        values you need to explicitly pass them as explained in case 3.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inject fake data for dataset into a temporary directory.\n\n        Args:\n            tmpdir (str): Path to a temporary directory. For most cases this acts as root directory for the dataset\n                to be created and in turn also for the fake data injected here.\n            config (Dict[str, Any]): Configuration that will be used to create the dataset.\n\n        Needs to return one of the following:\n\n            1. (int): Number of examples in the dataset to be created, or\n            2. (Dict[str, Any]): Additional information about the injected fake data. Must contain the field\n                ``\"num_examples\"`` that corresponds to the number of examples in the dataset to be created.", "file": "test_datasets_utils.py", "function": "DatasetTestCase.inject_fake_data", "project": "vision", "change_stats": {"code_changes": 6, "doc_changes": 12, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_job(self, target: Callable[..., None], *args: Any) -> None:\n        \n        \n        if self.pool is None:\n            run_callback_threadsafe(self.pool, self.async_init_pool).result()\n        self.pool.add_job((target,) + args)```\n\nUpdated code:\n```python\ndef add_job(self, target: Callable[..., None], *args: Any) -> None:\n        \n        \n        run_callback_threadsafe(\n            self.loop, self.async_add_job, target, *args).result()\n```\n\nOriginal documentation:\n```python\nAdd job to the worker pool.\n\n        target: target to call.\n        args: parameters for method to call.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add job to the executor pool.\n\n        target: target to call.\n        args: parameters for method to call.", "file": "homeassistant_core.py", "function": "HomeAssistant.add_job", "project": "core", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unique(self):\n        \n        \n        \n        result = super(Series, self).unique()\n\n        if is_datetime64tz_dtype(self.dtype):\n            # we are special casing datetime64tz_dtype\n            # to return an object array of tz-aware Timestamps\n\n            # TODO: it must return DatetimeArray with tz in pandas 2.0\n            result = result.astype(object).values\n\n        return result```\n\nUpdated code:\n```python\ndef unique(self):\n        \n        \n        \n        result = super(Series, self).unique()\n        if isinstance(result, DatetimeIndex):\n            # TODO: This should be unnecessary after Series._values returns\n            #  DatetimeArray\n            result = result._eadata\n        return result\n```\n\nOriginal documentation:\n```python\nReturn unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or Categorical\n            The unique values returned as a NumPy array. In case of categorical\n            data type, returned as a Categorical.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        array([Timestamp('2016-01-01 00:00:00-0500', tz='US/Eastern')],\n              dtype=object)\n\n        An unordered Categorical will return categories in the order of\n        appearance.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        [b, a, c]\n        Categories (3, object): [b, a, c]\n\n        An ordered Categorical preserves the category ordering.\n\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        [b, a, c]\n        Categories (3, object): [a < b < c]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. In case of an\n            extension-array backed Series, a new\n            :class:`~api.extensions.ExtensionArray` of that type with just\n            the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArrayMixin>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An unordered Categorical will return categories in the order of\n        appearance.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        [b, a, c]\n        Categories (3, object): [b, a, c]\n\n        An ordered Categorical preserves the category ordering.\n\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        [b, a, c]\n        Categories (3, object): [a < b < c]", "file": "pandas_core_series.py", "function": "Series.unique", "project": "pandas", "change_stats": {"code_changes": 14, "doc_changes": 22, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_title(self, title):\n        ''\n        self._legend_title_box._text.set_text(title)\n\n        if title:\n            self._legend_title_box.set_visible(True)\n        else:\n            self._legend_title_box.set_visible(False)```\n\nUpdated code:\n```python\ndef set_title(self, title, prop=None):\n        \n        \n        \n        self._legend_title_box._text.set_text(title)\n\n        if prop is not None:\n            if isinstance(prop, dict):\n                prop = FontProperties(**prop)\n            self._legend_title_box._text.set_fontproperties(prop)\n\n        if title:\n            self._legend_title_box.set_visible(True)\n        else:\n            self._legend_title_box.set_visible(False)\n```\n\nOriginal documentation:\n```python\nset the legend title\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "set the legend title. Fontproperties can be optionally set\n        with *prop* parameter.", "file": "lib_matplotlib_legend.py", "function": "Legend.set_title", "project": "matplotlib", "change_stats": {"code_changes": 13, "doc_changes": 5, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_json_get_subscribers(self) -> None:\n        \n        \n        \n        stream_name = gather_subscriptions(self.user_profile)[0][0]['name']\n        stream_id = get_stream(stream_name, self.user_profile.realm).id\n        expected_subscribers = gather_subscriptions(self.user_profile)[0][0]['subscribers']\n        result = self.client_get(\"/json/streams/%d/members\" % (stream_id,))\n        self.assert_json_success(result)\n        result_dict = result.json()\n        self.assertIn('subscribers', result_dict)\n        self.assertIsInstance(result_dict['subscribers'], list)\n        subscribers = []  # type: List[str]\n        for subscriber in result_dict['subscribers']:\n            self.assertIsInstance(subscriber, str)\n            subscribers.append(subscriber)\n        self.assertEqual(set(subscribers), set(expected_subscribers))```\n\nUpdated code:\n```python\ndef test_json_get_subscribers(self) -> None:\n        \n        \n        \n        stream_name = gather_subscriptions(self.user_profile)[0][0]['name']\n        stream_id = get_stream(stream_name, self.user_profile.realm).id\n        expected_subscribers = gather_subscriptions(\n            self.user_profile, include_subscribers=True)[0][0]['subscribers']\n        result = self.client_get(\"/json/streams/%d/members\" % (stream_id,))\n        self.assert_json_success(result)\n        result_dict = result.json()\n        self.assertIn('subscribers', result_dict)\n        self.assertIsInstance(result_dict['subscribers'], list)\n        subscribers = []  # type: List[str]\n        for subscriber in result_dict['subscribers']:\n            self.assertIsInstance(subscriber, str)\n            subscribers.append(subscriber)\n        self.assertEqual(set(subscribers), set(expected_subscribers))\n```\n\nOriginal documentation:\n```python\njson_get_subscribers in zerver/views/streams.py\n        also returns the list of subscribers for a stream.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "json_get_subscribers in zerver/views/streams.py\n        also returns the list of subscribers for a stream, when requested.", "file": "zerver_tests_test_subs.py", "function": "GetSubscribersTest.test_json_get_subscribers", "project": "zulip", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef match_template(self, image):\n        \n        \n        \n        color = cv2.mean(crop(image, self.template_area))[:3]\n        # Match frequently hit templates first\n        names = np.array(list(self.templates.keys()))[np.argsort(list(self.templates_hit.values()))][::-1]\n        # Match known templates first\n        names = [name for name in names if not name.isdigit()] + [name for name in names if name.isdigit()]\n        for name in names:\n            if color_similar(color1=color, color2=self.colors[name], threshold=30):\n                res = cv2.matchTemplate(image, self.templates[name], cv2.TM_CCOEFF_NORMED)\n                _, similarity, _, _ = cv2.minMaxLoc(res)\n                if similarity > self.similarity:\n                    self.templates_hit[name] += 1\n                    return name\n\n        self.next_template_index += 1\n        name = str(self.next_template_index)\n        logger.info(f'New template: {name}')\n        image = crop(image, self.template_area)\n        self.colors[name] = cv2.mean(image)[:3]\n        self.templates[name] = image\n        self.templates_hit[name] = self.templates_hit.get(name, 0) + 1\n        return name```\n\nUpdated code:\n```python\ndef match_template(self, image, similarity=None):\n        \n        \n        \n        if similarity is None:\n            similarity = self.similarity\n        color = cv2.mean(crop(image, self.template_area))[:3]\n        # Match frequently hit templates first\n        names = np.array(list(self.templates.keys()))[np.argsort(list(self.templates_hit.values()))][::-1]\n        # Match known templates first\n        names = [name for name in names if not name.isdigit()] + [name for name in names if name.isdigit()]\n        for name in names:\n            if color_similar(color1=color, color2=self.colors[name], threshold=30):\n                res = cv2.matchTemplate(image, self.templates[name], cv2.TM_CCOEFF_NORMED)\n                _, sim, _, _ = cv2.minMaxLoc(res)\n                if sim > similarity:\n                    self.templates_hit[name] += 1\n                    return name\n\n        self.next_template_index += 1\n        name = str(self.next_template_index)\n        logger.info(f'New template: {name}')\n        image = crop(image, self.template_area)\n        self.colors[name] = cv2.mean(image)[:3]\n        self.templates[name] = image\n        self.templates_hit[name] = self.templates_hit.get(name, 0) + 1\n        return name\n```\n\nOriginal documentation:\n```python\nMatch templates, try most frequent hit templates first.\n\n        Args:\n            image (np.ndarray):\n\n        Returns:\n            str: Template name.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Match templates, try most frequent hit templates first.\n\n        Args:\n            image (np.ndarray):\n            similarity (float):\n\n        Returns:\n            str: Template name.", "file": "module_statistics_item.py", "function": "ItemGrid.match_template", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef masked_softmax(X, valid_len):\n    \n    # X: 3-D tensor, valid_len: 1-D or 2-D tensor\n    if valid_len is None:\n        return npx.softmax(X)\n    else:\n        shape = X.shape\n        if valid_len.ndim == 1:\n            valid_len = valid_len.repeat(shape[1], axis=0)\n        else:\n            valid_len = valid_len.reshape(-1)\n        # Fill masked elements with a large negative, whose exp is 0\n        X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_len, True,\n                              axis=1, value=-1e6)\n        return npx.softmax(X).reshape(shape)```\n\nUpdated code:\n```python\ndef masked_softmax(X, valid_lens):\n    \n    # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\n    if valid_lens is None:\n        return npx.softmax(X)\n    else:\n        shape = X.shape\n        if valid_lens.ndim == 1:\n            valid_lens = valid_lens.repeat(shape[1])\n        else:\n            valid_lens = valid_lens.reshape(-1)\n        # On the last axis, replace masked elements with a very large negative\n        # value, whose exponentiation outputs 0\n        X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, True,\n                              value=-1e6, axis=1)\n        return npx.softmax(X).reshape(shape)\n```\n\nOriginal documentation:\n```python\nPerform softmax by filtering out some elements.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform softmax operation by masking elements on the last axis.", "file": "d2l_mxnet.py", "function": "masked_softmax", "project": "d2l-en", "change_stats": {"code_changes": 21, "doc_changes": 4, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef do_list_modules(self, arg):\n\t\t  \n\t\tfor m,d in self.pupsrv.list_modules():\n\t\t\tself.stdout.write(\"{:<20}\t{}\\n\".format(m, color(d,'grey')))```\n\nUpdated code:\n```python\ndef do_list_modules(self, arg):\n\t\t  \n\t\tfor m,d in self.pupsrv.list_modules():\n\t\t\tself.stdout.write(\"{:<20}\t{}\\n\".format(m, color(d.split(\"\\n\",1)[0],'grey')))\n```\n\nOriginal documentation:\n```python\nList available modules with a brief description\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "List available modules with a brief description (the first description line)", "file": "pupy_pupylib_PupyCmd.py", "function": "PupyCmd.do_list_modules", "project": "pupy", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        \n        \n        with torch.no_grad():\n            # 1. Encoder\n            xs_pad, ys_pad = self.target_forcing(xs_pad, ys_pad)\n            hpad, hlens, _ = self.enc(self.dropout(self.embed(xs_pad)), ilens)\n\n            # 2. Decoder\n            att_ws = self.dec.calculate_all_attentions(hpad, hlens, ys_pad)\n\n        return att_ws```\n\nUpdated code:\n```python\ndef calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        \n        \n        with torch.no_grad():\n            # 1. Encoder\n            xs_pad, ys_pad = self.target_language_biasing(xs_pad, ilens, ys_pad)\n            hpad, hlens, _ = self.enc(self.dropout(self.embed(xs_pad)), ilens)\n\n            # 2. Decoder\n            att_ws = self.dec.calculate_all_attentions(hpad, hlens, ys_pad)\n\n        return att_ws\n```\n\nOriginal documentation:\n```python\nE2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray", "file": "espnet_nets_pytorch_backend_e2e_mt.py", "function": "E2E.calculate_all_attentions", "project": "espnet", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_collective_group(self, backend, world_size, rank, group_name):\n        \n        \n        \n        backend = types.Backend(backend)\n        if backend == types.Backend.MPI:\n            raise NotImplementedError()\n        elif backend == types.Backend.NCCL:\n            # create the ncclUniqueID\n            if rank == 0:\n                # availability has been checked before entering here.\n                group_uid = nccl_util.get_nccl_unique_id()\n                store_name = get_nccl_store_name(group_name)\n                # Avoid a potential circular dependency in ray/actor.py\n                from ray.util.collective.util import NCCLUniqueIDStore\n                store = NCCLUniqueIDStore.options(\n                    name=store_name, lifetime=\"detached\").remote(store_name)\n                ray.wait([store.set_id.remote(group_uid)])\n\n            logger.debug(\"creating NCCL group: '{}'\".format(group_name))\n            g = NCCLGroup(world_size, rank, group_name)\n            self._name_group_map[group_name] = g\n            self._group_name_map[g] = group_name\n        return self._name_group_map[group_name]```\n\nUpdated code:\n```python\ndef create_collective_group(self, backend, world_size, rank, group_name):\n        \n        \n        backend = types.Backend(backend)\n        if backend == types.Backend.MPI:\n            raise NotImplementedError()\n        elif backend == types.Backend.NCCL:\n            # create the ncclUniqueID\n            if rank == 0:\n                # availability has been checked before entering here.\n                group_uid = nccl_util.get_nccl_unique_id()\n                store_name = get_nccl_store_name(group_name)\n                # Avoid a potential circular dependency in ray/actor.py\n                from ray.util.collective.util import NCCLUniqueIDStore\n                store = NCCLUniqueIDStore.options(\n                    name=store_name, lifetime=\"detached\").remote(store_name)\n                ray.wait([store.set_id.remote(group_uid)])\n\n            logger.debug(\"creating NCCL group: '{}'\".format(group_name))\n            g = NCCLGroup(world_size, rank, group_name)\n            self._name_group_map[group_name] = g\n            self._group_name_map[g] = group_name\n        return self._name_group_map[group_name]\n```\n\nOriginal documentation:\n```python\nThe entry to create new collective groups and register in the manager.\n\n        Put the registration and the group information into the manager\n        metadata as well.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "The entry to create new collective groups in the manager.\n\n        Put the registration and the group information into the manager\n        metadata as well.", "file": "python_ray_util_collective_collective.py", "function": "GroupManager.create_collective_group", "project": "ray", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mute(self):\n          \n        self.muted = True\n        self.wrapped_stream.stop_stream()```\n\nUpdated code:\n```python\ndef mute(self):\n        \n        with self.read_lock:\n            self.muted = True\n            self.wrapped_stream.stop_stream()\n```\n\nOriginal documentation:\n```python\nStop the stream and set the muted flag\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Stop the stream and set the muted flag.", "file": "mycroft_client_speech_mic.py", "function": "MutableStream.mute", "project": "mycroft-core", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_issue_1386(self):\n        \n        \n        \n        correct_urls = [\n            \"asdf.com\",\n            \"asdf.net\",\n            \"www.as_df.org\",\n            \"as.d8f.ghj8.gov\",\n        ]\n        for i in correct_urls:\n            res = urlize_quoted_links(i)\n            self.assertNotEqual(res, i)\n            self.assertIn(i, res)\n\n        incorrect_urls = [\n            \"mailto://asdf@fdf.com\",\n            \"asdf.netnet\",\n        ]\n        for i in incorrect_urls:\n            res = urlize_quoted_links(i)\n            self.assertEqual(i, res)\n\n        # example from issue #1386, this shouldn't raise an exception\n        urlize_quoted_links(\"asdf:[/p]zxcv.com\")```\n\nUpdated code:\n```python\ndef test_issue_1386(self):\n        \n        \n        \n        correct_urls = [\n            \"asdf.com\",\n            \"asdf.net\",\n            \"www.as_df.org\",\n            \"as.d8f.ghj8.gov\",\n        ]\n        for i in correct_urls:\n            res = urlize(i)\n            self.assertNotEqual(res, i)\n            self.assertIn(i, res)\n\n        incorrect_urls = [\n            \"mailto://asdf@fdf.com\",\n            \"asdf.netnet\",\n        ]\n        for i in incorrect_urls:\n            res = urlize(i)\n            self.assertEqual(i, res)\n\n        # example from issue #1386, this shouldn't raise an exception\n        urlize(\"asdf:[/p]zxcv.com\")\n```\n\nOriginal documentation:\n```python\nTest function urlize_quoted_links with different args\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test function urlize with different args", "file": "tests_test_templatetags.py", "function": "Issue1386Tests.test_issue_1386", "project": "django-rest-framework", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _img_array(self):\n        \n        \n        pilIm = self.im\n        pilIm.seek(0)\n\n        # Read all images inside\n        image_data = []\n        try:\n            while True:\n                img_tmp = pilIm\n                img_tmp = self.img_correct(img_tmp)\n                image_data.append(ImageData(img_tmp.size[0], img_tmp.size[1],\n                                img_tmp.mode.lower(), img_tmp.tostring()))\n                pilIm.seek(pilIm.tell()+1)\n        except EOFError:\n            pass\n        # Done\n        return image_data```\n\nUpdated code:\n```python\ndef _img_array(self):\n        '''\n        '''\n        pilIm = self.im\n        pilIm.seek(0)\n\n        # Read all images inside\n        image_data = []\n        try:\n            while True:\n                img_tmp = pilIm\n                img_tmp = self.img_correct(img_tmp)\n                image_data.append(ImageData(img_tmp.size[0], img_tmp.size[1],\n                                img_tmp.mode.lower(), img_tmp.tostring()))\n                pilIm.seek(pilIm.tell()+1)\n        except EOFError:\n            pass\n        # Done\n        return image_data\n```\n\nOriginal documentation:\n```python\nRead images from an animated file.\n        Returns a list/array of typ ImageData\n\n        NOTE:\n        gif animation has a lot of Issues(transparency/color depths... etc).\n        In order to keep simple what is implimented here is what is natively\n        supported by pil\n\n        As a general rule try to use gifs that have no transparency\n        gif's with transparency will work but be ready for some\n        artifacts for now.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Read images from an animated file.\n        Returns a list/array of typ ImageData", "file": "kivy_core_image_img_pil.py", "function": "ImageSequence._img_array", "project": "kivy", "change_stats": {"code_changes": 6, "doc_changes": 11, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef flag(\n        self, \n        interface: gr.Interface, \n        input_data: List[Any], \n        output_data: List[Any], \n        flag_option: Optional[str] = None, \n        flag_index: Optional[int] = None, \n        username: Optional[str] = None) -> int:\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef flag(\n        self,\n        interface: gr.Interface,\n        input_data: List[Any],\n        output_data: List[Any],\n        flag_option: Optional[str] = None,\n        flag_index: Optional[int] = None,\n        username: Optional[str] = None,\n    ) -> int:\n        \n        \n        \n        pass\n```\n\nOriginal documentation:\n```python\nThis method should be overridden by the FlaggingCallback subclass and may contain optional additional arguments.\n        This gets called every time the <flag> button is pressed.\n        Parameters:\n        interface: The Interface object that is being used to launch the flagging interface.\n        input_data: The input data to be flagged.\n        output_data: The output data to be flagged.        \n        flag_option (optional): In the case that flagging_options are provided, the flag option that is being used.\n        flag_index (optional): The index of the sample that is being flagged.\n        username (optional): The username of the user that is flagging the data, if logged in.\n        Returns:\n        (int) The total number of samples that have been flagged.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method should be overridden by the FlaggingCallback subclass and may contain optional additional arguments.\n        This gets called every time the <flag> button is pressed.\n        Parameters:\n        interface: The Interface object that is being used to launch the flagging interface.\n        input_data: The input data to be flagged.\n        output_data: The output data to be flagged.\n        flag_option (optional): In the case that flagging_options are provided, the flag option that is being used.\n        flag_index (optional): The index of the sample that is being flagged.\n        username (optional): The username of the user that is flagging the data, if logged in.\n        Returns:\n        (int) The total number of samples that have been flagged.", "file": "gradio_flagging.py", "function": "FlaggingCallback.flag", "project": "gradio", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef change_bar_values(self, values: Iterable[float]):\n        \n        \n\n        for i, (bar, value) in enumerate(zip(self.bars, values)):\n            chart_val = self.values[i]\n\n            if chart_val > 0:\n                bar_lim = bar.get_bottom()\n                aligned_edge = DOWN\n            else:\n                bar_lim = bar.get_top()\n                aligned_edge = UP\n\n            try:\n                quotient = value / chart_val\n                if quotient < 0:\n\n                    aligned_edge = UP if chart_val > 0 else DOWN\n\n                    # if the bar is already positive, then we now want to move it\n                    # so that it is negative. So, we move the top edge of the bar\n                    # to the location of the previous bottom\n\n                    # if already negative, then we move the bottom edge of the bar\n                    # to the location of the previous top\n\n                bar.stretch_to_fit_height(quotient * bar.height)\n\n            except ZeroDivisionError:\n                bar.height = 0\n\n            bar.move_to(bar_lim, aligned_edge)\n\n        self.values[: len(values)] = values```\n\nUpdated code:\n```python\ndef change_bar_values(self, values: Iterable[float], update_colors: bool = True):\n        \n        \n\n        for i, (bar, value) in enumerate(zip(self.bars, values)):\n            chart_val = self.values[i]\n\n            if chart_val > 0:\n                bar_lim = bar.get_bottom()\n                aligned_edge = DOWN\n            else:\n                bar_lim = bar.get_top()\n                aligned_edge = UP\n\n            # check if the bar has height\n            if chart_val != 0:\n                quotient = value / chart_val\n                if quotient < 0:\n\n                    aligned_edge = UP if chart_val > 0 else DOWN\n\n                    # if the bar is already positive, then we now want to move it\n                    # so that it is negative. So, we move the top edge of the bar\n                    # to the location of the previous bottom\n\n                    # if already negative, then we move the bottom edge of the bar\n                    # to the location of the previous top\n\n                bar.stretch_to_fit_height(abs(quotient) * bar.height)\n\n            else:\n                # create a new bar since the current one has a height of zero (doesn't exist)\n                temp_bar = self._create_bar(i, value)\n                self.bars.remove(bar)\n                self.bars.insert(i, temp_bar)\n\n            bar.move_to(bar_lim, aligned_edge)\n\n        if update_colors:\n            self._update_colors()\n\n        self.values[: len(values)] = values\n```\n\nOriginal documentation:\n```python\nUpdates the height of the bars of the chart.\n\n        Parameters\n        ----------\n        values\n            The values that will be used to update the height of the bars.\n            Does not have to match the number of bars.\n\n        Examples\n        --------\n        .. manim:: ChangeBarValuesExample\n            :save_last_frame:\n\n            class ChangeBarValuesExample(Scene):\n                def construct(self):\n                    values=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10]\n\n                    chart = BarChart(\n                        values,\n                        y_range=[-10, 10, 2],\n                        y_axis_config={\"font_size\": 24},\n                    )\n                    self.add(chart)\n\n                    chart.change_bar_values(list(reversed(values)))\n                    self.add(chart.get_bar_labels(font_size=24))\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Updates the height of the bars of the chart.\n\n        Parameters\n        ----------\n        values\n            The values that will be used to update the height of the bars.\n            Does not have to match the number of bars.\n        update_colors\n            Whether to re-initalize the colors of the bars based on ``self.bar_colors``.\n\n        Examples\n        --------\n        .. manim:: ChangeBarValuesExample\n            :save_last_frame:\n\n            class ChangeBarValuesExample(Scene):\n                def construct(self):\n                    values=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10]\n\n                    chart = BarChart(\n                        values,\n                        y_range=[-10, 10, 2],\n                        y_axis_config={\"font_size\": 24},\n                    )\n                    self.add(chart)\n\n                    chart.change_bar_values(list(reversed(values)))\n                    self.add(chart.get_bar_labels(font_size=24))", "file": "manim_mobject_graphing_probability.py", "function": "BarChart.change_bar_values", "project": "manim", "change_stats": {"code_changes": 19, "doc_changes": 4, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lookup_future_symbol(self, symbol):\n         \n\n        \n\n        data = self._select_asset_by_symbol(self.futures_contracts, symbol)\\\n                   .execute().fetchone()\n\n        # If no data found, raise an exception\n        if not data:\n            raise SymbolNotFound(symbol=symbol)\n        return self.retrieve_asset(data['sid'])```\n\nUpdated code:\n```python\ndef lookup_future_symbol(self, symbol):\n        \n\n        \n\n        data = self._select_asset_by_symbol(self.futures_contracts, symbol)\\\n                   .execute().fetchone()\n\n        # If no data found, raise an exception\n        if not data:\n            raise SymbolNotFound(symbol=symbol)\n        return self.retrieve_asset(data['sid'])\n```\n\nOriginal documentation:\n```python\nReturn the Future object for a given symbol.\n\n        Parameters\n        ----------\n        symbol : str\n            The symbol of the desired contract.\n\n        Returns\n        -------\n        Future\n            A Future object.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when no contract named 'symbol' is found.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Lookup a future contract by symbol.\n\n        Parameters\n        ----------\n        symbol : str\n            The symbol of the desired contract.\n\n        Returns\n        -------\n        future : Future\n            The future contract referenced by ``symbol``.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when no contract named 'symbol' is found.", "file": "zipline_assets_assets.py", "function": "AssetFinder.lookup_future_symbol", "project": "zipline", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, providing_args=None):\n        \n        \n        self.receivers = []\n        if providing_args is None:\n            providing_args = []\n        self.providing_args = set(providing_args)```\n\nUpdated code:\n```python\ndef __init__(self, providing_args=None):\n        \n        \n        \n        self.receivers = []\n        if providing_args is None:\n            providing_args = []\n        self.providing_args = set(providing_args)\n```\n\nOriginal documentation:\n```python\nproviding_args -- A list of the arguments this signal can pass along in\n                       a send() call.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a new signal.\n        \n        providing_args\n            A list of the arguments this signal can pass along in a send() call.", "file": "django_dispatch_dispatcher.py", "function": "Signal.__init__", "project": "django", "change_stats": {"code_changes": 3, "doc_changes": 8, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_verts(self):\n        \n        \n        \n        trans = self.get_transform()\n        path = self.get_path()\n        tverts = trans.transform(path.vertices)\n        return tverts```\n\nUpdated code:\n```python\ndef get_verts(self):\n        \n        \n        \n        trans = self.get_transform()\n        path = self.get_path()\n        polygons = path.to_polygons(trans)\n        if len(polygons):\n            return polygons[0]\n        return []\n```\n\nOriginal documentation:\n```python\nreturn a copy of the vertices used in this patch\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "return a copy of the vertices used in this patch\n\n        If the patch contains Bezier curves, the curves will be\n        interpolated by line segments.  To access the curves as\n        curves, use :meth:`get_path`.", "file": "lib_matplotlib_patches.py", "function": "Patch.get_verts", "project": "matplotlib", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unwrap(p, discont=numpy.pi, axis=-1):\n    \n    \n\n    p = cupy.asarray(p)\n    nd = p.ndim\n    dd = sumprod.diff(p, axis=axis)\n    slice1 = [slice(None, None)]*nd     # full slices\n    slice1[axis] = slice(1, None)\n    slice1 = tuple(slice1)\n    ph_correct = _unwrap_correct(dd, discont)\n    dtype = 'd'\n    if numpy.dtype(p.dtype).kind == 'f':\n        dtype = p.dtype\n    up = cupy.array(p, copy=True, dtype=dtype)\n    up[slice1] = p[slice1] + cupy.cumsum(ph_correct, axis=axis)\n    return up```\n\nUpdated code:\n```python\ndef unwrap(p, discont=None, axis=-1, *, period=2*numpy.pi):\n    \n    \n\n    p = cupy.asarray(p)\n    nd = p.ndim\n    dd = sumprod.diff(p, axis=axis)\n    if discont is None:\n        discont = period/2\n    slice1 = [slice(None, None)]*nd     # full slices\n    slice1[axis] = slice(1, None)\n    slice1 = tuple(slice1)\n    ph_correct = _unwrap_correct(dd, discont)\n    dtype = numpy.result_type(dd.dtype, period)\n    if numpy.core.numeric.issubdtype(dtype, numpy.core.numeric.integer):\n        interval_high, rem = divmod(period, 2)\n        boundary_ambiguous = rem == 0\n    else:\n        interval_high = period / 2\n        boundary_ambiguous = True\n    interval_low = -interval_high\n    ddmod = cupy.mod(dd - interval_low, period) + interval_low\n    if boundary_ambiguous:\n        # for `mask = (abs(dd) == period/2)`, the above line made\n        # `ddmod[mask] == -period/2`. correct these such that\n        # `ddmod[mask] == sign(dd[mask])*period/2`.\n        ddmod[(ddmod == interval_low) & (dd > 0)] = interval_high\n\n    up = cupy.array(p, copy=True, dtype=dtype)\n    up[slice1] = p[slice1] + cupy.cumsum(ph_correct, axis=axis)\n    return up\n```\n\nOriginal documentation:\n```python\nUnwrap by changing deltas between values to 2*pi complement.\n\n    Args:\n        p (cupy.ndarray): Input array.\n        discont (float): Maximum discontinuity between values, default is\n            ``pi``.\n        axis (int): Axis along which unwrap will operate, default is the last\n            axis.\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.unwrap`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Unwrap by taking the complement of large deltas w.r.t. the period.\n\n    This unwraps a signal `p` by changing elements which have an absolute\n    difference from their predecessor of more than ``max(discont, period/2)``\n    to their `period`-complementary values.\n\n    For the default case where `period` is ``2 * pi`` and is `discont` is\n    ``pi``, this unwraps a radian phase `p` such that adjacent differences\n    are never greater than ``pi`` by adding ``2ki*pi`` for some\n    integer ``k``.\n\n    Args:\n        p (cupy.ndarray): Input array.\n        discont (float): Maximum discontinuity between values, default is\n        ``period/2``. Values below ``period/2`` are treated as if they were\n        ``period/2``. To have an effect different from the default, `discont`\n        should be larger than ``period/2``.\n        axis (int): Axis along which unwrap will operate, default is the last\n            axis.\n        period: float, optional\n        Size of the range over which the input wraps. By default, it is\n        ``2 pi``.\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.unwrap`", "file": "cupy__math_trigonometric.py", "function": "unwrap", "project": "cupy", "change_stats": {"code_changes": 24, "doc_changes": 20, "total_changes": 44}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef count_slot_mapping_statistics(self) -> Tuple[int, int]:\n        \n        \n        total_mappings = 0\n        custom_mappings = 0\n        for slot in self.slots:\n            for mapping in slot.mappings:\n                total_mappings += 1\n                if mapping.get(\"type\") == str(SlotMapping.CUSTOM):\n                    custom_mappings += 1\n\n        return (total_mappings, custom_mappings)```\n\nUpdated code:\n```python\ndef count_slot_mapping_statistics(self) -> Tuple[int, int, int]:\n        \n        \n        total_mappings = 0\n        custom_mappings = 0\n        conditional_mappings = 0\n\n        for slot in self.slots:\n            for mapping in slot.mappings:\n                total_mappings += 1\n                if mapping.get(\"type\") == str(SlotMapping.CUSTOM):\n                    custom_mappings += 1\n\n                if \"conditions\" in mapping:\n                    conditional_mappings += 1\n\n        return (total_mappings, custom_mappings, conditional_mappings)\n```\n\nOriginal documentation:\n```python\nCounts the total number of slot mappings and custom slot mappings.\n\n        Returns:\n            A pair of integers where the first entry is the total number of mappings\n            and the second entry is the total number of custom mappings.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Counts the total number of slot mappings and custom slot mappings.\n\n        Returns:\n            A triple of integers where the first entry is the total number of mappings,\n            the second entry is the total number of custom mappings, and the third entry\n            is the total number of mappings which have conditions attached.", "file": "rasa_shared_core_domain.py", "function": "Domain.count_slot_mapping_statistics", "project": "rasa", "change_stats": {"code_changes": 11, "doc_changes": 7, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef current_app(self):\n        \n        \n        \n        # try: adb shell dumpsys activity top\n        _activityRE = re.compile(\n            r'ACTIVITY (?P<package>[^/]+)/(?P<activity>[^/\\s]+) \\w+ pid=(?P<pid>\\d+)'\n        )\n        m = _activityRE.search(self.shell(['dumpsys', 'activity', 'top'])[0])\n        if m:\n            return dict(\n                package=m.group('package'),\n                activity=m.group('activity'),\n                pid=int(m.group('pid')))\n\n        # try: adb shell dumpsys window windows\n        _focusedRE = re.compile(\n            'mFocusedApp=.*ActivityRecord{\\w+ \\w+ (?P<package>.*)/(?P<activity>.*) .*'\n        )\n        m = _focusedRE.search(self.shell(['dumpsys', 'window', 'windows']))\n        if m:\n            return dict(\n                package=m.group('package'), activity=m.group('activity'))\n        # empty result\n        warnings.warn(\"Couldn't get focused app\", stacklevel=2)\n        return dict(package=None, activity=None)```\n\nUpdated code:\n```python\ndef current_app(self):\n        \n        \n        \n        # try: adb shell dumpsys window windows\n        _focusedRE = re.compile(\n            r'mFocusedApp=.*ActivityRecord{\\w+ \\w+ (?P<package>.*)/(?P<activity>.*) .*'\n        )\n        m = _focusedRE.search(self.shell(['dumpsys', 'window', 'windows'])[0])\n        if m:\n            return dict(\n                package=m.group('package'), activity=m.group('activity'))\n\n        # try: adb shell dumpsys activity top\n        _activityRE = re.compile(\n            r'ACTIVITY (?P<package>[^/]+)/(?P<activity>[^/\\s]+) \\w+ pid=(?P<pid>\\d+)'\n        )\n        output, _ = self.shell(['dumpsys', 'activity', 'top'])\n        ms = _activityRE.finditer(output)\n        ret = None\n        for m in ms:\n            ret = dict(\n                package=m.group('package'),\n                activity=m.group('activity'),\n                pid=int(m.group('pid')))\n        if ret:\n            return ret\n        # empty result\n        warnings.warn(\"Couldn't get focused app\", stacklevel=2)\n        return dict(package=None, activity=None)\n```\n\nOriginal documentation:\n```python\nReturn: dict(package, activity, pid?)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns:\n            dict(package, activity, pid?)\n\n        For developer:\n            Function healthcheck need this function, so can't use jsonrpc here.", "file": "uiautomator2___init__.py", "function": "UIAutomatorServer.current_app", "project": "uiautomator2", "change_stats": {"code_changes": 30, "doc_changes": 8, "total_changes": 38}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_bare_dict(typ: Type[Any]) -> bool:\n    \n    \n    \n    return typ in (typing.Dict, Dict, dict)```\n\nUpdated code:\n```python\ndef is_bare_dict(typ: type[Any]) -> bool:\n    \n    \n    \n    return typ is dict\n```\n\nOriginal documentation:\n```python\nTest if the type is `typing.Dict` without type args.\n\n    >>> from typing import Dict\n    >>> is_bare_dict(Dict[int, str])\n    False\n    >>> is_bare_dict(Dict)\n    True\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test if the type is `dict` without type args.\n\n    >>> is_bare_dict(dict[int, str])\n    False\n    >>> is_bare_dict(dict)\n    True", "file": "serde_compat.py", "function": "is_bare_dict", "project": "pyserde", "change_stats": {"code_changes": 6, "doc_changes": 9, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef launch_multi_processes(args):\n    \n    \n    \n    assert isinstance(args, Config)\n\n    # cannot accept hosts and hostfile at the same time\n    if args.host and args.hostfile:\n        click.echo(\"Error: hostfile and hosts are mutually exclusive, only one is required\")\n\n    # check if hostfile is given\n    if args.hostfile:\n        device_pool = fetch_hostfile(args.hostfile)\n    else:\n        device_pool = None\n\n    # filter and only keep the ones needed\n    active_devices = None\n    if device_pool:\n        active_devices = parse_inclusion_exclusion(device_pool, args.include, args.exclude)\n\n        if args.num_nodes > 0:\n            # only keep the first num_nodes to execute jobs\n            updated_active_devices = collections.OrderedDict()\n            for count, hostname in enumerate(active_devices.keys()):\n                if args.num_nodes == count:\n                    break\n                updated_active_devices[hostname] = active_devices[hostname]\n            active_devices = updated_active_devices\n\n        if args.nproc_per_node > 0:\n            # only keep the first\n            updated_active_devices = collections.OrderedDict()\n            for hostname, active_devices in active_devices.items():\n                if len(active_devices) < args.nproc_per_node:\n                    click.echo(\n                        f\"Error: The number of available GPUs on {hostname} is smaller than the argument nproc_per_node\"\n                    )\n                    exit()\n                updated_active_devices[hostname] = active_devices[args.nproc_per_node]\n            active_devices = updated_active_devices\n\n    env = os.environ.copy()\n\n    # use hosts if hostfile is not given\n    if args.host and active_devices is None:\n        hostinfo = collections.OrderedDict()\n        host_list = args.host.strip().split(',')\n        for hostname in host_list:\n            hostinfo[hostname] = args.nproc_per_node\n        active_devices = hostinfo\n\n    # run on local node if not hosts or hostfile is given\n    if not active_devices:\n        if args.nproc_per_node == -1 or args.nproc_per_node > torch.cuda.device_count():\n            nproc_per_node = torch.cuda.device_count()\n        else:\n            nproc_per_node = args.nproc_per_node\n\n        if torch.__version__ <= \"1.9\":\n            cmd = [\n                sys.executable, \"-u\", \"-m\", \"torch.distributed.launch\", f\"--nproc_per_node={nproc_per_node}\",\n                f\"--master_addr={args.master_addr}\", f\"--master_port={args.master_port}\"\n            ] + [args.user_script] + args.user_args\n        else:\n            cmd = [\n                \"torchrun\", f\"--nproc_per_node={nproc_per_node}\", f\"--master_addr={args.master_addr}\",\n                f\"--master_port={args.master_port}\"\n            ] + [args.user_script] + args.user_args\n    else:\n        runner = PDSHRunner(args)\n\n        curr_path = os.path.abspath('.')\n        if 'PYTHONPATH' in env:\n            env['PYTHONPATH'] = curr_path + \":\" + env['PYTHONPATH']\n        else:\n            env['PYTHONPATH'] = curr_path\n\n        cmd = runner.get_cmd(env, active_devices, args)\n\n    result = subprocess.Popen(cmd, env=env)\n    result.wait()\n    if result.returncode > 0:\n        sys.exit(result.returncode)```\n\nUpdated code:\n```python\ndef launch_multi_processes(args: Config) -> None:\n    \n    \n\n    \n    assert isinstance(args, Config)\n\n    if args.nproc_per_node is None:\n        click.echo(\"--nproc_per_node did not receive any value\")\n        exit()\n\n    # cannot accept hosts and hostfile at the same time\n    if args.host and args.hostfile:\n        click.echo(\"Error: hostfile and hosts are mutually exclusive, only one is required\")\n\n    # check if hostfile is given\n    if args.hostfile:\n        device_pool = fetch_hostfile(args.hostfile, ssh_port=args.ssh_port)\n        active_device_pool = parse_device_filter(device_pool, args.include, args.exclude)\n\n        if args.num_nodes > 0:\n            # only keep the first num_nodes to execute jobs\n            updated_active_device_pool = HostInfoList()\n            for count, hostinfo in enumerate(active_device_pool):\n                if args.num_nodes == count:\n                    break\n                updated_active_device_pool.append(hostinfo)\n            active_device_pool = updated_active_device_pool\n    else:\n        active_device_pool = None\n\n    env = os.environ.copy()\n\n    # use hosts if hostfile is not given\n    if args.host and active_device_pool is None:\n        active_device_pool = HostInfoList()\n        host_list = args.host.strip().split(NODE_SEP)\n        for hostname in host_list:\n            hostinfo = HostInfo(hostname=hostname, port=args.ssh_port)\n            active_device_pool.append(hostinfo)\n\n    if not active_device_pool:\n        # run on local node if not hosts or hostfile is given\n        # add local node to host info list\n        active_device_pool = HostInfoList()\n        localhost_info = HostInfo(hostname='127.0.0.1', port=args.ssh_port)\n        active_device_pool.append(localhost_info)\n\n    # launch distributed processes\n    runner = MultiNodeRunner()\n    curr_path = os.path.abspath('.')\n\n    # collect current path env\n    env = dict()\n    for k, v in os.environ.items():\n        # do not support multi-line env var\n        if v and '\\n' not in v:\n            env[k] = v\n\n    # establish remote connection\n    runner.connect(host_info_list=active_device_pool, workdir=curr_path, env=env)\n\n    # execute distributed launching command\n    for node_id, hostinfo in enumerate(active_device_pool):\n        cmd = get_launch_command(master_addr=args.master_addr,\n                                 master_port=args.master_port,\n                                 nproc_per_node=args.nproc_per_node,\n                                 user_script=args.user_script,\n                                 user_args=args.user_args,\n                                 node_rank=node_id,\n                                 num_nodes=len(active_device_pool),\n                                 extra_launch_args=args.extra_launch_args)\n        runner.send(hostinfo=hostinfo, cmd=cmd)\n\n    runner.recv_from_all()\n    runner.stop_all()\n    runner.recv_from_all()\n```\n\nOriginal documentation:\n```python\nLaunch multiple processes on a single node or multiple nodes.\n\n    The overall logic can be summarized as the pseudo code below:\n    \n    if hostfile given:\n        hostinfo = parse_hostfile(hostfile)\n        hostinfo = include_or_exclude_hosts(hostinfo)\n        launch_on_multi_nodes(hostinfo)\n    elif hosts given:\n        hostinfo = parse_hosts(hosts)\n        launch_on_multi_nodes(hostinfo)\n    else:\n        launch_on_current_node()\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Launch multiple processes on a single node or multiple nodes.\n\n    The overall logic can be summarized as the pseudo code below:\n    \n        if hostfile given:\n            hostinfo = parse_hostfile(hostfile)\n            hostinfo = include_or_exclude_hosts(hostinfo)\n            launch_on_multi_nodes(hostinfo)\n        elif hosts given:\n            hostinfo = parse_hosts(hosts)\n            launch_on_multi_nodes(hostinfo)\n        else:\n            launch_on_current_node()\n    \n    Args:\n        args (Config): the arguments taken from command line", "file": "colossalai_cli_launcher_run.py", "function": "launch_multi_processes", "project": "ColossalAI", "change_stats": {"code_changes": 111, "doc_changes": 23, "total_changes": 134}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef post(self, api_key_api):\n        \n        \n        \n        api_key_db = None\n        try:\n            api_key_api.user = self._get_user()\n            api_key, api_key_hash = auth_util.generate_api_key_and_hash()\n            # store key_hash in DB\n            api_key_api.key_hash = api_key_hash\n            api_key_db = ApiKey.add_or_update(ApiKeyAPI.to_model(api_key_api))\n        except (ValidationError, ValueError) as e:\n            LOG.exception('Validation failed for api_key data=%s.', api_key_api)\n            abort(http_client.BAD_REQUEST, str(e))\n\n        extra = {'api_key_db': api_key_db}\n        LOG.audit('ApiKey created. ApiKey.id=%s' % (api_key_db.id), extra=extra)\n\n        api_key_create_response_api = ApiKeyCreateResponseAPI.from_model(api_key_db)\n        # Return real api_key back to user. A one-way hash of the api_key is stored in the DB\n        # only the real value only returned at create time. Also, no masking of key here since\n        # the user needs to see this value atleast once.\n        api_key_create_response_api.key = api_key\n        return api_key_create_response_api```\n\nUpdated code:\n```python\ndef post(self, api_key_api):\n        \n        \n        \n        api_key_db = None\n        api_key = None\n        try:\n            if not getattr(api_key_api, 'user', None):\n                api_key_api.user = self._get_user()\n            # If key_hash is provided use that and do not create a new key. The assumption\n            # is user already has the original api-key\n            if not getattr(api_key_api, 'key_hash', None):\n                api_key, api_key_hash = auth_util.generate_api_key_and_hash()\n                # store key_hash in DB\n                api_key_api.key_hash = api_key_hash\n            api_key_db = ApiKey.add_or_update(ApiKeyAPI.to_model(api_key_api))\n        except (ValidationError, ValueError) as e:\n            LOG.exception('Validation failed for api_key data=%s.', api_key_api)\n            abort(http_client.BAD_REQUEST, str(e))\n\n        extra = {'api_key_db': api_key_db}\n        LOG.audit('ApiKey created. ApiKey.id=%s' % (api_key_db.id), extra=extra)\n\n        api_key_create_response_api = ApiKeyCreateResponseAPI.from_model(api_key_db)\n        # Return real api_key back to user. A one-way hash of the api_key is stored in the DB\n        # only the real value only returned at create time. Also, no masking of key here since\n        # the user needs to see this value atleast once.\n        api_key_create_response_api.key = api_key\n        return api_key_create_response_api\n```\n\nOriginal documentation:\n```python\nCreate a new entry or update an existing one.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a new entry.", "file": "st2api_st2api_controllers_v1_auth.py", "function": "ApiKeyController.post", "project": "st2", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extract(self, images, layers=['pool5'], size=(224, 224)):\n        \n\n        \n\n        x = chainer.dataset.concat_examples(\n            [self.prepare(img, size=size) for img in images])\n        x = Variable(self.xp.asarray(x))\n        return self(x, layers=layers)```\n\nUpdated code:\n```python\ndef extract(self, images, layers=['pool5'], size=(224, 224)):\n        \n\n        \n\n        x = chainer.dataset.concat_examples(\n            [prepare(img, size=size) for img in images])\n        x = Variable(self.xp.asarray(x))\n        return self(x, layers=layers)\n```\n\nOriginal documentation:\n```python\nExtracts all the feature maps of given images.\n\n        The difference of directly executing ``__call__`` is that\n        it directly accepts the list of images as an input, and\n        automatically transforms them to a proper variable. That is,\n        it is also interpreted as a shortcut method that implicitly call\n        ``prepare`` and ``__call__`` methods.\n\n        Args:\n            image (list of PIL.Image or numpy.ndarray): Input images.\n            layers (list of str): The list of layernames you want to extract.\n            size (pair of ints): The resolution of resized images used as\n                an input of CNN. All the given images are not resized\n                if this argument is ``None``, but the resolutions of\n                all the images should be the same.\n\n        Returns:\n            Dictionary of ~chainer.Variable: The directory in which\n            the key contains the layer name and the value contains\n            the corresponding variable.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extracts all the feature maps of given images.\n\n        The difference of directly executing ``__call__`` is that\n        it directly accepts the list of images as an input, and\n        automatically transforms them to a proper variable. That is,\n        it is also interpreted as a shortcut method that implicitly call\n        ``prepare`` and ``__call__`` functions.\n\n        Args:\n            image (list of PIL.Image or numpy.ndarray): Input images.\n            layers (list of str): The list of layernames you want to extract.\n            size (pair of ints): The resolution of resized images used as\n                an input of CNN. All the given images are not resized\n                if this argument is ``None``, but the resolutions of\n                all the images should be the same.\n\n        Returns:\n            Dictionary of ~chainer.Variable: The directory in which\n            the key contains the layer name and the value contains\n            the corresponding variable.", "file": "chainer_links_model_vision_resnet.py", "function": "ResNet50Layers.extract", "project": "chainer", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef all_completions(self, text):\n        \n        \n        \n        return self.complete(text)[1]```\n\nUpdated code:\n```python\ndef all_completions(self, text) -> List[str]:\n        \n        \n        \n        prefix = text[:text.rfind(\".\") + 1]\n        with provisionalcompleter():\n            return list(map(lambda c: prefix + c.text,\n                   self.completions(text, len(text))))\n\n        return self.complete(text)[1]\n```\n\nOriginal documentation:\n```python\nWrapper around the complete method for the benefit of emacs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Wrapper around the completions method for the benefit of emacs.", "file": "IPython_core_completer.py", "function": "IPCompleter.all_completions", "project": "ipython", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef prelu(x, W):\n    \n\n    \n    return PReLUFunction()(x, W)```\n\nUpdated code:\n```python\ndef prelu(x, W):\n    \n\n    \n    return PReLUFunction().apply((x, W))[0]\n```\n\nOriginal documentation:\n```python\nParametric ReLU function.\n\n    It accepts two arguments: an input ``x`` and a weight array ``W``\n    and computes the output as :math:`PReLU(x) = \\\\max(x, W*x)`,\n    where :math:`*` is an elementwise multiplication for each sample in the\n    batch.\n\n    When the PReLU function is combined with two-dimensional convolution, the\n    elements of parameter :math:`a` are typically shared across the same filter\n    of different pixels. In order to support such usage, this function supports\n    the shape of parameter array that indicates leading dimensions of input\n    arrays except the batch dimension.\n\n    For example :math:`W` has the shape of :math:`(2, 3, 4)`,\n    :math:`x` must have the shape of :math:`(B, 2, 3, 4, S1, ..., SN)`\n    where B is batch size and the number of trailing S's\n    is arbitrary non-negative integer.\n\n    Args:\n        x (~chainer.Variable): Input variable.\n            Its first argument is assumed to be the minibatch dimension.\n        W (~chainer.Variable): Weight variable.\n\n    Returns:\n        ~chainer.Variable: Output variable\n\n    .. seealso:: :class:`~chainer.links.PReLU`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parametric ReLU function.\n\n    It accepts two arguments: an input ``x`` and a weight array ``W``\n    and computes the output as :math:`PReLU(x) = \\\\max(x, W*x)`,\n    where :math:`*` is an elementwise multiplication for each sample in the\n    batch.\n\n    When the PReLU function is combined with two-dimensional convolution, the\n    elements of parameter :math:`W` are typically shared across the same filter\n    of different pixels. In order to support such usage, this function supports\n    the shape of parameter array that indicates leading dimensions of input\n    arrays except the batch dimension.\n\n    For example, if :math:`W` has the shape of :math:`(2, 3, 4)`,\n    :math:`x` must have the shape of :math:`(B, 2, 3, 4, S_1, ..., S_N)`\n    where :math:`B` is the batch size and the number of trailing :math:`S`'s\n    :math:`N` is an arbitrary non-negative integer.\n\n    Args:\n        x (~chainer.Variable): Input variable.\n            Its first argument is assumed to be the minibatch dimension.\n        W (~chainer.Variable): Weight variable.\n\n    Returns:\n        ~chainer.Variable: Output variable\n\n    .. seealso:: :class:`~chainer.links.PReLU`", "file": "chainer_", "function": "prelu", "project": "chainer", "change_stats": {"code_changes": 4, "doc_changes": 12, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        outputs_per_step: int = 1,\n        compute_linear_spec: bool = False,\n        ap: AudioProcessor = None,\n        samples: List[Dict] = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        compute_f0: bool = False,\n        f0_cache_path: str = None,\n        return_wav: bool = False,\n        batch_group_size: int = 0,\n        min_text_len: int = 0,\n        max_text_len: int = float(\"inf\"),\n        min_audio_len: int = 0,\n        max_audio_len: int = float(\"inf\"),\n        phoneme_cache_path: str = None,\n        precompute_num_workers: int = 0,\n        speaker_id_mapping: Dict = None,\n        d_vector_mapping: Dict = None,\n        language_id_mapping: Dict = None,\n        use_noise_augment: bool = False,\n        start_by_longest: bool = False,\n        verbose: bool = False,\n    ):\n        \n        \n        super().__init__()\n        self.batch_group_size = batch_group_size\n        self._samples = samples\n        self.outputs_per_step = outputs_per_step\n        self.compute_linear_spec = compute_linear_spec\n        self.return_wav = return_wav\n        self.compute_f0 = compute_f0\n        self.f0_cache_path = f0_cache_path\n        self.min_audio_len = min_audio_len\n        self.max_audio_len = max_audio_len\n        self.min_text_len = min_text_len\n        self.max_text_len = max_text_len\n        self.ap = ap\n        self.phoneme_cache_path = phoneme_cache_path\n        self.speaker_id_mapping = speaker_id_mapping\n        self.d_vector_mapping = d_vector_mapping\n        self.language_id_mapping = language_id_mapping\n        self.use_noise_augment = use_noise_augment\n        self.start_by_longest = start_by_longest\n\n        self.verbose = verbose\n        self.rescue_item_idx = 1\n        self.pitch_computed = False\n        self.tokenizer = tokenizer\n\n        if self.tokenizer.use_phonemes:\n            self.phoneme_dataset = PhonemeDataset(\n                self.samples, self.tokenizer, phoneme_cache_path, precompute_num_workers=precompute_num_workers\n            )\n\n        if compute_f0:\n            self.f0_dataset = F0Dataset(\n                self.samples, self.ap, cache_path=f0_cache_path, precompute_num_workers=precompute_num_workers\n            )\n\n        if self.verbose:\n            self.print_logs()```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        outputs_per_step: int = 1,\n        compute_linear_spec: bool = False,\n        ap: AudioProcessor = None,\n        samples: List[Dict] = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        compute_f0: bool = False,\n        compute_energy: bool = False,\n        f0_cache_path: str = None,\n        energy_cache_path: str = None,\n        return_wav: bool = False,\n        batch_group_size: int = 0,\n        min_text_len: int = 0,\n        max_text_len: int = float(\"inf\"),\n        min_audio_len: int = 0,\n        max_audio_len: int = float(\"inf\"),\n        phoneme_cache_path: str = None,\n        precompute_num_workers: int = 0,\n        speaker_id_mapping: Dict = None,\n        d_vector_mapping: Dict = None,\n        language_id_mapping: Dict = None,\n        use_noise_augment: bool = False,\n        start_by_longest: bool = False,\n        verbose: bool = False,\n    ):\n        \n        \n        super().__init__()\n        self.batch_group_size = batch_group_size\n        self._samples = samples\n        self.outputs_per_step = outputs_per_step\n        self.compute_linear_spec = compute_linear_spec\n        self.return_wav = return_wav\n        self.compute_f0 = compute_f0\n        self.compute_energy = compute_energy\n        self.f0_cache_path = f0_cache_path\n        self.energy_cache_path = energy_cache_path\n        self.min_audio_len = min_audio_len\n        self.max_audio_len = max_audio_len\n        self.min_text_len = min_text_len\n        self.max_text_len = max_text_len\n        self.ap = ap\n        self.phoneme_cache_path = phoneme_cache_path\n        self.speaker_id_mapping = speaker_id_mapping\n        self.d_vector_mapping = d_vector_mapping\n        self.language_id_mapping = language_id_mapping\n        self.use_noise_augment = use_noise_augment\n        self.start_by_longest = start_by_longest\n\n        self.verbose = verbose\n        self.rescue_item_idx = 1\n        self.pitch_computed = False\n        self.tokenizer = tokenizer\n\n        if self.tokenizer.use_phonemes:\n            self.phoneme_dataset = PhonemeDataset(\n                self.samples, self.tokenizer, phoneme_cache_path, precompute_num_workers=precompute_num_workers\n            )\n\n        if compute_f0:\n            self.f0_dataset = F0Dataset(\n                self.samples, self.ap, cache_path=f0_cache_path, precompute_num_workers=precompute_num_workers\n            )\n        if compute_energy:\n            self.energy_dataset = EnergyDataset(\n                self.samples, self.ap, cache_path=energy_cache_path, precompute_num_workers=precompute_num_workers\n            )\n        if self.verbose:\n            self.print_logs()\n```\n\nOriginal documentation:\n```python\nGeneric \ud83d\udcc2 data loader for `tts` models. It is configurable for different outputs and needs.\n\n        If you need something different, you can subclass and override.\n\n        Args:\n            outputs_per_step (int): Number of time frames predicted per step.\n\n            compute_linear_spec (bool): compute linear spectrogram if True.\n\n            ap (TTS.tts.utils.AudioProcessor): Audio processor object.\n\n            samples (list): List of dataset samples.\n\n            tokenizer (TTSTokenizer): tokenizer to convert text to sequence IDs. If None init internally else\n                use the given. Defaults to None.\n\n            compute_f0 (bool): compute f0 if True. Defaults to False.\n\n            f0_cache_path (str): Path to store f0 cache. Defaults to None.\n\n            return_wav (bool): Return the waveform of the sample. Defaults to False.\n\n            batch_group_size (int): Range of batch randomization after sorting\n                sequences by length. It shuffles each batch with bucketing to gather similar lenght sequences in a\n                batch. Set 0 to disable. Defaults to 0.\n\n            min_text_len (int): Minimum length of input text to be used. All shorter samples will be ignored.\n                Defaults to 0.\n\n            max_text_len (int): Maximum length of input text to be used. All longer samples will be ignored.\n                Defaults to float(\"inf\").\n\n            min_audio_len (int): Minimum length of input audio to be used. All shorter samples will be ignored.\n                Defaults to 0.\n\n            max_audio_len (int): Maximum length of input audio to be used. All longer samples will be ignored.\n                The maximum length in the dataset defines the VRAM used in the training. Hence, pay attention to\n                this value if you encounter an OOM error in training. Defaults to float(\"inf\").\n\n            phoneme_cache_path (str): Path to cache computed phonemes. It writes phonemes of each sample to a\n                separate file. Defaults to None.\n\n            precompute_num_workers (int): Number of workers to precompute features. Defaults to 0.\n\n            speaker_id_mapping (dict): Mapping of speaker names to IDs used to compute embedding vectors by the\n                embedding layer. Defaults to None.\n\n            d_vector_mapping (dict): Mapping of wav files to computed d-vectors. Defaults to None.\n\n            use_noise_augment (bool): Enable adding random noise to wav for augmentation. Defaults to False.\n\n            start_by_longest (bool): Start by longest sequence. It is especially useful to check OOM. Defaults to False.\n\n            verbose (bool): Print diagnostic information. Defaults to false.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generic \ud83d\udcc2 data loader for `tts` models. It is configurable for different outputs and needs.\n\n        If you need something different, you can subclass and override.\n\n        Args:\n            outputs_per_step (int): Number of time frames predicted per step.\n\n            compute_linear_spec (bool): compute linear spectrogram if True.\n\n            ap (TTS.tts.utils.AudioProcessor): Audio processor object.\n\n            samples (list): List of dataset samples.\n\n            tokenizer (TTSTokenizer): tokenizer to convert text to sequence IDs. If None init internally else\n                use the given. Defaults to None.\n\n            compute_f0 (bool): compute f0 if True. Defaults to False.\n\n            compute_energy (bool): compute energy if True. Defaults to False.\n\n            f0_cache_path (str): Path to store f0 cache. Defaults to None.\n\n            energy_cache_path (str): Path to store energy cache. Defaults to None.\n\n            return_wav (bool): Return the waveform of the sample. Defaults to False.\n\n            batch_group_size (int): Range of batch randomization after sorting\n                sequences by length. It shuffles each batch with bucketing to gather similar lenght sequences in a\n                batch. Set 0 to disable. Defaults to 0.\n\n            min_text_len (int): Minimum length of input text to be used. All shorter samples will be ignored.\n                Defaults to 0.\n\n            max_text_len (int): Maximum length of input text to be used. All longer samples will be ignored.\n                Defaults to float(\"inf\").\n\n            min_audio_len (int): Minimum length of input audio to be used. All shorter samples will be ignored.\n                Defaults to 0.\n\n            max_audio_len (int): Maximum length of input audio to be used. All longer samples will be ignored.\n                The maximum length in the dataset defines the VRAM used in the training. Hence, pay attention to\n                this value if you encounter an OOM error in training. Defaults to float(\"inf\").\n\n            phoneme_cache_path (str): Path to cache computed phonemes. It writes phonemes of each sample to a\n                separate file. Defaults to None.\n\n            precompute_num_workers (int): Number of workers to precompute features. Defaults to 0.\n\n            speaker_id_mapping (dict): Mapping of speaker names to IDs used to compute embedding vectors by the\n                embedding layer. Defaults to None.\n\n            d_vector_mapping (dict): Mapping of wav files to computed d-vectors. Defaults to None.\n\n            use_noise_augment (bool): Enable adding random noise to wav for augmentation. Defaults to False.\n\n            start_by_longest (bool): Start by longest sequence. It is especially useful to check OOM. Defaults to False.\n\n            verbose (bool): Print diagnostic information. Defaults to false.", "file": "TTS_tts_datasets_dataset.py", "function": "TTSDataset.__init__", "project": "TTS", "change_stats": {"code_changes": 11, "doc_changes": 6, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_cfn_attribute(self, attribute_name):\n        \n        if attribute_name in REF_ARN_ATTRS and hasattr(self, \"arn\"):\n            return self.arn\n        if attribute_name in REF_ATTRS:\n            result = self.get_physical_resource_id(attribute=attribute_name)\n            if result:\n                return result\n        props = self.props\n        if attribute_name in props:\n            return props.get(attribute_name)\n        return None```\n\nUpdated code:\n```python\ndef get_cfn_attribute(self, attribute_name):\n        \n        return self.props.get(attribute_name)\n```\n\nOriginal documentation:\n```python\nRetrieve the given CF attribute for this resource (inherited from moto's CloudFormationModel)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieve the given CF attribute for this resource", "file": "localstack_services_cloudformation_service_models.py", "function": "GenericBaseModel.get_cfn_attribute", "project": "localstack", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_prediction_confidence(self, tests: List[str]) -> Dict[str, float]:\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef get_prediction_confidence(self, tests: List[str]) -> TestPrioritizations:\n        \n        \n        \n        pass\n```\n\nOriginal documentation:\n```python\nLike get_test_priorities, but instead returns a float ranking ranging\n        from -1 to 1, where negative means skip, positive means run, 0 means no\n        idea, and magnitude = how confident the heuristic is. Used by\n        AggregatedHeuristicsRankings.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a float ranking ranging from -1 to 1, where negative means skip,\n        positive means run, 0 means no idea, and magnitude = how confident the\n        heuristic is. Used by AggregatedHeuristicsRankings.", "file": "tools_testing_target_determination_heuristics_interface.py", "function": "HeuristicInterface.get_prediction_confidence", "project": "pytorch", "change_stats": {"code_changes": 4, "doc_changes": 9, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef remove_disabled_jobs(\n    workflow: str, job_name: str, test_matrix: Dict[str, List[Any]]\n) -> Dict[str, List[Any]]:\n    \n    \n    \n    try:\n        # The job name from github is in the PLATFORM / JOB (CONFIG) format, so breaking\n        # it into its two components first\n        current_platform, _ = [n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n]\n    except ValueError as error:\n        warnings.warn(f\"Invalid job name {job_name}, returning\")\n        return test_matrix\n\n    # The result will be stored here\n    filtered_test_matrix: Dict[str, List[Any]] = {\"include\": []}\n\n    for _, record in download_json(url=DISABLED_JOBS_URL, headers={}).items():\n        (\n            author,\n            _,\n            disabled_url,\n            disabled_workflow,\n            disabled_platform,\n            disabled_job_cfg,\n        ) = record\n\n        if disabled_workflow != workflow:\n            # The current workflow is not disabled by this record\n            continue\n\n        cleanup_regex = rf\"(-{BUILD_JOB_NAME}|-{TEST_JOB_NAME})$\"\n        # There is an exception here for binary build workflows in which the platform\n        # names have the build and test suffix. For example, we have a build job called\n        # manywheel-py3-cuda11_8-build / build and its subsequent test job called\n        # manywheel-py3-cuda11_8-test / test. So they are linked, but their suffixes\n        # are different\n        disabled_platform_no_suffix = re.sub(cleanup_regex, \"\", disabled_platform)\n        current_platform_no_suffix = re.sub(cleanup_regex, \"\", current_platform)\n\n        if (\n            disabled_platform != current_platform\n            and disabled_platform_no_suffix != current_platform_no_suffix\n        ):\n            # The current platform is not disabled by this record\n            continue\n\n        # The logic after this is fairly complicated:\n        #\n        # - If the disabled record doesn't have the optional job (config) name,\n        #   i.e. pull / linux-bionic-py3.8-clang9, all build and test jobs will\n        #   be skipped\n        #\n        # - If the disabled record has the job name and it's a build job, i.e.\n        #   pull / linux-bionic-py3.8-clang9 / build, all build and test jobs\n        #   will be skipped, because the latter requires the former\n        #\n        # - If the disabled record has the job name and it's a test job without\n        #   the config part, i.e. pull / linux-bionic-py3.8-clang9 / test, all\n        #   test jobs will be skipped. TODO: At the moment, the script uses the\n        #   short-circuiting logic to skip the build job automatically when there\n        #   is no test job assuming that it would be a waste of effort building\n        #   for nothing. This might not be the desirable behavior, and could be\n        #   fixed later if needed\n        #\n        # - If the disabled record has the job (config) name, only that test config\n        #   will be skipped, i.e. pull / linux-bionic-py3.8-clang9 / test (dynamo)\n        if not disabled_job_cfg:\n            print(\n                f\"Issue {disabled_url} created by {author} has disabled all CI jobs for {workflow} / {job_name}\"\n            )\n            return filtered_test_matrix\n\n        if disabled_job_cfg == BUILD_JOB_NAME:\n            print(\n                f\"Issue {disabled_url} created by {author} has disabled the build job for {workflow} / {job_name}\"\n            )\n            return filtered_test_matrix\n\n        if disabled_job_cfg in (TEST_JOB_NAME, BUILD_AND_TEST_JOB_NAME):\n            print(\n                f\"Issue {disabled_url} created by {author} has disabled all the test jobs for {workflow} / {job_name}\"\n            )\n            return filtered_test_matrix\n\n        m = JOB_NAME_CFG_REGEX.match(disabled_job_cfg)\n        if m:\n            disabled_job = m.group(\"job\")\n            # Make sure that the job name is a valid test job name first before checking the config\n            if disabled_job in (TEST_JOB_NAME, BUILD_AND_TEST_JOB_NAME):\n                disabled_cfg = m.group(\"cfg\")\n                # Remove the disabled config from the test matrix\n                filtered_test_matrix[\"include\"] = [\n                    r\n                    for r in test_matrix[\"include\"]\n                    if r.get(\"config\", \"\") != disabled_cfg\n                ]\n                return filtered_test_matrix\n\n        warnings.warn(\n            f\"Found a matching disabled issue {disabled_url} for {workflow} / {job_name}, \"\n            f\"but the name {disabled_job_cfg} is invalid\"\n        )\n\n    # Found no matching disabled issue, return the same input test matrix\n    return test_matrix```\n\nUpdated code:\n```python\ndef remove_disabled_jobs(\n    workflow: str, job_name: str, test_matrix: Dict[str, List[Any]]\n) -> Dict[str, List[Any]]:\n    \n    \n    \n    return process_jobs(\n        workflow=workflow,\n        job_name=job_name,\n        test_matrix=test_matrix,\n        issue_type=IssueType.DISABLED,\n        url=DISABLED_JOBS_URL,\n    )\n```\n\nOriginal documentation:\n```python\nCheck the list of disabled jobs, remove the current job and all its dependents\n    if it exists in the list. The list of disabled jobs is as follows:\n\n    {\n        \"WORKFLOW / PLATFORM / JOB (CONFIG)\": [\n            AUTHOR,\n            ISSUE_NUMBER,\n            ISSUE_URL,\n            WORKFLOW,\n            PLATFORM,\n            JOB (CONFIG),\n        ],\n        \"pull / linux-bionic-py3.8-clang9 / test (dynamo)\": [\n            \"pytorchbot\",\n            \"94861\",\n            \"https://github.com/pytorch/pytorch/issues/94861\",\n            \"pull\",\n            \"linux-bionic-py3.8-clang9\",\n            \"test (dynamo)\",\n        ],\n    }\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check the list of disabled jobs, remove the current job and all its dependents\n    if it exists in the list", "file": ".github_scripts_filter_test_configs.py", "function": "remove_disabled_jobs", "project": "pytorch", "change_stats": {"code_changes": 109, "doc_changes": 23, "total_changes": 132}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef move_to_device(item, device):\n    \n    \n    \n    if torch.is_tensor(item):\n        return item.to(device)\n    elif isinstance(item, list):\n        return [move_to_device(v, device) for v in item]\n    elif isinstance(item, tuple):\n        return tuple([move_to_device(v, device) for v in item])\n    elif isinstance(item, dict):\n        return {k: move_to_device(v, device) for k, v in item.items()}\n    else:\n        return item```\n\nUpdated code:\n```python\ndef move_to_device(item, device, criterion_func):\n    \n    \n    \n    if criterion_func(item):\n        device_copy = item.to(device)\n        item.data = device_copy.data\n        return item\n    elif isinstance(item, list):\n        return [move_to_device(v, device, criterion_func) for v in item]\n    elif isinstance(item, tuple):\n        return tuple([move_to_device(v, device, criterion_func) for v in item])\n    elif isinstance(item, dict):\n        return {k: move_to_device(v, device, criterion_func) for k, v in item.items()}\n    else:\n        return item\n```\n\nOriginal documentation:\n```python\nMove tensor onto device. Works on individual tensors, and tensors contained/nested in lists, tuples, and dicts.\n    Parameters:\n        item: tensor to move or (possibly nested) container of tensors to move.\n        device: target device\n\n    Returns:\n        None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Move tensor on to specified device by changing the storage.\n    Works on individual tensors, and tensors contained/nested in lists, tuples, and dicts.\n    Parameters:\n        item: tensor to move or (possibly nested) container of tensors to move.\n        device: target device\n        criterion_func: Function to restrict move operation to items meet criterion\n\n    Returns:\n        None", "file": "deepspeed_runtime_utils.py", "function": "move_to_device", "project": "DeepSpeed", "change_stats": {"code_changes": 16, "doc_changes": 6, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef websocket_get_panels(hass, connection, msg):\n    \n    \n    user_is_admin = connection.user.is_admin\n    panels = {\n        panel_key: panel.to_response()\n        for panel_key, panel in connection.hass.data[DATA_PANELS].items()\n        if user_is_admin or not panel.require_admin\n    }\n\n    connection.send_message(websocket_api.result_message(msg[\"id\"], panels))```\n\nUpdated code:\n```python\ndef websocket_get_panels(hass, connection, msg):\n    \n    user_is_admin = connection.user.is_admin\n    panels = {\n        panel_key: panel.to_response()\n        for panel_key, panel in connection.hass.data[DATA_PANELS].items()\n        if user_is_admin or not panel.require_admin\n    }\n\n    connection.send_message(websocket_api.result_message(msg[\"id\"], panels))\n```\n\nOriginal documentation:\n```python\nHandle get panels command.\n\n    Async friendly.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Handle get panels command.", "file": "homeassistant_components_frontend___init__.py", "function": "websocket_get_panels", "project": "core", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef resize_local_mean(image, output_shape, grid_mode=True,\n                      preserve_range=False):\n    \n\n    \n    resized, output_shape = _preprocess_resize_output_shape(image,\n                                                            output_shape)\n\n    resized = convert_to_float(resized, preserve_range)\n    dtype = resized.dtype\n\n    for axis, (old_size, new_size) in enumerate(zip(image.shape,\n                                                    output_shape)):\n        if old_size == new_size:\n            continue\n        weights = _local_mean_weights(old_size, new_size, grid_mode, dtype)\n        product = np.tensordot(resized, weights, [[axis], [-1]])\n        resized = np.moveaxis(product, -1, axis)\n\n    return resized```\n\nUpdated code:\n```python\ndef resize_local_mean(image, output_shape, grid_mode=True,\n                      preserve_range=False, *, channel_axis=None):\n    \n\n    \n    if channel_axis is not None:\n        if channel_axis < -image.ndim or channel_axis >= image.ndim:\n            raise ValueError(\"invalid channel_axis\")\n\n        # move channels to last position\n        image = np.moveaxis(image, channel_axis, -1)\n        nc = image.shape[-1]\n\n        output_ndim = len(output_shape)\n        if output_ndim == image.ndim - 1:\n            # insert channels dimension at the end\n            output_shape = output_shape + (nc,)\n        elif output_ndim == image.ndim:\n            if output_shape[channel_axis] != nc:\n                raise ValueError(\n                    \"Cannot reshape along the channel_axis. Use \"\n                    \"channel_axis=None to reshape along all axes.\"\n                )\n            # move channels to last position in output_shape\n            channel_axis = channel_axis % image.ndim\n            output_shape = (\n                output_shape[:channel_axis] + output_shape[channel_axis:] +\n                (nc,)\n            )\n        else:\n            raise ValueError(\n                \"len(output_shape) must be image.ndim or (image.ndim - 1) \"\n                \"when a channel_axis is specified.\"\n            )\n        resized = image\n    else:\n        resized, output_shape = _preprocess_resize_output_shape(image,\n                                                                output_shape)\n    resized = convert_to_float(resized, preserve_range)\n    dtype = resized.dtype\n\n    for axis, (old_size, new_size) in enumerate(zip(image.shape,\n                                                    output_shape)):\n        if old_size == new_size:\n            continue\n        weights = _local_mean_weights(old_size, new_size, grid_mode, dtype)\n        product = np.tensordot(resized, weights, [[axis], [-1]])\n        resized = np.moveaxis(product, -1, axis)\n\n    if channel_axis is not None:\n        # restore channels to original axis\n        resized = np.moveaxis(resized, -1, channel_axis)\n\n    return resized\n```\n\nOriginal documentation:\n```python\nResize an array with the local mean / bilinear scaling.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    output_shape : tuple or ndarray\n        Size of the generated output image `(rows, cols[, ...][, dim])`. If\n        `dim` is not provided, the number of channels is preserved. In case the\n        number of input channels does not equal the number of output channels\n        an n-dimensional interpolation is applied.\n    grid_mode : bool, optional\n        If False, the distance from the pixel centers is zoomed. Otherwise, the\n        distance including the full pixel extent is used. For example, a 1d\n        signal of length 5 is considered to have length 4 when `grid_mode` is\n        False, but length 5 when `grid_mode` is True. See the following\n        visual illustration:\n\n        .. code-block:: text\n\n                | pixel 1 | pixel 2 | pixel 3 | pixel 4 | pixel 5 |\n                     |<-------------------------------------->|\n                                        vs.\n                |<----------------------------------------------->|\n\n        The starting point of the arrow in the diagram above corresponds to\n        coordinate location 0 in each mode.\n    preserve_range : bool, optional\n        Whether to keep the original range of values. Otherwise, the input\n        image is converted according to the conventions of `img_as_float`.\n        Also see\n        https://scikit-image.org/docs/dev/user_guide/data_types.html\n\n    Returns\n    -------\n    resized : ndarray\n        Resized version of the input.\n\n    See Also\n    --------\n    resize, downscale_local_mean\n\n    Notes\n    -----\n    This method is sometimes referred to as \"area-based\" interpolation or\n    \"pixel mixing\" interpolation [1]_. When `grid_mode` is True, it is\n    equivalent to using OpenCV's resize with `INTER_AREA` interpolation mode.\n    It is commonly used for image downsizing. If the downsizing factors are\n    integers, then ``downscale_local_mean`` should be preferred instead.\n\n    References\n    ----------\n    .. [1] http://entropymine.com/imageworsener/pixelmixing/\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.transform import resize_local_mean\n    >>> image = data.camera()\n    >>> resize_local_mean(image, (100, 100)).shape\n    (100, 100)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Resize an array with the local mean / bilinear scaling.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image. If this is a multichannel image, the axis corresponding\n        to channels should be specified using `channel_axis`\n    output_shape : tuple or ndarray\n        Size of the generated output image. When `channel_axis` is not None,\n        the `channel_axis` should either be omitted from `output_shape` or the\n        ``output_shape[channel_axis]`` must match\n        ``image.shape[channel_axis]``. If the length of `output_shape` exceeds\n        image.ndim, additional singleton dimensions will be appended to the\n        input ``image`` as needed.\n    grid_mode : bool, optional\n        If False, the distance from the pixel centers is zoomed. Otherwise, the\n        distance including the full pixel extent is used. For example, a 1d\n        signal of length 5 is considered to have length 4 when `grid_mode` is\n        False, but length 5 when `grid_mode` is True. See the following\n        visual illustration:\n\n        .. code-block:: text\n\n                | pixel 1 | pixel 2 | pixel 3 | pixel 4 | pixel 5 |\n                     |<-------------------------------------->|\n                                        vs.\n                |<----------------------------------------------->|\n\n        The starting point of the arrow in the diagram above corresponds to\n        coordinate location 0 in each mode.\n    preserve_range : bool, optional\n        Whether to keep the original range of values. Otherwise, the input\n        image is converted according to the conventions of `img_as_float`.\n        Also see\n        https://scikit-image.org/docs/dev/user_guide/data_types.html\n\n    Returns\n    -------\n    resized : ndarray\n        Resized version of the input.\n\n    See Also\n    --------\n    resize, downscale_local_mean\n\n    Notes\n    -----\n    This method is sometimes referred to as \"area-based\" interpolation or\n    \"pixel mixing\" interpolation [1]_. When `grid_mode` is True, it is\n    equivalent to using OpenCV's resize with `INTER_AREA` interpolation mode.\n    It is commonly used for image downsizing. If the downsizing factors are\n    integers, then `downscale_local_mean` should be preferred instead.\n\n    References\n    ----------\n    .. [1] http://entropymine.com/imageworsener/pixelmixing/\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.transform import resize_local_mean\n    >>> image = data.camera()\n    >>> resize_local_mean(image, (100, 100)).shape\n    (100, 100)", "file": "skimage_transform__warps.py", "function": "resize_local_mean", "project": "scikit-image", "change_stats": {"code_changes": 42, "doc_changes": 17, "total_changes": 59}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_trials(self, trials: Iterable[FrozenTrial]) -> None:\n        \n        \n\n        for trial in trials:\n            self.add_trial(trial)```\n\nUpdated code:\n```python\ndef add_trials(self, trials: Iterable[FrozenTrial]) -> None:\n        \n\n        \n\n        for trial in trials:\n            self.add_trial(trial)\n```\n\nOriginal documentation:\n```python\nAdd trials to study.\n\n        The trials are validated before being added.\n\n        Example:\n\n            .. testcode::\n\n                import optuna\n\n\n                def objective(trial):\n                    x = trial.suggest_float(\"x\", 0, 10)\n                    return x**2\n\n\n                study = optuna.create_study()\n                study.optimize(objective, n_trials=3)\n                assert len(study.trials) == 3\n\n                other_study = optuna.create_study()\n                other_study.add_trials(study.trials)\n                assert len(other_study.trials) == len(study.trials)\n\n                other_study.optimize(objective, n_trials=2)\n                assert len(other_study.trials) == len(study.trials) + 2\n\n        .. seealso::\n\n            See :func:`~optuna.study.Study.add_trial` for addition of each trial.\n\n        Args:\n            trials: Trials to add.\n\n        Raises:\n            :exc:`ValueError`:\n                If ``trials`` include invalid trial.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add trials to study.\n\n        The trials are validated before being added.\n\n        Example:\n\n            .. testcode::\n\n                import optuna\n\n\n                def objective(trial):\n                    x = trial.suggest_float(\"x\", 0, 10)\n                    return x**2\n\n\n                study = optuna.create_study()\n                study.optimize(objective, n_trials=3)\n                assert len(study.trials) == 3\n\n                other_study = optuna.create_study()\n                other_study.add_trials(study.trials)\n                assert len(other_study.trials) == len(study.trials)\n\n                other_study.optimize(objective, n_trials=2)\n                assert len(other_study.trials) == len(study.trials) + 2\n\n        .. seealso::\n\n            See :func:`~optuna.study.Study.add_trial` for addition of each trial.\n\n        Args:\n            trials: Trials to add.", "file": "optuna_study_study.py", "function": "Study.add_trials", "project": "optuna", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef uri_to_iri(uri):\n    \n    \n    \n    if uri is None:\n        return uri\n    uri = force_bytes(uri)\n    iri = unquote_to_bytes(uri)\n    return repercent_broken_unicode(iri).decode()```\n\nUpdated code:\n```python\ndef uri_to_iri(uri):\n    \n    \n    \n    if uri is None:\n        return uri\n    uri = force_bytes(uri)\n    # Fast selective unqote: First, split on '%' and then starting with the\n    # second block, decode the first 2 bytes if they represent a hex code to\n    # decode. The rest of the block is the part after '%AB', not containing\n    # any '%'. Add that to the output without further processing.\n    bits = uri.split(b'%')\n    if len(bits) == 1:\n        iri = uri\n    else:\n        parts = [bits[0]]\n        append = parts.append\n        hextobyte = _hextobyte\n        for item in bits[1:]:\n            hex = item[:2]\n            if hex in hextobyte:\n                append(hextobyte[item[:2]])\n                append(item[2:])\n            else:\n                append(b'%')\n                append(item)\n        iri = b''.join(parts)\n    return repercent_broken_unicode(iri).decode()\n```\n\nOriginal documentation:\n```python\nConverts a Uniform Resource Identifier(URI) into an Internationalized\n    Resource Identifier(IRI).\n\n    This is the algorithm from section 3.2 of RFC 3987.\n\n    Takes an URI in ASCII bytes (e.g. '/I%20%E2%99%A5%20Django/') and returns\n    a string containing the encoded result (e.g. '/I \\xe2\\x99\\xa5 Django/').\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Converts a Uniform Resource Identifier(URI) into an Internationalized\n    Resource Identifier(IRI).\n\n    This is the algorithm from section 3.2 of RFC 3987, excluding step 4.\n\n    Takes an URI in ASCII bytes (e.g. '/I%20%E2%99%A5%20Django/') and returns\n    a string containing the encoded result (e.g. '/I%20\u2665%20Django/').", "file": "django_utils_encoding.py", "function": "uri_to_iri", "project": "django", "change_stats": {"code_changes": 23, "doc_changes": 6, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                     encoding=None):\n    \n    \n\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n    keyvals.sort()\n    query = urlencode(keyvals)\n\n    # XXX: copied from w3lib.url.safe_url_string to add encoding argument\n    # path = to_native_str(path, encoding)\n    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'\n\n    path = safe_url_string(_unquotepath(path)) or '/'\n    fragment = '' if not keep_fragments else fragment\n    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))```\n\nUpdated code:\n```python\ndef canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                     encoding=None):\n    \n    \n    # If supplied `encoding` is not compatible with all characters in `url`,\n    # fallback to UTF-8 as safety net.\n    # UTF-8 can handle all Unicode characters,\n    # so we should be covered regarding URL normalization,\n    # if not for proper URL expected by remote website.\n    try:\n        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n            parse_url(url), encoding=encoding)\n    except UnicodeError as e:\n        if encoding != 'utf8':\n            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n                parse_url(url), encoding='utf8')\n        else:\n            raise\n\n    # 1. decode query-string as UTF-8 (or keep raw bytes),\n    #    sort values,\n    #    and percent-encode them back\n    if not six.PY2:\n        # Python3's urllib.parse.parse_qsl does not work as wanted\n        # for percent-encoded characters that do not match passed encoding,\n        # they get lost.\n        #\n        # e.g., 'q=b%a3' becomes [('q', 'b\\ufffd')]\n        # (ie. with 'REPLACEMENT CHARACTER' (U+FFFD),\n        #      instead of \\xa3 that you get with Python2's parse_qsl)\n        #\n        # what we want here is to keep raw bytes, and percent encode them\n        # so as to preserve whatever encoding what originally used.\n        #\n        # See https://tools.ietf.org/html/rfc3987#section-6.4:\n        #\n        # For example, it is possible to have a URI reference of\n        # \"http://www.example.org/r%E9sum%E9.xml#r%C3%A9sum%C3%A9\", where the\n        # document name is encoded in iso-8859-1 based on server settings, but\n        # where the fragment identifier is encoded in UTF-8 according to\n        # [XPointer]. The IRI corresponding to the above URI would be (in XML\n        # notation)\n        # \"http://www.example.org/r%E9sum%E9.xml#r&#xE9;sum&#xE9;\".\n        # Similar considerations apply to query parts.  The functionality of\n        # IRIs (namely, to be able to include non-ASCII characters) can only be\n        # used if the query part is encoded in UTF-8.\n        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n    else:\n        keyvals = parse_qsl(query, keep_blank_values)\n    keyvals.sort()\n    query = urlencode(keyvals)\n\n    # 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)\n    #    and percent-encode path again (this normalizes to upper-case %XX)\n    uqp = _unquotepath(path)\n    path = quote(uqp, _safe_chars) or '/'\n\n    fragment = '' if not keep_fragments else fragment\n\n    # every part should be safe already\n    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n```\n\nOriginal documentation:\n```python\nCanonicalize the given url by applying the following procedures:\n\n    - sort query arguments, first by key, then by value\n    - percent encode paths and query arguments. non-ASCII characters are\n      percent-encoded using UTF-8 (RFC-3986)\n    - normalize all spaces (in query arguments) '+' (plus symbol)\n    - normalize percent encodings case (%2f -> %2F)\n    - remove query arguments with blank values (unless keep_blank_values is True)\n    - remove fragments (unless keep_fragments is True)\n\n    The url passed can be a str or unicode, while the url returned is always a\n    str.\n\n    For examples see the tests in tests/test_utils_url.py\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Canonicalize the given url by applying the following procedures:\n\n    - sort query arguments, first by key, then by value\n    - percent encode paths ; non-ASCII characters are percent-encoded\n      using UTF-8 (RFC-3986)\n    - percent encode query arguments ; non-ASCII characters are percent-encoded\n      using passed `encoding` (UTF-8 by default)\n    - normalize all spaces (in query arguments) '+' (plus symbol)\n    - normalize percent encodings case (%2f -> %2F)\n    - remove query arguments with blank values (unless `keep_blank_values` is True)\n    - remove fragments (unless `keep_fragments` is True)\n\n    The url passed can be bytes or unicode, while the url returned is\n    always a native str (bytes in Python 2, unicode in Python 3).\n\n    For examples see the tests in tests/test_utils_url.py", "file": "scrapy_utils_url.py", "function": "canonicalize_url", "project": "scrapy", "change_stats": {"code_changes": 58, "doc_changes": 16, "total_changes": 74}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_credit_card_number(credit_card_number: str) -> bool:\n    \n    \n    \n    error_message = f\"{credit_card_number} is an invalid credit card number because\"\n    if not credit_card_number.isdigit():\n        print(f\"{error_message} it has nonnumerical characters.\")\n        return False\n\n    if not 13 <= len(credit_card_number) <= 16:\n        print(f\"{error_message} of its length.\")\n        return False\n\n    if not validate_initial_digits(credit_card_number):\n        print(f\"{error_message} of its first two digits.\")\n        return False\n\n    if not luhn_validation(credit_card_number):\n        print(f\"{error_message} it fails the Lhun check.\")\n        return False\n\n    print(f\"{credit_card_number} is a valid credit card number.\")\n    return True```\n\nUpdated code:\n```python\ndef validate_credit_card_number(credit_card_number: str) -> bool:\n    \n    \n    \n    error_message = f\"{credit_card_number} is an invalid credit card number because\"\n    if not credit_card_number.isdigit():\n        print(f\"{error_message} it has nonnumerical characters.\")\n        return False\n\n    if not 13 <= len(credit_card_number) <= 16:\n        print(f\"{error_message} of its length.\")\n        return False\n\n    if not validate_initial_digits(credit_card_number):\n        print(f\"{error_message} of its first two digits.\")\n        return False\n\n    if not luhn_validation(credit_card_number):\n        print(f\"{error_message} it fails the Luhn check.\")\n        return False\n\n    print(f\"{credit_card_number} is a valid credit card number.\")\n    return True\n```\n\nOriginal documentation:\n```python\nFunction to validate the given credit card number.\n    >>> validate_credit_card_number('4111111111111111')\n    4111111111111111 is a valid credit card number.\n    True\n    >>> validate_credit_card_number('helloworld$')\n    helloworld$ is an invalid credit card number because it has nonnumerical characters.\n    False\n    >>> validate_credit_card_number('32323')\n    32323 is an invalid credit card number because of its length.\n    False\n    >>> validate_credit_card_number('32323323233232332323')\n    32323323233232332323 is an invalid credit card number because of its length.\n    False\n    >>> validate_credit_card_number('36111111111111')\n    36111111111111 is an invalid credit card number because of its first two digits.\n    False\n    >>> validate_credit_card_number('41111111111111')\n    41111111111111 is an invalid credit card number because it fails the Lhun check.\n    False\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Function to validate the given credit card number.\n    >>> validate_credit_card_number('4111111111111111')\n    4111111111111111 is a valid credit card number.\n    True\n    >>> validate_credit_card_number('helloworld$')\n    helloworld$ is an invalid credit card number because it has nonnumerical characters.\n    False\n    >>> validate_credit_card_number('32323')\n    32323 is an invalid credit card number because of its length.\n    False\n    >>> validate_credit_card_number('32323323233232332323')\n    32323323233232332323 is an invalid credit card number because of its length.\n    False\n    >>> validate_credit_card_number('36111111111111')\n    36111111111111 is an invalid credit card number because of its first two digits.\n    False\n    >>> validate_credit_card_number('41111111111111')\n    41111111111111 is an invalid credit card number because it fails the Luhn check.\n    False", "file": "strings_credit_card_validator.py", "function": "validate_credit_card_number", "project": "Python", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate(self, x, h, n_samples=None, interval=None, mode=\"sampling\"):\n        \n\n        \n        # reshape inputs\n        assert len(x.shape) == 1\n        assert len(h.shape) == 2 and h.shape[1] == self.n_aux\n        x = x.unsqueeze(0)\n        h = h.transpose(0, 1).unsqueeze(0)\n\n        # check number of samples to be generated\n        if n_samples is None:\n            if self.upsampling_factor > 0:\n                n_samples = h.shape[1] * self.upsampling_factor - 1\n            else:\n                n_samples = h.shape[1] - 1\n\n        # perform upsampling\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # padding if the length less than\n        n_pad = self.receptive_field - x.size(1)\n        if n_pad > 0:\n            x = F.pad(x, (n_pad, 0), \"constant\", self.n_quantize // 2)\n            h = F.pad(h, (n_pad, 0), \"replicate\")\n\n        # prepare buffer\n        output = self._preprocess(x)\n        h_ = h[:, :, :x.size(1)]\n        output_buffer = []\n        buffer_size = []\n        for l, d in enumerate(self.dilations):\n            output, _ = self._residual_forward(\n                output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                self.skip_1x1[l], self.res_1x1[l])\n            if d == 2 ** (self.dilation_depth - 1):\n                buffer_size.append(self.kernel_size - 1)\n            else:\n                buffer_size.append(d * 2 * (self.kernel_size - 1))\n            output_buffer.append(output[:, :, -buffer_size[l] - 1: -1])\n\n        # generate\n        samples = x[0]\n        start = time.time()\n        for i in range(n_samples):\n            output = samples[-self.kernel_size * 2 + 1:].unsqueeze(0)\n            output = self._preprocess(output)\n            h_ = h[:, :, samples.size(0) - 1].contiguous().view(1, self.n_aux, 1)\n            output_buffer_next = []\n            skip_connections = []\n            for l, d in enumerate(self.dilations):\n                output, skip = self._generate_residual_forward(\n                    output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                    self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                    self.skip_1x1[l], self.res_1x1[l])\n                output = torch.cat([output_buffer[l], output], dim=2)\n                output_buffer_next.append(output[:, :, -buffer_size[l]:])\n                skip_connections.append(skip)\n\n            # update buffer\n            output_buffer = output_buffer_next\n\n            # get predicted sample\n            output = sum(skip_connections)\n            output = self._postprocess(output)[0]\n            if mode == \"sampling\":\n                posterior = F.softmax(output[-1], dim=0)\n                dist = torch.distributions.Categorical(posterior)\n                sample = dist.sample().unsqueeze(0)\n            elif mode == \"argmax\":\n                sample = output.argmax(-1)\n            else:\n                logging.error(\"mode should be sampling or argmax\")\n                sys.exit(1)\n            samples = torch.cat([samples, sample], dim=0)\n\n            # show progress\n            if interval is not None and (i + 1) % interval == 0:\n                logging.info(\"%d/%d estimated time = %.3f sec (%.3f sec / sample)\" % (\n                    i + 1, n_samples,\n                    (n_samples - i - 1) * ((time.time() - start) / interval),\n                    (time.time() - start) / interval))\n                start = time.time()\n\n        return samples[-n_samples:].cpu().numpy()```\n\nUpdated code:\n```python\ndef generate(self, x, h, n_samples, interval=None, mode=\"sampling\"):\n        \n\n        \n        # reshape inputs\n        assert len(x.shape) == 1\n        assert len(h.shape) == 2 and h.shape[1] == self.n_aux\n        x = x.unsqueeze(0)\n        h = h.transpose(0, 1).unsqueeze(0)\n\n        # perform upsampling\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # padding if the length less than\n        n_pad = self.receptive_field - x.size(1)\n        if n_pad > 0:\n            x = F.pad(x, (n_pad, 0), \"constant\", self.n_quantize // 2)\n            h = F.pad(h, (n_pad, 0), \"replicate\")\n\n        # prepare buffer\n        output = self._preprocess(x)\n        h_ = h[:, :, :x.size(1)]\n        output_buffer = []\n        buffer_size = []\n        for l, d in enumerate(self.dilations):\n            output, _ = self._residual_forward(\n                output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                self.skip_1x1[l], self.res_1x1[l])\n            if d == 2 ** (self.dilation_depth - 1):\n                buffer_size.append(self.kernel_size - 1)\n            else:\n                buffer_size.append(d * 2 * (self.kernel_size - 1))\n            output_buffer.append(output[:, :, -buffer_size[l] - 1: -1])\n\n        # generate\n        samples = x[0]\n        start = time.time()\n        for i in range(n_samples):\n            output = samples[-self.kernel_size * 2 + 1:].unsqueeze(0)\n            output = self._preprocess(output)\n            h_ = h[:, :, samples.size(0) - 1].contiguous().view(1, self.n_aux, 1)\n            output_buffer_next = []\n            skip_connections = []\n            for l, d in enumerate(self.dilations):\n                output, skip = self._generate_residual_forward(\n                    output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                    self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                    self.skip_1x1[l], self.res_1x1[l])\n                output = torch.cat([output_buffer[l], output], dim=2)\n                output_buffer_next.append(output[:, :, -buffer_size[l]:])\n                skip_connections.append(skip)\n\n            # update buffer\n            output_buffer = output_buffer_next\n\n            # get predicted sample\n            output = sum(skip_connections)\n            output = self._postprocess(output)[0]\n            if mode == \"sampling\":\n                posterior = F.softmax(output[-1], dim=0)\n                dist = torch.distributions.Categorical(posterior)\n                sample = dist.sample().unsqueeze(0)\n            elif mode == \"argmax\":\n                sample = output.argmax(-1)\n            else:\n                logging.error(\"mode should be sampling or argmax\")\n                sys.exit(1)\n            samples = torch.cat([samples, sample], dim=0)\n\n            # show progress\n            if interval is not None and (i + 1) % interval == 0:\n                logging.info(\"%d/%d estimated time = %.3f sec (%.3f sec / sample)\" % (\n                    i + 1, n_samples,\n                    (n_samples - i - 1) * ((time.time() - start) / interval),\n                    (time.time() - start) / interval))\n                start = time.time()\n\n        return samples[-n_samples:].cpu().numpy()\n```\n\nOriginal documentation:\n```python\nGenerate a waveform with fast genration algorithm.\n\n        This generation based on `Fast WaveNet Generation Algorithm`_.\n\n        Args:\n            x (LongTensor): Initial wavenform tensor with the shape  (T).\n            h (Tensor): Auxiliary feature tensor with the shape  (n_samples + T, n_aux).\n            n_samples (int, optional): Number of samples to be generated.\n            interval (int, optional): Log interval.\n            mode (str, optional): \"sampling\" or \"argmax\".\n\n        Return:\n            ndarray: Generated quantized wavenform (n_samples).\n\n        .. _`Fast WaveNet Generation Algorithm`: https://arxiv.org/abs/1611.09482\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate a waveform with fast genration algorithm.\n\n        This generation based on `Fast WaveNet Generation Algorithm`_.\n\n        Args:\n            x (LongTensor): Initial wavenform tensor with the shape  (T).\n            h (Tensor): Auxiliary feature tensor with the shape  (n_samples + T, n_aux).\n            n_samples (int): Number of samples to be generated.\n            interval (int, optional): Log interval.\n            mode (str, optional): \"sampling\" or \"argmax\".\n\n        Return:\n            ndarray: Generated quantized wavenform (n_samples).\n\n        .. _`Fast WaveNet Generation Algorithm`: https://arxiv.org/abs/1611.09482", "file": "espnet_nets_pytorch_backend_wavenet.py", "function": "WaveNet.generate", "project": "espnet", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_resolution_directory(self):\n        \n        \n        pixel_height = self.video_quality_config[\"pixel_height\"]\n        frame_rate = self.video_quality_config[\"frame_rate\"]\n        return \"{}p{}\".format(pixel_height, frame_rate)```\n\nUpdated code:\n```python\ndef get_resolution_directory(self):\n        \n        \n        pixel_height = self.scene.camera.pixel_height\n        frame_rate = self.scene.camera.frame_rate\n        return \"{}p{}\".format(pixel_height, frame_rate)\n```\n\nOriginal documentation:\n```python\nGet the name of the resolution directory directly containing\n        the video file.\n\n        This method gets the name of the directory that immediately contains the\n        video file. This name is ``<height_in_pixels_of_video>p<frame_rate>``.\n        For example, if you are rendering an 854x480 px animation at 15fps,\n        the name of the directory that immediately contains the video file\n        will be ``480p15``.\n\n        The file structure should look something like::\n\n            MEDIA_DIR\n                |--Tex\n                |--texts\n                |--videos\n                |--<name_of_file_containing_scene>\n                    |--<height_in_pixels_of_video>p<frame_rate>\n                        |--<scene_name>.mp4\n\n        Returns\n        -------\n        :class:`str`\n            The name of the directory.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the name of the resolution directory directly containing\n        the video file.\n\n        This method gets the name of the directory that immediately contains the\n        video file. This name is ``<height_in_pixels_of_video>p<frame_rate>``.\n        For example, if you are rendering an 854x480 px animation at 15fps,\n        the name of the directory that immediately contains the video,  file\n        will be ``480p15``.\n\n        The file structure should look something like::\n\n            MEDIA_DIR\n                |--Tex\n                |--texts\n                |--videos\n                |--<name_of_file_containing_scene>\n                    |--<height_in_pixels_of_video>p<frame_rate>\n                        |--<scene_name>.mp4\n\n        Returns\n        -------\n        :class:`str`\n            The name of the directory.", "file": "manim_scene_scene_file_writer.py", "function": "SceneFileWriter.get_resolution_directory", "project": "manim", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef recon_loss(self, z, pos_edge_index,neg_edge_index=False):\n        \n        \n\n        pos_loss = -torch.log(\n            self.decoder(z, pos_edge_index, sigmoid=True) + EPS).mean()\n\n        # Do not include self-loops in negative samples\n        pos_edge_index, _ = remove_self_loops(pos_edge_index)\n        pos_edge_index, _ = add_self_loops(pos_edge_index)\n        if not neg_edge_index:\n            neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n        neg_loss = -torch.log(1 -\n                            self.decoder(z, neg_edge_index, sigmoid=True) +\n                            EPS).mean()\n\n        return pos_loss + neg_loss```\n\nUpdated code:\n```python\ndef recon_loss(self, z, pos_edge_index, neg_edge_index=None):\n        \n        \n\n        pos_loss = -torch.log(\n            self.decoder(z, pos_edge_index, sigmoid=True) + EPS).mean()\n\n        # Do not include self-loops in negative samples\n        pos_edge_index, _ = remove_self_loops(pos_edge_index)\n        pos_edge_index, _ = add_self_loops(pos_edge_index)\n        if neg_edge_index is None:\n            neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n        neg_loss = -torch.log(1 -\n                              self.decoder(z, neg_edge_index, sigmoid=True) +\n                              EPS).mean()\n\n        return pos_loss + neg_loss\n```\n\nOriginal documentation:\n```python\nr\"\"\"Given latent variables :obj:`z`, computes the binary cross\n        entropy loss for positive edges :obj:`pos_edge_index` and negative\n        sampled edges.\n\n        Args:\n            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n            pos_edge_index (LongTensor): The positive edges to train against.\n            neg_edge_index (LongTensor): The negative edges to train against. \n            If not given uses negative sampling to calculate negative edges.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n        entropy loss for positive edges :obj:`pos_edge_index` and negative\n        sampled edges.\n\n        Args:\n            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n            pos_edge_index (LongTensor): The positive edges to train against.\n            neg_edge_index (LongTensor, optional): The negative edges to train\n                against. If not given, uses negative sampling to calculate\n                negative edges. (default: :obj:`None`)", "file": "torch_geometric_nn_models_autoencoder.py", "function": "GAE.recon_loss", "project": "pytorch_geometric", "change_stats": {"code_changes": 10, "doc_changes": 7, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef match_baseline_cfg(cfg_dict, cfg_dict_baseline, verbose=True):\n    '''\n    \n\n\n    '''\n    from yacs.config import CfgNode as CN\n    stats_baseline = dict_to_stats(cfg_dict_baseline)\n    set_cfg(cfg)\n    cfg_new = CN(cfg_dict)\n    cfg.merge_from_other_cfg(cfg_new)\n    stats = match_computation(stats_baseline, key=['gnn', 'dim_inner'])\n    if 'gnn' in cfg_dict:\n        cfg_dict['gnn']['dim_inner'] = cfg.gnn.dim_inner\n    else:\n        cfg_dict['gnn'] = {'dim_inner', cfg.gnn.dim_inner}\n    set_cfg(cfg)\n    if verbose:\n        print(f\"Computational budget has matched - Baseline params: \"\n              f\"{stats_baseline}, Current params: {stats}\")\n    return cfg_dict```\n\nUpdated code:\n```python\ndef match_baseline_cfg(cfg_dict, cfg_dict_baseline, verbose=True):\n    \n    \n    from yacs.config import CfgNode as CN\n    stats_baseline = dict_to_stats(cfg_dict_baseline)\n    set_cfg(cfg)\n    cfg_new = CN(cfg_dict)\n    cfg.merge_from_other_cfg(cfg_new)\n    stats = match_computation(stats_baseline, key=['gnn', 'dim_inner'])\n    if 'gnn' in cfg_dict:\n        cfg_dict['gnn']['dim_inner'] = cfg.gnn.dim_inner\n    else:\n        cfg_dict['gnn'] = {'dim_inner', cfg.gnn.dim_inner}\n    set_cfg(cfg)\n    if verbose:\n        print(f\"Computational budget has matched - Baseline params: \"\n              f\"{stats_baseline}, Current params: {stats}\")\n    return cfg_dict\n```\n\nOriginal documentation:\n```python\nMatch the computational budget of a given baseline model. THe current\n    configuration dictionary will be modifed and returned.\n\n    Args:\n        cfg_dict (dict): Current experiment's configuration\n        cfg_dict_baseline (dict): Baseline configuration\n        verbose (str, optional): If printing matched paramter conunts\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Match the computational budget of a given baseline model. The current\n    configuration dictionary will be modifed and returned.\n\n    Args:\n        cfg_dict (dict): Current experiment's configuration\n        cfg_dict_baseline (dict): Baseline configuration\n        verbose (str, optional): If printing matched paramter conunts", "file": "torch_geometric_graphgym_utils_comp_budget.py", "function": "match_baseline_cfg", "project": "pytorch_geometric", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _best_policy_prediction(\n        self,\n        tracker: DialogueStateTracker,\n        domain: Domain,\n        interpreter: NaturalLanguageInterpreter,\n    ) -> Tuple[PolicyPrediction, Optional[Text]]:\n        \n        \n        # find rejected action before running the policies\n        # because some of them might add events\n        rejected_action_name = None\n        last_action_event = next(\n            (\n                event\n                for event in reversed(tracker.events)\n                if isinstance(event, (ActionExecutionRejected, ActionExecuted))\n            ),\n            None,\n        )\n\n        if len(tracker.events) > 0 and isinstance(\n            last_action_event, ActionExecutionRejected\n        ):\n            rejected_action_name = last_action_event.action_name\n\n        predictions = {\n            f\"policy_{i}_{type(p).__name__}\": self._get_prediction(\n                p, tracker, domain, interpreter\n            )\n            for i, p in enumerate(self.policies)\n        }\n\n        if rejected_action_name:\n            logger.debug(\n                f\"Execution of '{rejected_action_name}' was rejected. \"\n                f\"Setting its confidence to 0.0 in all predictions.\"\n            )\n            for prediction in predictions.values():\n                prediction.probabilities[\n                    domain.index_for_action(rejected_action_name)\n                ] = 0.0\n\n        return self._pick_best_policy(predictions, tracker, domain)```\n\nUpdated code:\n```python\ndef _best_policy_prediction(\n        self,\n        tracker: DialogueStateTracker,\n        domain: Domain,\n        interpreter: NaturalLanguageInterpreter,\n    ) -> PolicyPrediction:\n        \n        \n        # find rejected action before running the policies\n        # because some of them might add events\n        rejected_action_name = None\n        last_action_event = next(\n            (\n                event\n                for event in reversed(tracker.events)\n                if isinstance(event, (ActionExecutionRejected, ActionExecuted))\n            ),\n            None,\n        )\n\n        if len(tracker.events) > 0 and isinstance(\n            last_action_event, ActionExecutionRejected\n        ):\n            rejected_action_name = last_action_event.action_name\n\n        predictions = {\n            f\"policy_{i}_{type(p).__name__}\": self._get_prediction(\n                p, tracker, domain, interpreter\n            )\n            for i, p in enumerate(self.policies)\n        }\n\n        if rejected_action_name:\n            logger.debug(\n                f\"Execution of '{rejected_action_name}' was rejected. \"\n                f\"Setting its confidence to 0.0 in all predictions.\"\n            )\n            for prediction in predictions.values():\n                prediction.probabilities[\n                    domain.index_for_action(rejected_action_name)\n                ] = 0.0\n\n        return self._pick_best_policy(predictions)\n```\n\nOriginal documentation:\n```python\nFinds the best policy prediction.\n\n        Args:\n            tracker: the :class:`rasa.core.trackers.DialogueStateTracker`\n            domain: the :class:`rasa.shared.core.domain.Domain`\n            interpreter: Interpreter which may be used by the policies to create\n                additional features.\n\n        Returns:\n            probabilities: the list of probabilities for the next actions\n            policy_name: the name of the picked policy\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Finds the best policy prediction.\n\n        Args:\n            tracker: the :class:`rasa.core.trackers.DialogueStateTracker`\n            domain: the :class:`rasa.shared.core.domain.Domain`\n            interpreter: Interpreter which may be used by the policies to create\n                additional features.\n\n        Returns:\n            The winning policy prediction.", "file": "rasa_core_policies_ensemble.py", "function": "SimplePolicyEnsemble._best_policy_prediction", "project": "rasa", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _local_mean_weights(old_size, new_size, reflect=False):\n    \n\n    \n    if reflect:\n        old, new = old_size - 1, new_size - 1\n        old_breaks = np.pad(np.linspace(0.5, old-0.5, old),\n                            1, 'constant', constant_values=(0, old))\n        val = 0.5 * old / new\n        new_breaks = np.pad(np.linspace(val, old-val, new),\n                            1, 'constant', constant_values=(0, old))\n    else:\n        old_breaks = np.linspace(0, old_size, num=old_size + 1)\n        new_breaks = np.linspace(0, old_size, num=new_size + 1)\n\n    upper = np.minimum(new_breaks[1:, np.newaxis], old_breaks[np.newaxis, 1:])\n    lower = np.maximum(new_breaks[:-1, np.newaxis],\n                       old_breaks[np.newaxis, :-1])\n\n    weights = np.maximum(upper - lower, 0)\n    weights /= weights.sum(axis=1, keepdims=True)\n\n    return weights```\n\nUpdated code:\n```python\ndef _local_mean_weights(old_size, new_size, grid_mode, dtype):\n    \n\n    \n    if grid_mode:\n        old_breaks = np.linspace(0, old_size, num=old_size + 1, dtype=dtype)\n        new_breaks = np.linspace(0, old_size, num=new_size + 1, dtype=dtype)\n    else:\n        old, new = old_size - 1, new_size - 1\n        old_breaks = np.pad(np.linspace(0.5, old-0.5, old, dtype=dtype),\n                            1, 'constant', constant_values=(0, old))\n        val = 0.5 * old / new\n        new_breaks = np.pad(np.linspace(val, old-val, new, dtype=dtype),\n                            1, 'constant', constant_values=(0, old))\n\n    upper = np.minimum(new_breaks[1:, np.newaxis], old_breaks[np.newaxis, 1:])\n    lower = np.maximum(new_breaks[:-1, np.newaxis],\n                       old_breaks[np.newaxis, :-1])\n\n    weights = np.maximum(upper - lower, 0)\n    weights /= weights.sum(axis=1, keepdims=True)\n\n    return weights\n```\n\nOriginal documentation:\n```python\nCreate a weight matrix for resizing with the local mean along an\n    axis.\n\n    Parameters\n    ----------\n    old_size: int\n        Old size.\n    new_size: int\n        New size.\n    reflect: bool\n        Whether to use reflecting boundary conditions or not.\n\n    Returns\n    -------\n    NumPy array with shape (new_size, old_size). Rows sum to 1.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a 2D weight matrix for resizing with the local mean.\n\n    Parameters\n    ----------\n    old_size: int\n        Old size.\n    new_size: int\n        New size.\n    grid_mode : bool\n        Whether to use grid data model of pixel/voxel model for\n        average weights computation.\n    dtype: dtype\n        Output array data type.\n\n    Returns\n    -------\n    weights: (new_size, old_size) array\n        Rows sum to 1.", "file": "skimage_transform__warps.py", "function": "_local_mean_weights", "project": "scikit-image", "change_stats": {"code_changes": 16, "doc_changes": 15, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        name: Text,\n        mappings: List[Dict[Text, Any]],\n        initial_value: Any = None,\n        value_reset_delay: Optional[int] = None,\n        auto_fill: bool = True,\n        influence_conversation: bool = True,\n    ) -> None:\n        \n        \n        self.name = name\n        self.mappings = mappings\n        self._value = initial_value\n        self.initial_value = initial_value\n        self._value_reset_delay = value_reset_delay\n        if auto_fill:\n            rasa.shared.utils.io.raise_deprecation_warning(\n                \"Slot auto-fill has been deactivated in 3.0 and replaced by a\"\n                \" new explicit mechanism to specify slot mappings. \"\n                \"Please refer to the docs to learn more.\",\n                \"4.0.0\",\n                docs=DOCS_URL_SLOTS,\n            )\n            auto_fill = False\n        self.auto_fill = auto_fill\n        self.influence_conversation = influence_conversation\n        self._has_been_set = False```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        name: Text,\n        mappings: List[Dict[Text, Any]],\n        initial_value: Any = None,\n        value_reset_delay: Optional[int] = None,\n        influence_conversation: bool = True,\n    ) -> None:\n        \n        \n        self.name = name\n        self.mappings = mappings\n        self._value = initial_value\n        self.initial_value = initial_value\n        self._value_reset_delay = value_reset_delay\n        self.influence_conversation = influence_conversation\n        self._has_been_set = False\n```\n\nOriginal documentation:\n```python\nCreate a Slot.\n\n        Args:\n            name: The name of the slot.\n            initial_value: The initial value of the slot.\n            mappings: List containing slot mappings.\n            value_reset_delay: After how many turns the slot should be reset to the\n                initial_value. This is behavior is currently not implemented.\n            auto_fill: `True` if the slot should be filled automatically by entities\n                with the same name.\n            influence_conversation: If `True` the slot will be featurized and hence\n                influence the predictions of the dialogue polices.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a Slot.\n\n        Args:\n            name: The name of the slot.\n            initial_value: The initial value of the slot.\n            mappings: List containing slot mappings.\n            value_reset_delay: After how many turns the slot should be reset to the\n                initial_value. This is behavior is currently not implemented.\n            influence_conversation: If `True` the slot will be featurized and hence\n                influence the predictions of the dialogue polices.", "file": "rasa_shared_core_slots.py", "function": "Slot.__init__", "project": "rasa", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef process_handler(cmd, callback, stderr=subprocess.PIPE):\n    \n    \n    sys.stdout.flush()\n    sys.stderr.flush()\n    # On win32, close_fds can't be true when using pipes for stdin/out/err\n    close_fds = sys.platform != 'win32'\n    p = subprocess.Popen(cmd, shell=True,\n                         stdin=subprocess.PIPE,\n                         stdout=subprocess.PIPE,\n                         stderr=stderr,\n                         close_fds=close_fds)\n\n    try:\n        out = callback(p)\n    except KeyboardInterrupt:\n        print('^C')\n        sys.stdout.flush()\n        sys.stderr.flush()\n        out = None\n    finally:\n        # Make really sure that we don't leave processes behind, in case the\n        # call above raises an exception\n        # We start by assuming the subprocess finished (to avoid NameErrors\n        # later depending on the path taken)\n        if p.returncode is None:\n            try:\n                p.terminate()\n                p.poll()\n            except OSError:\n                pass\n        # One last try on our way out\n        if p.returncode is None:\n            try:\n                p.kill()\n            except OSError:\n                pass\n\n    return out```\n\nUpdated code:\n```python\ndef process_handler(cmd, callback, stderr=subprocess.PIPE):\n    \n    \n    sys.stdout.flush()\n    sys.stderr.flush()\n    # On win32, close_fds can't be true when using pipes for stdin/out/err\n    close_fds = sys.platform != 'win32'\n    p = subprocess.Popen(cmd, shell=isinstance(cmd, py3compat.string_types),\n                         stdin=subprocess.PIPE,\n                         stdout=subprocess.PIPE,\n                         stderr=stderr,\n                         close_fds=close_fds)\n\n    try:\n        out = callback(p)\n    except KeyboardInterrupt:\n        print('^C')\n        sys.stdout.flush()\n        sys.stderr.flush()\n        out = None\n    finally:\n        # Make really sure that we don't leave processes behind, in case the\n        # call above raises an exception\n        # We start by assuming the subprocess finished (to avoid NameErrors\n        # later depending on the path taken)\n        if p.returncode is None:\n            try:\n                p.terminate()\n                p.poll()\n            except OSError:\n                pass\n        # One last try on our way out\n        if p.returncode is None:\n            try:\n                p.kill()\n            except OSError:\n                pass\n\n    return out\n```\n\nOriginal documentation:\n```python\nOpen a command in a shell subprocess and execute a callback.\n\n    This function provides common scaffolding for creating subprocess.Popen()\n    calls.  It creates a Popen object and then calls the callback with it.\n\n    Parameters\n    ----------\n    cmd : str\n      A string to be executed with the underlying system shell (by calling\n    :func:`Popen` with ``shell=True``.\n\n    callback : callable\n      A one-argument function that will be called with the Popen object.\n\n    stderr : file descriptor number, optional\n      By default this is set to ``subprocess.PIPE``, but you can also pass the\n      value ``subprocess.STDOUT`` to force the subprocess' stderr to go into\n      the same file descriptor as its stdout.  This is useful to read stdout\n      and stderr combined in the order they are generated.\n\n    Returns\n    -------\n    The return value of the provided callback is returned.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Open a command in a shell subprocess and execute a callback.\n\n    This function provides common scaffolding for creating subprocess.Popen()\n    calls.  It creates a Popen object and then calls the callback with it.\n\n    Parameters\n    ----------\n    cmd : str or list\n      A command to be executed by the system, using :class:`subprocess.Popen`.\n      If a string is passed, it will be run in the system shell. If a list is\n      passed, it will be used directly as arguments.\n\n    callback : callable\n      A one-argument function that will be called with the Popen object.\n\n    stderr : file descriptor number, optional\n      By default this is set to ``subprocess.PIPE``, but you can also pass the\n      value ``subprocess.STDOUT`` to force the subprocess' stderr to go into\n      the same file descriptor as its stdout.  This is useful to read stdout\n      and stderr combined in the order they are generated.\n\n    Returns\n    -------\n    The return value of the provided callback is returned.", "file": "IPython_utils__process_common.py", "function": "process_handler", "project": "ipython", "change_stats": {"code_changes": 4, "doc_changes": 9, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _resolve_neighborhood(footprint, connectivity, ndim):\n    \n    \n    if footprint is None:\n        if connectivity is None:\n            connectivity = ndim\n        footprint = ndi.generate_binary_structure(ndim, connectivity)\n    else:\n        # Validate custom structured element\n        footprint = np.asarray(footprint, dtype=bool)\n        # Must specify neighbors for all dimensions\n        if footprint.ndim != ndim:\n            raise ValueError(\n                \"number of dimensions in image and footprint do not\"\n                \"match\"\n            )\n        # Must only specify direct neighbors\n        if any(s != 3 for s in footprint.shape):\n            raise ValueError(\"dimension size in footprint is not 3\")\n\n    return footprint```\n\nUpdated code:\n```python\ndef _resolve_neighborhood(footprint, connectivity, ndim,\n                          enforce_adjacency=True):\n    \n    \n    if footprint is None:\n        if connectivity is None:\n            connectivity = ndim\n        footprint = ndi.generate_binary_structure(ndim, connectivity)\n    else:\n        # Validate custom structured element\n        footprint = np.asarray(footprint, dtype=bool)\n        # Must specify neighbors for all dimensions\n        if footprint.ndim != ndim:\n            raise ValueError(\n                \"number of dimensions in image and footprint do not\"\n                \"match\"\n            )\n        # Must only specify direct neighbors\n        if enforce_adjacency and any(s != 3 for s in footprint.shape):\n            raise ValueError(\"dimension size in footprint is not 3\")\n        elif any((s % 2 != 1) for s in footprint.shape):\n            raise ValueError(\"footprint size must be odd along all dimensions\")\n\n    return footprint\n```\n\nOriginal documentation:\n```python\nValidate or create a footprint (structuring element).\n\n    Depending on the values of `connectivity` and `footprint` this function\n    either creates a new footprint (`footprint` is None) using `connectivity`\n    or validates the given footprint (`footprint` is not None).\n\n    Parameters\n    ----------\n    footprint : ndarray\n        The footprint (structuring) element used to determine the neighborhood\n        of each evaluated pixel (``True`` denotes a connected pixel). It must\n        be a boolean array and have the same number of dimensions as `image`.\n        If neither `footprint` nor `connectivity` are given, all adjacent\n        pixels are considered as part of the neighborhood.\n    connectivity : int\n        A number used to determine the neighborhood of each evaluated pixel.\n        Adjacent pixels whose squared distance from the center is less than or\n        equal to `connectivity` are considered neighbors. Ignored if\n        `footprint` is not None.\n    ndim : int\n        Number of dimensions `footprint` ought to have.\n\n    Returns\n    -------\n    footprint : ndarray\n        Validated or new footprint specifying the neighborhood.\n\n    Examples\n    --------\n    >>> _resolve_neighborhood(None, 1, 2)\n    array([[False,  True, False],\n           [ True,  True,  True],\n           [False,  True, False]])\n    >>> _resolve_neighborhood(None, None, 3).shape\n    (3, 3, 3)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Validate or create a footprint (structuring element).\n\n    Depending on the values of `connectivity` and `footprint` this function\n    either creates a new footprint (`footprint` is None) using `connectivity`\n    or validates the given footprint (`footprint` is not None).\n\n    Parameters\n    ----------\n    footprint : ndarray\n        The footprint (structuring) element used to determine the neighborhood\n        of each evaluated pixel (``True`` denotes a connected pixel). It must\n        be a boolean array and have the same number of dimensions as `image`.\n        If neither `footprint` nor `connectivity` are given, all adjacent\n        pixels are considered as part of the neighborhood.\n    connectivity : int\n        A number used to determine the neighborhood of each evaluated pixel.\n        Adjacent pixels whose squared distance from the center is less than or\n        equal to `connectivity` are considered neighbors. Ignored if\n        `footprint` is not None.\n    ndim : int\n        Number of dimensions `footprint` ought to have.\n    enforce_adjacency : bool\n        A boolean that determines whether footprint must only specify direct\n        neighbors.\n\n    Returns\n    -------\n    footprint : ndarray\n        Validated or new footprint specifying the neighborhood.\n\n    Examples\n    --------\n    >>> _resolve_neighborhood(None, 1, 2)\n    array([[False,  True, False],\n           [ True,  True,  True],\n           [False,  True, False]])\n    >>> _resolve_neighborhood(None, None, 3).shape\n    (3, 3, 3)", "file": "skimage_morphology__util.py", "function": "_resolve_neighborhood", "project": "scikit-image", "change_stats": {"code_changes": 9, "doc_changes": 5, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_num_bytes(*args):\n    \n    \n    \n    return sum(\n        arg.numel() * arg.element_size()\n        for arg in args\n        if isinstance(arg, torch.Tensor)\n    )```\n\nUpdated code:\n```python\ndef get_num_bytes(*args, num_in_out_args=0):\n    \n    \n    \n    return sum(\n        arg.numel() * arg.element_size() * (1 + int(i < num_in_out_args))\n        for i, arg in enumerate(args)\n        if isinstance(arg, torch.Tensor)\n    )\n```\n\nOriginal documentation:\n```python\nReturn the total number of bytes the arguments of tensor type takes.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the total number of bytes the arguments of tensor type takes.\n\n    For in/out args, tensor sizes are counted twice: once for reading and\n    once for writing.\n\n    The first num_in_out_args arguments are in out tensors.", "file": "torch__inductor_utils.py", "function": "get_num_bytes", "project": "pytorch", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _load_align(self,\n                    aligner: Optional[str],\n                    configfile: Optional[str],\n                    normalize_method: Optional[str],\n                    re_feed: int) -> Optional[\"Aligner\"]:\n          \n        if aligner is None or aligner.lower() == \"none\":\n            logger.debug(\"No aligner selected. Returning None\")\n            return None\n        aligner_name = aligner.replace(\"-\", \"_\").lower()\n        logger.debug(\"Loading Aligner: '%s'\", aligner_name)\n        plugin = PluginLoader.get_aligner(aligner_name)(exclude_gpus=self._exclude_gpus,\n                                                        configfile=configfile,\n                                                        normalize_method=normalize_method,\n                                                        re_feed=re_feed,\n                                                        instance=self._instance)\n        return plugin```\n\nUpdated code:\n```python\ndef _load_align(self,\n                    aligner: Optional[str],\n                    configfile: Optional[str],\n                    normalize_method: Optional[Literal[\"none\", \"clahe\", \"hist\", \"mean\"]],\n                    re_feed: int) -> Optional[\"Aligner\"]:\n         \n        \n        if aligner is None or aligner.lower() == \"none\":\n            logger.debug(\"No aligner selected. Returning None\")\n            return None\n        aligner_name = aligner.replace(\"-\", \"_\").lower()\n        logger.debug(\"Loading Aligner: '%s'\", aligner_name)\n        plugin = PluginLoader.get_aligner(aligner_name)(exclude_gpus=self._exclude_gpus,\n                                                        configfile=configfile,\n                                                        normalize_method=normalize_method,\n                                                        re_feed=re_feed,\n                                                        instance=self._instance)\n        return plugin\n```\n\nOriginal documentation:\n```python\nSet global arguments and load aligner plugin\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set global arguments and load aligner plugin\n\n        Parameters\n        ----------\n        aligner: str\n            The aligner plugin to load or ``None`` for no aligner\n        configfile: str\n            Optional full path to custom config file\n        normalize_method: str\n            Optional normalization method to use\n        re_feed: int\n            The number of times to adjust the image and re-feed to get an average score\n\n        Returns\n        -------\n        Aligner plugin if one is specified otherwise ``None``", "file": "plugins_extract_pipeline.py", "function": "Extractor._load_align", "project": "faceswap", "change_stats": {"code_changes": 7, "doc_changes": 17, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef write_data(self, chunksize):\n          \n\n        # create the masks & values\n        masks = []\n        for a in self.values_axes:\n\n            # figure the mask: only do if we can successfully process this\n            # column, otherwise ignore the mask\n            mask = com.isnull(a.data).all(axis=0)\n            masks.append(mask.astype('u1'))\n\n        # consolidate masks\n        mask = masks[0]\n        for m in masks[1:]:\n            mask = mask & m\n\n        # the arguments\n        indexes = [a.cvalues for a in self.index_axes]\n        values = [a.take_data() for a in self.values_axes]\n\n        # transpose the values so first dimension is last\n        values = [ v.transpose(np.roll(np.arange(v.ndim),v.ndim-1)) for v in values ]\n\n        # write the chunks\n        if chunksize is None:\n            chunksize = 100000\n\n        rows = self.nrows_expected\n        chunks = int(rows / chunksize) + 1\n        for i in range(chunks):\n            start_i = i * chunksize\n            end_i = min((i + 1) * chunksize, rows)\n            if start_i >= end_i:\n                break\n\n            self.write_data_chunk(\n                indexes=[a[start_i:end_i] for a in indexes],\n                mask=mask[start_i:end_i],\n                values=[v[start_i:end_i] for v in values])```\n\nUpdated code:\n```python\ndef write_data(self, chunksize):\n          \n\n        names = self.dtype.names\n        nrows = self.nrows_expected\n\n        # create the masks & values\n        masks = []\n        for a in self.values_axes:\n\n            # figure the mask: only do if we can successfully process this\n            # column, otherwise ignore the mask\n            mask = com.isnull(a.data).all(axis=0)\n            masks.append(mask.astype('u1'))\n\n        # consolidate masks\n        mask = masks[0]\n        for m in masks[1:]:\n            mask = mask & m\n        mask = mask.ravel()\n\n        # broadcast the indexes if needed\n        indexes = [ a.cvalues for a in self.index_axes ]\n        nindexes = len(indexes)\n        bindexes = []\n        for i, idx in enumerate(indexes):\n\n            # broadcast to all other indexes except myself\n            if i > 0 and i < nindexes:\n                repeater = np.prod([indexes[bi].shape[0] for bi in range(0,i)])\n                idx = np.tile(idx,repeater)\n\n            if i < nindexes-1:\n                repeater = np.prod([indexes[bi].shape[0] for bi in range(i+1,nindexes)])\n                idx = np.repeat(idx,repeater)\n\n            bindexes.append(idx)\n\n        # transpose the values so first dimension is last\n        # reshape the values if needed\n        values = [ a.take_data() for a in self.values_axes]\n        values = [ v.transpose(np.roll(np.arange(v.ndim),v.ndim-1)) for v in values ]\n        bvalues = []\n        for i, v in enumerate(values):\n            new_shape = (nrows,) + self.dtype[names[nindexes + i]].shape\n            bvalues.append(values[i].ravel().reshape(new_shape))\n\n        # write the chunks\n        if chunksize is None:\n            chunksize = 100000\n\n        chunks = int(nrows / chunksize) + 1\n        for i in range(chunks):\n            start_i = i * chunksize\n            end_i = min((i + 1) * chunksize, nrows)\n            if start_i >= end_i:\n                break\n\n            self.write_data_chunk(\n                indexes=[a[start_i:end_i] for a in bindexes],\n                mask=mask[start_i:end_i],\n                values=[v[start_i:end_i] for v in bvalues])\n```\n\nOriginal documentation:\n```python\nfast writing of data: requires specific cython routines each axis shape\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "we form the data into a 2-d including indexes,values,mask\n            write chunk-by-chunk", "file": "pandas_io_pytables.py", "function": "AppendableTable.write_data", "project": "pandas", "change_stats": {"code_changes": 40, "doc_changes": 5, "total_changes": 45}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _slice(self, slicer) -> ExtensionArray:\n        \n        \n        \n        # return same dims as we currently have\n        if not isinstance(slicer, tuple) and self.ndim == 2:\n            # reached via getitem_block via _slice_take_blocks_ax0\n            # TODO(EA2D): won't be necessary with 2D EAs\n            slicer = (slicer, slice(None))\n\n        if isinstance(slicer, tuple) and len(slicer) == 2:\n            first = slicer[0]\n            if not isinstance(first, slice):\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", first\n                )\n            # GH#32959 only full-slicers along fake-dim0 are valid\n            # TODO(EA2D): won't be necessary with 2D EAs\n            # range(1) instead of self._mgr_locs to avoid exception on [::-1]\n            #  see test_iloc_getitem_slice_negative_step_ea_block\n            new_locs = range(1)[first]\n            if len(new_locs):\n                # effectively slice(None)\n                slicer = slicer[1]\n            else:\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", slicer\n                )\n\n        return self.values[slicer]```\n\nUpdated code:\n```python\ndef _slice(\n        self, slicer: slice | npt.NDArray[np.bool_] | npt.NDArray[np.intp]\n    ) -> ExtensionArray:\n        \n        \n        \n        # Notes: ndarray[bool] is only reachable when via getitem_mgr, which\n        #  is only for Series, i.e. self.ndim == 1.\n\n        # return same dims as we currently have\n        if self.ndim == 2:\n            # reached via getitem_block via _slice_take_blocks_ax0\n            # TODO(EA2D): won't be necessary with 2D EAs\n\n            if not isinstance(slicer, slice):\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", slicer\n                )\n            # GH#32959 only full-slicers along fake-dim0 are valid\n            # TODO(EA2D): won't be necessary with 2D EAs\n            # range(1) instead of self._mgr_locs to avoid exception on [::-1]\n            #  see test_iloc_getitem_slice_negative_step_ea_block\n            new_locs = range(1)[slicer]\n            if not len(new_locs):\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", slicer\n                )\n            slicer = slice(None)\n\n        return self.values[slicer]\n```\n\nOriginal documentation:\n```python\nReturn a slice of my values.\n\n        Parameters\n        ----------\n        slicer : slice, ndarray[int], or a tuple of these\n            Valid (non-reducing) indexer for self.values.\n\n        Returns\n        -------\n        ExtensionArray\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a slice of my values.\n\n        Parameters\n        ----------\n        slicer : slice, ndarray[int], or ndarray[bool]\n            Valid (non-reducing) indexer for self.values.\n\n        Returns\n        -------\n        ExtensionArray", "file": "pandas_core_internals_blocks.py", "function": "ExtensionBlock._slice", "project": "pandas", "change_stats": {"code_changes": 26, "doc_changes": 4, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _fast_pad(image, value):\n    \n    \n    # Allocate padded image\n    new_shape = np.array(image.shape) + 2\n    new_image = np.empty(new_shape, dtype=image.dtype, order=\"C\")\n\n    # Copy old image into new space\n    original_slice = tuple(slice(1, -1) for _ in range(image.ndim))\n    new_image[original_slice] = image\n    # and set the edge values\n    _set_edge_values_inplace(new_image, value)\n\n    return new_image```\n\nUpdated code:\n```python\ndef _fast_pad(image, value, *, order=\"C\"):\n    \n    \n    # Allocate padded image\n    new_shape = np.array(image.shape) + 2\n    new_image = np.empty(new_shape, dtype=image.dtype, order=order)\n\n    # Copy old image into new space\n    original_slice = tuple(slice(1, -1) for _ in range(image.ndim))\n    new_image[original_slice] = image\n    # and set the edge values\n    _set_edge_values_inplace(new_image, value)\n\n    return new_image\n```\n\nOriginal documentation:\n```python\nPad an array on all axes with one constant value.\n\n    Parameters\n    ----------\n    image : ndarray\n        Image to pad.\n    value : scalar\n         The value to use. Should be compatible with `image`'s dtype.\n\n    Returns\n    -------\n    padded_image : ndarray\n        The new image.\n\n    Notes\n    -----\n    The output of this function is equivalent to::\n\n        np.pad(image, mode=\"constant\", constant_values=value)\n\n    However, this method needs to only allocate and copy once which can result\n    in significant speed gains if `image` is large.\n\n    Examples\n    --------\n    >>> _fast_pad(np.zeros((2, 3), dtype=int), 4)\n    array([[4, 4, 4, 4, 4],\n           [4, 0, 0, 0, 4],\n           [4, 0, 0, 0, 4],\n           [4, 4, 4, 4, 4]])\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Pad an array on all axes with one constant value.\n\n    Parameters\n    ----------\n    image : ndarray\n        Image to pad.\n    value : scalar\n         The value to use. Should be compatible with `image`'s dtype.\n    order : \"C\" or \"F\"\n        Specify the memory layout of the padded image (C or Fortran style).\n\n    Returns\n    -------\n    padded_image : ndarray\n        The new image.\n\n    Notes\n    -----\n    The output of this function is equivalent to::\n\n        np.pad(image, mode=\"constant\", constant_values=value)\n\n    However, this method needs to only allocate and copy once which can result\n    in significant speed gains if `image` is large.\n\n    Examples\n    --------\n    >>> _fast_pad(np.zeros((2, 3), dtype=int), 4)\n    array([[4, 4, 4, 4, 4],\n           [4, 0, 0, 0, 4],\n           [4, 0, 0, 0, 4],\n           [4, 4, 4, 4, 4]])", "file": "skimage_morphology_extrema.py", "function": "_fast_pad", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _parse_source_state(self, state):\n        \n        self._state = state.state\n        self._source = state.entity_id\n        self._latitude = state.attributes.get(ATTR_LATITUDE)\n        self._longitude = state.attributes.get(ATTR_LONGITUDE)```\n\nUpdated code:\n```python\ndef _parse_source_state(self, state):\n        \n        \n        self._state = state.state\n        self._source = state.entity_id\n        self._latitude = state.attributes.get(ATTR_LATITUDE)\n        self._longitude = state.attributes.get(ATTR_LONGITUDE)\n```\n\nOriginal documentation:\n```python\nParse source state and set person attributes.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parse source state and set person attributes.\n\n        This is a device tracker state or the restored person state.", "file": "homeassistant_components_person___init__.py", "function": "Person._parse_source_state", "project": "core", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef push_data(self, data, raw_data):\n        \n        _LOGGER.debug(\"PUSH >> %s: %s\", self, data)\n        was_unavailable = self._async_track_unavailable()\n        is_data = self.parse_data(data, raw_data)\n        is_voltage = self.parse_voltage(data)\n        if is_data or is_voltage or was_unavailable:\n            self.async_write_ha_state()```\n\nUpdated code:\n```python\ndef push_data(self, data: dict[str, Any], raw_data: dict[Any, Any]) -> None:\n        \n        self.hass.loop.call_soon(self.async_push_data, data, raw_data)\n```\n\nOriginal documentation:\n```python\nPush from Hub.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Push from Hub running in another thread.", "file": "homeassistant_components_xiaomi_aqara___init__.py", "function": "XiaomiDevice.push_data", "project": "core", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef Gamma0_2_swr(Gamma0):\n    \n\n    \n    return (1 + npy.abs(Gamma0)) / (1 - npy.abs(Gamma0))```\n\nUpdated code:\n```python\ndef Gamma0_2_swr(Gamma0: NumberLike):\n    \n    \n\n    \n    return (1 + npy.abs(Gamma0)) / (1 - npy.abs(Gamma0))\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n    Return Standing Wave Ratio (SWR) for a given reflection coefficient.\n\n    Standing Wave Ratio value is defined by:\n\n        .. math::\n            VSWR = \\\\frac{1 + |\\\\Gamma_0|}{1 - |\\Gamma_0|}\n\n    Parameters\n    ----------\n    Gamma0 : number or array-like\n        Reflection coefficient\n\n    Returns\n    -------\n    swr : number or array-like\n        Standing Wave Ratio.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Standing Wave Ratio (SWR) for a given reflection coefficient.\n\n    Standing Wave Ratio value is defined by:\n\n    .. math::\n        VSWR = \\\\frac{1 + |\\\\Gamma_0|}{1 - |\\\\Gamma_0|}\n\n    Parameters\n    ----------\n    Gamma0 : number or array-like\n        Reflection coefficient\n\n    Returns\n    -------\n    swr : number or array-like\n        Standing Wave Ratio.", "file": "skrf_tlineFunctions.py", "function": "Gamma0_2_swr", "project": "scikit-rf", "change_stats": {"code_changes": 5, "doc_changes": 9, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef raid_ocr(raid, mode):\n    \n    \n    \n    raid = raid_name_shorten(raid)\n    key = f'{raid}_OCR_REMAIN_{mode.upper()}'\n    try:\n        button = globals()[key]\n        # Old raids use RaidCounter to compatible with old OCR model and its assets\n        # New raids use DigitCounter\n        if raid == 'ESSEX':\n            return RaidCounter(button, letter=(57, 52, 255), threshold=128)\n        elif raid == 'SURUGA':\n            return RaidCounter(button, letter=(49, 48, 49), threshold=128)\n        elif raid == 'BRISTOL':\n            return RaidCounter(button, letter=(214, 231, 219), threshold=128)\n        elif raid == 'IRIS':\n            # Font is not in model 'azur_lane', so use general ocr model\n            if server.server == 'en':\n                # Bold in EN\n                return RaidCounter(button, letter=(148, 138, 123), threshold=80, lang='cnocr')\n            if server.server == 'jp':\n                return RaidCounter(button, letter=(148, 138, 123), threshold=128, lang='cnocr')\n            else:\n                return DigitCounter(button, letter=(148, 138, 123), threshold=128, lang='cnocr')\n        elif raid == \"ALBION\":\n            return DigitCounter(button, letter=(99, 73, 57), threshold=128)\n        elif raid == 'KUYBYSHEY':\n            return DigitCounter(button, letter=(231, 239, 247), threshold=128)\n    except KeyError:\n        raise ScriptError(f'Raid entrance asset not exists: {key}')```\n\nUpdated code:\n```python\ndef raid_ocr(raid, mode):\n    \n    \n    \n    raid = raid_name_shorten(raid)\n    key = f'{raid}_OCR_REMAIN_{mode.upper()}'\n    try:\n        button = globals()[key]\n        # Old raids use RaidCounter to compatible with old OCR model and its assets\n        # New raids use DigitCounter\n        if raid == 'ESSEX':\n            return RaidCounter(button, letter=(57, 52, 255), threshold=128)\n        elif raid == 'SURUGA':\n            return RaidCounter(button, letter=(49, 48, 49), threshold=128)\n        elif raid == 'BRISTOL':\n            return RaidCounter(button, letter=(214, 231, 219), threshold=128)\n        elif raid == 'IRIS':\n            # Font is not in model 'azur_lane', so use general ocr model\n            if server.server == 'en':\n                # Bold in EN\n                return RaidCounter(button, letter=(148, 138, 123), threshold=80, lang='cnocr')\n            if server.server == 'jp':\n                return RaidCounter(button, letter=(148, 138, 123), threshold=128, lang='cnocr')\n            else:\n                return DigitCounter(button, letter=(148, 138, 123), threshold=128, lang='cnocr')\n        elif raid == \"ALBION\":\n            return DigitCounter(button, letter=(99, 73, 57), threshold=128)\n        elif raid == 'KUYBYSHEY':\n            if mode == 'ex':\n                return Digit(button, letter=(189, 203, 214), threshold=128)\n            else:\n                return DigitCounter(button, letter=(231, 239, 247), threshold=128)\n    except KeyError:\n        raise ScriptError(f'Raid entrance asset not exists: {key}')\n```\n\nOriginal documentation:\n```python\nArgs:\n        raid (str): Raid name, such as raid_20200624, raid_20210708.\n        mode (str): easy, normal, hard\n\n    Returns:\n        DigitCounter:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n        raid (str): Raid name, such as raid_20200624, raid_20210708.\n        mode (str): easy, normal, hard, ex\n\n    Returns:\n        DigitCounter:", "file": "module_raid_raid.py", "function": "raid_ocr", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _EmbedIPython(variables):\n  \n  \n  lcl = locals()\n  lcl.update(**variables)\n  IPython.embed()```\n\nUpdated code:\n```python\ndef _EmbedIPython(variables, argv=None):\n  \n  \n  argv = argv or []\n  IPython.start_ipython(argv=argv, user_ns=variables)\n```\n\nOriginal documentation:\n```python\nDrops into an IPython REPL with variables available as local variables.\n\n  Args:\n    variables: A dict of variables to make available. Keys are variable names.\n        Values are variable values.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Drops into an IPython REPL with variables available for use.\n\n  Args:\n    variables: A dict of variables to make available. Keys are variable names.\n        Values are variable values.\n    argv: The argv to use for starting ipython. Defaults to an empty list.", "file": "fire_interact.py", "function": "_EmbedIPython", "project": "python-fire", "change_stats": {"code_changes": 9, "doc_changes": 5, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_label(self, label, *, loc=None, **kwargs):\n        \n        \n        \n        _pos_xy = 'y' if self.orientation == 'vertical' else 'x'\n        _protected_kw = [_pos_xy, 'horizontalalignment', 'ha']\n        if any([k in kwargs for k in _protected_kw]):\n            if loc is not None:\n                raise TypeError(f'Specifying *loc* is disallowed when any of '\n                                f'its corresponding low level keyword '\n                                f'arguments {_protected_kw} are also supplied')\n            loc = 'center'\n        else:\n            if loc is None:\n                loc = mpl.rcParams['%saxis.labellocation' % _pos_xy]\n        if self.orientation == 'vertical':\n            cbook._check_in_list(('bottom', 'center', 'top'), loc=loc)\n        else:\n            cbook._check_in_list(('left', 'center', 'right'), loc=loc)\n        if loc in ['right', 'top']:\n            kwargs[_pos_xy] = 1.\n            kwargs['horizontalalignment'] = 'right'\n        elif loc in ['left', 'bottom']:\n            kwargs[_pos_xy] = 0.\n            kwargs['horizontalalignment'] = 'left'\n        self._label = label\n        self._labelkw = kwargs\n        self._set_label()```\n\nUpdated code:\n```python\ndef set_label(self, label, *, loc=None, **kwargs):\n        \n        _pos_xy = 'y' if self.orientation == 'vertical' else 'x'\n        _protected_kw = [_pos_xy, 'horizontalalignment', 'ha']\n        if any([k in kwargs for k in _protected_kw]):\n            if loc is not None:\n                raise TypeError(f'Specifying *loc* is disallowed when any of '\n                                f'its corresponding low level keyword '\n                                f'arguments {_protected_kw} are also supplied')\n            loc = 'center'\n        else:\n            if loc is None:\n                loc = mpl.rcParams['%saxis.labellocation' % _pos_xy]\n        if self.orientation == 'vertical':\n            cbook._check_in_list(('bottom', 'center', 'top'), loc=loc)\n        else:\n            cbook._check_in_list(('left', 'center', 'right'), loc=loc)\n        if loc in ['right', 'top']:\n            kwargs[_pos_xy] = 1.\n            kwargs['horizontalalignment'] = 'right'\n        elif loc in ['left', 'bottom']:\n            kwargs[_pos_xy] = 0.\n            kwargs['horizontalalignment'] = 'left'\n        if self.orientation == 'vertical':\n            self.ax.set_ylabel(label, **kwargs)\n        else:\n            self.ax.set_xlabel(label, **kwargs)\n        self.stale = True\n```\n\nOriginal documentation:\n```python\nAdd a label to the long axis of the colorbar.\n\n        Parameters\n        ----------\n        label : str\n            The label text.\n        loc : str, optional\n            The location of the label.\n\n            - For horizontal orientation one of {'left', 'center', 'right'}\n            - For vertical orientation one of {'bottom', 'center', 'top'}\n\n            Defaults to :rc:`xaxis.labellocation` or :rc:`yaxis.labellocation`\n            depending on the orientation.\n        **kwargs\n            Keyword arguments are passed to `~.Axes.set_xlabel` /\n            `~.Axes.set_ylabel`.\n            Supported keywords are *labelpad* and `.Text` properties.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add a label to the long axis of the colorbar.", "file": "lib_matplotlib_colorbar.py", "function": "ColorbarBase.set_label", "project": "matplotlib", "change_stats": {"code_changes": 12, "doc_changes": 19, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef attach_cluster(config_file: str,\n                   start: bool,\n                   use_screen: bool,\n                   use_tmux: bool,\n                   override_cluster_name: Optional[str],\n                   no_config_cache: bool = False,\n                   new: bool = False,\n                   port_forward: Any = None):\n    \n    \n\n    if use_tmux:\n        if new:\n            cmd = \"tmux new\"\n        else:\n            cmd = \"tmux attach || tmux new\"\n    elif use_screen:\n        if new:\n            cmd = \"screen -L\"\n        else:\n            cmd = \"screen -L -xRR\"\n    else:\n        if new:\n            raise ValueError(\n                \"--new only makes sense if passing --screen or --tmux\")\n        cmd = \"$SHELL\"\n\n    exec_cluster(\n        config_file,\n        cmd=cmd,\n        run_env=\"auto\",\n        screen=False,\n        tmux=False,\n        stop=False,\n        start=start,\n        override_cluster_name=override_cluster_name,\n        no_config_cache=no_config_cache,\n        port_forward=port_forward,\n    )```\n\nUpdated code:\n```python\ndef attach_cluster(config_file: str,\n                   start: bool,\n                   use_screen: bool,\n                   use_tmux: bool,\n                   override_cluster_name: Optional[str],\n                   no_config_cache: bool = False,\n                   new: bool = False,\n                   port_forward: Optional[Port_forward] = None) -> None:\n    \n    \n\n    if use_tmux:\n        if new:\n            cmd = \"tmux new\"\n        else:\n            cmd = \"tmux attach || tmux new\"\n    elif use_screen:\n        if new:\n            cmd = \"screen -L\"\n        else:\n            cmd = \"screen -L -xRR\"\n    else:\n        if new:\n            raise ValueError(\n                \"--new only makes sense if passing --screen or --tmux\")\n        cmd = \"$SHELL\"\n\n    exec_cluster(\n        config_file,\n        cmd=cmd,\n        run_env=\"auto\",\n        screen=False,\n        tmux=False,\n        stop=False,\n        start=start,\n        override_cluster_name=override_cluster_name,\n        no_config_cache=no_config_cache,\n        port_forward=port_forward,\n    )\n```\n\nOriginal documentation:\n```python\nAttaches to a screen for the specified cluster.\n\n    Arguments:\n        config_file: path to the cluster yaml\n        start: whether to start the cluster if it isn't up\n        use_screen: whether to use screen as multiplexer\n        use_tmux: whether to use tmux as multiplexer\n        override_cluster_name: set the name of the cluster\n        new: whether to force a new screen\n        port_forward (int or list[int]): port(s) to forward\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Attaches to a screen for the specified cluster.\n\n    Arguments:\n        config_file: path to the cluster yaml\n        start: whether to start the cluster if it isn't up\n        use_screen: whether to use screen as multiplexer\n        use_tmux: whether to use tmux as multiplexer\n        override_cluster_name: set the name of the cluster\n        new: whether to force a new screen\n        port_forward ( (int,int) or list[(int,int)] ): port(s) to forward", "file": "python_ray_autoscaler__private_commands.py", "function": "attach_cluster", "project": "ray", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_service_exception(\n    response: Response, parsed_response: Dict\n) -> Optional[ServiceException]:\n    \n    \n    \n    if response.status_code < 301 or \"Error\" not in parsed_response:\n        return None\n    error = parsed_response[\"Error\"]\n    service_exception = CommonServiceException(\n        code=error.get(\"Code\", f\"'{response.status_code}'\"),\n        status_code=response.status_code,\n        message=error.get(\"Message\", \"\"),\n        sender_fault=error.get(\"Type\") == \"Sender\",\n    )\n    # Add all additional fields in the parsed response as members of the exception\n    for key, value in parsed_response.items():\n        if key.lower() not in [\"code\", \"message\", \"type\"] and not hasattr(service_exception, key):\n            setattr(service_exception, key, value)\n    return service_exception```\n\nUpdated code:\n```python\ndef parse_service_exception(\n    response: Response, parsed_response: Dict\n) -> Optional[ServiceException]:\n    \n    \n    \n    if response.status_code < 301 or \"Error\" not in parsed_response:\n        return None\n    error = parsed_response[\"Error\"]\n    service_exception = CommonServiceException(\n        code=error.get(\"Code\", f\"'{response.status_code}'\"),\n        status_code=response.status_code,\n        message=error.get(\"Message\", \"\"),\n        sender_fault=error.get(\"Type\") == \"Sender\",\n    )\n    # Add all additional fields in the parsed response as members of the exception\n    for key, value in parsed_response.items():\n        if key.lower() not in [\"code\", \"message\", \"type\", \"error\"] and not hasattr(\n            service_exception, key\n        ):\n            setattr(service_exception, key, value)\n    return service_exception\n```\n\nOriginal documentation:\n```python\nCreates a ServiceException from a parsed response (one that botocore would return).\n    It does not automatically raise the exception (see #raise_service_exception).\n    :param response: Un-parsed response\n    :param parsed_response: Parsed response\n    :return: ServiceException or None (if it's not an error response)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates a ServiceException (one ASF can handle) from a parsed response (one that botocore would return).\n    It does not automatically raise the exception (see #raise_service_exception).\n    :param response: Un-parsed response\n    :param parsed_response: Parsed response\n    :return: ServiceException or None (if it's not an error response)", "file": "localstack_aws_client.py", "function": "parse_service_exception", "project": "localstack", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef translate(self, x, recog_args, char_list, rnnlm=None):\n        \n        \n        prev = self.training\n        self.eval()\n        ilen = [len(x[0])]\n\n        # 1. encoder\n        # make a utt list (1) to use the same interface for encoder\n        if self.replace_sos:\n            id2token = {i: x for i, x in enumerate(char_list)}\n            logging.info('src (multilingual): %s', ' '.join([id2token[int(y)] for y in x[0][1:]]))\n            h = to_device(self, torch.from_numpy(np.fromiter(map(int, x[0][1:]), dtype=np.int64)))\n            h = h.contiguous()\n            h_emb = self.dropout_emb_src(self.embed_src(h.unsqueeze(0)))\n        else:\n            h = to_device(self, torch.from_numpy(np.fromiter(map(int, x[0]), dtype=np.int64)))\n            h = h.contiguous()\n            h_emb = self.dropout_emb_src(self.embed_src(h.unsqueeze(0)))\n        hs, _, _ = self.enc(h_emb, ilen)\n\n        # 2. decoder\n        # decode the first utterance\n        y = self.dec.recognize_beam(hs[0], None, recog_args, char_list, rnnlm)\n\n        if prev:\n            self.train()\n        return y```\n\nUpdated code:\n```python\ndef translate(self, x, trans_args, char_list, rnnlm=None):\n        \n        \n        prev = self.training\n        self.eval()\n        ilen = [len(x[0])]\n\n        # 1. encoder\n        # make a utt list (1) to use the same interface for encoder\n        if self.replace_sos:\n            id2token = {i: x for i, x in enumerate(char_list)}\n            logging.info('src (multilingual): %s', ' '.join([id2token[int(y)] for y in x[0][1:]]))\n            h = to_device(self, torch.from_numpy(np.fromiter(map(int, x[0][1:]), dtype=np.int64)))\n            h = h.contiguous()\n            h_emb = self.dropout_emb_src(self.embed_src(h.unsqueeze(0)))\n        else:\n            h = to_device(self, torch.from_numpy(np.fromiter(map(int, x[0]), dtype=np.int64)))\n            h = h.contiguous()\n            h_emb = self.dropout_emb_src(self.embed_src(h.unsqueeze(0)))\n        hs, _, _ = self.enc(h_emb, ilen)\n\n        # 2. decoder\n        # decode the first utterance\n        y = self.dec.recognize_beam(hs[0], None, trans_args, char_list, rnnlm)\n\n        if prev:\n            self.train()\n        return y\n```\n\nOriginal documentation:\n```python\nE2E beam search\n\n        :param ndarray x: input source text feature (T, D)\n        :param Namespace recog_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "E2E beam search\n\n        :param ndarray x: input source text feature (T, D)\n        :param Namespace trans_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list", "file": "espnet_nets_pytorch_backend_e2e_mt.py", "function": "E2E.translate", "project": "espnet", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, name, data):\n        \n        self._name = name\n        self._data = data\n        self._state = None\n        self._description = None```\n\nUpdated code:\n```python\ndef __init__(self, name, data):\n        \n        self._data = data\n        self._description = None\n        self._name = name\n        self._state = None\n        self.attrs = {ATTR_ATTRIBUTION: ATTRIBUTION}\n```\n\nOriginal documentation:\n```python\nInitialize the sensor.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the London Underground sensor.", "file": "homeassistant_components_london_underground_sensor.py", "function": "LondonTubeSensor.__init__", "project": "core", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef from_data(\n        cls,\n        data: Union[str, bytes],\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        path: Optional[str] = None,\n    ) -> Blob:\n        \n        \n        return cls(data=data, mimetype=mime_type, encoding=encoding, path=path)```\n\nUpdated code:\n```python\ndef from_data(\n        cls,\n        data: Union[str, bytes],\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        path: Optional[str] = None,\n        metadata: Optional[dict] = None,\n    ) -> Blob:\n        \n        \n        return cls(\n            data=data,\n            mimetype=mime_type,\n            encoding=encoding,\n            path=path,\n            metadata=metadata if metadata is not None else {},\n        )\n```\n\nOriginal documentation:\n```python\nInitialize the blob from in-memory data.\n\n        Args:\n            data: the in-memory data associated with the blob\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            path: if provided, will be set as the source from which the data came\n\n        Returns:\n            Blob instance\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the blob from in-memory data.\n\n        Args:\n            data: the in-memory data associated with the blob\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            path: if provided, will be set as the source from which the data came\n            metadata: Metadata to associate with the blob\n\n        Returns:\n            Blob instance", "file": "libs_langchain_langchain_document_loaders_blob_loaders_schema.py", "function": "Blob.from_data", "project": "langchain", "change_stats": {"code_changes": 11, "doc_changes": 3, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_redis_database(self, redis_host='localhost', redis_port=6379) -> bool:\n        \n        \n        \n        try:\n            r = redis.StrictRedis(host=redis_host, port=redis_port, db=0, charset=\"utf-8\",\n                                       decode_responses=True)\n            r.ping()\n        except Exception as ex:\n            print('[DB] Error: Is redis database running? You can run it as: \"redis-server --daemonize yes\"')\n            return False\n        return True```\n\nUpdated code:\n```python\ndef check_redis_database(self, redis_host='localhost', redis_port=6379) -> bool:\n        \n        \n        \n        try:\n            r = redis.StrictRedis(host=redis_host, port=redis_port, db=0, charset=\"utf-8\",\n                                       decode_responses=True)\n            r.ping()\n        except Exception as ex:\n            print('[DB] Error: Is redis cache database running? '\n                  'You can run it as: \"redis-server --daemonize yes\"')\n            return False\n\n        return True\n```\n\nOriginal documentation:\n```python\nCheck if we have redis-server running\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if we have redis-server running (this is the cache db it should always be running)", "file": "slips.py", "function": "Main.check_redis_database", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef tokenize(self, text, never_split=None):\n        \n        \n        \n        # union() returns a new set by concatenating the two sets.\n        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if token not in never_split:\n                if self.do_lower_case:\n                    token = token.lower()\n                    if self.strip_accents is not False:\n                        token = self._run_strip_accents(token)\n                elif self.strip_accents:\n                    token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token, never_split))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens```\n\nUpdated code:\n```python\ndef tokenize(self, text, never_split=None):\n        \n        \n        \n        # union() returns a new set by concatenating the two sets.\n        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        # prevents treating the same character with different unicode codepoints as different characters\n        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n        split_tokens = []\n        for token in orig_tokens:\n            if token not in never_split:\n                if self.do_lower_case:\n                    token = token.lower()\n                    if self.strip_accents is not False:\n                        token = self._run_strip_accents(token)\n                elif self.strip_accents:\n                    token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token, never_split))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n```\n\nOriginal documentation:\n```python\nBasic Tokenization of a piece of text. Split on \"white spaces\" only, for sub-word tokenization, see\n        WordPieceTokenizer.\n\n        Args:\n            never_split (`List[str]`, *optional*)\n                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n\n        Args:\n            never_split (`List[str]`, *optional*)\n                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n                [`PreTrainedTokenizer.tokenize`]) List of token not to split.", "file": "src_transformers_models_roc_bert_tokenization_roc_bert.py", "function": "RoCBertBasicTokenizer.tokenize", "project": "transformers", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef center_crop_image(image):\n  \n  \n  with tf.name_scope('center_crop_image'):\n    image_size = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n    crop_size = (\n        CENTER_CROP_FRACTION * tf.math.minimum(image_size[0], image_size[1]))\n    crop_offset = tf.cast((image_size - crop_size) / 2.0, dtype=tf.int32)\n    crop_size = tf.cast(crop_size, dtype=tf.int32)\n    cropped_image = image[\n        crop_offset[0]:crop_offset[0] + crop_size,\n        crop_offset[1]:crop_offset[1] + crop_size, :]\n    return cropped_image```\n\nUpdated code:\n```python\ndef center_crop_image(\n    image, center_crop_fraction: float = CENTER_CROP_FRACTION):\n  \n  \n  with tf.name_scope('center_crop_image'):\n    image_size = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n    crop_size = (\n        center_crop_fraction * tf.math.minimum(image_size[0], image_size[1]))\n    crop_offset = tf.cast((image_size - crop_size) / 2.0, dtype=tf.int32)\n    crop_size = tf.cast(crop_size, dtype=tf.int32)\n    cropped_image = image[\n        crop_offset[0]:crop_offset[0] + crop_size,\n        crop_offset[1]:crop_offset[1] + crop_size, :]\n    return cropped_image\n```\n\nOriginal documentation:\n```python\nCenter crop a square shape slice from the input image.\n\n  It crops a square shape slice from the image. The side of the actual crop\n  is 224 / 256 = 0.875 of the short side of the original image. References:\n  [1] Very Deep Convolutional Networks for Large-Scale Image Recognition\n      https://arxiv.org/abs/1409.1556\n  [2] Deep Residual Learning for Image Recognition\n      https://arxiv.org/abs/1512.03385\n\n  Args:\n    image: a Tensor of shape [height, width, 3] representing the input image.\n\n  Returns:\n    cropped_image: a Tensor representing the center cropped image.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Center crop a square shape slice from the input image.\n\n  It crops a square shape slice from the image. The side of the actual crop\n  is 224 / 256 = 0.875 of the short side of the original image. References:\n  [1] Very Deep Convolutional Networks for Large-Scale Image Recognition\n      https://arxiv.org/abs/1409.1556\n  [2] Deep Residual Learning for Image Recognition\n      https://arxiv.org/abs/1512.03385\n\n  Args:\n    image: a Tensor of shape [height, width, 3] representing the input image.\n    center_crop_fraction: a float of ratio between the side of the cropped image\n      and the short side of the original image\n\n  Returns:\n    cropped_image: a Tensor representing the center cropped image.", "file": "official_vision_ops_preprocess_ops.py", "function": "center_crop_image", "project": "models", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update(self):\n        ''\n        if self.useblit:\n            if self.background is not None:\n                self.canvas.restore_region(self.background)\n            self.ax.draw_artist(self.to_draw)\n            self.canvas.blit(self.ax.bbox)\n        else:\n            self.canvas.draw_idle()\n        return False```\n\nUpdated code:\n```python\ndef update(self):\n        \n\n        \n        if self.useblit:\n            if self.background is not None:\n                self.canvas.restore_region(self.background)\n            self.ax.draw_artist(self.to_draw)\n            self.canvas.blit(self.ax.bbox)\n        else:\n            self.canvas.draw_idle()\n        return False\n```\n\nOriginal documentation:\n```python\ndraw using newfangled blit or oldfangled draw depending on useblit\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "draw using newfangled blit or oldfangled draw depending on\n        useblit", "file": "lib_matplotlib_widgets.py", "function": "RectangleSelector.update", "project": "matplotlib", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef prune_folder_files(kb_names: List[str]):\n    '''\n    \n    '''\n    for kb_name in kb_names:\n        kb = KBServiceFactory.get_service_by_name(kb_name)\n        if kb and kb.exists():\n            files_in_db = kb.list_files()\n            files_in_folder = list_files_from_folder(kb_name)\n            files = list(set(files_in_folder) - set(files_in_db))\n            for file in files:\n                os.remove(get_file_path(kb_name, file))\n                print(f\"success to delete file: {kb_name}/{file}\")```\n\nUpdated code:\n```python\ndef prune_folder_files(kb_names: List[str]):\n    \n    \n    \n    for kb_name in kb_names:\n        kb = KBServiceFactory.get_service_by_name(kb_name)\n        if kb and kb.exists():\n            files_in_db = kb.list_files()\n            files_in_folder = list_files_from_folder(kb_name)\n            files = list(set(files_in_folder) - set(files_in_db))\n            for file in files:\n                os.remove(get_file_path(kb_name, file))\n                print(f\"success to delete file: {kb_name}/{file}\")\n```\n\nOriginal documentation:\n```python\ndelete doc files in local folder that not existed in database.\n    is is used to free local disk space by delete unused doc files.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "delete doc files in local folder that not existed in database.\n    it is used to free local disk space by delete unused doc files.", "file": "server_knowledge_base_migrate.py", "function": "prune_folder_files", "project": "Langchain-Chatchat", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _IsWindowsAbsPath(path):\n  \n  \n  \n  return path.startswith('c:') or path.startswith('C:')```\n\nUpdated code:\n```python\ndef _IsWindowsAbsPath(path):\n    \n  \n  \n    return path.startswith(\"c:\") or path.startswith(\"C:\")\n```\n\nOriginal documentation:\n```python\nOn Cygwin systems Python needs a little help determining if a path is an absolute Windows path or not, so that\n  it does not treat those as relative, which results in bad paths like:\n  '..\\C:\\<some path>\\some_source_code_file.cc'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "On Cygwin systems Python needs a little help determining if a path is an absolute Windows path or not, so that\n  it does not treat those as relative, which results in bad paths like:\n  '..\\\\C:\\\\<some path>\\\\some_source_code_file.cc'", "file": "gyp_pylib_gyp_generator_msvs.py", "function": "_IsWindowsAbsPath", "project": "node-gyp", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extract_data(\n        query: SecCompanyFilingsQueryParams,  # pylint: disable=unused-argument\n        credentials: Optional[Dict[str, str]],\n        **kwargs: Any,\n    ) -> List[Dict]:\n        \n        filings = pd.DataFrame()\n\n        if query.symbol and not query.cik:\n            query.cik = symbol_map(query.symbol.lower(), use_cache=query.use_cache)\n            if not query.cik:\n                return []\n        if query.cik is None:\n            return []\n\n        # The leading 0s need to be inserted but are typically removed from the data to store as an integer.\n        if len(query.cik) != 10:\n            cik_: str = \"\"\n            temp = 10 - len(query.cik)\n            for i in range(temp):\n                cik_ = cik_ + \"0\"\n            query.cik = cik_ + query.cik\n\n        url = f\"https://data.sec.gov/submissions/CIK{query.cik}.json\"\n        r = (\n            requests.get(url, headers=HEADERS, timeout=5)\n            if query.use_cache is False\n            else sec_session_company_filings.get(url, headers=HEADERS, timeout=5)\n        )\n        if r.status_code == 200:\n            data = r.json()\n            filings = pd.DataFrame.from_records(data[\"filings\"][\"recent\"])\n            if len(filings) >= 1000:\n                new_urls = pd.DataFrame(data[\"filings\"][\"files\"])\n                for i in new_urls.index:\n                    new_cik: str = data[\"filings\"][\"files\"][i][\"name\"]\n                    new_url: str = \"https://data.sec.gov/submissions/\" + new_cik\n                    r_ = (\n                        requests.get(new_url, headers=HEADERS, timeout=5)\n                        if query.use_cache is False\n                        else sec_session_company_filings.get(\n                            new_url, headers=HEADERS, timeout=5\n                        )\n                    )\n                    if r_.status_code == 200:\n                        data_ = r_.json()\n                        additional_data = pd.DataFrame.from_records(data_)\n                        filings = pd.concat([filings, additional_data], axis=0)\n        cols = [\n            \"reportDate\",\n            \"filingDate\",\n            \"acceptanceDateTime\",\n            \"act\",\n            \"form\",\n            \"items\",\n            \"primaryDocDescription\",\n            \"primaryDocument\",\n            \"accessionNumber\",\n            \"fileNumber\",\n            \"filmNumber\",\n            \"isInlineXBRL\",\n            \"isXBRL\",\n            \"size\",\n        ]\n        filings = (\n            pd.DataFrame(filings, columns=cols)\n            .fillna(value=\"N/A\")\n            .replace(\"N/A\", None)\n            .astype(str)\n        )\n        filings = filings.sort_values(by=[\"reportDate\", \"filingDate\"], ascending=False)\n        base_url = f\"https://www.sec.gov/Archives/edgar/data/{query.cik}/\"\n        filings[\"primaryDocumentUrl\"] = (\n            base_url\n            + filings[\"accessionNumber\"].str.replace(\"-\", \"\")\n            + \"/\"\n            + filings[\"primaryDocument\"]\n        )\n        filings[\"completeSubmissionUrl\"] = (\n            base_url + filings[\"accessionNumber\"] + \".txt\"\n        )\n        filings[\"filingDetailUrl\"] = (\n            base_url + filings[\"accessionNumber\"] + \"-index.htm\"\n        )\n        if \"type\" in query.model_dump() and query.type is not None:\n            filings = filings[filings[\"form\"] == query.type]\n\n        if \"limit\" in query.model_dump():\n            filings = filings.head(query.limit) if query.limit != 0 else filings\n\n        return filings.to_dict(\"records\")```\n\nUpdated code:\n```python\ndef extract_data(\n        query: SecCompanyFilingsQueryParams,  # pylint: disable=unused-argument\n        credentials: Optional[Dict[str, str]],\n        **kwargs: Any,\n    ) -> List[Dict]:\n        \n        filings = pd.DataFrame()\n\n        if query.symbol and not query.cik:\n            query.cik = symbol_map(query.symbol.lower(), use_cache=query.use_cache)\n            if not query.cik:\n                return []\n        if query.cik is None:\n            return []\n\n        # The leading 0s need to be inserted but are typically removed from the data to store as an integer.\n        if len(query.cik) != 10:  # type: ignore\n            cik_: str = \"\"\n            temp = 10 - len(query.cik)  # type: ignore\n            for i in range(temp):\n                cik_ = cik_ + \"0\"\n            query.cik = cik_ + str(query.cik)  # type: ignore\n\n        url = f\"https://data.sec.gov/submissions/CIK{query.cik}.json\"\n        r = (\n            requests.get(url, headers=HEADERS, timeout=5)\n            if query.use_cache is False\n            else sec_session_company_filings.get(url, headers=HEADERS, timeout=5)\n        )\n        if r.status_code == 200:\n            data = r.json()\n            filings = pd.DataFrame.from_records(data[\"filings\"][\"recent\"])\n            if len(filings) >= 1000:\n                new_urls = pd.DataFrame(data[\"filings\"][\"files\"])\n                for i in new_urls.index:\n                    new_cik: str = data[\"filings\"][\"files\"][i][\"name\"]\n                    new_url: str = \"https://data.sec.gov/submissions/\" + new_cik\n                    r_ = (\n                        requests.get(new_url, headers=HEADERS, timeout=5)\n                        if query.use_cache is False\n                        else sec_session_company_filings.get(\n                            new_url, headers=HEADERS, timeout=5\n                        )\n                    )\n                    if r_.status_code == 200:\n                        data_ = r_.json()\n                        additional_data = pd.DataFrame.from_records(data_)\n                        filings = pd.concat([filings, additional_data], axis=0)\n        cols = [\n            \"reportDate\",\n            \"filingDate\",\n            \"acceptanceDateTime\",\n            \"act\",\n            \"form\",\n            \"items\",\n            \"primaryDocDescription\",\n            \"primaryDocument\",\n            \"accessionNumber\",\n            \"fileNumber\",\n            \"filmNumber\",\n            \"isInlineXBRL\",\n            \"isXBRL\",\n            \"size\",\n        ]\n        filings = (\n            pd.DataFrame(filings, columns=cols)\n            .fillna(value=\"N/A\")\n            .replace(\"N/A\", None)\n            .astype(str)\n        )\n        filings = filings.sort_values(by=[\"reportDate\", \"filingDate\"], ascending=False)\n        base_url = f\"https://www.sec.gov/Archives/edgar/data/{query.cik}/\"\n        filings[\"primaryDocumentUrl\"] = (\n            base_url\n            + filings[\"accessionNumber\"].str.replace(\"-\", \"\")\n            + \"/\"\n            + filings[\"primaryDocument\"]\n        )\n        filings[\"completeSubmissionUrl\"] = (\n            base_url + filings[\"accessionNumber\"] + \".txt\"\n        )\n        filings[\"filingDetailUrl\"] = (\n            base_url + filings[\"accessionNumber\"] + \"-index.htm\"\n        )\n        if \"type\" in query.model_dump() and query.type is not None:\n            filings = filings[filings[\"form\"] == query.type]\n\n        if \"limit\" in query.model_dump():\n            filings = filings.head(query.limit) if query.limit != 0 else filings\n\n        return filings.to_dict(\"records\")\n```\n\nOriginal documentation:\n```python\nExtracts the data from the SEC endpoint.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extract the data from the SEC endpoint.", "file": "openbb_platform_providers_sec_openbb_sec_models_company_filings.py", "function": "SecCompanyFilingsFetcher.extract_data", "project": "OpenBBTerminal", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, parent, title, text, modal=True,\n                 _htest=False, _utest=False):\n        \n        \n        Toplevel.__init__(self, parent)\n        self.configure(borderwidth=5)\n        # Place dialog below parent if running htest.\n        self.geometry(\"=%dx%d+%d+%d\" % (750, 500,\n                           parent.winfo_rootx() + 10,\n                           parent.winfo_rooty() + (10 if not _htest else 100)))\n        # TODO: get fg/bg from theme.\n        self.bg = '#ffffff'\n        self.fg = '#000000'\n\n        self.CreateWidgets()\n        self.title(title)\n        self.protocol(\"WM_DELETE_WINDOW\", self.Ok)\n        self.parent = parent\n        self.textView.focus_set()\n        # Bind keys for closing this dialog.\n        self.bind('<Return>',self.Ok)\n        self.bind('<Escape>',self.Ok)\n        self.textView.insert(0.0, text)\n        self.textView.config(state=DISABLED)\n\n        if modal:\n            self.transient(parent)\n            self.grab_set()\n            if not _utest:\n                self.wait_window()```\n\nUpdated code:\n```python\ndef __init__(self, parent, title, text, modal=True,\n                 _htest=False, _utest=False):\n        \n        \n        Toplevel.__init__(self, parent)\n        self.configure(borderwidth=5)\n        # Place dialog below parent if running htest.\n        self.geometry(\"=%dx%d+%d+%d\" % (750, 500,\n                           parent.winfo_rootx() + 10,\n                           parent.winfo_rooty() + (10 if not _htest else 100)))\n        # TODO: get fg/bg from theme.\n        self.bg = '#ffffff'\n        self.fg = '#000000'\n\n        self.create_widgets()\n        self.title(title)\n        self.protocol(\"WM_DELETE_WINDOW\", self.ok)\n        self.parent = parent\n        self.text.focus_set()\n        # Bind keys for closing this dialog.\n        self.bind('<Return>', self.ok)\n        self.bind('<Escape>', self.ok)\n        self.text.insert(0.0, text)\n        self.text.config(state=DISABLED)\n\n        if modal:\n            self.transient(parent)\n            self.grab_set()\n            if not _utest:\n                self.wait_window()\n```\n\nOriginal documentation:\n```python\nShow the given text in a scrollable window with a 'close' button.\n\n        If modal is left True, users cannot interact with other windows\n        until the textview window is closed.\n\n        _htest - bool; change box location when running htest.\n        _utest - bool; don't wait_window when running unittest.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Show the given text in a scrollable window with a 'close' button.\n\n        If modal is left True, users cannot interact with other windows\n        until the textview window is closed.\n\n        parent - parent of this dialog\n        title - string which is title of popup dialog\n        text - text to display in dialog\n        _htest - bool; change box location when running htest.\n        _utest - bool; don't wait_window when running unittest.", "file": "Lib_idlelib_textview.py", "function": "TextViewer.__init__", "project": "cpython", "change_stats": {"code_changes": 16, "doc_changes": 5, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef NamedTemporaryFile(mode='w+b', buffering=-1, encoding=None,\n                       newline=None, suffix=\"\", prefix=template,\n                       dir=None, delete=True):\n    \n    \n\n    if dir is None:\n        dir = gettempdir()\n\n    if 'b' in mode:\n        flags = _bin_openflags\n    else:\n        flags = _text_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete:\n        flags |= _os.O_TEMPORARY\n\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    file = _io.open(fd, mode, buffering=buffering,\n                    newline=newline, encoding=encoding)\n\n    return _TemporaryFileWrapper(file, name, delete)```\n\nUpdated code:\n```python\ndef NamedTemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                       prefix=template, dir=None, delete=True):\n    \n    \n\n    if dir is None:\n        dir = gettempdir()\n\n    if 'b' in mode:\n        flags = _bin_openflags\n    else:\n        flags = _text_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete:\n        flags |= _os.O_TEMPORARY\n\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    file = _os.fdopen(fd, mode, bufsize)\n    return _TemporaryFileWrapper(file, name, delete)\n```\n\nOriginal documentation:\n```python\nCreate and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to io.open (default \"w+b\").\n    'buffering' -- the buffer size argument to io.open (default -1).\n    'encoding' -- the encoding argument to io.open (default None)\n    'newline' -- the newline argument to io.open (default None)\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as file.name.  The file will be automatically deleted\n    when it is closed unless the 'delete' argument is set to False.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to os.fdopen (default \"w+b\").\n    'bufsize' -- the buffer size argument to os.fdopen (default -1).\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as file.name.  The file will be automatically deleted\n    when it is closed unless the 'delete' argument is set to False.", "file": "Lib_tempfile.py", "function": "NamedTemporaryFile", "project": "cpython", "change_stats": {"code_changes": 11, "doc_changes": 8, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_timesteps(\n        self, state: DDIMSchedulerState, num_inference_steps: int, offset: int = 0\n    ) -> DDIMSchedulerState:\n        \n        \n        \n        step_ratio = self.config.num_train_timesteps // num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # casting to int to avoid issues when num_inference_step is power of 3\n        timesteps = (jnp.arange(0, num_inference_steps) * step_ratio).round()[::-1]\n        timesteps = timesteps + offset\n\n        return state.replace(num_inference_steps=num_inference_steps, timesteps=timesteps)```\n\nUpdated code:\n```python\ndef set_timesteps(self, state: DDIMSchedulerState, num_inference_steps: int) -> DDIMSchedulerState:\n        \n        \n        \n        offset = self.config.steps_offset\n\n        step_ratio = self.config.num_train_timesteps // num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # casting to int to avoid issues when num_inference_step is power of 3\n        timesteps = (jnp.arange(0, num_inference_steps) * step_ratio).round()[::-1]\n        timesteps = timesteps + offset\n\n        return state.replace(num_inference_steps=num_inference_steps, timesteps=timesteps)\n```\n\nOriginal documentation:\n```python\nSets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`DDIMSchedulerState`):\n                the `FlaxDDIMScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n            offset (`int`):\n                optional value to shift timestep values up by. A value of 1 is used in stable diffusion for inference.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`DDIMSchedulerState`):\n                the `FlaxDDIMScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.", "file": "src_diffusers_schedulers_scheduling_ddim_flax.py", "function": "FlaxDDIMScheduler.set_timesteps", "project": "diffusers", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef all_gather_transform_layouts(self, source_layout: Layout) -> Dict[Layout, CommSpec]:\n        '''\n        \n        '''\n        valid_spec_dict = {}\n        comm_pattern = CollectiveCommPattern.GATHER_FWD_SPLIT_BWD\n        source_spec = source_layout.sharding_spec\n\n        # the key of the dict is the axis\n        # the value is the process group\n        current_rank = source_layout.device_mesh._global_rank_of_current_process\n        process_group_dict = source_layout.device_mesh._process_group_dict[current_rank]\n\n        for target_pair in source_spec.dim_partition_dict.items():\n            shard_list = all_gather_simulator(target_pair)\n            index = target_pair[0]\n            new_dim_partition_dict = deepcopy(source_spec.dim_partition_dict)\n\n            # We won't add empty list into dim_partition_dict\n            # The key will be popped if the related shard_list is empty\n            if shard_list:\n                new_dim_partition_dict[index] = shard_list\n            else:\n                new_dim_partition_dict.pop(index)\n\n            # generate the CommSpec to record the action of source_sharding_spec->new_sharding_spec\n            gather_dim = index\n            logical_process_axis = target_pair[1][-1]\n            comm_spec = CommSpec(\n                comm_pattern,\n                process_group_dict=process_group_dict,\n                gather_dim=gather_dim,\n            # shard_dim will be used during backward\n                shard_dim=gather_dim,\n                logical_process_axis=logical_process_axis)\n\n            # generate new sharding spec\n            try:\n                new_sharding_spec = ShardingSpec(source_spec.dims, dim_partition_dict=new_dim_partition_dict)\n                new_layout = Layout(device_mesh=source_layout.device_mesh,\n                                    sharding_spec=new_sharding_spec,\n                                    global_shape=source_layout.global_shape)\n\n                valid_spec_dict[new_layout] = comm_spec\n            except LayoutException:\n                pass\n        return valid_spec_dict```\n\nUpdated code:\n```python\ndef all_gather_transform_layouts(self, source_layout: Layout) -> Dict[Layout, CommSpec]:\n        '''\n        \n        '''\n        valid_spec_dict = {}\n        comm_pattern = CollectiveCommPattern.GATHER_FWD_SPLIT_BWD\n        source_spec = source_layout.sharding_spec\n        process_groups_dict = source_layout.device_mesh.process_groups_dict\n        for target_pair in source_spec.dim_partition_dict.items():\n            shard_list = all_gather_simulator(target_pair)\n            index = target_pair[0]\n            new_dim_partition_dict = deepcopy(source_spec.dim_partition_dict)\n\n            # We won't add empty list into dim_partition_dict\n            # The key will be popped if the related shard_list is empty\n            if shard_list:\n                new_dim_partition_dict[index] = shard_list\n            else:\n                new_dim_partition_dict.pop(index)\n\n            # generate the CommSpec to record the action of source_sharding_spec->new_sharding_spec\n            gather_dim = index\n            logical_process_axis = target_pair[1][-1]\n            comm_spec = CommSpec(\n                comm_pattern,\n                process_groups_dict=process_groups_dict,\n                gather_dim=gather_dim,\n            # shard_dim will be used during backward\n                shard_dim=gather_dim,\n                logical_process_axis=logical_process_axis)\n\n            # generate new sharding spec\n            try:\n                new_sharding_spec = ShardingSpec(source_spec.dims, dim_partition_dict=new_dim_partition_dict)\n                new_layout = Layout(device_mesh=source_layout.device_mesh,\n                                    sharding_spec=new_sharding_spec,\n                                    device_type=source_layout.device_type,\n                                    entire_shape=source_layout.entire_shape)\n\n                valid_spec_dict[new_layout] = comm_spec\n            except LayoutException:\n                pass\n        return valid_spec_dict\n```\n\nOriginal documentation:\n```python\nGet all valid layouts from source_layout with single all-gather operation.\n        For the all-gather operation, we just care about the S dimension.\n\n        Argument:\n            source_layout: the layout to be transformed.\n\n        Return:\n            valid_spec_dict(Dict[Layout, CommSpec]): all valid layouts from source_layout with single all-gather operation.\n\n        Example:\n            layout_converter = LayoutConverter()\n            physical_mesh_id = torch.arange(0, 4)\n            mesh_shape = (2, 2)\n            # [[0, 1,\n            #  [2, 3]]\n            device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)\n            global_shape = (4, 4, 4)\n            dim_partition_dict = {0: [0], 1: [1]}\n\n            # [S0,S1,R]\n            sharding_spec = ShardingSpec(dim_size=3, dim_partition_dict=dim_partition_dict)\n            layout = Layout(device_mesh=device_mesh,\n                            sharding_spec=sharding_spec,\n                            global_shape=global_shape)\n\n            rst_dict = layout_converter.all_gather_transform_layouts(layout)\n            for layout, comm_spec in rst_dict.items():\n                print(f'{layout.sharding_spec.sharding_sequence}: {comm_spec}')\n\n        Output:\n            [R, S1, R]: CommSpec:(comm_pattern:GATHER_FWD_SPLIT_BWD, gather_dim:0, shard_dim:0, logical_process_axis:0)\n            [S0, R, R]: CommSpec:(comm_pattern:GATHER_FWD_SPLIT_BWD, gather_dim:1, shard_dim:1, logical_process_axis:1)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get all valid layouts from source_layout with single all-gather operation.\n        For the all-gather operation, we just care about the S dimension.\n\n        Argument:\n            source_layout: the layout to be transformed.\n\n        Return:\n            valid_spec_dict(Dict[Layout, CommSpec]): all valid layouts from source_layout with single all-gather operation.\n\n        Example:\n            layout_converter = LayoutConverter()\n            physical_mesh_id = torch.arange(0, 4)\n            mesh_shape = (2, 2)\n            # [[0, 1,\n            #  [2, 3]]\n            device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)\n            entire_shape = (4, 4, 4)\n            dim_partition_dict = {0: [0], 1: [1]}\n\n            # [S0,S1,R]\n            sharding_spec = ShardingSpec(dim_size=3, dim_partition_dict=dim_partition_dict)\n            layout = Layout(device_mesh=device_mesh,\n                            device_type=torch.device('cuda'),\n                            sharding_spec=sharding_spec,\n                            entire_shape=entire_shape)\n\n            rst_dict = layout_converter.all_gather_transform_layouts(layout)\n            for layout, comm_spec in rst_dict.items():\n                print(f'{layout.sharding_spec.sharding_sequence}: {comm_spec}')\n\n        Output:\n            [R, S1, R]: CommSpec:(comm_pattern:GATHER_FWD_SPLIT_BWD, gather_dim:0, shard_dim:0, logical_process_axis:0)\n            [S0, R, R]: CommSpec:(comm_pattern:GATHER_FWD_SPLIT_BWD, gather_dim:1, shard_dim:1, logical_process_axis:1)", "file": "colossalai_tensor_d_tensor_layout_converter.py", "function": "LayoutConverter.all_gather_transform_layouts", "project": "ColossalAI", "change_stats": {"code_changes": 14, "doc_changes": 7, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_balance(self, params={}):\n        \n        \n        \n        self.load_markets()\n        response = self.v2PrivateGetClientHolding(params)\n        #\n        # {\n        #     holding: [\n        #       {\n        #         token: 'USDT',\n        #         holding: '23.56',  # free balance\n        #         frozen: '888.0',  # i.e. if in processing withdrawal\n        #         interest: '0.0',\n        #         outstanding_holding: '-56.7',  # self value is set(negative number) if there is an open limit order, and self is QUOTE currency of order\n        #         pending_exposure: '333.45',  #  self value is set  (positive number) if there is an open limit order, and self is BASE currency of order\n        #         opening_cost: '0.00000000',\n        #         holding_cost: '0.00000000',\n        #         realised_pnl: '0.00000000',\n        #         settled_pnl: '0.00000000',\n        #         fee_24_h: '0',\n        #         settled_pnl_24_h: '0',\n        #         updated_time: '1641370779'\n        #       },\n        #       ...\n        #     ],\n        #     success: True\n        # }\n        #\n        return self.parse_balance(response)```\n\nUpdated code:\n```python\ndef fetch_balance(self, params={}):\n        \n        \n        \n        self.load_markets()\n        response = self.v3PrivateGetBalances(params)\n        #\n        #     {\n        #         \"success\": True,\n        #         \"data\": {\n        #             \"holding\": [\n        #                 {\n        #                     \"token\": \"0_token\",\n        #                     \"holding\": 1,\n        #                     \"frozen\": 0,\n        #                     \"staked\": 0,\n        #                     \"unbonding\": 0,\n        #                     \"vault\": 0,\n        #                     \"interest\": 0,\n        #                     \"pendingShortQty\": 0,\n        #                     \"pendingLongQty\": 0,\n        #                     \"availableBalance\": 0,\n        #                     \"updatedTime\": 312321.121\n        #                 }\n        #             ]\n        #         },\n        #         \"timestamp\": 1673323746259\n        #     }\n        #\n        data = self.safe_value(response, 'data')\n        return self.parse_balance(data)\n```\n\nOriginal documentation:\n```python\nquery for balance and get the amount of funds available for trading or funds locked in orders\n        :param dict params: extra parameters specific to the woo api endpoint\n        :returns dict: a `balance structure <https://docs.ccxt.com/en/latest/manual.html?#balance-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "query for balance and get the amount of funds available for trading or funds locked in orders\n        see https://docs.woo.org/#get-current-holding-get-balance-new\n        :param dict params: extra parameters specific to the woo api endpoint\n        :returns dict: a `balance structure <https://docs.ccxt.com/en/latest/manual.html?#balance-structure>`", "file": "python_ccxt_woo.py", "function": "woo.fetch_balance", "project": "ccxt", "change_stats": {"code_changes": 49, "doc_changes": 3, "total_changes": 52}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _disassemble(self, lineno_width=3, mark_as_current=False, offset_width=0):\n        \n        \n        fields = []\n        # Column: Source code line number\n        if lineno_width:\n            if self.starts_line:\n                lineno_fmt = \"%%%dd\" if self.line_number is not None else \"%%%ds\"\n                lineno_fmt = lineno_fmt % lineno_width\n                lineno = self.line_number if self.line_number is not None else '--'\n                fields.append(lineno_fmt % lineno)\n            else:\n                fields.append(' ' * lineno_width)\n        # Column: Label\n        if self.label is not None:\n            lbl = f\"L{self.label}:\"\n            fields.append(f\"{lbl:>{self.label_width}}\")\n        else:\n            fields.append(' ' * self.label_width)\n        # Column: Instruction offset from start of code sequence\n        if offset_width > 0:\n            fields.append(f\"{repr(self.offset):>{offset_width}}  \")\n        # Column: Current instruction indicator\n        if mark_as_current:\n            fields.append('-->')\n        else:\n            fields.append('   ')\n        # Column: Opcode name\n        fields.append(self.opname.ljust(_OPNAME_WIDTH))\n        # Column: Opcode argument\n        if self.arg is not None:\n            arg = repr(self.arg)\n            # If opname is longer than _OPNAME_WIDTH, we allow it to overflow into\n            # the space reserved for oparg. This results in fewer misaligned opargs\n            # in the disassembly output.\n            opname_excess = max(0, len(self.opname) - _OPNAME_WIDTH)\n            fields.append(repr(self.arg).rjust(_OPARG_WIDTH - opname_excess))\n            # Column: Opcode argument details\n            if self.argrepr:\n                fields.append('(' + self.argrepr + ')')\n        return ' '.join(fields).rstrip()```\n\nUpdated code:\n```python\ndef _disassemble(self, lineno_width=3, mark_as_current=False, offset_width=0,\n                           label_width=0):\n        \n        \n        fields = []\n        # Column: Source code line number\n        if lineno_width:\n            if self.starts_line:\n                lineno_fmt = \"%%%dd\" if self.line_number is not None else \"%%%ds\"\n                lineno_fmt = lineno_fmt % lineno_width\n                lineno = self.line_number if self.line_number is not None else '--'\n                fields.append(lineno_fmt % lineno)\n            else:\n                fields.append(' ' * lineno_width)\n        # Column: Label\n        if self.label is not None:\n            lbl = f\"L{self.label}:\"\n            fields.append(f\"{lbl:>{label_width}}\")\n        else:\n            fields.append(' ' * label_width)\n        # Column: Instruction offset from start of code sequence\n        if offset_width > 0:\n            fields.append(f\"{repr(self.offset):>{offset_width}}  \")\n        # Column: Current instruction indicator\n        if mark_as_current:\n            fields.append('-->')\n        else:\n            fields.append('   ')\n        # Column: Opcode name\n        fields.append(self.opname.ljust(_OPNAME_WIDTH))\n        # Column: Opcode argument\n        if self.arg is not None:\n            arg = repr(self.arg)\n            # If opname is longer than _OPNAME_WIDTH, we allow it to overflow into\n            # the space reserved for oparg. This results in fewer misaligned opargs\n            # in the disassembly output.\n            opname_excess = max(0, len(self.opname) - _OPNAME_WIDTH)\n            fields.append(repr(self.arg).rjust(_OPARG_WIDTH - opname_excess))\n            # Column: Opcode argument details\n            if self.argrepr:\n                fields.append('(' + self.argrepr + ')')\n        return ' '.join(fields).rstrip()\n```\n\nOriginal documentation:\n```python\nFormat instruction details for inclusion in disassembly output.\n\n        *lineno_width* sets the width of the line number field (0 omits it)\n        *mark_as_current* inserts a '-->' marker arrow as part of the line\n        *offset_width* sets the width of the instruction offset field\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Format instruction details for inclusion in disassembly output.\n\n        *lineno_width* sets the width of the line number field (0 omits it)\n        *mark_as_current* inserts a '-->' marker arrow as part of the line\n        *offset_width* sets the width of the instruction offset field\n        *label_width* sets the width of the label field", "file": "Lib_dis.py", "function": "Instruction._disassemble", "project": "cpython", "change_stats": {"code_changes": 9, "doc_changes": 3, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rel_shift(self, x, zero_triu=False):\n        \n\n        \n        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=-1)\n\n        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n        x = x_padded[:, :, 1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(2), x.size(3)))\n            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n\n        return x```\n\nUpdated code:\n```python\ndef rel_shift(self, x, zero_triu=False):\n        \n\n        \n        idx = torch.arange(0, x.size(3), device=x.device)\n        k_idx, q_idx = idx.unsqueeze(0), idx.unsqueeze(1)\n        # for pos_idx[i, j], i means the postion of query, j means the position of key\n        # the range of i-j is from -(L-1) to L-1\n        # use the absolute value of i-j, so the range is from 0 to L-1\n        rel_pos_idx = torch.abs(k_idx - q_idx)\n        # original postional encodings are generated with reversed order\n        rel_pos_idx = x.size(3) - 1 - rel_pos_idx\n        rel_pos_idx = rel_pos_idx.repeat(x.size(0), x.size(1), 1, 1)\n        x_shift = torch.gather(x, dim=3, index=rel_pos_idx)\n\n        if zero_triu:\n            ones = torch.ones((x_shift.size(2), x_shift.size(3)), device=x_shift.device)\n            x_shift = (\n                x_shift\n                * torch.tril(ones, x_shift.size(3) - x_shift.size(2))[None, None, :, :]\n            )\n\n        return x_shift\n```\n\nOriginal documentation:\n```python\nCompute relative positinal encoding.\n\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, size).\n            zero_triu (bool): If true, return the lower triangular part of the matrix.\n\n        Returns:\n            torch.Tensor: Output tensor.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute relative positinal encoding.\n\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, time2).\n            zero_triu (bool): If true, return the lower triangular part of the matrix.\n\n        Returns:\n            torch.Tensor: Output tensor.", "file": "espnet_nets_pytorch_backend_transformer_attention.py", "function": "RelPositionMultiHeadedAttention.rel_shift", "project": "espnet", "change_stats": {"code_changes": 26, "doc_changes": 4, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_txt_record(self, domain_name, record_name, record_content):\n        \n        \n        \n\n        try:\n            domain = self._find_domain(domain_name)\n        except digitalocean.Error as e:\n            hint = None\n\n            if str(e).startswith(\"Unable to authenticate\"):\n                hint = 'Did you provide a valid API token?'\n\n            logger.debug('Error finding domain using the DigitalOcean API: %s', e)\n            raise errors.PluginError('Error finding domain using the DigitalOcean API: {0}{1}'\n                                     .format(e, ' ({0})'.format(hint) if hint else ''))\n\n        try:\n            result = domain.create_new_domain_record(\n                type='TXT',\n                name=self._compute_record_name(domain, record_name),\n                data=record_content)\n\n            record_id = result['domain_record']['id']\n\n            logger.debug('Successfully added TXT record with id: %d', record_id)\n        except digitalocean.Error as e:\n            logger.debug('Error adding TXT record using the DigitalOcean API: %s', e)\n            raise errors.PluginError('Error adding TXT record using the DigitalOcean API: {0}'\n                                     .format(e))```\n\nUpdated code:\n```python\ndef add_txt_record(self, domain_name: str, record_name: str, record_content: str,\n                       record_ttl: int):\n        \n        \n        \n\n        try:\n            domain = self._find_domain(domain_name)\n        except digitalocean.Error as e:\n            hint = None\n\n            if str(e).startswith(\"Unable to authenticate\"):\n                hint = 'Did you provide a valid API token?'\n\n            logger.debug('Error finding domain using the DigitalOcean API: %s', e)\n            raise errors.PluginError('Error finding domain using the DigitalOcean API: {0}{1}'\n                                     .format(e, ' ({0})'.format(hint) if hint else ''))\n\n        try:\n            result = domain.create_new_domain_record(\n                type='TXT',\n                name=self._compute_record_name(domain, record_name),\n                data=record_content,\n                ttl=record_ttl) # ttl kwarg is only effective starting python-digitalocean 1.15.0\n\n            record_id = result['domain_record']['id']\n\n            logger.debug('Successfully added TXT record with id: %d', record_id)\n        except digitalocean.Error as e:\n            logger.debug('Error adding TXT record using the DigitalOcean API: %s', e)\n            raise errors.PluginError('Error adding TXT record using the DigitalOcean API: {0}'\n                                     .format(e))\n```\n\nOriginal documentation:\n```python\nAdd a TXT record using the supplied information.\n\n        :param str domain_name: The domain to use to associate the record with.\n        :param str record_name: The record name (typically beginning with '_acme-challenge.').\n        :param str record_content: The record content (typically the challenge validation).\n        :raises certbot.errors.PluginError: if an error occurs communicating with the DigitalOcean\n                                            API\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add a TXT record using the supplied information.\n\n        :param str domain_name: The domain to use to associate the record with.\n        :param str record_name: The record name (typically beginning with '_acme-challenge.').\n        :param str record_content: The record content (typically the challenge validation).\n        :param int record_ttl: The record TTL.\n        :raises certbot.errors.PluginError: if an error occurs communicating with the DigitalOcean\n                                            API", "file": "certbot-dns-digitalocean_certbot_dns_digitalocean__internal_dns_digitalocean.py", "function": "_DigitalOceanClient.add_txt_record", "project": "certbot", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef git_clone(repo):\n    \n    \n    \n\n    # Return repo dir\n    tail = os.path.split(repo)[1]\n    repo_dir = tail.rsplit('.git')[0]\n    logging.debug('repo_dir is {0}'.format(repo_dir))\n\n    if os.path.isdir(repo_dir):\n        delete_repo(repo_dir)\n\n    os.system('git clone {0}'.format(repo))\n\n    return repo_dir```\n\nUpdated code:\n```python\ndef git_clone(repo, checkout=None):\n    \n    \n    \n\n    # Return repo dir\n    tail = os.path.split(repo)[1]\n    repo_dir = tail.rsplit('.git')[0]\n    logging.debug('repo_dir is {0}'.format(repo_dir))\n\n    if os.path.isdir(repo_dir):\n        delete_repo(repo_dir)\n\n    os.system('git clone {0}'.format(repo))\n\n    if checkout is not None:\n        subprocess.check_call(['git', 'checkout', checkout], cwd=repo_dir)\n\n    return repo_dir\n```\n\nOriginal documentation:\n```python\nClone a git repo to the current directory.\n\n    :param repo: Git repo URL ending with .git.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Clone a git repo to the current directory.\n\n    :param repo: Git repo URL ending with .git.\n    :param checkout: The branch, tag or commit ID to checkout after clone", "file": "cookiecutter_vcs.py", "function": "git_clone", "project": "cookiecutter", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef planning(self, animation=True, search_until_max_iter=True):\n        \n        \n        \n\n        self.node_list = [self.start]\n        for i in range(self.max_iter):\n            print(\"Iter:\", i, \", number of nodes:\", len(self.node_list))\n            rnd = self.get_random_node()\n            nearest_ind = self.get_nearest_list_index(self.node_list, rnd)\n            new_node = self.steer(self.node_list[nearest_ind], rnd)\n\n            if self.check_collision(new_node, self.obstacle_list):\n                near_indexes = self.find_near_nodes(new_node)\n                new_node = self.choose_parent(new_node, near_indexes)\n                if new_node:\n                    self.node_list.append(new_node)\n                    self.rewire(new_node, near_indexes)\n\n            if animation and i % 5 == 0:\n                self.plot_start_goal_arrow()\n                self.draw_graph(rnd)\n\n            if (not search_until_max_iter) and new_node:  # check reaching the goal\n                last_index = self.search_best_goal_node()\n                if last_index:\n                    return self.generate_final_course(last_index)\n\n        print(\"reached max iteration\")\n\n        last_index = self.search_best_goal_node()\n        if last_index:\n            return self.generate_final_course(last_index)\n        else:\n            print(\"Cannot find path\")\n\n        return None```\n\nUpdated code:\n```python\ndef planning(self, animation=True, search_until_max_iter=True):\n        \n        \n        \n\n        self.node_list = [self.start]\n        for i in range(self.max_iter):\n            print(\"Iter:\", i, \", number of nodes:\", len(self.node_list))\n            rnd = self.get_random_node()\n            nearest_ind = self.get_nearest_node_index(self.node_list, rnd)\n            new_node = self.steer(self.node_list[nearest_ind], rnd)\n\n            if self.check_collision(new_node, self.obstacle_list):\n                near_indexes = self.find_near_nodes(new_node)\n                new_node = self.choose_parent(new_node, near_indexes)\n                if new_node:\n                    self.node_list.append(new_node)\n                    self.rewire(new_node, near_indexes)\n                    self.try_goal_path(new_node)\n\n            if animation and i % 5 == 0:\n                self.plot_start_goal_arrow()\n                self.draw_graph(rnd)\n\n            if (not search_until_max_iter) and new_node:  # check reaching the goal\n                last_index = self.search_best_goal_node()\n                if last_index:\n                    return self.generate_final_course(last_index)\n\n        print(\"reached max iteration\")\n\n        last_index = self.search_best_goal_node()\n        if last_index:\n            return self.generate_final_course(last_index)\n        else:\n            print(\"Cannot find path\")\n\n        return None\n```\n\nOriginal documentation:\n```python\nRRT Star planning\n\n        animation: flag for animation on or off\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "planning\n\n        animation: flag for animation on or off", "file": "PathPlanning_RRTStarReedsShepp_rrt_star_reeds_shepp.py", "function": "RRTStarReedsShepp.planning", "project": "PythonRobotics", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_transaction_fees(self, codes=None, params={}):\n        \n        \n        \n        self.load_markets()\n        response = self.privatePostAccountFees(params)\n        fees = response['withdraw']\n        withdraw = {}\n        ids = list(fees.keys())\n        for i in range(0, len(ids)):\n            id = ids[i]\n            code = self.safe_currency_code(id)\n            withdraw[code] = self.safe_number(fees, id)\n        return {\n            'info': response,\n            'withdraw': withdraw,\n            'deposit': withdraw,  # only for deposits of less than $1000\n        }```\n\nUpdated code:\n```python\ndef fetch_transaction_fees(self, codes=None, params={}):\n        \n        \n        \n        self.load_markets()\n        result = {}\n        response = self.privatePostAccountFees(params)\n        #\n        # {\n        #     'withdraw': {\n        #         'BTC': '0.0004',\n        #     }\n        # }\n        #\n        fees = self.safe_value(response, 'withdraw')\n        ids = list(fees.keys())\n        for i in range(0, len(ids)):\n            id = ids[i]\n            code = self.safe_currency_code(id)\n            if codes is not None and not self.in_array(code, codes):\n                continue\n            result[code] = {\n                'withdraw': self.safe_number(fees, id),\n                'deposit': {},\n                'info': self.safe_number(fees, id),\n            }\n        return result\n```\n\nOriginal documentation:\n```python\nfetch transaction fees\n        :param [str]|None codes: not used by bitfinex2 fetchTransactionFees()\n        :param dict params: extra parameters specific to the bitfinex api endpoint\n        :returns [dict]: a list of `fees structures <https://docs.ccxt.com/en/latest/manual.html#fee-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch transaction fees\n        see https://docs.bitfinex.com/v1/reference/rest-auth-fees\n        :param [str]|None codes: not used by bitfinex2 fetchTransactionFees()\n        :param dict params: extra parameters specific to the bitfinex api endpoint\n        :returns [dict]: a list of `fees structures <https://docs.ccxt.com/en/latest/manual.html#fee-structure>`", "file": "python_ccxt_bitfinex.py", "function": "bitfinex.fetch_transaction_fees", "project": "ccxt", "change_stats": {"code_changes": 27, "doc_changes": 3, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef close(self):\n          \n        return```\n\nUpdated code:\n```python\ndef close(self) -> None:\n          \n        return\n```\n\nOriginal documentation:\n```python\nImage writer does not need a close method\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Does nothing as OpenCV writer does not need a close method", "file": "plugins_convert_writer_opencv.py", "function": "Writer.close", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef between(self, left, right, inclusive=True):\n        \n        \n        \n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        mask = lmask & rmask\n        if mask.dtype == np.object_:\n            np.putmask(mask, isnull(mask), False)\n            mask = mask.astype(bool)\n\n        return mask```\n\nUpdated code:\n```python\ndef between(self, left, right, inclusive=True):\n        \n        \n        \n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n```\n\nOriginal documentation:\n```python\nReturn boolean Series equivalent to left <= series <= right. NA values\n        will be treated as False\n\n        Parameters\n        ----------\n        left : scalar\n            Left boundary\n        right : scalar\n            Right boundary\n\n        Returns\n        -------\n        is_between : Series\n            NAs, if any, will be preserved\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return boolean Series equivalent to left <= series <= right. NA values\n        will be treated as False\n\n        Parameters\n        ----------\n        left : scalar\n            Left boundary\n        right : scalar\n            Right boundary\n\n        Returns\n        -------\n        is_between : Series", "file": "pandas_core_series.py", "function": "Series.between", "project": "pandas", "change_stats": {"code_changes": 9, "doc_changes": 3, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef keras_backend_quiet():\n      \n    stderr = sys.stderr\n    sys.stderr = open(os.devnull, 'w')\n    from keras import backend as K\n    sys.stderr = stderr\n    return K```\n\nUpdated code:\n```python\ndef keras_backend_quiet():\n      \n    stderr = sys.stderr\n    sys.stderr = open(os.devnull, 'w')\n    try:\n        from keras import backend as K  # pylint:disable=import-outside-toplevel\n    except:\n        sys.stderr = stderr\n        raise\n    sys.stderr = stderr\n    return K\n```\n\nOriginal documentation:\n```python\nSuppresses the \"Using x backend\" message when importing\n        backend from keras\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Suppresses the \"Using x backend\" message when importing backend from keras.\n\n    Make sure output is redirected back to stderr if there is an error.", "file": "lib_utils.py", "function": "keras_backend_quiet", "project": "faceswap", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef task_flush(taskid):\n    \n    \n    \n    global tasks\n\n    if is_admin(taskid):\n        for task in tasks:\n            tasks[task].clean_filesystem()\n\n        tasks = dict()\n        return jsonize({\"success\": True})\n    else:\n        abort(401)```\n\nUpdated code:\n```python\ndef task_flush(taskid):\n    \n    \n    \n    global tasks\n\n    if is_admin(taskid):\n        for task in tasks:\n            tasks[task].clean_filesystem()\n\n        tasks = dict()\n        logger.debug(\"Flushed task poll\")\n        return jsonize({\"success\": True})\n    else:\n        abort(401)\n```\n\nOriginal documentation:\n```python\nFlush task spool (destroy all tasks)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Flush task spool (delete all tasks)", "file": "lib_utils_api.py", "function": "task_flush", "project": "sqlmap", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_error_html(self, status_code):\n        \n        return \"<html><title>%(code)d: %(message)s</title>\" \\\n               \"<body>%(code)d: %(message)s</body></html>\" % {\n            \"code\": status_code,\n            \"message\": httplib.responses[status_code],\n        }```\n\nUpdated code:\n```python\ndef get_error_html(self, status_code, **kwargs):\n        \n        \n        return \"<html><title>%(code)d: %(message)s</title>\" \\\n               \"<body>%(code)d: %(message)s</body></html>\" % {\n            \"code\": status_code,\n            \"message\": httplib.responses[status_code],\n        }\n```\n\nOriginal documentation:\n```python\nOverride to implement custom error pages.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Override to implement custom error pages.\n\n        If this error was caused by an uncaught exception, the\n        exception object can be found in kwargs e.g. kwargs['exception']", "file": "tornado_web.py", "function": "RequestHandler.get_error_html", "project": "tornado", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_well_formed_result(\n        self, result: Dict[str, Any], stream_name: str, realm: Realm\n    ) -> None:\n        \n        \n        \n        self.assertIn(\"subscribers\", result)\n        self.assertIsInstance(result[\"subscribers\"], list)\n        true_subscribers = [\n            user_profile.email\n            for user_profile in self.users_subscribed_to_stream(stream_name, realm)\n        ]\n        self.assertEqual(sorted(result[\"subscribers\"]), sorted(true_subscribers))```\n\nUpdated code:\n```python\ndef check_well_formed_result(\n        self, result: Dict[str, Any], stream_name: str, realm: Realm\n    ) -> None:\n        \n        \n        \n        self.assertIn(\"subscribers\", result)\n        self.assertIsInstance(result[\"subscribers\"], list)\n        true_subscribers = [\n            user_profile.id for user_profile in self.users_subscribed_to_stream(stream_name, realm)\n        ]\n        self.assertEqual(sorted(result[\"subscribers\"]), sorted(true_subscribers))\n```\n\nOriginal documentation:\n```python\nA successful call to get_subscribers returns the list of subscribers in\n        the form:\n\n        {\"msg\": \"\",\n         \"result\": \"success\",\n         \"subscribers\": [self.example_email(\"hamlet\"), self.example_email(\"prospero\")]}\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A successful call to get_subscribers returns the list of subscribers in\n        the form:\n\n        {\"msg\": \"\",\n         \"result\": \"success\",\n         \"subscribers\": [hamlet_user.id, prospero_user.id]}", "file": "zerver_tests_test_subs.py", "function": "GetSubscribersTest.check_well_formed_result", "project": "zulip", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef meow_feed_select(self):\n        \n        \n        \n        current = 0\n        while 1:\n            # Scan for feed, exit if none\n            buttons = self.meow_feed_scan()\n            if not len(buttons):\n                break\n\n            # Else click each button to\n            # apply green check mark\n            # Sleep for stable image\n            for button in buttons:\n                self.device.click(button)\n            self.device.sleep((0.3, 0.5))\n            self.device.screenshot()\n\n            # Exit if maximum clicked\n            current, remain, total = MEOWFFICER_FEED.ocr(self.device.image)\n            if not remain:\n                break\n\n        # Use current to pass appropriate button for ui_click\n        # route back to MEOWFFICER_ENHANCE\n        logger.info(f'Confirm selected feed material, total: {current} / 10') if current else \\\n            logger.info('Lack of feed material to complete enhancement, cancelling')\n        self.ui_click(MEOWFFICER_FEED_CONFIRM if current else MEOWFFICER_FEED_CANCEL,\n                      check_button=MEOWFFICER_ENHANCE_CONFIRM, offset=(20, 20))```\n\nUpdated code:\n```python\ndef meow_feed_select(self):\n        \n        \n        \n        current = 0\n        while 1:\n            # Scan for feed, exit if none\n            buttons = self.meow_feed_scan()\n            if not len(buttons):\n                break\n\n            # Else click each button to\n            # apply green check mark\n            # Sleep for stable image\n            for button in buttons:\n                self.device.click(button)\n            self.device.sleep((0.3, 0.5))\n            self.device.screenshot()\n\n            # Exit if maximum clicked\n            current, remain, total = MEOWFFICER_FEED.ocr(self.device.image)\n            if not remain:\n                break\n\n        # Use current to pass appropriate button for ui_click\n        # route back to MEOWFFICER_ENHANCE\n        logger.info(f'Confirm selected feed material, total: {current} / 10') if current else \\\n            logger.info('Lack of feed material to complete enhancement, cancelling')\n        self.ui_click(MEOWFFICER_FEED_CONFIRM if current else MEOWFFICER_FEED_CANCEL,\n                      check_button=MEOWFFICER_ENHANCE_CONFIRM, offset=(20, 20))\n        return current\n```\n\nOriginal documentation:\n```python\nClick and confirm the meowfficers that\n        can be used as feed to enhance the target\n        meowfficer\n\n        Pages:\n            in: MEOWFFICER_FEED\n            out: MEOWFFICER_ENHANCE\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Click and confirm the meowfficers that\n        can be used as feed to enhance the target\n        meowfficer\n\n        Pages:\n            in: MEOWFFICER_FEED\n            out: MEOWFFICER_ENHANCE\n\n        Returns:\n            int: non-zero positive, some selected\n                 zero, none selected", "file": "module_meowfficer_enhance.py", "function": "MeowfficerEnhance.meow_feed_select", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef gcc_llvm():\n     \n    \n    if gcc_llvm.is_llvm is None:\n        pass\n        p = None\n        try:\n            p = call_subprocess_Popen(['g++', '--version'],\n                                      stdout=subprocess.PIPE,\n                                      stderr=subprocess.PIPE)\n            p.wait()\n            output = p.stdout.read() + p.stderr.read()\n        except OSError:\n            # Typically means g++ cannot be found.\n            # So it is not an llvm compiler.\n\n            # Normally this should not happen as we should not try to\n            # compile when g++ is not available. If this happen, it\n            # will crash later so supposing it is not llvm is \"safe\".\n            output = ''\n        del p\n        gcc_llvm.is_llvm = \"llvm\" in output\n    return gcc_llvm.is_llvm```\n\nUpdated code:\n```python\ndef gcc_llvm():\n     \n    \n    if gcc_llvm.is_llvm is None:\n        p = None\n        try:\n            p = call_subprocess_Popen(['g++', '--version'],\n                                      stdout=subprocess.PIPE,\n                                      stderr=subprocess.PIPE)\n            p.wait()\n            output = p.stdout.read() + p.stderr.read()\n        except OSError:\n            # Typically means g++ cannot be found.\n            # So it is not an llvm compiler.\n\n            # Normally this should not happen as we should not try to\n            # compile when g++ is not available. If this happens, it\n            # will crash later so supposing it is not llvm is \"safe\".\n            output = ''\n        del p\n        gcc_llvm.is_llvm = \"llvm\" in output\n    return gcc_llvm.is_llvm\n```\n\nOriginal documentation:\n```python\nDetect if the g++ version used is the llvm one or not.\n\n    It don't support all g++ parameters even if it support many of them.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Detect if the g++ version used is the llvm one or not.\n\n    It does not support all g++ parameters even if it supports many of them.", "file": "theano_gof_cmodule.py", "function": "gcc_llvm", "project": "Theano", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_map_data(self, use_loop=False):\n        \n        \n        \n        if not len(self.map_data_loop):\n            self._load_map_data(self.map_data)\n        else:\n            if use_loop:\n                self._load_map_data(self.map_data_loop)\n            else:\n                self._load_map_data(self.map_data)```\n\nUpdated code:\n```python\ndef load_map_data(self, use_loop=False):\n        \n        \n        \n        has_loop = len(self.map_data_loop)\n        logger.info(f'Load map_data, has_loop={has_loop}, use_loop={use_loop}')\n        if has_loop and use_loop:\n            self._load_map_data(self.map_data_loop)\n        else:\n            self._load_map_data(self.map_data)\n```\n\nOriginal documentation:\n```python\nArgs:\n            use_loop (bool): If at clearing mode.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            use_loop (bool): If at clearing mode.\n                             clearing mode (Correct name) == fast forward (in old Alas) == loop (in lua files)", "file": "module_map_map_base.py", "function": "CampaignMap.load_map_data", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 13, "doc_changes": 3, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef normal(loc=0.0, scale=1.0, size=None, dtype=float):\n    \n\n    \n    rs = _generator.get_random_state()\n    x = rs.normal(0, 1, size, dtype)\n    cupy.multiply(x, scale, out=x)\n    cupy.add(x, loc, out=x)\n    return x```\n\nUpdated code:\n```python\ndef normal(loc=0.0, scale=1.0, size=None, dtype=float):\n    \n\n    \n    rs = _generator.get_random_state()\n    if size is None and any(isinstance(arg, cupy.ndarray)\n                            for arg in [scale, loc]):\n        size = cupy.broadcast_arrays(loc, scale)[0].shape\n    x = rs.normal(0, 1, size, dtype)\n    cupy.multiply(x, scale, out=x)\n    cupy.add(x, loc, out=x)\n    return x\n```\n\nOriginal documentation:\n```python\nReturns an array of normally distributed samples.\n\n    Args:\n        loc (float or array_like of floats): Mean of the normal distribution.\n        scale (float or array_like of floats):\n            Standard deviation of the normal distribution.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Normally distributed samples.\n\n    .. seealso:: :func:`numpy.random.normal`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns an array of normally distributed samples.\n\n    Args:\n        loc (float or array_like of floats): Mean of the normal distribution.\n        scale (float or array_like of floats):\n            Standard deviation of the normal distribution.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Normally distributed samples.\n\n    .. seealso:: :meth:`numpy.random.normal\n                 <numpy.random.mtrand.RandomState.normal>`", "file": "cupy_random__distributions.py", "function": "normal", "project": "cupy", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convert(\n        cls, value: float, from_unit: str, to_unit: str, *, interval: bool = False\n    ) -> float:\n        \n        if from_unit == to_unit:\n            return value\n\n        if from_unit == TEMP_CELSIUS:\n            if to_unit == TEMP_FAHRENHEIT:\n                return cls._celsius_to_fahrenheit(value, interval)\n            if to_unit == TEMP_KELVIN:\n                return cls._celsius_to_kelvin(value, interval)\n            raise HomeAssistantError(\n                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)\n            )\n\n        if from_unit == TEMP_FAHRENHEIT:\n            if to_unit == TEMP_CELSIUS:\n                return cls._fahrenheit_to_celsius(value, interval)\n            if to_unit == TEMP_KELVIN:\n                return cls._celsius_to_kelvin(\n                    cls._fahrenheit_to_celsius(value, interval), interval\n                )\n            raise HomeAssistantError(\n                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)\n            )\n\n        if from_unit == TEMP_KELVIN:\n            if to_unit == TEMP_CELSIUS:\n                return cls._kelvin_to_celsius(value, interval)\n            if to_unit == TEMP_FAHRENHEIT:\n                return cls._celsius_to_fahrenheit(\n                    cls._kelvin_to_celsius(value, interval), interval\n                )\n            raise HomeAssistantError(\n                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)\n            )\n        raise HomeAssistantError(\n            UNIT_NOT_RECOGNIZED_TEMPLATE.format(from_unit, cls.UNIT_CLASS)\n        )```\n\nUpdated code:\n```python\ndef convert(cls, value: float, from_unit: str, to_unit: str) -> float:\n        \n        \n        # We cannot use the implementation from BaseUnitConverter here because the temperature\n        # units do not use the same floor: 0\u00b0C, 0\u00b0F and 0K do not align\n        if from_unit == to_unit:\n            return value\n\n        if from_unit == TEMP_CELSIUS:\n            if to_unit == TEMP_FAHRENHEIT:\n                return cls._celsius_to_fahrenheit(value)\n            if to_unit == TEMP_KELVIN:\n                return cls._celsius_to_kelvin(value)\n            raise HomeAssistantError(\n                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)\n            )\n\n        if from_unit == TEMP_FAHRENHEIT:\n            if to_unit == TEMP_CELSIUS:\n                return cls._fahrenheit_to_celsius(value)\n            if to_unit == TEMP_KELVIN:\n                return cls._celsius_to_kelvin(cls._fahrenheit_to_celsius(value))\n            raise HomeAssistantError(\n                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)\n            )\n\n        if from_unit == TEMP_KELVIN:\n            if to_unit == TEMP_CELSIUS:\n                return cls._kelvin_to_celsius(value)\n            if to_unit == TEMP_FAHRENHEIT:\n                return cls._celsius_to_fahrenheit(cls._kelvin_to_celsius(value))\n            raise HomeAssistantError(\n                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)\n            )\n        raise HomeAssistantError(\n            UNIT_NOT_RECOGNIZED_TEMPLATE.format(from_unit, cls.UNIT_CLASS)\n        )\n```\n\nOriginal documentation:\n```python\nConvert a temperature from one unit to another.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert a temperature from one unit to another.\n\n        eg. 10\u00b0C will return 50\u00b0F\n\n        For converting an interval between two temperatures, please use\n        `convert_interval` instead.", "file": "homeassistant_util_unit_conversion.py", "function": "TemperatureConverter.convert", "project": "core", "change_stats": {"code_changes": 25, "doc_changes": 7, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef recognize(self, x_block, recog_args, char_list=None, rnnlm=None, use_jit=False):\n        '''\n        '''\n\n        xp = self.xp\n        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n            ilens = [x_block.shape[0]]\n            batch = len(ilens)\n            xs, x_mask, ilens = self.encoder(x_block[None, :, :], ilens)\n            logging.info('Encoder size: ' + str(xs.shape))\n            if recog_args.ctc_weight > 0.0:\n                raise NotImplementedError('use joint ctc/tranformer decoding. WIP')\n            if recog_args.beam_size == 1:\n                logging.info('Use greedy search implementation')\n                ys = xp.full((1, 1), self.sos)\n                score = xp.zeros(1)\n                maxlen = xs.shape[1] + 1\n                for step in range(maxlen):\n                    yy_mask = self.make_attention_mask(ys, ys)\n                    yy_mask *= self.make_history_mask(ys)\n                    xy_mask = self.make_attention_mask(ys, xp.array(x_mask))\n                    out = self.decoder(ys, yy_mask, xs, xy_mask).reshape(batch, -1, self.odim)\n                    prob = F.log_softmax(out[:, -1], axis=-1)\n                    max_prob = prob.array.max(axis=1)\n                    next_id = F.argmax(prob, axis=1).array.astype(np.int64)\n                    score += max_prob\n                    if step == maxlen - 1:\n                        next_id[0] = self.eos\n                    ys = F.concat((ys, next_id[None, :]), axis=1).data\n                    if next_id[0] == self.eos:\n                        break\n                nbest_hyps = [{\"score\": score, \"yseq\": ys[0].tolist()}]\n            else:\n                raise NotImplementedError('use beam search implementation. WIP')\n        return nbest_hyps```\n\nUpdated code:\n```python\ndef recognize(self, x_block, recog_args, char_list=None, rnnlm=None):\n        \n        \n        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n            # 1. encoder\n            ilens = [x_block.shape[0]]\n            batch = len(ilens)\n            xs, _, _ = self.encoder(x_block[None, :, :], ilens)\n\n            # calculate log P(z_t|X) for CTC scores\n            if recog_args.ctc_weight > 0.0:\n                lpz = self.ctc.log_softmax(xs.reshape(batch, -1, self.dims)).data[0]\n            else:\n                lpz = None\n            # 2. decoder\n            y = self.recognize_beam(xs, lpz, recog_args, char_list, rnnlm)\n\n        return y\n```\n\nOriginal documentation:\n```python\nE2E beam search\n\n        :param ndarray x: input acouctic feature (B, T, D) or (T, D)\n        :param namespace recog_args: argment namespace contraining options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "E2E greedy/beam search\n        :param x:\n        :param recog_args:\n        :param char_list:\n        :param rnnlm:\n        :return:", "file": "espnet_nets_chainer_backend_e2e_asr_transformer.py", "function": "E2E.recognize", "project": "espnet", "change_stats": {"code_changes": 45, "doc_changes": 16, "total_changes": 61}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef coosort(x):\n    \n\n    \n    nnz = x.nnz\n    if nnz == 0:\n        return\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    buffer_size = cusparse.xcoosort_bufferSizeExt(\n        handle, m, n, nnz, x.row.data.ptr, x.col.data.ptr)\n    buf = cupy.empty(buffer_size, 'b')\n    P = cupy.empty(nnz, 'i')\n    data_orig = x.data.copy()\n    cusparse.createIdentityPermutation(handle, nnz, P.data.ptr)\n    cusparse.xcoosortByRow(\n        handle, m, n, nnz, x.row.data.ptr, x.col.data.ptr,\n        P.data.ptr, buf.data.ptr)\n    _call_cusparse(\n        'gthr', x.dtype,\n        handle, nnz, data_orig.data.ptr, x.data.data.ptr,\n        P.data.ptr, cusparse.CUSPARSE_INDEX_BASE_ZERO)```\n\nUpdated code:\n```python\ndef coosort(x, sort_by='r'):\n    \n\n    \n    if not check_availability('coosort'):\n        raise RuntimeError('coosort is not available.')\n\n    nnz = x.nnz\n    if nnz == 0:\n        return\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    buffer_size = cusparse.xcoosort_bufferSizeExt(\n        handle, m, n, nnz, x.row.data.ptr, x.col.data.ptr)\n    buf = cupy.empty(buffer_size, 'b')\n    P = cupy.empty(nnz, 'i')\n    data_orig = x.data.copy()\n    cusparse.createIdentityPermutation(handle, nnz, P.data.ptr)\n    if sort_by == 'r':\n        cusparse.xcoosortByRow(\n            handle, m, n, nnz, x.row.data.ptr, x.col.data.ptr,\n            P.data.ptr, buf.data.ptr)\n    elif sort_by == 'c':\n        cusparse.xcoosortByColumn(\n            handle, m, n, nnz, x.row.data.ptr, x.col.data.ptr,\n            P.data.ptr, buf.data.ptr)\n    else:\n        raise ValueError(\"sort_by must be either 'r' or 'c'\")\n    _call_cusparse(\n        'gthr', x.dtype,\n        handle, nnz, data_orig.data.ptr, x.data.data.ptr,\n        P.data.ptr, cusparse.CUSPARSE_INDEX_BASE_ZERO)\n    if sort_by == 'c':  # coo is sorted by row first\n        x._has_canonical_format = False\n```\n\nOriginal documentation:\n```python\nSorts indices of COO-matrix in place.\n\n    Args:\n        x (cupyx.scipy.sparse.coo_matrix): A sparse matrix to sort.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sorts indices of COO-matrix in place.\n\n    Args:\n        x (cupyx.scipy.sparse.coo_matrix): A sparse matrix to sort.\n        sort_by (str): Sort the indices by row ('r', default) or column ('c').", "file": "cupy_cusparse.py", "function": "coosort", "project": "cupy", "change_stats": {"code_changes": 22, "doc_changes": 3, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_qconfig_dtypes(qconfig):\n    \n    \n    assert qconfig is not None\n    activation = qconfig.activation()\n    weight = qconfig.weight()\n    compute_dtype = activation.compute_dtype if hasattr(activation, 'compute_dtype') else None\n    return (activation.dtype, weight.dtype, compute_dtype)```\n\nUpdated code:\n```python\ndef get_qconfig_dtypes(qconfig):\n    \n    \n    assert qconfig is not None\n    activation = qconfig.activation()\n    weight = qconfig.weight()\n    act_is_dynamic = activation.is_dynamic if hasattr(activation, 'is_dynamic') else False\n    return (activation.dtype, weight.dtype, act_is_dynamic)\n```\n\nOriginal documentation:\n```python\nr\"\"\" returns the qconfig tuple for qconfig:\n    (activation_dtype, weight_dtype, activation_compute_dtype)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\" returns the qconfig tuple for qconfig:\n    (activation_dtype, weight_dtype, activation_is_dynamic)", "file": "torch_ao_quantization_utils.py", "function": "get_qconfig_dtypes", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef makedirs_rm_exist(dir):\n    '''\n    \n\n    '''\n    if os.path.isdir(dir):\n        shutil.rmtree(dir)\n    os.makedirs(dir, exist_ok=True)```\n\nUpdated code:\n```python\ndef makedirs_rm_exist(dir):\n    \n    \n    if os.path.isdir(dir):\n        shutil.rmtree(dir)\n    os.makedirs(dir, exist_ok=True)\n```\n\nOriginal documentation:\n```python\nMake a directory, remove any existing data.\n\n    Args:\n        dir (str): The directory to be created.\n\n    Returns:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Make a directory, remove any existing data.\n\n    Args:\n        dir (str): The directory to be created.", "file": "torch_geometric_graphgym_utils_io.py", "function": "makedirs_rm_exist", "project": "pytorch_geometric", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef shop_currency(self):\n        \n        \n        \n        self._shop_guild_coins = OCR_SHOP_GUILD_COINS.ocr(self.device.image)\n        logger.info(f'Guild coins: {self._shop_guild_coins}')\n        return self._shop_guild_coins```\n\nUpdated code:\n```python\ndef shop_currency(self):\n        \n        \n        \n        self._currency = OCR_SHOP_GUILD_COINS.ocr(self.device.image)\n        logger.info(f'Guild coins: {self._currency}')\n        return self._currency\n```\n\nOriginal documentation:\n```python\nOcr shop guild currency\n\n        Returns:\n            int: guild coin amount\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ocr shop guild currency\n        Then return guild coin count\n\n        Returns:\n            int: guild coin amount", "file": "module_shop_shop_guild.py", "function": "GuildShop.shop_currency", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _duration_handler(time1, lang=None, speech=True, *, time2=None,\n                      use_years=True, resolution=TimeResolution.SECONDS):\n     \n    \n    _leapdays = 0\n    _input_resolution = resolution\n    milliseconds = 0\n\n    type1 = type(time1)\n\n    if time2:\n        type2 = type(time2)\n        if type1 is not type2:\n            raise Exception(\"nice_duration(\" + str(time1) + \", \" +\n                            str(time2) + \"): \\n\\t can't \"\n                            \"combine data types: \" + str(type(time1)) +\n                            \" and \" + str(type(time2)))\n        elif type1 is datetime.datetime:\n            duration = time1 - time2\n            _leapdays = (abs(leapdays(time1.year, time2.year)))\n\n            # when operating on datetimes, refuse resolutions that\n            # would result in bunches of trailing zeroes\n            if all([time1.second == 0, time2.second == 0,\n                    resolution.value >= 5]):\n                resolution = TimeResolution.MINUTES\n            if all([time1.minute == 0, time2.minute == 0,\n                    resolution.value == 4]):\n                resolution = TimeResolution.HOURS\n            if all([time1.hour == 0, time2.hour == 0,\n                    resolution.value == 3]):\n                resolution = TimeResolution.DAYS\n\n        else:\n            _tmp = warnings.formatwarning\n            warnings.formatwarning = lambda msg, * \\\n                args, **kwargs: \"{}\\n\".format(msg)\n            warning = (\"WARN: mycroft.util.format.nice_duration_dt() can't \"\n                       \"subtract \" + str(type1) + \". Ignoring 2nd \"\n                       \"argument '\" + str(time2) + \"'.\")\n            warnings.warn(warning)\n            warnings.formatwarning = _tmp\n            duration = time1\n    else:\n        duration = time1\n\n    # Pull decimal portion of seconds, if present, to use for milliseconds\n    if isinstance(duration, float):\n        milliseconds = str(duration).split('.')[1]\n        if speech:\n            milliseconds = milliseconds[:2]\n        else:\n            milliseconds = milliseconds[:3]\n        milliseconds = float(\"0.\" + milliseconds)\n\n    # Cast duration to datetime.timedelta for human-friendliness\n    if not isinstance(duration, datetime.timedelta):\n        duration = datetime.timedelta(seconds=duration)\n\n    days = duration.days\n    if use_years:\n        days -= _leapdays if days > 365 else 0\n        years = days // 365\n    else:\n        years = 0\n    days = days % 365 if years > 0 else days\n\n    # We already stored milliseconds. Now we want the integer part.\n    seconds = duration.seconds\n    minutes = seconds // 60\n    seconds %= 60\n    hours = minutes // 60\n    minutes %= 60\n\n    if speech:\n        out = \"\"\n        if years > 0:\n            out += pronounce_number(years, lang) + \" \"\n            out += _translate_word(\"year\" if years == 1 else \"years\", lang)\n\n        if days > 0 and resolution.value > TimeResolution.YEARS.value:\n            if out:\n                out += \" \"\n            out += pronounce_number(days, lang) + \" \"\n            out += _translate_word(\"day\" if days == 1 else \"days\", lang)\n\n        if hours > 0 and resolution.value > TimeResolution.DAYS.value:\n            if out:\n                out += \" \"\n            out += pronounce_number(hours, lang) + \" \"\n            out += _translate_word(\"hour\" if hours == 1 else \"hours\", lang)\n\n        if minutes > 0 and resolution.value > TimeResolution.HOURS.value:\n            if out:\n                out += \" \"\n            out += pronounce_number(minutes, lang) + \" \"\n            out += _translate_word(\"minute\" if minutes ==\n                                   1 else \"minutes\", lang)\n\n        if ((seconds > 0 and resolution.value >=\n             TimeResolution.SECONDS.value) or\n            (milliseconds > 0 and resolution.value ==\n             TimeResolution.MILLISECONDS.value)):\n\n            if resolution.value == TimeResolution.MILLISECONDS.value:\n                seconds += milliseconds\n            if out:\n                out += \" \"\n                # Throw \"and\" between minutes and seconds if duration < 1 hour\n                if len(out.split()) > 3 or seconds < 1:\n                    out += _translate_word(\"and\", lang) + \" \"\n            # speaking \"zero point five seconds\" is better than \"point five\"\n            if seconds < 1:\n                out += pronounce_number(0, lang)\n            out += pronounce_number(seconds, lang) + \" \"\n            out += _translate_word(\"second\" if seconds ==\n                                   1 else \"seconds\", lang)\n\n    else:\n        # M:SS, MM:SS, H:MM:SS, Dd H:MM:SS format\n\n        _seconds_str = (\"0\" + str(seconds)) if seconds < 10 else str(seconds)\n\n        out = \"\"\n        if years > 0:\n            out = str(years) + \"y \"\n        if days > 0 and resolution.value > TimeResolution.YEARS.value:\n            out += str(days) + \"d \"\n        if hours > 0 and resolution.value > TimeResolution.DAYS.value:\n            out += str(hours)\n\n        if resolution.value == TimeResolution.MINUTES.value:\n            out += ((\"h \" + str(minutes) + \"m\") if hours > 0\n                    else str(minutes) + \"m\")\n        elif minutes > 0 and resolution.value > TimeResolution.HOURS.value:\n            if hours != 0:\n                out += \":\"\n                if minutes < 10:\n                    out += \"0\"\n            out += str(minutes) + \":\"\n            if seconds > 0 and resolution.value > TimeResolution.MINUTES.value:\n                out += _seconds_str\n            else:\n                out += \"00\"\n        # if we have seconds but no minutes...\n        elif seconds > 0 and resolution.value > TimeResolution.MINUTES.value:\n            # check if output ends in hours\n            try:\n                if str(hours) == out.split()[-1]:\n                    out += \":\"\n            except IndexError:\n                pass\n            out += (\"00:\" if hours > 0 else \"0:\") + _seconds_str\n\n        if milliseconds > 0 and resolution.value \\\n                == TimeResolution.MILLISECONDS.value:\n            _mill = str(milliseconds).split(\".\")[1]\n            # right-pad milliseconds to three decimal places\n            while len(_mill) < 3:\n                _mill += \"0\"\n            # make sure output < 1s still formats correctly\n            if out == \"\":\n                out = \"0:00\"\n            else:\n                if (str(hours) == out.split()[-1]) and \":\" not in out:\n                    out += \":00:00\"\n            # only append milliseconds to output that contains\n            # minutes and/or seconds\n            if \":\" in out:\n                out += \".\" + _mill\n\n        # If this evaluates True, out currently ends in hours: \"1d 12\"\n        if out and all([resolution.value >= TimeResolution.HOURS.value,\n                        \":\" not in out, out[-1] != \"m\", hours > 0]):\n            # to \"1d 12h\"\n            out += \"h\"\n        out = out.strip()\n\n    if not out:\n        if _input_resolution == TimeResolution.YEARS:\n            out = \"zero years\" if speech else \"0y\"\n        elif _input_resolution == TimeResolution.DAYS:\n            out = \"zero days\" if speech else \"0d\"\n        elif _input_resolution == TimeResolution.HOURS:\n            out = \"zero hours\" if speech else \"0h\"\n        elif _input_resolution == TimeResolution.MINUTES:\n            if speech:\n                out = \"under a minute\" if seconds > 0 else \"zero minutes\"\n            else:\n                out = \"0m\"\n        else:\n            out = \"zero seconds\" if speech else \"0:00\"\n\n    return out```\n\nUpdated code:\n```python\ndef _duration_handler(time1, lang=None, speech=True, *, time2=None,\n                      use_years=True, clock=False,\n                      resolution=TimeResolution.SECONDS):\n     \n    \n    _leapdays = 0\n    _input_resolution = resolution\n    milliseconds = 0\n\n    type1 = type(time1)\n\n    if time2:\n        type2 = type(time2)\n        if type1 is not type2:\n            raise Exception(\"nice_duration(\" + str(time1) + \", \" +\n                            str(time2) + \"): \\n\\t can't \"\n                            \"combine data types: \" + str(type(time1)) +\n                            \" and \" + str(type(time2)))\n        elif type1 is datetime.datetime:\n            duration = time1 - time2\n            _leapdays = (abs(leapdays(time1.year, time2.year)))\n\n            # when operating on datetimes, refuse resolutions that\n            # would result in bunches of trailing zeroes\n            if all([time1.second == 0, time2.second == 0,\n                    resolution.value >= 5]):\n                resolution = TimeResolution.MINUTES\n            if all([time1.minute == 0, time2.minute == 0,\n                    resolution.value == 4]):\n                resolution = TimeResolution.HOURS\n            if all([time1.hour == 0, time2.hour == 0,\n                    resolution.value == 3]):\n                resolution = TimeResolution.DAYS\n\n        else:\n            _tmp = warnings.formatwarning\n            warnings.formatwarning = lambda msg, * \\\n                args, **kwargs: \"{}\\n\".format(msg)\n            warning = (\"WARN: mycroft.util.format.nice_duration_dt() can't \"\n                       \"subtract \" + str(type1) + \". Ignoring 2nd \"\n                       \"argument '\" + str(time2) + \"'.\")\n            warnings.warn(warning)\n            warnings.formatwarning = _tmp\n            duration = time1\n    else:\n        duration = time1\n\n    # Pull decimal portion of seconds, if present, to use for milliseconds\n    if isinstance(duration, float):\n        milliseconds = str(duration).split('.')[1]\n        if speech:\n            milliseconds = milliseconds[:2]\n        else:\n            milliseconds = milliseconds[:3]\n        milliseconds = float(\"0.\" + milliseconds)\n\n    # Cast duration to datetime.timedelta for human-friendliness\n    if not isinstance(duration, datetime.timedelta):\n        duration = datetime.timedelta(seconds=duration)\n\n    days = duration.days\n    if use_years:\n        days -= _leapdays if days > 365 else 0\n        years = days // 365\n    else:\n        years = 0\n    days = days % 365 if years > 0 else days\n\n    # We already stored milliseconds. Now we want the integer part.\n    seconds = duration.seconds\n    minutes = seconds // 60\n    seconds %= 60\n    hours = minutes // 60\n    minutes %= 60\n\n    if speech:\n        out = \"\"\n        if years > 0:\n            out += pronounce_number(years, lang) + \" \"\n            out += _translate_word(\"year\" if years == 1 else \"years\", lang)\n\n        if days > 0 and resolution.value > TimeResolution.YEARS.value:\n            if out:\n                out += \" \"\n            out += pronounce_number(days, lang) + \" \"\n            out += _translate_word(\"day\" if days == 1 else \"days\", lang)\n\n        if hours > 0 and resolution.value > TimeResolution.DAYS.value:\n            if out:\n                out += \" \"\n            out += pronounce_number(hours, lang) + \" \"\n            out += _translate_word(\"hour\" if hours == 1 else \"hours\", lang)\n\n        if minutes > 0 and resolution.value > TimeResolution.HOURS.value:\n            if out:\n                out += \" \"\n            out += pronounce_number(minutes, lang) + \" \"\n            out += _translate_word(\"minute\" if minutes ==\n                                   1 else \"minutes\", lang)\n\n        if ((seconds > 0 and resolution.value >=\n             TimeResolution.SECONDS.value) or\n            (milliseconds > 0 and resolution.value ==\n             TimeResolution.MILLISECONDS.value)):\n\n            if resolution.value == TimeResolution.MILLISECONDS.value:\n                seconds += milliseconds\n            if out:\n                out += \" \"\n                # Throw \"and\" between minutes and seconds if duration < 1 hour\n                if len(out.split()) > 3 or seconds < 1:\n                    out += _translate_word(\"and\", lang) + \" \"\n            # speaking \"zero point five seconds\" is better than \"point five\"\n            if seconds < 1:\n                out += pronounce_number(0, lang)\n            out += pronounce_number(seconds, lang) + \" \"\n            out += _translate_word(\"second\" if seconds ==\n                                   1 else \"seconds\", lang)\n\n    else:\n        # M:SS, MM:SS, H:MM:SS, Dd H:MM:SS format\n\n        _seconds_str = (\"0\" + str(seconds)) if seconds < 10 else str(seconds)\n\n        out = \"\"\n        if years > 0:\n            out = str(years) + \"y \"\n        if days > 0 and resolution.value > TimeResolution.YEARS.value:\n            out += str(days) + \"d \"\n        if (hours > 0 and resolution.value > TimeResolution.DAYS.value) or \\\n                (clock and resolution is TimeResolution.HOURS):\n            out += str(hours)\n\n        if resolution.value == TimeResolution.MINUTES.value and not clock:\n            out += ((\"h \" + str(minutes) + \"m\") if hours > 0\n                    else str(minutes) + \"m\")\n        elif (minutes > 0 and resolution.value > TimeResolution.HOURS.value) \\\n                or (clock and resolution.value >= TimeResolution.HOURS.value):\n            if hours != 0 or (clock and resolution is TimeResolution.HOURS):\n                out += \":\"\n                if minutes < 10:\n                    out += \"0\"\n            out += str(minutes) + \":\"\n            if (seconds > 0 and resolution.value >\n                    TimeResolution.MINUTES.value) or clock:\n                out += _seconds_str\n            else:\n                out += \"00\"\n        # if we have seconds but no minutes...\n        elif (seconds > 0 or clock) and resolution.value > \\\n                TimeResolution.MINUTES.value:\n            # check if output ends in hours\n            try:\n                if str(hours) == out.split()[-1]:\n                    out += \":\"\n            except IndexError:\n                pass\n            out += (\"00:\" if hours > 0 else \"0:\") + _seconds_str\n\n        if (milliseconds > 0 or clock) and resolution.value \\\n                == TimeResolution.MILLISECONDS.value:\n            _mill = str(milliseconds).split(\".\")[1]\n            # right-pad milliseconds to three decimal places\n            while len(_mill) < 3:\n                _mill += \"0\"\n            # make sure output < 1s still formats correctly\n            if out == \"\":\n                out = \"0:00\"\n            else:\n                if (str(hours) == out.split()[-1]) and \":\" not in out:\n                    out += \":00:00\"\n            # only append milliseconds to output that contains\n            # minutes and/or seconds\n            if \":\" in out:\n                out += \".\" + _mill\n\n        # If this evaluates True, out currently ends in hours: \"1d 12\"\n        if out and all([resolution.value >= TimeResolution.HOURS.value,\n                        \":\" not in out, out[-1] != \"m\", hours > 0]):\n            # to \"1d 12h\"\n            out += \"h\"\n        out = out.strip()\n\n    if not out:\n        if _input_resolution == TimeResolution.YEARS:\n            out = \"zero years\" if speech else \"0y\"\n        elif _input_resolution == TimeResolution.DAYS:\n            out = \"zero days\" if speech else \"0d\"\n        elif _input_resolution == TimeResolution.HOURS:\n            out = \"zero hours\" if speech else \"0h\"\n        elif _input_resolution == TimeResolution.MINUTES:\n            if speech:\n                out = \"under a minute\" if seconds > 0 else \"zero minutes\"\n            else:\n                out = \"0m\"\n        else:\n            out = \"zero seconds\" if speech else \"0:00\"\n\n    return out\n```\n\nOriginal documentation:\n```python\nConvert duration in seconds to a nice spoken timespan\n        Used as a handler by nice_duration and nice_duration_dt\n\n    Accepts:\n        datetime.timedelta, or\n        seconds (int/float), or\n        2 x datetime.datetime\n\n    Examples:\n       time1 = 60  ->  \"1:00\" or \"one minute\"\n       time1 = 163  ->  \"2:43\" or \"two minutes forty three seconds\"\n       time1 = timedelta(seconds=120)  ->  \"2:00\" or \"two minutes\"\n\n       time1 = datetime(2019, 3, 12),\n       time2 = datetime(2019, 1, 1)  ->  \"seventy days\"\n\n    Args:\n        time1: int/float seconds, OR datetime.timedelta, OR datetime.datetime\n        time2 (datetime, optional): subtracted from time1 if time1 is datetime\n        lang (str, optional): a BCP-47 language code, None for default\n        speech (bool, opt): format output for speech (True) or display (False)\n        use_years (bool, opt): rtn years and days if True, total days if False\n        resolution (mycroft.util.format.TimeResolution, optional): lower bound\n\n            mycroft.util.format.TimeResolution values:\n                TimeResolution.YEARS\n                TimeResolution.DAYS\n                TimeResolution.HOURS\n                TimeResolution.MINUTES\n                TimeResolution.SECONDS\n                TimeResolution.MILLISECONDS\n            NOTE: nice_duration will not produce milliseconds\n            unless that resolution is passed.\n\n    Returns:\n        str: timespan as a string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert duration in seconds to a nice spoken timespan\n        Used as a handler by nice_duration and nice_duration_dt\n\n    Accepts:\n        datetime.timedelta, or\n        seconds (int/float), or\n        2 x datetime.datetime\n\n    Examples:\n       time1 = 60  ->  \"1:00\" or \"one minute\"\n       time1 = 163  ->  \"2:43\" or \"two minutes forty three seconds\"\n       time1 = timedelta(seconds=120)  ->  \"2:00\" or \"two minutes\"\n\n       time1 = datetime(2019, 3, 12),\n       time2 = datetime(2019, 1, 1)  ->  \"seventy days\"\n\n    Args:\n        time1: int/float seconds, OR datetime.timedelta, OR datetime.datetime\n        time2 (datetime, optional): subtracted from time1 if time1 is datetime\n        lang (str, optional): a BCP-47 language code, None for default\n        speech (bool, opt): format output for speech (True) or display (False)\n        use_years (bool, opt): rtn years and days if True, total days if False\n        clock (bool, opt): always format output like digital clock (see below)\n        resolution (mycroft.util.format.TimeResolution, optional): lower bound\n\n            mycroft.util.format.TimeResolution values:\n                TimeResolution.YEARS\n                TimeResolution.DAYS\n                TimeResolution.HOURS\n                TimeResolution.MINUTES\n                TimeResolution.SECONDS\n                TimeResolution.MILLISECONDS\n            NOTE: nice_duration will not produce milliseconds\n            unless that resolution is passed.\n\n            NOTE: clock will produce digital clock-like output appropriate to\n            resolution. Has no effect on resolutions DAYS or YEARS. Only\n            applies to displayed output.\n\n    Returns:\n        str: timespan as a string", "file": "mycroft_util_format.py", "function": "_duration_handler", "project": "mycroft-core", "change_stats": {"code_changes": 23, "doc_changes": 7, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef skew(self, bias=False):\n        \n        \n        \n\n        from scipy.stats import skew\n        nona = remove_na(self.values())\n\n        if len(nona) < 3:\n            return NaN\n\n        theSkew = skew(nona, bias=bias)\n\n        # Hack for SciPy < 0.8\n        if isinstance(theSkew, ndarray):\n            theSkew = theSkew.item()\n\n        return theSkew```\n\nUpdated code:\n```python\ndef skew(self):\n        \n        \n        \n        y = np.array(self.values())\n        mask = notnull(y)\n        count = mask.sum()\n        y[-mask] = 0\n\n        A = y.sum() / count\n        B = (y**2).sum() / count  - A**2\n        C = (y**3).sum() / count - A**3 - 3*A*B\n\n        return (np.sqrt((count**2-count))*C) / ((count-2)*np.sqrt(B)**3)\n```\n\nOriginal documentation:\n```python\nComputes the skewness of the non-null values\n\n        Parameters\n        ----------\n        bias : bool\n            If False, then the calculations are corrected for\n            statistical bias.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes the skewness of the non-null values\n\n        Returns\n        -------\n        float", "file": "pandas_core_series.py", "function": "Series.skew", "project": "pandas", "change_stats": {"code_changes": 24, "doc_changes": 10, "total_changes": 34}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cert_time_to_seconds(cert_time):\n    \n\n    import time\n    return time.mktime(time.strptime(cert_time, \"%b %d %H:%M:%S %Y GMT\"))```\n\nUpdated code:\n```python\ndef cert_time_to_seconds(cert_time):\n    \n    \n    from time import strptime\n    from calendar import timegm\n\n    months = (\n        \"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n        \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"\n    )\n    time_format = ' %d %H:%M:%S %Y GMT' # NOTE: no month, fixed GMT\n    try:\n        month_number = months.index(cert_time[:3].title()) + 1\n    except ValueError:\n        raise ValueError('time data %r does not match '\n                         'format \"%%b%s\"' % (cert_time, time_format))\n    else:\n        # found valid month\n        tt = strptime(cert_time[3:], time_format)\n        # return an integer, the previous mktime()-based implementation\n        # returned a float (fractional seconds are always zero here).\n        return timegm((tt[0], month_number) + tt[2:6])\n```\n\nOriginal documentation:\n```python\nTakes a date-time string in standard ASN1_print form\n    (\"MON DAY 24HOUR:MINUTE:SEC YEAR TIMEZONE\") and return\n    a Python time value in seconds past the epoch.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the time in seconds since the Epoch, given the timestring\n    representing the \"notBefore\" or \"notAfter\" date from a certificate\n    in ``\"%b %d %H:%M:%S %Y %Z\"`` strptime format (C locale).\n\n    \"notBefore\" or \"notAfter\" dates must use UTC (RFC 5280).\n\n    Month is one of: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n    UTC should be specified as GMT (see ASN1_TIME_print())", "file": "Lib_ssl.py", "function": "cert_time_to_seconds", "project": "cpython", "change_stats": {"code_changes": 23, "doc_changes": 13, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _extract_proposal_features(self, preprocessed_inputs, scope):\n    \n    \n\n    preprocessed_inputs.get_shape().assert_has_rank(4)\n    shape_assert = tf.Assert(\n        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n        ['image size must at least be 33 in both height and width.'])\n\n    with tf.control_dependencies([shape_assert]):\n      with tf.variable_scope('InceptionV2',\n                             reuse=self._reuse_weights) as scope:\n        with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],\n                                   batch_norm_scale=True,\n                                   train_batch_norm=self._train_batch_norm):\n          _, activations = inception_v2.inception_v2_base(\n              preprocessed_inputs,\n              final_endpoint='Mixed_4e',\n              min_depth=self._min_depth,\n              depth_multiplier=self._depth_multiplier,\n              scope=scope)\n\n    return activations['Mixed_4e']```\n\nUpdated code:\n```python\ndef _extract_proposal_features(self, preprocessed_inputs, scope):\n    \n    \n\n    preprocessed_inputs.get_shape().assert_has_rank(4)\n    shape_assert = tf.Assert(\n        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n        ['image size must at least be 33 in both height and width.'])\n\n    with tf.control_dependencies([shape_assert]):\n      with tf.variable_scope('InceptionV2',\n                             reuse=self._reuse_weights) as scope:\n        with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],\n                                   batch_norm_scale=True,\n                                   train_batch_norm=self._train_batch_norm):\n          _, activations = inception_v2.inception_v2_base(\n              preprocessed_inputs,\n              final_endpoint='Mixed_4e',\n              min_depth=self._min_depth,\n              depth_multiplier=self._depth_multiplier,\n              scope=scope)\n\n    return activations['Mixed_4e'], activations\n```\n\nOriginal documentation:\n```python\nExtracts first stage RPN features.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n    Raises:\n      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n        (height or width) is less than 33.\n      ValueError: If the created network is missing the required activation.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extracts first stage RPN features.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n      activations: A dictionary mapping feature extractor tensor names to\n        tensors\n\n    Raises:\n      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n        (height or width) is less than 33.\n      ValueError: If the created network is missing the required activation.", "file": "research_object_detection_models_faster_rcnn_inception_v2_feature_extractor.py", "function": "FasterRCNNInceptionV2FeatureExtractor._extract_proposal_features", "project": "models", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef finalize_predictions(self, bounding_boxes_scales):\n          \n        ret = list()\n        batch_size = range(bounding_boxes_scales[0].shape[0])\n        for img in batch_size:\n            bboxlist = [scale[img:img+1] for scale in bounding_boxes_scales]\n            boxes = self._post_process(bboxlist)\n            bboxlist = self._nms(boxes, 0.5)\n            ret.append(bboxlist)\n        return ret```\n\nUpdated code:\n```python\ndef finalize_predictions(self, bounding_boxes_scales):\n         \n        \n        ret = list()\n        batch_size = range(bounding_boxes_scales[0].shape[0])\n        for img in batch_size:\n            bboxlist = [scale[img:img+1] for scale in bounding_boxes_scales]\n            boxes = self._post_process(bboxlist)\n            bboxlist = self._nms(boxes, 0.5)\n            ret.append(bboxlist)\n        return ret\n```\n\nOriginal documentation:\n```python\nDetect faces\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Process the output from the model to obtain faces\n\n        Parameters\n        ----------\n        bounding_boxes_scales: list\n            The output predictions from the S3FD model", "file": "plugins_extract_detect_s3fd.py", "function": "S3fd.finalize_predictions", "project": "faceswap", "change_stats": {"code_changes": 5, "doc_changes": 9, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, ax, labels, actives=None):\n        \n        \n        \n        super().__init__(ax)\n\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_navigate(False)\n\n        if actives is None:\n            actives = [False] * len(labels)\n\n        if len(labels) > 1:\n            dy = 1. / (len(labels) + 1)\n            ys = np.linspace(1 - dy, dy, len(labels))\n        else:\n            dy = 0.25\n            ys = [0.5]\n\n        axcolor = ax.get_facecolor()\n\n        self.labels = []\n        self.lines = []\n        self.rectangles = []\n\n        lineparams = {'color': 'k', 'linewidth': 1.25,\n                      'transform': ax.transAxes, 'solid_capstyle': 'butt'}\n        for y, label, active in zip(ys, labels, actives):\n            t = ax.text(0.25, y, label, transform=ax.transAxes,\n                        horizontalalignment='left',\n                        verticalalignment='center')\n\n            w, h = dy / 2, dy / 2\n            x, y = 0.05, y - h / 2\n\n            p = Rectangle(xy=(x, y), width=w, height=h, edgecolor='black',\n                          facecolor=axcolor, transform=ax.transAxes)\n\n            l1 = Line2D([x, x + w], [y + h, y], **lineparams)\n            l2 = Line2D([x, x + w], [y, y + h], **lineparams)\n\n            l1.set_visible(active)\n            l2.set_visible(active)\n            self.labels.append(t)\n            self.rectangles.append(p)\n            self.lines.append((l1, l2))\n            ax.add_patch(p)\n            ax.add_line(l1)\n            ax.add_line(l2)\n\n        self.connect_event('button_press_event', self._clicked)\n\n        self._observers = cbook.CallbackRegistry()```\n\nUpdated code:\n```python\ndef __init__(self, ax, labels, actives=None):\n        \n        \n        \n        super().__init__(ax)\n\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_navigate(False)\n\n        if actives is None:\n            actives = [False] * len(labels)\n\n        if len(labels) > 1:\n            dy = 1. / (len(labels) + 1)\n            ys = np.linspace(1 - dy, dy, len(labels))\n        else:\n            dy = 0.25\n            ys = [0.5]\n\n        axcolor = ax.get_facecolor()\n\n        self.labels = []\n        self.lines = []\n        self.rectangles = []\n\n        lineparams = {'color': 'k', 'linewidth': 1.25,\n                      'transform': ax.transAxes, 'solid_capstyle': 'butt'}\n        for y, label, active in zip(ys, labels, actives):\n            t = ax.text(0.25, y, label, transform=ax.transAxes,\n                        horizontalalignment='left',\n                        verticalalignment='center')\n\n            w, h = dy / 2, dy / 2\n            x, y = 0.05, y - h / 2\n\n            p = Rectangle(xy=(x, y), width=w, height=h, edgecolor='black',\n                          facecolor=axcolor, transform=ax.transAxes)\n\n            l1 = Line2D([x, x + w], [y + h, y], **lineparams)\n            l2 = Line2D([x, x + w], [y, y + h], **lineparams)\n\n            l1.set_visible(active)\n            l2.set_visible(active)\n            self.labels.append(t)\n            self.rectangles.append(p)\n            self.lines.append((l1, l2))\n            ax.add_patch(p)\n            ax.add_line(l1)\n            ax.add_line(l2)\n\n        self.connect_event('button_press_event', self._clicked)\n\n        self._observers = cbook.CallbackRegistry(signals=[\"clicked\"])\n```\n\nOriginal documentation:\n```python\nAdd check buttons to `matplotlib.axes.Axes` instance *ax*.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            The parent axes for the widget.\n\n        labels : list of str\n            The labels of the check buttons.\n\n        actives : list of bool, optional\n            The initial check states of the buttons. The list must have the\n            same length as *labels*. If not given, all buttons are unchecked.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add check buttons to `matplotlib.axes.Axes` instance *ax*.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            The parent Axes for the widget.\n\n        labels : list of str\n            The labels of the check buttons.\n\n        actives : list of bool, optional\n            The initial check states of the buttons. The list must have the\n            same length as *labels*. If not given, all buttons are unchecked.", "file": "lib_matplotlib_widgets.py", "function": "CheckButtons.__init__", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, x: Tensor, index: Optional[Tensor] = None,\n                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n                dim: int = -2) -> Tensor:\n        \n        \n        pass```\n\nUpdated code:\n```python\ndef forward(\n        self,\n        x: Tensor,\n        index: Optional[Tensor] = None,\n        ptr: Optional[Tensor] = None,\n        dim_size: Optional[int] = None,\n        dim: int = -2,\n        max_num_elements: Optional[int] = None,\n    ) -> Tensor:\n        \n        \n        pass\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Args:\n            x (torch.Tensor): The source tensor.\n            index (torch.Tensor, optional): The indices of elements for\n                applying the aggregation.\n                One of :obj:`index` or :obj:`ptr` must be defined.\n                (default: :obj:`None`)\n            ptr (torch.Tensor, optional): If given, computes the aggregation\n                based on sorted inputs in CSR representation.\n                One of :obj:`index` or :obj:`ptr` must be defined.\n                (default: :obj:`None`)\n            dim_size (int, optional): The size of the output tensor at\n                dimension :obj:`dim` after aggregation. (default: :obj:`None`)\n            dim (int, optional): The dimension in which to aggregate.\n                (default: :obj:`-2`)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Args:\n            x (torch.Tensor): The source tensor.\n            index (torch.Tensor, optional): The indices of elements for\n                applying the aggregation.\n                One of :obj:`index` or :obj:`ptr` must be defined.\n                (default: :obj:`None`)\n            ptr (torch.Tensor, optional): If given, computes the aggregation\n                based on sorted inputs in CSR representation.\n                One of :obj:`index` or :obj:`ptr` must be defined.\n                (default: :obj:`None`)\n            dim_size (int, optional): The size of the output tensor at\n                dimension :obj:`dim` after aggregation. (default: :obj:`None`)\n            dim (int, optional): The dimension in which to aggregate.\n                (default: :obj:`-2`)\n            max_num_elements: (int, optional): The maximum number of elements\n                within a single aggregation group. (default: :obj:`None`)", "file": "torch_geometric_nn_aggr_base.py", "function": "Aggregation.forward", "project": "pytorch_geometric", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_bounds(self, low, high):\n        \n        if self.spine_type == 'circle':\n            raise ValueError(\n                'set_bounds() method incompatible with circular spines')\n        self._bounds = (low, high)\n        self.stale = True```\n\nUpdated code:\n```python\ndef set_bounds(self, low=None, high=None):\n        \n        \n        \n        if self.spine_type == 'circle':\n            raise ValueError(\n                'set_bounds() method incompatible with circular spines')\n        if high is None and np.iterable(low):\n            low, high = low\n        old_low, old_high = self.get_bounds() or (None, None)\n        if low is None:\n            low = old_low\n        if high is None:\n            high = old_high\n        self._bounds = (low, high)\n        self.stale = True\n```\n\nOriginal documentation:\n```python\nSet the bounds of the spine.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the spine bounds.\n\n        .. ACCEPTS: (low: float, high: float)\n\n        Parameters\n        ----------\n        low : scalar, optional\n            The lower spine bound. Passing *None* leaves the limit unchanged.\n\n            The bounds may also be passed as the tuple (*low*, *high*) as the\n            first positional argument.\n\n        high : scalar, optional\n            The higher spine bound. Passing *None* leaves the limit unchanged.", "file": "lib_matplotlib_spines.py", "function": "Spine.set_bounds", "project": "matplotlib", "change_stats": {"code_changes": 13, "doc_changes": 17, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _DetectVisualStudioVersions(versions_to_check, force_express):\n  \n  \n  version_to_year = {\n      '8.0': '2005',\n      '9.0': '2008',\n      '10.0': '2010',\n      '11.0': '2012',\n      '12.0': '2013',\n  }\n  versions = []\n  for version in versions_to_check:\n    # Old method of searching for which VS version is installed\n    # We don't use the 2010-encouraged-way because we also want to get the\n    # path to the binaries, which it doesn't offer.\n    keys = [r'HKLM\\Software\\Microsoft\\VisualStudio\\%s' % version,\n            r'HKLM\\Software\\Wow6432Node\\Microsoft\\VisualStudio\\%s' % version,\n            r'HKLM\\Software\\Microsoft\\VCExpress\\%s' % version,\n            r'HKLM\\Software\\Wow6432Node\\Microsoft\\VCExpress\\%s' % version]\n    for index in range(len(keys)):\n      path = _RegistryGetValue(keys[index], 'InstallDir')\n      if not path:\n        continue\n      path = _ConvertToCygpath(path)\n      # Check for full.\n      full_path = os.path.join(path, 'devenv.exe')\n      express_path = os.path.join(path, '*express.exe')\n      if not force_express and os.path.exists(full_path):\n        # Add this one.\n        versions.append(_CreateVersion(version_to_year[version],\n            os.path.join(path, '..', '..')))\n      # Check for express.\n      elif glob.glob(express_path):\n        # Add this one.\n        versions.append(_CreateVersion(version_to_year[version] + 'e',\n            os.path.join(path, '..', '..')))\n\n    # The old method above does not work when only SDK is installed.\n    keys = [r'HKLM\\Software\\Microsoft\\VisualStudio\\SxS\\VC7',\n            r'HKLM\\Software\\Wow6432Node\\Microsoft\\VisualStudio\\SxS\\VC7']\n    for index in range(len(keys)):\n      path = _RegistryGetValue(keys[index], version)\n      if not path:\n        continue\n      path = _ConvertToCygpath(path)\n      versions.append(_CreateVersion(version_to_year[version] + 'e',\n          os.path.join(path, '..'), sdk_based=True))\n\n  return versions```\n\nUpdated code:\n```python\ndef _DetectVisualStudioVersions(versions_to_check, force_express):\n  \n  \n  version_to_year = {\n      '8.0': '2005',\n      '9.0': '2008',\n      '10.0': '2010',\n      '11.0': '2012',\n      '12.0': '2013',\n      '14.0': '2015',\n  }\n  versions = []\n  for version in versions_to_check:\n    # Old method of searching for which VS version is installed\n    # We don't use the 2010-encouraged-way because we also want to get the\n    # path to the binaries, which it doesn't offer.\n    keys = [r'HKLM\\Software\\Microsoft\\VisualStudio\\%s' % version,\n            r'HKLM\\Software\\Wow6432Node\\Microsoft\\VisualStudio\\%s' % version,\n            r'HKLM\\Software\\Microsoft\\VCExpress\\%s' % version,\n            r'HKLM\\Software\\Wow6432Node\\Microsoft\\VCExpress\\%s' % version]\n    for index in range(len(keys)):\n      path = _RegistryGetValue(keys[index], 'InstallDir')\n      if not path:\n        continue\n      path = _ConvertToCygpath(path)\n      # Check for full.\n      full_path = os.path.join(path, 'devenv.exe')\n      express_path = os.path.join(path, '*express.exe')\n      if not force_express and os.path.exists(full_path):\n        # Add this one.\n        versions.append(_CreateVersion(version_to_year[version],\n            os.path.join(path, '..', '..')))\n      # Check for express.\n      elif glob.glob(express_path):\n        # Add this one.\n        versions.append(_CreateVersion(version_to_year[version] + 'e',\n            os.path.join(path, '..', '..')))\n\n    # The old method above does not work when only SDK is installed.\n    keys = [r'HKLM\\Software\\Microsoft\\VisualStudio\\SxS\\VC7',\n            r'HKLM\\Software\\Wow6432Node\\Microsoft\\VisualStudio\\SxS\\VC7']\n    for index in range(len(keys)):\n      path = _RegistryGetValue(keys[index], version)\n      if not path:\n        continue\n      path = _ConvertToCygpath(path)\n      if version != '14.0':  # There is no Express edition for 2015.\n        versions.append(_CreateVersion(version_to_year[version] + 'e',\n            os.path.join(path, '..'), sdk_based=True))\n\n  return versions\n```\n\nOriginal documentation:\n```python\nCollect the list of installed visual studio versions.\n\n  Returns:\n    A list of visual studio versions installed in descending order of\n    usage preference.\n    Base this on the registry and a quick check if devenv.exe exists.\n    Only versions 8-10 are considered.\n    Possibilities are:\n      2005(e) - Visual Studio 2005 (8)\n      2008(e) - Visual Studio 2008 (9)\n      2010(e) - Visual Studio 2010 (10)\n      2012(e) - Visual Studio 2012 (11)\n      2013(e) - Visual Studio 2013 (11)\n    Where (e) is e for express editions of MSVS and blank otherwise.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Collect the list of installed visual studio versions.\n\n  Returns:\n    A list of visual studio versions installed in descending order of\n    usage preference.\n    Base this on the registry and a quick check if devenv.exe exists.\n    Only versions 8-10 are considered.\n    Possibilities are:\n      2005(e) - Visual Studio 2005 (8)\n      2008(e) - Visual Studio 2008 (9)\n      2010(e) - Visual Studio 2010 (10)\n      2012(e) - Visual Studio 2012 (11)\n      2013(e) - Visual Studio 2013 (12)\n      2015    - Visual Studio 2015 (14)\n    Where (e) is e for express editions of MSVS and blank otherwise.", "file": "gyp_pylib_gyp_MSVSVersion.py", "function": "_DetectVisualStudioVersions", "project": "node-gyp", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef from_sql(\n        sql: Union[str, \"sqlalchemy.sql.Selectable\"],\n        con: str,\n        features: Optional[Features] = None,\n        cache_dir: str = None,\n        keep_in_memory: bool = False,\n        **kwargs,\n    ):\n        \n        \n        from .io.sql import SqlDatasetReader\n\n        return SqlDatasetReader(\n            sql,\n            con,\n            features=features,\n            cache_dir=cache_dir,\n            keep_in_memory=keep_in_memory,\n            **kwargs,\n        ).read()```\n\nUpdated code:\n```python\ndef from_sql(\n        sql: Union[str, \"sqlalchemy.sql.Selectable\"],\n        con: Union[str, \"sqlalchemy.engine.Connection\", \"sqlalchemy.engine.Engine\", \"sqlite3.Connection\"],\n        features: Optional[Features] = None,\n        cache_dir: str = None,\n        keep_in_memory: bool = False,\n        **kwargs,\n    ):\n        \n        \n        from .io.sql import SqlDatasetReader\n\n        return SqlDatasetReader(\n            sql,\n            con,\n            features=features,\n            cache_dir=cache_dir,\n            keep_in_memory=keep_in_memory,\n            **kwargs,\n        ).read()\n```\n\nOriginal documentation:\n```python\nCreate Dataset from SQL query or database table.\n\n        Args:\n            sql (`str` or :obj:`sqlalchemy.sql.Selectable`): SQL query to be executed or a table name.\n            con (`str`): A connection URI string used to instantiate a database connection.\n            features (:class:`Features`, optional): Dataset features.\n            cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n            keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n            **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`SqlConfig`.\n\n        Returns:\n            :class:`Dataset`\n\n        Example:\n\n        ```py\n        >>> # Fetch a database table\n        >>> ds = Dataset.from_sql(\"test_data\", \"postgres:///db_name\")\n        >>> # Execute a SQL query on the table\n        >>> ds = Dataset.from_sql(\"SELECT sentence FROM test_data\", \"postgres:///db_name\")\n        >>> # Use a Selectable object to specify the query\n        >>> from sqlalchemy import select, text\n        >>> stmt = select([text(\"sentence\")]).select_from(text(\"test_data\"))\n        >>> ds = Dataset.from_sql(stmt, \"postgres:///db_name\")\n        ```\n\n        <Tip {warning=true}>\n        `sqlalchemy` needs to be installed to use this function.\n        </Tip>\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create Dataset from SQL query or database table.\n\n        Args:\n            sql (`str` or :obj:`sqlalchemy.sql.Selectable`): SQL query to be executed or a table name.\n            con (`str` or :obj:`sqlite3.Connection` or :obj:`sqlalchemy.engine.Connection` or :obj:`sqlalchemy.engine.Connection`):\n                A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.\n            features (:class:`Features`, optional): Dataset features.\n            cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n            keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n            **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`SqlConfig`.\n\n        Returns:\n            :class:`Dataset`\n\n        Example:\n\n        ```py\n        >>> # Fetch a database table\n        >>> ds = Dataset.from_sql(\"test_data\", \"postgres:///db_name\")\n        >>> # Execute a SQL query on the table\n        >>> ds = Dataset.from_sql(\"SELECT sentence FROM test_data\", \"postgres:///db_name\")\n        >>> # Use a Selectable object to specify the query\n        >>> from sqlalchemy import select, text\n        >>> stmt = select([text(\"sentence\")]).select_from(text(\"test_data\"))\n        >>> ds = Dataset.from_sql(stmt, \"postgres:///db_name\")\n        ```\n\n        <Tip {warning=true}>\n        The returned dataset can only be cached if `con` is specified as URI string.\n        </Tip>", "file": "src_datasets_arrow_dataset.py", "function": "Dataset.from_sql", "project": "datasets", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sharded_model(self) -> Generator:\n        \n        \n        if isinstance(self._strategy, _Sharded):\n            with self._strategy.module_sharded_context():\n                yield\n        else:\n            yield```\n\nUpdated code:\n```python\ndef sharded_model(self) -> Generator:\n        \n        \n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\n        with _old_sharded_model_context(self._strategy):\n            yield\n```\n\nOriginal documentation:\n```python\nShard the parameters of the model instantly when instantiating the layers.\n\n        Use this context manager with strategies that support sharding the model parameters to save peak memory usage.\n\n        Example::\n\n            with self.sharded_model():\n                model = MyModel()\n\n        The context manager is strategy-agnostic and for the ones that don't do sharding, it is a no-op.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Shard the parameters of the model instantly when instantiating the layers.\n\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.", "file": "src_lightning_fabric_fabric.py", "function": "Fabric.sharded_model", "project": "pytorch-lightning", "change_stats": {"code_changes": 8, "doc_changes": 11, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fit(self, X, Y):\n        \n        \n        # copy since this will contains the centered data\n        check_consistent_length(X, Y)\n        X = self._validate_data(X, dtype=np.float64, copy=self.copy,\n                                ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        if self.n_components > max(Y.shape[1], X.shape[1]):\n            raise ValueError(\"Invalid number of components n_components=%d\"\n                             \" with X of shape %s and Y of shape %s.\"\n                             % (self.n_components, str(X.shape), str(Y.shape)))\n\n        # Scale (in place)\n        X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_ = (\n            _center_scale_xy(X, Y, self.scale))\n        # svd(X'Y)\n        C = np.dot(X.T, Y)\n\n        # The arpack svds solver only works if the number of extracted\n        # components is smaller than rank(X) - 1. Hence, if we want to extract\n        # all the components (C.shape[1]), we have to use another one. Else,\n        # let's use arpacks to compute only the interesting components.\n        if self.n_components >= np.min(C.shape):\n            U, s, V = svd(C, full_matrices=False)\n        else:\n            U, s, V = svds(C, k=self.n_components)\n        # Deterministic output\n        U, V = svd_flip(U, V)\n        V = V.T\n        self.x_scores_ = np.dot(X, U)\n        self.y_scores_ = np.dot(Y, V)\n        self.x_weights_ = U\n        self.y_weights_ = V\n        return self```\n\nUpdated code:\n```python\ndef fit(self, X, Y):\n        \n        \n        check_consistent_length(X, Y)\n        X = self._validate_data(X, dtype=np.float64, copy=self.copy,\n                                ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(Y)\n        # This matrix rank is at most min(n_samples, n_features, n_targets) so\n        # n_components cannot be bigger than that.\n        n_components = self.n_components\n        rank_upper_bound = min(X.shape[0], X.shape[1], Y.shape[1])\n        if not 1 <= n_components <= rank_upper_bound:\n            # TODO: raise an error in 0.26\n            warnings.warn(\n                f\"As of version 0.24, n_components({n_components}) should be \"\n                f\"in [1, min(n_features, n_samples, n_targets)] = \"\n                f\"[1, {rank_upper_bound}]. \"\n                f\"n_components={rank_upper_bound} will be used instead. \"\n                f\"In version 0.26, an error will be raised.\",\n                FutureWarning\n            )\n            n_components = rank_upper_bound\n\n        X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_ = (\n            _center_scale_xy(X, Y, self.scale))\n\n        # Compute SVD of cross-covariance matrix\n        C = np.dot(X.T, Y)\n        U, s, Vt = svd(C, full_matrices=False)\n        U = U[:, :n_components]\n        Vt = Vt[:n_components]\n        U, Vt = svd_flip(U, Vt)\n        V = Vt.T\n\n        self._x_scores = np.dot(X, U)  # TODO: remove in 0.26\n        self._y_scores = np.dot(Y, V)  # TODO: remove in 0.26\n        self.x_weights_ = U\n        self.y_weights_ = V\n        return self\n```\n\nOriginal documentation:\n```python\nFit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        Y : array-like of shape (n_samples, n_targets)\n            Target vectors, where n_samples is the number of samples and\n            n_targets is the number of response variables.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Targets", "file": "sklearn_cross_decomposition__pls.py", "function": "PLSSVD.fit", "project": "scikit-learn", "change_stats": {"code_changes": 47, "doc_changes": 10, "total_changes": 57}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_unsharded_model(self, model: nn.Module, checkpoint: str, strict: bool = True):\n        \n        \n        \n        # the model should be unwrapped in self.load_model via ModelWrapper.unwrap\n        return super().load_unsharded_model(model, checkpoint, strict=strict)```\n\nUpdated code:\n```python\ndef load_unsharded_model(self, model: ModelWrapper, checkpoint: str, strict: bool = True):\n        \n        \n        \n        assert isinstance(model, ModelWrapper), \"Please boost the model before loading!\"\n        super().load_unsharded_model(model.unwrap(), checkpoint, strict=strict)\n```\n\nOriginal documentation:\n```python\nLoad model from checkpoint with automatic unwrapping.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load model from checkpoint.", "file": "colossalai_booster_plugin_torch_ddp_plugin.py", "function": "TorchDDPCheckpointIO.load_unsharded_model", "project": "ColossalAI", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_traces(self, model, guide, *args, **kwargs):\n        \n        \n        \n        # enable parallel enumeration\n        guide = poutine.enum(guide, first_available_dim=self.max_iarange_nesting)\n\n        for i in range(self.num_particles):\n            for guide_trace in iter_discrete_traces(\"flat\", guide, *args, **kwargs):\n                model_trace = poutine.trace(poutine.replay(model, trace=guide_trace),\n                                            graph_type=\"flat\").get_trace(*args, **kwargs)\n\n                if is_validation_enabled():\n                    check_model_guide_match(model_trace, guide_trace, self.max_iarange_nesting)\n                guide_trace = prune_subsample_sites(guide_trace)\n                model_trace = prune_subsample_sites(model_trace)\n                if is_validation_enabled():\n                    check_traceenum_requirements(model_trace, guide_trace)\n\n                model_trace.compute_log_prob()\n                guide_trace.compute_score_parts()\n                if is_validation_enabled():\n                    for site in model_trace.nodes.values():\n                        if site[\"type\"] == \"sample\":\n                            check_site_shape(site, self.max_iarange_nesting)\n                    any_enumerated = False\n                    for site in guide_trace.nodes.values():\n                        if site[\"type\"] == \"sample\":\n                            check_site_shape(site, self.max_iarange_nesting)\n                            if site[\"infer\"].get(\"enumerate\"):\n                                any_enumerated = True\n                    if self.strict_enumeration_warning and not any_enumerated:\n                        warnings.warn('TraceEnum_ELBO found no sample sites configured for enumeration. '\n                                      'If you want to enumerate sites, you need to @config_enumerate or set '\n                                      'infer={\"enumerate\": \"sequential\"} or infer={\"enumerate\": \"parallel\"}? '\n                                      'If you do not want to enumerate, consider using Trace_ELBO instead.')\n\n                yield model_trace, guide_trace```\n\nUpdated code:\n```python\ndef _get_traces(self, model, guide, *args, **kwargs):\n        \n        \n        \n        if self.vectorize_particles:\n            # enable parallel enumeration over the vectorized guide.\n            guide = poutine.enum(self._vectorized_num_particles(guide),\n                                 first_available_dim=self.max_iarange_nesting)\n            model = self._vectorized_num_particles(model)\n            for model_trace, guide_trace in self._get_trace(model, guide, *args, **kwargs):\n                yield model_trace, guide_trace\n        else:\n            # enable parallel enumeration.\n            guide = poutine.enum(guide, first_available_dim=self.max_iarange_nesting)\n            for i in range(self.num_particles):\n                for model_trace, guide_trace in self._get_trace(model, guide, *args, **kwargs):\n                    yield model_trace, guide_trace\n```\n\nOriginal documentation:\n```python\nruns the guide and runs the model against the guide with\n        the result packaged as a trace generator\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Runs the guide and runs the model against the guide with\n        the result packaged as a trace generator.", "file": "pyro_infer_traceenum_elbo.py", "function": "TraceEnum_ELBO._get_traces", "project": "pyro", "change_stats": {"code_changes": 47, "doc_changes": 6, "total_changes": 53}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fill_leverage_tiers(self) -> None:\n        \n        \n        \n        if self._api.has['fetchLeverageTiers']:\n            if self.trading_mode == TradingMode.FUTURES:\n                leverage_tiers = self.load_leverage_tiers()\n                for pair, tiers in leverage_tiers.items():\n                    tiers = []\n                    for tier in tiers:\n                        tiers.append(self.parse_leverage_tier(tier))\n                    self._leverage_tiers[pair] = tiers```\n\nUpdated code:\n```python\ndef fill_leverage_tiers(self) -> None:\n        \n        \n        \n        leverage_tiers = self.load_leverage_tiers()\n        for pair, tiers in leverage_tiers.items():\n            tiers = []\n            for tier in tiers:\n                tiers.append(self.parse_leverage_tier(tier))\n            self._leverage_tiers[pair] = tiers\n```\n\nOriginal documentation:\n```python\nAssigns property _leverage_tiers to a dictionary of information about the leverage\n        allowed on each pair\n        After exectution, self._leverage_tiers = {\n            \"pair_name\": [\n                [notional_floor, maintenenace_margin_ratio, maintenance_amt],\n                ...\n            ],\n            ...\n        }\n        e.g. {\n            \"ETH/USDT:USDT\": [\n                [0.0, 0.01, 0.0],\n                [10000, 0.02, 0.01],\n                ...\n            ],\n            ...\n        }\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Assigns property _leverage_tiers to a dictionary of information about the leverage\n        allowed on each pair", "file": "freqtrade_exchange_exchange.py", "function": "Exchange.fill_leverage_tiers", "project": "freqtrade", "change_stats": {"code_changes": 16, "doc_changes": 17, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __setattr__(self, key, val):\n        \n        if key == MESSAGE_ROUTE_CAUSE_BY:\n            self.set_cause_by(val)\n            return\n        if key == MESSAGE_ROUTE_FROM:\n            self.set_from(any_to_str(val))\n        super().__setattr__(key, val)```\n\nUpdated code:\n```python\ndef __setattr__(self, key, val):\n        \n        if key == MESSAGE_ROUTE_CAUSE_BY:\n            new_val = any_to_str(val)\n        elif key == MESSAGE_ROUTE_FROM:\n            new_val = any_to_str(val)\n        elif key == MESSAGE_ROUTE_TO:\n            new_val = any_to_str_set(val)\n        else:\n            new_val = val\n        super().__setattr__(key, new_val)\n```\n\nOriginal documentation:\n```python\nOverride `@property.setter`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Override `@property.setter`, convert non-string parameters into string parameters.", "file": "metagpt_schema.py", "function": "Message.__setattr__", "project": "MetaGPT", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_absolute_path(cls, settings, path):\n        \n        \n        root = settings[\"static_path\"]\n        abspath = os.path.abspath(os.path.join(root, path))\n        return abspath```\n\nUpdated code:\n```python\ndef get_absolute_path(cls, root, path):\n        \n        \n        abspath = os.path.abspath(os.path.join(root, path))\n        return abspath\n```\n\nOriginal documentation:\n```python\nReturns the absolute location of ``path``.\n\n        This class method may be overridden in subclasses.  By default\n        it returns a filesystem path, but other strings may be used\n        as long as they are unique and understood by the subclass's\n        overridden `get_content`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the absolute location of ``path`` relative to ``root``.\n\n        ``root`` is the path configured for this `StaticFileHandler`\n        (in most cases the ``static_path`` `Application` setting).\n\n        This class method may be overridden in subclasses.  By default\n        it returns a filesystem path, but other strings may be used\n        as long as they are unique and understood by the subclass's\n        overridden `get_content`.", "file": "tornado_web.py", "function": "StaticFileHandler.get_absolute_path", "project": "tornado", "change_stats": {"code_changes": 5, "doc_changes": 7, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef search(self, charset, *criteria):\n        \n        \n        name = 'SEARCH'\n        if charset:\n            typ, dat = self._simple_command(name, 'CHARSET', charset, *criteria)\n        else:\n            typ, dat = self._simple_command(name, *criteria)\n        return self._untagged_response(typ, dat, name)```\n\nUpdated code:\n```python\ndef search(self, charset, *criteria):\n        \n        \n        name = 'SEARCH'\n        if charset:\n            if self.utf8_enabled:\n                raise IMAP4.error(\"Non-None charset not valid in UTF8 mode\")\n            typ, dat = self._simple_command(name, 'CHARSET', charset, *criteria)\n        else:\n            typ, dat = self._simple_command(name, *criteria)\n        return self._untagged_response(typ, dat, name)\n```\n\nOriginal documentation:\n```python\nSearch mailbox for matching messages.\n\n        (typ, [data]) = <instance>.search(charset, criterion, ...)\n\n        'data' is space separated list of matching message numbers.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Search mailbox for matching messages.\n\n        (typ, [data]) = <instance>.search(charset, criterion, ...)\n\n        'data' is space separated list of matching message numbers.\n        If UTF8 is enabled, charset MUST be None.", "file": "Lib_imaplib.py", "function": "IMAP4.search", "project": "cpython", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _make_skel_func(code, cell_count, base_globals=None):\n     \n    \n    # This function is deprecated and should be removed in cloudpickle 1.7\n    warnings.warn(\n        \"A pickle file created using an old (<=1.4.1) version of cloudpickle \"\n        \"is currently being loaded. This is not supported by cloudpickle and \"\n        \"will break in cloudpickle 1.7\", category=UserWarning\n    )\n    # This is backward-compatibility code: for cloudpickle versions between\n    # 0.5.4 and 0.7, base_globals could be a string or None. base_globals\n    # should now always be a dictionary.\n    if base_globals is None or isinstance(base_globals, str):\n        base_globals = {}\n\n    base_globals['__builtins__'] = __builtins__\n\n    closure = (\n        tuple(_make_empty_cell() for _ in range(cell_count))\n        if cell_count >= 0 else\n        None\n    )\n    return types.FunctionType(code, base_globals, None, None, closure)```\n\nUpdated code:\n```python\ndef _make_skel_func(code, cell_count, base_globals=None):\n    \n    \n    # This function is deprecated and should be removed in cloudpickle 1.7\n    warnings.warn(\n        \"A pickle file created using an old (<=1.4.1) version of cloudpickle \"\n        \"is currently being loaded. This is not supported by cloudpickle and \"\n        \"will break in cloudpickle 1.7\",\n        category=UserWarning,\n    )\n    # This is backward-compatibility code: for cloudpickle versions between\n    # 0.5.4 and 0.7, base_globals could be a string or None. base_globals\n    # should now always be a dictionary.\n    if base_globals is None or isinstance(base_globals, str):\n        base_globals = {}\n\n    base_globals[\"__builtins__\"] = __builtins__\n\n    closure = (\n        tuple(_make_empty_cell() for _ in range(cell_count))\n        if cell_count >= 0\n        else None\n    )\n    return types.FunctionType(code, base_globals, None, None, closure)\n```\n\nOriginal documentation:\n```python\nCreates a skeleton function object that contains just the provided\n        code and the correct number of cells in func_closure.  All other\n        func attributes (e.g. func_globals) are empty.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates a skeleton function object that contains just the provided\n    code and the correct number of cells in func_closure.  All other\n    func attributes (e.g. func_globals) are empty.", "file": "python_ray_cloudpickle_cloudpickle.py", "function": "_make_skel_func", "project": "ray", "change_stats": {"code_changes": 13, "doc_changes": 6, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_orientation(image: Image.Image) -> int:\n    \n    \n    \n    exif_data = image.getexif() if hasattr(image, 'getexif') else image._getexif()\n    if exif_data is not None:\n        orientation = exif_data.get(274) # 274 corresponds to the orientation tag in EXIF\n        if orientation is not None:\n            return orientation```\n\nUpdated code:\n```python\ndef get_orientation(image: Image) -> int:\n    \n    \n    \n    exif_data = image.getexif() if hasattr(image, 'getexif') else image._getexif()\n    if exif_data is not None:\n        orientation = exif_data.get(274) # 274 corresponds to the orientation tag in EXIF\n        if orientation is not None:\n            return orientation\n```\n\nOriginal documentation:\n```python\nGets the orientation of the given image.\n\n    Args:\n        image (Image.Image): The image.\n\n    Returns:\n        int: The orientation value.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets the orientation of the given image.\n\n    Args:\n        image (Image): The image.\n\n    Returns:\n        int: The orientation value.", "file": "g4f_image.py", "function": "get_orientation", "project": "gpt4free", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef stories_to_yaml(\n        self, story_steps: List[StoryStep], is_test_story: bool = False\n    ) -> Dict[Text, Any]:\n        \n        \n        from rasa.shared.utils.validation import KEY_TRAINING_DATA_FORMAT_VERSION\n\n        self._is_test_story = is_test_story\n\n        stories = []\n        rules = []\n        for story_step in story_steps:\n            if isinstance(story_step, RuleStep):\n                rules.append(self.process_rule_step(story_step))\n            else:\n                stories.append(self.process_story_step(story_step))\n\n        result = OrderedDict()\n        result[KEY_TRAINING_DATA_FORMAT_VERSION] = DoubleQuotedScalarString(\n            LATEST_TRAINING_DATA_FORMAT_VERSION\n        )\n\n        if stories:\n            result[KEY_STORIES] = stories\n        if rules:\n            result[KEY_RULES] = rules\n        return result```\n\nUpdated code:\n```python\ndef stories_to_yaml(\n        self, story_steps: List[StoryStep], is_test_story: bool = False\n    ) -> Dict[Text, Any]:\n        \n        \n        from rasa.shared.utils.validation import KEY_TRAINING_DATA_FORMAT_VERSION\n\n        self._is_test_story = is_test_story\n\n        stories = []\n        rules = []\n        for story_step in story_steps:\n            if isinstance(story_step, RuleStep):\n                rules.append(self.process_rule_step(story_step))\n            else:\n                stories.append(self.process_story_step(story_step))\n\n        result = OrderedDict()\n        result[KEY_TRAINING_DATA_FORMAT_VERSION] = DoubleQuotedScalarString(\n            LATEST_TRAINING_DATA_FORMAT_VERSION\n        )\n\n        if stories:\n            result[KEY_STORIES] = stories\n        if rules:\n            result[KEY_RULES] = rules\n\n        return result\n```\n\nOriginal documentation:\n```python\nConverts a sequence of story steps into yaml format.\n\n        Args:\n            story_steps: Original story steps to be converted to the YAML.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Converts a sequence of story steps into yaml format.\n\n        Args:\n            story_steps: Original story steps to be converted to the YAML.\n            is_test_story: `True` if the story is an end-to-end conversation test story.", "file": "rasa_shared_core_training_data_story_writer_yaml_story_writer.py", "function": "YAMLStoryWriter.stories_to_yaml", "project": "rasa", "change_stats": {"code_changes": 3, "doc_changes": 3, "total_changes": 6}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _diff(path: Path) -> Tuple[Set[str], Set[str]]:\n        \n        \n        ext_map = PackageBuilder._read(path)\n\n        add: Set[str] = set()\n        remove: Set[str] = set()\n        groups = OpenBBGroups.groups()\n\n        for g in groups:\n            built = set(ext_map.get(g, {}))\n            installed = set(\n                f\"{e.name}@{getattr(e.dist, 'version', '')}\"\n                for e in entry_points(group=g)\n            )\n            add = add.union(installed - built)\n            remove = remove.union(built - installed)\n\n        return add, remove```\n\nUpdated code:\n```python\ndef _diff(ext_map: Dict[str, List[str]]) -> Tuple[Set[str], Set[str]]:\n        \n        \n        add: Set[str] = set()\n        remove: Set[str] = set()\n        groups = OpenBBGroups.groups()\n\n        for g in groups:\n            built = set(ext_map.get(g, {}))\n            installed = set(\n                f\"{e.name}@{getattr(e.dist, 'version', '')}\"\n                for e in entry_points(group=g)\n            )\n            add = add.union(installed - built)\n            remove = remove.union(built - installed)\n\n        return add, remove\n```\n\nOriginal documentation:\n```python\nCheck differences between built and installed extensions.\n\n        Parameters\n        ----------\n        path: Path\n            The path to the folder where the extension map is stored.\n\n        Returns\n        -------\n        Tuple[Set[str], Set[str]]\n            First element: set of installed extensions that are not in the package.\n            Second element: set of extensions in the package that are not installed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check differences between built and installed extensions.\n\n        Parameters\n        ----------\n        ext_map: Dict[str, List[str]]\n            Dictionary containing the extensions.\n            Example:\n                {\n                    \"openbb_core_extension\": [\n                        \"commodity@1.0.1\",\n                        ...\n                    ],\n                    \"openbb_provider_extension\": [\n                        \"benzinga@1.1.3\",\n                        ...\n                    ],\n                    \"openbb_obbject_extension\": [\n                        \"openbb_charting@1.0.0\",\n                        ...\n                    ]\n                }\n\n        Returns\n        -------\n        Tuple[Set[str], Set[str]]\n            First element: set of installed extensions that are not in the package.\n            Second element: set of extensions in the package that are not installed.", "file": "openbb_platform_core_openbb_core_app_static_package_builder.py", "function": "PackageBuilder._diff", "project": "OpenBBTerminal", "change_stats": {"code_changes": 6, "doc_changes": 21, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef elastic_project_search(request, project_slug):\n    \n    \n    \n    queryset = Project.objects.protected(request.user)\n    project = get_object_or_404(queryset, slug=project_slug)\n    version_slug = request.GET.get('version', LATEST)\n    query = request.GET.get('q', None)\n    if query:\n        user = ''\n        if request.user.is_authenticated():\n            user = request.user\n        log.info(LOG_TEMPLATE.format(\n            user=user,\n            project=project or '',\n            type='inproject',\n            version=version_slug or '',\n            language='',\n            msg=query or '',\n        ))\n\n    if query:\n\n        kwargs = {}\n        body = {\n            \"query\": {\n                \"bool\": {\n                    \"should\": [\n                        {\"match\": {\"title\": {\"query\": query, \"boost\": 10}}},\n                        {\"match\": {\"headers\": {\"query\": query, \"boost\": 5}}},\n                        {\"match\": {\"content\": {\"query\": query}}},\n                    ]\n                }\n            },\n            \"highlight\": {\n                \"fields\": {\n                    \"title\": {},\n                    \"headers\": {},\n                    \"content\": {},\n                }\n            },\n            \"fields\": [\"title\", \"project\", \"version\", \"path\"],\n            \"filter\": {\n                \"and\": [\n                    {\"term\": {\"project\": project_slug}},\n                    {\"term\": {\"version\": version_slug}},\n                ]\n            },\n            \"size\": 50  # TODO: Support pagination.\n        }\n\n        # Add routing to optimize search by hitting the right shard.\n        kwargs['routing'] = project_slug\n\n        results = PageIndex().search(body, **kwargs)\n    else:\n        results = {}\n\n    if results:\n        # pre and post 1.0 compat\n        for num, hit in enumerate(results['hits']['hits']):\n            for key, val in hit['fields'].items():\n                if isinstance(val, list):\n                    results['hits']['hits'][num]['fields'][key] = val[0]\n\n    return render_to_response(\n        'search/elastic_project_search.html',\n        {\n            'project': project,\n            'query': query,\n            'results': results,\n        },\n        context_instance=RequestContext(request),\n    )```\n\nUpdated code:\n```python\ndef elastic_project_search(request, project_slug):\n    \n    queryset = Project.objects.protected(request.user)\n    project = get_object_or_404(queryset, slug=project_slug)\n    version_slug = request.GET.get('version', LATEST)\n    query = request.GET.get('q', None)\n    if query:\n        user = ''\n        if request.user.is_authenticated():\n            user = request.user\n        log.info(LOG_TEMPLATE.format(\n            user=user,\n            project=project or '',\n            type='inproject',\n            version=version_slug or '',\n            language='',\n            msg=query or '',\n        ))\n\n    if query:\n\n        kwargs = {}\n        body = {\n            \"query\": {\n                \"bool\": {\n                    \"should\": [\n                        {\"match\": {\"title\": {\"query\": query, \"boost\": 10}}},\n                        {\"match\": {\"headers\": {\"query\": query, \"boost\": 5}}},\n                        {\"match\": {\"content\": {\"query\": query}}},\n                    ]\n                }\n            },\n            \"highlight\": {\n                \"fields\": {\n                    \"title\": {},\n                    \"headers\": {},\n                    \"content\": {},\n                }\n            },\n            \"fields\": [\"title\", \"project\", \"version\", \"path\"],\n            \"filter\": {\n                \"and\": [\n                    {\"term\": {\"project\": project_slug}},\n                    {\"term\": {\"version\": version_slug}},\n                ]\n            },\n            \"size\": 50  # TODO: Support pagination.\n        }\n\n        # Add routing to optimize search by hitting the right shard.\n        kwargs['routing'] = project_slug\n\n        results = PageIndex().search(body, **kwargs)\n    else:\n        results = {}\n\n    if results:\n        # pre and post 1.0 compat\n        for num, hit in enumerate(results['hits']['hits']):\n            for key, val in hit['fields'].items():\n                if isinstance(val, list):\n                    results['hits']['hits'][num]['fields'][key] = val[0]\n\n    return render_to_response(\n        'search/elastic_project_search.html',\n        {\n            'project': project,\n            'query': query,\n            'results': results,\n        },\n        context_instance=RequestContext(request),\n    )\n```\n\nOriginal documentation:\n```python\nUse elastic search to search in a project.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Use elastic search to search in a project", "file": "readthedocs_projects_views_public.py", "function": "elastic_project_search", "project": "readthedocs.org", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _set_flow(detector: Optional[str],\n                  aligner: Optional[str],\n                  masker: List[Optional[str]]) -> List[str]:\n         \n        \n        logger.debug(\"detector: %s, aligner: %s, masker: %s\", detector, aligner, masker)\n        retval = []\n        if detector is not None and detector.lower() != \"none\":\n            retval.append(\"detect\")\n        if aligner is not None and aligner.lower() != \"none\":\n            retval.append(\"align\")\n        retval.extend([f\"mask_{idx}\"\n                       for idx, mask in enumerate(masker)\n                       if mask is not None and mask.lower() != \"none\"])\n        logger.debug(\"flow: %s\", retval)\n        return retval```\n\nUpdated code:\n```python\ndef _set_flow(detector: Optional[str],\n                  aligner: Optional[str],\n                  masker: List[Optional[str]],\n                  recognition: Optional[str]) -> List[str]:\n         \n        \n        logger.debug(\"detector: %s, aligner: %s, masker: %s recognition: %s\",\n                     detector, aligner, masker, recognition)\n        retval = []\n        if detector is not None and detector.lower() != \"none\":\n            retval.append(\"detect\")\n        if aligner is not None and aligner.lower() != \"none\":\n            retval.append(\"align\")\n        if recognition is not None and recognition.lower() != \"none\":\n            retval.append(\"recognition\")\n        retval.extend([f\"mask_{idx}\"\n                       for idx, mask in enumerate(masker)\n                       if mask is not None and mask.lower() != \"none\"])\n        logger.debug(\"flow: %s\", retval)\n        return retval\n```\n\nOriginal documentation:\n```python\nSet the flow list based on the input plugins\n\n        Parameters\n        ----------\n        detector: str or ``None``\n            The name of a detector plugin as exists in :mod:`plugins.extract.detect`\n        aligner: str or ``None\n            The name of an aligner plugin as exists in :mod:`plugins.extract.align`\n        masker: str or list or ``None\n            The name of a masker plugin(s) as exists in :mod:`plugins.extract.mask`.\n            This can be a single masker or a list of multiple maskers\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the flow list based on the input plugins\n\n        Parameters\n        ----------\n        detector: str or ``None``\n            The name of a detector plugin as exists in :mod:`plugins.extract.detect`\n        aligner: str or ``None\n            The name of an aligner plugin as exists in :mod:`plugins.extract.align`\n        masker: str or list or ``None\n            The name of a masker plugin(s) as exists in :mod:`plugins.extract.mask`.\n            This can be a single masker or a list of multiple maskers\n        recognition: str or ``None``\n            The name of the recognition plugin to use. ``None`` to not do face recognition.", "file": "plugins_extract_pipeline.py", "function": "Extractor._set_flow", "project": "faceswap", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __truediv__(self, other):\n        \n        \n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef __truediv__(self, other):\n        \n        raise NotImplementedError\n```\n\nOriginal documentation:\n```python\nself / other with __future__ division.\n\n        Should promote to float when necessary.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "self / other: Should promote to float when necessary.", "file": "Lib_numbers.py", "function": "Complex.__truediv__", "project": "cpython", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef full_inference(\n        self,\n        text,\n        ref_audio_path,\n        language,\n        # GPT inference\n        temperature=0.65,\n        length_penalty=1,\n        repetition_penalty=2.0,\n        top_k=50,\n        top_p=0.85,\n        do_sample=True,\n        # Cloning\n        gpt_cond_len=6,\n        max_ref_len=10,\n        sound_norm_refs=False,\n        **hf_generate_kwargs,\n    ):\n        \n        \n        \n        (gpt_cond_latent, speaker_embedding) = self.get_conditioning_latents(\n            audio_path=ref_audio_path,\n            gpt_cond_len=gpt_cond_len,\n            max_ref_length=max_ref_len,\n            sound_norm_refs=sound_norm_refs,\n        )\n\n        return self.inference(\n            text,\n            language,\n            gpt_cond_latent,\n            speaker_embedding,\n            temperature=temperature,\n            length_penalty=length_penalty,\n            repetition_penalty=repetition_penalty,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=do_sample,\n            **hf_generate_kwargs,\n        )```\n\nUpdated code:\n```python\ndef full_inference(\n        self,\n        text,\n        ref_audio_path,\n        language,\n        # GPT inference\n        temperature=0.65,\n        length_penalty=1,\n        repetition_penalty=2.0,\n        top_k=50,\n        top_p=0.85,\n        do_sample=True,\n        # Cloning\n        gpt_cond_len=30,\n        gpt_cond_chunk_len=6,\n        max_ref_len=10,\n        sound_norm_refs=False,\n        **hf_generate_kwargs,\n    ):\n        \n        \n        \n        (gpt_cond_latent, speaker_embedding) = self.get_conditioning_latents(\n            audio_path=ref_audio_path,\n            gpt_cond_len=gpt_cond_len,\n            gpt_cond_chunk_len=gpt_cond_chunk_len,\n            max_ref_length=max_ref_len,\n            sound_norm_refs=sound_norm_refs,\n        )\n\n        return self.inference(\n            text,\n            language,\n            gpt_cond_latent,\n            speaker_embedding,\n            temperature=temperature,\n            length_penalty=length_penalty,\n            repetition_penalty=repetition_penalty,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=do_sample,\n            **hf_generate_kwargs,\n        )\n```\n\nOriginal documentation:\n```python\nThis function produces an audio clip of the given text being spoken with the given reference voice.\n\n        Args:\n            text: (str) Text to be spoken.\n\n            ref_audio_path: (str) Path to a reference audio file to be used for cloning. This audio file should be >3\n                seconds long.\n\n            language: (str) Language of the voice to be generated.\n\n            temperature: (float) The softmax temperature of the autoregressive model. Defaults to 0.65.\n\n            length_penalty: (float) A length penalty applied to the autoregressive decoder. Higher settings causes the\n                model to produce more terse outputs. Defaults to 1.0.\n\n            repetition_penalty: (float) A penalty that prevents the autoregressive decoder from repeating itself during\n                decoding. Can be used to reduce the incidence of long silences or \"uhhhhhhs\", etc. Defaults to 2.0.\n\n            top_k: (int) K value used in top-k sampling. [0,inf]. Lower values mean the decoder produces more \"likely\"\n                (aka boring) outputs. Defaults to 50.\n\n            top_p: (float) P value used in nucleus sampling. (0,1]. Lower values mean the decoder produces more \"likely\"\n                (aka boring) outputs. Defaults to 0.8.\n\n            gpt_cond_len: (int) Length of the audio used for cloning. If audio is shorter, then audio length is used\n                else the first `gpt_cond_len` secs is used. Defaults to 6 seconds.\n\n            hf_generate_kwargs: (**kwargs) The huggingface Transformers generate API is used for the autoregressive\n                transformer. Extra keyword args fed to this function get forwarded directly to that API. Documentation\n                here: https://huggingface.co/docs/transformers/internal/generation_utils\n\n        Returns:\n            Generated audio clip(s) as a torch tensor. Shape 1,S if k=1 else, (k,1,S) where S is the sample length.\n            Sample rate is 24kHz.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This function produces an audio clip of the given text being spoken with the given reference voice.\n\n        Args:\n            text: (str) Text to be spoken.\n\n            ref_audio_path: (str) Path to a reference audio file to be used for cloning. This audio file should be >3\n                seconds long.\n\n            language: (str) Language of the voice to be generated.\n\n            temperature: (float) The softmax temperature of the autoregressive model. Defaults to 0.65.\n\n            length_penalty: (float) A length penalty applied to the autoregressive decoder. Higher settings causes the\n                model to produce more terse outputs. Defaults to 1.0.\n\n            repetition_penalty: (float) A penalty that prevents the autoregressive decoder from repeating itself during\n                decoding. Can be used to reduce the incidence of long silences or \"uhhhhhhs\", etc. Defaults to 2.0.\n\n            top_k: (int) K value used in top-k sampling. [0,inf]. Lower values mean the decoder produces more \"likely\"\n                (aka boring) outputs. Defaults to 50.\n\n            top_p: (float) P value used in nucleus sampling. (0,1]. Lower values mean the decoder produces more \"likely\"\n                (aka boring) outputs. Defaults to 0.8.\n\n            gpt_cond_len: (int) Length of the audio used for cloning. If audio is shorter, then audio length is used\n                else the first `gpt_cond_len` secs is used. Defaults to 30 seconds.\n\n            gpt_cond_chunk_len: (int) Chunk length used for cloning. It must be <= `gpt_cond_len`.\n                If gpt_cond_len == gpt_cond_chunk_len, no chunking. Defaults to 6 seconds.\n\n            hf_generate_kwargs: (**kwargs) The huggingface Transformers generate API is used for the autoregressive\n                transformer. Extra keyword args fed to this function get forwarded directly to that API. Documentation\n                here: https://huggingface.co/docs/transformers/internal/generation_utils\n\n        Returns:\n            Generated audio clip(s) as a torch tensor. Shape 1,S if k=1 else, (k,1,S) where S is the sample length.\n            Sample rate is 24kHz.", "file": "TTS_tts_models_xtts.py", "function": "Xtts.full_inference", "project": "TTS", "change_stats": {"code_changes": 6, "doc_changes": 7, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _initial_imputation(self, X, in_fit=False):\n        \n        \n        if is_scalar_nan(self.missing_values):\n            force_all_finite = \"allow-nan\"\n        else:\n            force_all_finite = True\n\n        X = self._validate_data(\n            X,\n            dtype=FLOAT_DTYPES,\n            order=\"F\",\n            reset=in_fit,\n            force_all_finite=force_all_finite,\n        )\n        _check_inputs_dtype(X, self.missing_values)\n\n        X_missing_mask = _get_mask(X, self.missing_values)\n        mask_missing_values = X_missing_mask.copy()\n        if self.initial_imputer_ is None:\n            self.initial_imputer_ = SimpleImputer(\n                missing_values=self.missing_values, strategy=self.initial_strategy\n            )\n            X_filled = self.initial_imputer_.fit_transform(X)\n        else:\n            X_filled = self.initial_imputer_.transform(X)\n\n        valid_mask = np.flatnonzero(\n            np.logical_not(np.isnan(self.initial_imputer_.statistics_))\n        )\n        Xt = X[:, valid_mask]\n        mask_missing_values = mask_missing_values[:, valid_mask]\n\n        return Xt, X_filled, mask_missing_values, X_missing_mask```\n\nUpdated code:\n```python\ndef _initial_imputation(self, X, in_fit=False):\n        \n        \n        if is_scalar_nan(self.missing_values):\n            force_all_finite = \"allow-nan\"\n        else:\n            force_all_finite = True\n\n        X = self._validate_data(\n            X,\n            dtype=FLOAT_DTYPES,\n            order=\"F\",\n            reset=in_fit,\n            force_all_finite=force_all_finite,\n        )\n        _check_inputs_dtype(X, self.missing_values)\n\n        X_missing_mask = _get_mask(X, self.missing_values)\n        mask_missing_values = X_missing_mask.copy()\n        if self.initial_imputer_ is None:\n            self.initial_imputer_ = SimpleImputer(\n                missing_values=self.missing_values,\n                strategy=self.initial_strategy,\n                keep_empty_features=self.keep_empty_features,\n            )\n            X_filled = self.initial_imputer_.fit_transform(X)\n        else:\n            X_filled = self.initial_imputer_.transform(X)\n\n        valid_mask = np.flatnonzero(\n            np.logical_not(np.isnan(self.initial_imputer_.statistics_))\n        )\n\n        if not self.keep_empty_features:\n            # drop empty features\n            Xt = X[:, valid_mask]\n            mask_missing_values = mask_missing_values[:, valid_mask]\n        else:\n            # mark empty features as not missing and keep the original\n            # imputation\n            mask_missing_values[:, valid_mask] = True\n            Xt = X\n\n        return Xt, X_filled, mask_missing_values, X_missing_mask\n```\n\nOriginal documentation:\n```python\nPerform initial imputation for input `X`.\n\n        Parameters\n        ----------\n        X : ndarray, shape (n_samples, n_features)\n            Input data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        in_fit : bool, default=False\n            Whether function is called in :meth:`fit`.\n\n        Returns\n        -------\n        Xt : ndarray, shape (n_samples, n_features)\n            Input data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        X_filled : ndarray, shape (n_samples, n_features)\n            Input data with the most recent imputations.\n\n        mask_missing_values : ndarray, shape (n_samples, n_features)\n            Input data's missing indicator matrix, where `n_samples` is the\n            number of samples and `n_features` is the number of features.\n\n        X_missing_mask : ndarray, shape (n_samples, n_features)\n            Input data's mask matrix indicating missing datapoints, where\n            `n_samples` is the number of samples and `n_features` is the\n            number of features.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform initial imputation for input `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Input data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        in_fit : bool, default=False\n            Whether function is called in :meth:`fit`.\n\n        Returns\n        -------\n        Xt : ndarray of shape (n_samples, n_features)\n            Input data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        X_filled : ndarray of shape (n_samples, n_features)\n            Input data with the most recent imputations.\n\n        mask_missing_values : ndarray of shape (n_samples, n_features)\n            Input data's missing indicator matrix, where `n_samples` is the\n            number of samples and `n_features` is the number of features,\n            masked by non-missing features.\n\n        X_missing_mask : ndarray, shape (n_samples, n_features)\n            Input data's mask matrix indicating missing datapoints, where\n            `n_samples` is the number of samples and `n_features` is the\n            number of features.", "file": "sklearn_impute__iterative.py", "function": "IterativeImputer._initial_imputation", "project": "scikit-learn", "change_stats": {"code_changes": 18, "doc_changes": 13, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef borrow_margin(self, code: str, amount, symbol: Str = None, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        request = {\n            'coin': currency['info']['coinName'],\n            'borrowAmount': self.currency_to_precision(code, amount),\n        }\n        response = None\n        marginMode = None\n        marginMode, params = self.handle_margin_mode_and_params('borrowMargin', params)\n        if (symbol is not None) or (marginMode == 'isolated'):\n            self.check_required_symbol('borrowMargin', symbol)\n            market = self.market(symbol)\n            marketId = market['id']\n            parts = marketId.split('_')\n            marginMarketId = self.safe_string_upper(parts, 0)\n            request['symbol'] = marginMarketId\n            response = self.privateMarginPostMarginV1IsolatedAccountBorrow(self.extend(request, params))\n        else:\n            response = self.privateMarginPostMarginV1CrossAccountBorrow(self.extend(request, params))\n        #\n        # isolated\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697250952516,\n        #         \"data\": {\n        #             \"clientOid\": null,\n        #             \"symbol\": \"BTCUSDT\",\n        #             \"coin\": \"BTC\",\n        #             \"borrowAmount\": \"0.001\"\n        #         }\n        #     }\n        #\n        # cross\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697251314271,\n        #         \"data\": {\n        #             \"clientOid\": null,\n        #             \"coin\": \"BTC\",\n        #             \"borrowAmount\": \"0.0001\"\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data', {})\n        return self.parse_margin_loan(data, currency)```\n\nUpdated code:\n```python\ndef borrow_margin(self, code: str, amount, symbol: Str = None, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        request = {\n            'coin': currency['info']['coinName'],\n            'borrowAmount': self.currency_to_precision(code, amount),\n        }\n        response = None\n        marginMode = None\n        marginMode, params = self.handle_margin_mode_and_params('borrowMargin', params)\n        if (symbol is not None) or (marginMode == 'isolated'):\n            if symbol is None:\n                raise ArgumentsRequired(self.id + ' borrowMargin() requires a symbol argument')\n            market = self.market(symbol)\n            marketId = market['id']\n            parts = marketId.split('_')\n            marginMarketId = self.safe_string_upper(parts, 0)\n            request['symbol'] = marginMarketId\n            response = self.privateMarginPostMarginV1IsolatedAccountBorrow(self.extend(request, params))\n        else:\n            response = self.privateMarginPostMarginV1CrossAccountBorrow(self.extend(request, params))\n        #\n        # isolated\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697250952516,\n        #         \"data\": {\n        #             \"clientOid\": null,\n        #             \"symbol\": \"BTCUSDT\",\n        #             \"coin\": \"BTC\",\n        #             \"borrowAmount\": \"0.001\"\n        #         }\n        #     }\n        #\n        # cross\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697251314271,\n        #         \"data\": {\n        #             \"clientOid\": null,\n        #             \"coin\": \"BTC\",\n        #             \"borrowAmount\": \"0.0001\"\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data', {})\n        return self.parse_margin_loan(data, currency)\n```\n\nOriginal documentation:\n```python\ncreate a loan to borrow margin\n        :see: https://bitgetlimited.github.io/apidoc/en/margin/#cross-borrow\n        :see: https://bitgetlimited.github.io/apidoc/en/margin/#isolated-borrow\n        :param str code: unified currency code of the currency to borrow\n        :param str amount: the amount to borrow\n        :param str [symbol]: unified market symbol\n        :param dict [params]: extra parameters specific to the bitget api endpoint\n        :param str [params.marginMode]: 'isolated' or 'cross', symbol is required for 'isolated'\n        :returns dict: a `margin loan structure <https://github.com/ccxt/ccxt/wiki/Manual#margin-loan-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "create a loan to borrow margin\n        :see: https://bitgetlimited.github.io/apidoc/en/margin/#cross-borrow\n        :see: https://bitgetlimited.github.io/apidoc/en/margin/#isolated-borrow\n        :param str code: unified currency code of the currency to borrow\n        :param str amount: the amount to borrow\n        :param str [symbol]: unified market symbol\n        :param dict [params]: extra parameters specific to the bitget api endpoint\n        :param str [params.marginMode]: 'isolated' or 'cross', symbol is required for 'isolated'\n        :returns dict: a `margin loan structure <https://docs.ccxt.com/#/?id=margin-loan-structure>`", "file": "python_ccxt_bitget.py", "function": "bitget.borrow_margin", "project": "ccxt", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef broken_barh(self, xranges, yrange, **kwargs):\n        \n        \n\n        \n        # process the unit information\n        if len(xranges):\n            xdata = cbook.safe_first_element(xranges)\n        else:\n            xdata = None\n        if len(yrange):\n            ydata = cbook.safe_first_element(yrange)\n        else:\n            ydata = None\n        self._process_unit_info(xdata=xdata,\n                                ydata=ydata,\n                                kwargs=kwargs)\n        xranges_conv = []\n        for xr in xranges:\n            if len(xr) != 2:\n                raise ValueError('each range in xrange must be a sequence '\n                                 'with two elements (i.e. an Nx2 array)')\n            # convert the absolute values, not the x and dx...\n            x_conv = np.asarray(self.convert_xunits(xr[0]))\n            x1 = self._convert_dx(xr[1], xr[0], x_conv, self.convert_xunits)\n            xranges_conv.append((x_conv, x1))\n\n        yrange_conv = self.convert_yunits(yrange)\n\n        col = mcoll.BrokenBarHCollection(xranges_conv, yrange_conv, **kwargs)\n        self.add_collection(col, autolim=True)\n        self.autoscale_view()\n\n        return col```\n\nUpdated code:\n```python\ndef broken_barh(self, xranges, yrange, **kwargs):\n        \n        \n        \n        # process the unit information\n        if len(xranges):\n            xdata = cbook.safe_first_element(xranges)\n        else:\n            xdata = None\n        if len(yrange):\n            ydata = cbook.safe_first_element(yrange)\n        else:\n            ydata = None\n        self._process_unit_info(xdata=xdata,\n                                ydata=ydata,\n                                kwargs=kwargs)\n        xranges_conv = []\n        for xr in xranges:\n            if len(xr) != 2:\n                raise ValueError('each range in xrange must be a sequence '\n                                 'with two elements (i.e. an Nx2 array)')\n            # convert the absolute values, not the x and dx...\n            x_conv = np.asarray(self.convert_xunits(xr[0]))\n            x1 = self._convert_dx(xr[1], xr[0], x_conv, self.convert_xunits)\n            xranges_conv.append((x_conv, x1))\n\n        yrange_conv = self.convert_yunits(yrange)\n\n        col = mcoll.BrokenBarHCollection(xranges_conv, yrange_conv, **kwargs)\n        self.add_collection(col, autolim=True)\n        self.autoscale_view()\n\n        return col\n```\n\nOriginal documentation:\n```python\nPlot a horizontal sequence of rectangles.\n\n        A rectangle is drawn for each element of *xranges*. All rectangles\n        have the same vertical position and size defined by *yrange*.\n\n        This is a convenience function for instantiating a\n        `.BrokenBarHCollection`, adding it to the axes and autoscaling the\n        view.\n\n        Parameters\n        ----------\n        xranges : sequence of tuples (*xmin*, *xwidth*)\n            The x-positions and extends of the rectangles. For each tuple\n            (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n            *xwidth*.\n        yrange : (*ymin*, *yheight*)\n            The y-position and extend for all the rectangles.\n\n        Other Parameters\n        ----------------\n        **kwargs : :class:`.BrokenBarHCollection` properties\n\n            Each *kwarg* can be either a single argument applying to all\n            rectangles, e.g.::\n\n                facecolors='black'\n\n            or a sequence of arguments over which is cycled, e.g.::\n\n                facecolors=('black', 'blue')\n\n            would create interleaving black and blue rectangles.\n\n            Supported keywords:\n\n            %(BrokenBarHCollection)s\n\n        Returns\n        -------\n        collection : A :class:`~.collections.BrokenBarHCollection`\n\n        Notes\n        -----\n        .. [Notes section required for data comment. See #10189.]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Plot a horizontal sequence of rectangles.\n\n        A rectangle is drawn for each element of *xranges*. All rectangles\n        have the same vertical position and size defined by *yrange*.\n\n        This is a convenience function for instantiating a\n        `.BrokenBarHCollection`, adding it to the axes and autoscaling the\n        view.\n\n        Parameters\n        ----------\n        xranges : sequence of tuples (*xmin*, *xwidth*)\n            The x-positions and extends of the rectangles. For each tuple\n            (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n            *xwidth*.\n        yrange : (*ymin*, *yheight*)\n            The y-position and extend for all the rectangles.\n\n        Other Parameters\n        ----------------\n        **kwargs : :class:`.BrokenBarHCollection` properties\n\n            Each *kwarg* can be either a single argument applying to all\n            rectangles, e.g.::\n\n                facecolors='black'\n\n            or a sequence of arguments over which is cycled, e.g.::\n\n                facecolors=('black', 'blue')\n\n            would create interleaving black and blue rectangles.\n\n            Supported keywords:\n\n            %(BrokenBarHCollection)s\n\n        Returns\n        -------\n        collection : A :class:`~.collections.BrokenBarHCollection`", "file": "lib_matplotlib_axes__axes.py", "function": "Axes.broken_barh", "project": "matplotlib", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef blur(\n        self,\n        fn: Callable,\n        inputs: Component | List[Component] | Set[Component] | None = None,\n        outputs: Component | List[Component] | None = None,\n        api_name: AnyStr = None,\n        scroll_to_output: bool = False,\n        show_progress: bool = True,\n        queue: Optional[bool] = None,\n        batch: bool = False,\n        max_batch_size: int = 4,\n        preprocess: bool = True,\n        postprocess: bool = True,\n        cancels: Dict[str, Any] | List[Dict[str, Any]] | None = None,\n        _js: Optional[str] = None,\n    ):\n        \n        \n        \n        # _js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components.\n\n        self.set_event_trigger(\n            \"blur\",\n            fn,\n            inputs,\n            outputs,\n            api_name=api_name,\n            scroll_to_output=scroll_to_output,\n            show_progress=show_progress,\n            js=_js,\n            preprocess=preprocess,\n            postprocess=postprocess,\n            queue=queue,\n            batch=batch,\n            max_batch_size=max_batch_size,\n        )\n        set_cancel_events(self, \"blur\", cancels)```\n\nUpdated code:\n```python\ndef blur(\n        self,\n        fn: Callable,\n        inputs: Component | List[Component] | Set[Component] | None = None,\n        outputs: Component | List[Component] | None = None,\n        api_name: AnyStr = None,\n        scroll_to_output: bool = False,\n        show_progress: bool = True,\n        queue: Optional[bool] = None,\n        batch: bool = False,\n        max_batch_size: int = 4,\n        preprocess: bool = True,\n        postprocess: bool = True,\n        cancels: Dict[str, Any] | List[Dict[str, Any]] | None = None,\n        every: float | None = None,\n        _js: Optional[str] = None,\n    ):\n        \n        \n        \n        # _js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components.\n\n        self.set_event_trigger(\n            \"blur\",\n            fn,\n            inputs,\n            outputs,\n            preprocess=preprocess,\n            postprocess=postprocess,\n            scroll_to_output=scroll_to_output,\n            show_progress=show_progress,\n            api_name=api_name,\n            js=_js,\n            queue=queue,\n            batch=batch,\n            max_batch_size=max_batch_size,\n            every=every,\n        )\n        set_cancel_events(self, \"blur\", cancels)\n```\n\nOriginal documentation:\n```python\nThis event is triggered when the component's is unfocused/blurred (e.g. when the user clicks outside of a textbox). This method can be used when this component is in a Gradio Blocks.\n\n        Parameters:\n            fn: Callable function\n            inputs: List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.\n            outputs: List of gradio.components to use as inputs. If the function returns no outputs, this should be an empty list.\n            api_name: Defining this parameter exposes the endpoint in the api docs\n            scroll_to_output: If True, will scroll to output component on completion\n            show_progress: If True, will show progress animation while pending\n            queue: If True, will place the request on the queue, if the queue exists\n            batch: If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.\n            max_batch_size: Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)\n            preprocess: If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).\n            postprocess: If False, will not run postprocessing of component data before returning 'fn' output to the browser.\n            cancels: A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This event is triggered when the component's is unfocused/blurred (e.g. when the user clicks outside of a textbox). This method can be used when this component is in a Gradio Blocks.\n\n        Parameters:\n            fn: Callable function\n            inputs: List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.\n            outputs: List of gradio.components to use as inputs. If the function returns no outputs, this should be an empty list.\n            api_name: Defining this parameter exposes the endpoint in the api docs\n            scroll_to_output: If True, will scroll to output component on completion\n            show_progress: If True, will show progress animation while pending\n            queue: If True, will place the request on the queue, if the queue exists\n            batch: If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.\n            max_batch_size: Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)\n            preprocess: If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component).\n            postprocess: If False, will not run postprocessing of component data before returning 'fn' output to the browser.\n            cancels: A list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.\n            every: Run this event 'every' number of seconds. Interpreted in seconds. Queue must be enabled.", "file": "gradio_events.py", "function": "Blurrable.blur", "project": "gradio", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_token_type_ids_from_sequences(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -> List[int]:\n        \n        \n        \n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep + token_ids_1 + sep) * [0]```\n\nUpdated code:\n```python\ndef create_token_type_ids_from_sequences(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -> List[int]:\n        \n        \n        \n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n```\n\nOriginal documentation:\n```python\nCreate a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n        sequence pair mask has the following format:\n\n        ```\n        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n        | first sequence    | second sequence |\n        ```\n\n        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n\n        Args:\n            token_ids_0 (`List[int]`):\n                List of IDs.\n            token_ids_1 (`List[int]`, *optional*):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n        sequence pair mask has the following format:\n\n        ```\n        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n        | first sequence    | second sequence |\n        ```\n\n        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n\n        Args:\n            token_ids_0 (`List[int]`):\n                List of IDs.\n            token_ids_1 (`List[int]`, *optional*):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).", "file": "src_transformers_models_deberta_tokenization_deberta_fast.py", "function": "DebertaTokenizerFast.create_token_type_ids_from_sequences", "project": "transformers", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_fields():\n    \"\"\n    fields = Metadata.model_fields.keys()\n    assert \"arguments\" in fields\n    assert \"duration\" in fields\n    assert \"route\" in fields\n    assert \"timestamp\" in fields```\n\nUpdated code:\n```python\ndef test_fields():\n    \n    fields = Metadata.model_fields.keys()\n    assert \"arguments\" in fields\n    assert \"duration\" in fields\n    assert \"route\" in fields\n    assert \"timestamp\" in fields\n```\n\nOriginal documentation:\n```python\nSmoke test\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run Smoke test.", "file": "openbb_platform_core_tests_app_model_test_metadata.py", "function": "test_fields", "project": "OpenBBTerminal", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_login(self, login):\n        \n        try:\n            statement = self.connection.cursor()\n            statement.execute(\"INSERT OR REPLACE INTO %s (id, username, token) \"\n                              \"VALUES (?, ?, ?)\" % USER_TABLE,\n                              (\"login\", login[0], login[1]))\n            self.connection.commit()\n        except Exception as e:\n            raise ConanException(\"Could not store credentials\", e)```\n\nUpdated code:\n```python\ndef set_login(self, login, remote_url):\n        \n        try:\n            statement = self.connection.cursor()\n            statement.execute(\"INSERT OR REPLACE INTO %s (remote_url, user, token) \"\n                              \"VALUES (?, ?, ?)\" % REMOTES_USER_TABLE,\n                              (remote_url, login[0], login[1]))\n            self.connection.commit()\n        except Exception as e:\n            raise ConanException(\"Could not store credentials %s\" % str(e))\n```\n\nOriginal documentation:\n```python\nLogin is a tuple of (login, token)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Login is a tuple of (user, token)", "file": "conans_client_store_localdb.py", "function": "LocalDB.set_login", "project": "conan", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_link(self, name, link):\n        \n\n        \n        warnings.warn('''\\\nChild link registeration via Chain.__init__ and Chain.add_link are deprecated.\nAssign a Link object directly to an attribute within a \\\n\"with link.init_scope():\" block instead.\n        ''', DeprecationWarning)\n        if name in self.__dict__:\n            raise AttributeError(\n                'cannot register a new link %s: attribute exists' % name)\n        if not isinstance(link, Link):\n            raise TypeError('cannot register a non-link object as a child')\n        with self.init_scope():\n            setattr(self, name, link)```\n\nUpdated code:\n```python\ndef add_link(self, name, link):\n        \n\n        \n        if name in self.__dict__:\n            raise AttributeError(\n                'cannot register a new link %s: attribute exists' % name)\n        if not isinstance(link, Link):\n            raise TypeError('cannot register a non-link object as a child')\n        with self.init_scope():\n            setattr(self, name, link)\n```\n\nOriginal documentation:\n```python\nRegisters a child link to this chain.\n\n        .. deprecated:: v2.0.0\n\n           Assign the child link directly to an attribute within\n           :meth:`~chainer.Chain.init_scope` instead.\n           For example, the following code\n\n           .. code-block:: python\n\n              chain.add_link('l1', L.Linear(3, 5))\n\n           can be replaced by the following line.\n\n           .. code-block:: python\n\n              with chain.init_scope():\n                  chain.l1 = L.Linear(3, 5)\n\n           The latter is easier for IDEs to keep track of the attribute's\n           type.\n\n        Args:\n            name (str): Name of the child link. This name is also used as the\n                attribute name.\n            link (Link): The link object to be registered.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Registers a child link to this chain.\n\n        Args:\n            name (str): Name of the child link. This name is also used as the\n                attribute name.\n            link (Link): The link object to be registered.", "file": "chainer_link.py", "function": "Chain.add_link", "project": "chainer", "change_stats": {"code_changes": 7, "doc_changes": 22, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef reindex(self, index=None, method=None, level=None, fill_value=pa.NA,\n                limit=None, copy=True):\n        \n        \n        if index is None:\n            raise ValueError('Must pass Index or sequence, not None')\n\n        index = _ensure_index(index)\n        if self.index.equals(index):\n            if copy:\n                result = self.copy()\n                result.index = index\n                return result\n            else:\n                return self\n\n        if len(self.index) == 0:\n            return Series(nan, index=index, name=self.name)\n\n        new_index, indexer = self.index.reindex(index, method=method,\n                                                level=level, limit=limit)\n        new_values = com.take_1d(self.values, indexer, fill_value=fill_value)\n        return Series(new_values, index=new_index, name=self.name)```\n\nUpdated code:\n```python\ndef reindex(self, index=None, method=None, level=None, fill_value=pa.NA,\n                limit=None, copy=True, takeable=False):\n        \n        \n        if index is None:\n            raise ValueError('Must pass Index or sequence, not None')\n\n        index = _ensure_index(index)\n        if self.index.equals(index):\n            if copy:\n                result = self.copy()\n                result.index = index\n                return result\n            else:\n                return self\n\n        if len(self.index) == 0:\n            return Series(nan, index=index, name=self.name)\n\n        new_index, indexer = self.index.reindex(index, method=method,\n                                                level=level, limit=limit,\n                                                takeable=takeable)\n        new_values = com.take_1d(self.values, indexer, fill_value=fill_value)\n        return Series(new_values, index=new_index, name=self.name)\n```\n\nOriginal documentation:\n```python\nConform Series to new index with optional filling logic, placing\n        NA/NaN in locations having no value in the previous index. A new object\n        is produced unless the new index is equivalent to the current one and\n        copy=False\n\n        Parameters\n        ----------\n        index : array-like or Index\n            New labels / index to conform to. Preferably an Index object to\n            avoid duplicating data\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate LAST valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap\n        copy : boolean, default True\n            Return a new object, even if the passed indexes are the same\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n        fill_value : scalar, default NaN\n            Value to use for missing values. Defaults to NaN, but can be any\n            \"compatible\" value\n        limit : int, default None\n            Maximum size gap to forward or backward fill\n\n        Returns\n        -------\n        reindexed : Series\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Conform Series to new index with optional filling logic, placing\n        NA/NaN in locations having no value in the previous index. A new object\n        is produced unless the new index is equivalent to the current one and\n        copy=False\n\n        Parameters\n        ----------\n        index : array-like or Index\n            New labels / index to conform to. Preferably an Index object to\n            avoid duplicating data\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate LAST valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap\n        copy : boolean, default True\n            Return a new object, even if the passed indexes are the same\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n        fill_value : scalar, default NaN\n            Value to use for missing values. Defaults to NaN, but can be any\n            \"compatible\" value\n        limit : int, default None\n            Maximum size gap to forward or backward fill\n        takeable : the labels are locations (and not labels)\n\n        Returns\n        -------\n        reindexed : Series", "file": "pandas_core_series.py", "function": "Series.reindex", "project": "pandas", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mode(self):\n        \n        \n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self)```\n\nUpdated code:\n```python\ndef mode(self, dropna=True):\n        \n        \n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n```\n\nOriginal documentation:\n```python\nReturn the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Returns\n        -------\n        modes : Series (sorted)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        -------\n        dropna : boolean, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        modes : Series (sorted)", "file": "pandas_core_series.py", "function": "Series.mode", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 9, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_transform(self, *args) -> Transform:\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef get_transform(self, *args) -> Transform:\n        \n        \n        \n        raise NotImplementedError\n```\n\nOriginal documentation:\n```python\nExecute the policy to use input data to create transform(s).\n\n        Args:\n            arguments must follow what's defined in :attr:`input_args`.\n\n        Returns:\n            Return a :class:`Transform` instance, which is the transform to apply to inputs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Execute the policy based on input data, and decide what transform to apply to inputs.\n\n        Args:\n            arguments must follow what's defined in :attr:`input_args`.\n\n        Returns:\n            Transform: Returns the deterministic transform to apply to the input.", "file": "detectron2_data_transforms_augmentation.py", "function": "Augmentation.get_transform", "project": "detectron2", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _maybe_cache(arg, format, cache, convert_listlike):\n    \n    \n    \n    from pandas import Series\n    cache_array = Series()\n    if cache:\n        # Perform a quicker unique check\n        from pandas import Index\n        unique_dates = Index(arg).unique()\n        if len(unique_dates) < len(arg):\n            cache_dates = convert_listlike(unique_dates.to_numpy(),\n                                           True, format)\n            cache_array = Series(cache_dates, index=unique_dates)\n    return cache_array```\n\nUpdated code:\n```python\ndef _maybe_cache(arg, format, cache, convert_listlike):\n    \n    \n    \n    from pandas import Series\n    cache_array = Series()\n    if cache:\n        # Perform a quicker unique check\n        if not should_cache(arg):\n            return cache_array\n\n        unique_dates = unique(arg)\n        if len(unique_dates) < len(arg):\n            cache_dates = convert_listlike(unique_dates, True, format)\n            cache_array = Series(cache_dates, index=unique_dates)\n    return cache_array\n```\n\nOriginal documentation:\n```python\nCreate a cache of unique dates from an array of dates\n\n    Parameters\n    ----------\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\n    format : string\n        Strftime format to parse time\n    cache : boolean\n        True attempts to create a cache of converted values\n    convert_listlike : function\n        Conversion function to apply on dates\n\n    Returns\n    -------\n    cache_array : Series\n        Cache of converted, unique dates. Can be empty\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a cache of unique dates from an array of dates\n\n    Parameters\n    ----------\n    arg : listlike, tuple, 1-d array, Series\n    format : string\n        Strftime format to parse time\n    cache : boolean\n        True attempts to create a cache of converted values\n    convert_listlike : function\n        Conversion function to apply on dates\n\n    Returns\n    -------\n    cache_array : Series\n        Cache of converted, unique dates. Can be empty", "file": "pandas_core_tools_datetimes.py", "function": "_maybe_cache", "project": "pandas", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extract_pandas_array(\n    values: ArrayLike, dtype: DtypeObj | None, ndim: int\n) -> tuple[ArrayLike, DtypeObj | None]:\n    \n    \n    \n    # For now, blocks should be backed by ndarrays when possible.\n    if isinstance(values, ABCPandasArray):\n        values = values.to_numpy()\n        if ndim and ndim > 1:\n            # TODO(EA2D): special case not needed with 2D EAs\n            values = np.atleast_2d(values)\n\n    if isinstance(dtype, PandasDtype):\n        dtype = dtype.numpy_dtype\n\n    return values, dtype```\n\nUpdated code:\n```python\ndef extract_pandas_array(\n    values: ArrayLike, dtype: DtypeObj | None, ndim: int\n) -> tuple[ArrayLike, DtypeObj | None]:\n    \n    \n    \n    # For now, blocks should be backed by ndarrays when possible.\n    if isinstance(values, ABCNumpyExtensionArray):\n        values = values.to_numpy()\n        if ndim and ndim > 1:\n            # TODO(EA2D): special case not needed with 2D EAs\n            values = np.atleast_2d(values)\n\n    if isinstance(dtype, NumpyEADtype):\n        dtype = dtype.numpy_dtype\n\n    return values, dtype\n```\n\nOriginal documentation:\n```python\nEnsure that we don't allow PandasArray / PandasDtype in internals.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure that we don't allow NumpyExtensionArray / NumpyEADtype in internals.", "file": "pandas_core_internals_blocks.py", "function": "extract_pandas_array", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef response_httprepr(response):\n    \n    \n\n    s = \"HTTP/1.1 %d %s\\r\\n\" % (response.status, RESPONSES.get(response.status, ''))\n    if response.headers:\n        s += response.headers.to_string() + \"\\r\\n\"\n    s += \"\\r\\n\"\n    s += response.body\n    return s```\n\nUpdated code:\n```python\ndef response_httprepr(response):\n    \n    \n    s = b\"HTTP/1.1 \" + to_bytes(str(response.status)) + b\" \" + \\\n        to_bytes(RESPONSES.get(response.status, b'')) + b\"\\r\\n\"\n    if response.headers:\n        s += response.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += response.body\n    return s\n```\n\nOriginal documentation:\n```python\nReturn raw HTTP representation (as string) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return raw HTTP representation (as bytes) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).", "file": "scrapy_utils_response.py", "function": "response_httprepr", "project": "scrapy", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef estimate(self, src, dst):\n        \n\n        \n\n        # forward piecewise affine\n        # triangulate input positions into mesh\n        self._tesselation = spatial.Delaunay(src)\n        # find affine mapping from source positions to destination\n        self.affines = []\n        for tri in self._tesselation.vertices:\n            affine = AffineTransform()\n            affine.estimate(src[tri, :], dst[tri, :])\n            self.affines.append(affine)\n\n        # inverse piecewise affine\n        # triangulate input positions into mesh\n        self._inverse_tesselation = spatial.Delaunay(dst)\n        # find affine mapping from source positions to destination\n        self.inverse_affines = []\n        for tri in self._inverse_tesselation.vertices:\n            affine = AffineTransform()\n            affine.estimate(dst[tri, :], src[tri, :])\n            self.inverse_affines.append(affine)\n\n        return True```\n\nUpdated code:\n```python\ndef estimate(self, src, dst):\n        \n\n        \n\n        ndim = src.shape[1]\n        # forward piecewise affine\n        # triangulate input positions into mesh\n        self._tesselation = spatial.Delaunay(src)\n        # find affine mapping from source positions to destination\n        self.affines = []\n        for tri in self._tesselation.vertices:\n            affine = AffineTransform(dimensionality=ndim)\n            affine.estimate(src[tri, :], dst[tri, :])\n            self.affines.append(affine)\n\n        # inverse piecewise affine\n        # triangulate input positions into mesh\n        self._inverse_tesselation = spatial.Delaunay(dst)\n        # find affine mapping from source positions to destination\n        self.inverse_affines = []\n        for tri in self._inverse_tesselation.vertices:\n            affine = AffineTransform(dimensionality=ndim)\n            affine.estimate(dst[tri, :], src[tri, :])\n            self.inverse_affines.append(affine)\n\n        return True\n```\n\nOriginal documentation:\n```python\nEstimate the transformation from a set of corresponding points.\n\n        Number of source and destination coordinates must match.\n\n        Parameters\n        ----------\n        src : (N, 2) array\n            Source coordinates.\n        dst : (N, 2) array\n            Destination coordinates.\n\n        Returns\n        -------\n        success : bool\n            True, if model estimation succeeds.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Estimate the transformation from a set of corresponding points.\n\n        Number of source and destination coordinates must match.\n\n        Parameters\n        ----------\n        src : (N, D) array\n            Source coordinates.\n        dst : (N, D) array\n            Destination coordinates.\n\n        Returns\n        -------\n        success : bool\n            True, if model estimation succeeds.", "file": "skimage_transform__geometric.py", "function": "PiecewiseAffineTransform.estimate", "project": "scikit-image", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_child(self, name, data):\n          \n        logger.debug(\"Adding child: %s\", name)\n        graph = TrainingGraph(self.subnotebook, data, \"Loss\")\n        graph.build()\n        graph = self.subnotebook_add_page(name, widget=graph)\n        Tooltip(graph, text=self.helptext, wrap_length=200)```\n\nUpdated code:\n```python\ndef add_child(self, name: str, data: Calculations) -> None:\n         \n        \n        logger.debug(\"Adding child: %s\", name)\n        graph = TrainingGraph(self.subnotebook, data, \"Loss\")\n        graph.build()\n        graph = self.subnotebook_add_page(name, widget=graph)\n        Tooltip(graph, text=self.helptext, wrap_length=200)\n```\n\nOriginal documentation:\n```python\nAdd the graph for the selected keys\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add the graph for the selected keys.\n\n        Parameters\n        ----------\n        name: str\n            The name of the graph to add to the notebook\n        data: :class:`~lib.gui.analysis.stats.Calculations`\n            The object holding the data to be graphed", "file": "lib_gui_display_command.py", "function": "GraphDisplay.add_child", "project": "faceswap", "change_stats": {"code_changes": 7, "doc_changes": 11, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef swipe(self, lx, ly, rx, ry, duration=0.5):\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef swipe(self, fx, fy, tx, ty, duration=0.5):\n        \n        \n        \n        return self.jsonrpc.swipe(fx, fy, tx, ty, int(duration*200))\n```\n\nOriginal documentation:\n```python\nArgs:\n            lx, ly: from position\n            rx, ry: to position\n            duration (float): duration\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            fx, fy: from position\n            tx, ty: to position\n            duration (float): duration\n        \n        Documents:\n            uiautomator use steps instead of duration\n            As the document say: Each step execution is throttled to 5ms per step.\n        \n        Links:\n            https://developer.android.com/reference/android/support/test/uiautomator/UiDevice.html#swipe%28int,%20int,%20int,%20int,%20int%29", "file": "uiautomator2___init__.py", "function": "Session.swipe", "project": "uiautomator2", "change_stats": {"code_changes": 6, "doc_changes": 13, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_unique_resource_ids(\n    stacks: List[Stack],\n    resource_ids: Optional[Union[List[str], Tuple[str]]],\n    resource_types: Optional[Union[List[str], Tuple[str]]],\n) -> Set[ResourceIdentifier]:\n    \n    \n    output_resource_ids: Set[ResourceIdentifier] = set()\n    if resource_ids:\n        for resources_id in resource_ids:\n            output_resource_ids.add(ResourceIdentifier(resources_id))\n\n    if resource_types:\n        for resource_type in resource_types:\n            resource_type_ids = get_resource_ids_by_type(stacks, resource_type)\n            for resource_id in resource_type_ids:\n                output_resource_ids.add(resource_id)\n    return output_resource_ids```\n\nUpdated code:\n```python\ndef get_unique_resource_ids(\n    stacks: List[Stack],\n    resource_ids: Optional[Union[List[str]]],\n    resource_types: Optional[Union[List[str]]],\n) -> Set[ResourceIdentifier]:\n    \n    \n    output_resource_ids: Set[ResourceIdentifier] = set()\n    if resource_ids:\n        for resources_id in resource_ids:\n            output_resource_ids.add(ResourceIdentifier(resources_id))\n\n    if resource_types:\n        for resource_type in resource_types:\n            resource_type_ids = get_resource_ids_by_type(stacks, resource_type)\n            for resource_id in resource_type_ids:\n                output_resource_ids.add(resource_id)\n    return output_resource_ids\n```\n\nOriginal documentation:\n```python\nGet unique resource IDs for resource_ids and resource_types\n\n    Parameters\n    ----------\n    stacks : List[Stack]\n        Stacks\n    resource_ids : Optional[Union[List[str], Tuple[str]]]\n        Resource ID strings\n    resource_types : Optional[Union[List[str], Tuple[str]]]\n        Resource types\n\n    Returns\n    -------\n    Set[ResourceIdentifier]\n        Set of ResourceIdentifier either in resource_ids or has the type in resource_types\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get unique resource IDs for resource_ids and resource_types\n\n    Parameters\n    ----------\n    stacks : List[Stack]\n        Stacks\n    resource_ids : Optional[Union[List[str]]]\n        Resource ID strings\n    resource_types : Optional[Union[List[str]]]\n        Resource types\n\n    Returns\n    -------\n    Set[ResourceIdentifier]\n        Set of ResourceIdentifier either in resource_ids or has the type in resource_types", "file": "samcli_lib_providers_provider.py", "function": "get_unique_resource_ids", "project": "aws-sam-cli", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extract_data_and_train_model(\n        self,\n        new_trained_timerange: TimeRange,\n        pair: str,\n        strategy: IStrategy,\n        dk: FreqaiDataKitchen,\n        data_load_timerange: TimeRange,\n    ):\n        \n        \n        \n\n        corr_dataframes, base_dataframes = self.dd.get_base_and_corr_dataframes(\n            data_load_timerange, pair, dk\n        )\n\n        unfiltered_dataframe = dk.use_strategy_to_populate_indicators(\n            strategy, corr_dataframes, base_dataframes, pair\n        )\n\n        unfiltered_dataframe = dk.slice_dataframe(new_trained_timerange, unfiltered_dataframe)\n\n        # find the features indicated by strategy and store in datakitchen\n        dk.find_features(unfiltered_dataframe)\n        dk.find_labels(unfiltered_dataframe)\n\n        model = self.train(unfiltered_dataframe, pair, dk)\n\n        self.dd.pair_dict[pair][\"trained_timestamp\"] = new_trained_timerange.stopts\n        dk.set_new_model_names(pair, new_trained_timerange.stopts)\n        self.dd.save_data(model, pair, dk)\n\n        if self.plot_features:\n            plot_feature_importance(model, pair, dk, self.plot_features)\n\n        self.dd.purge_old_models()```\n\nUpdated code:\n```python\ndef extract_data_and_train_model(\n        self,\n        new_trained_timerange: TimeRange,\n        pair: str,\n        strategy: IStrategy,\n        dk: FreqaiDataKitchen,\n        data_load_timerange: TimeRange,\n    ):\n        \n        \n        \n\n        corr_dataframes, base_dataframes = self.dd.get_base_and_corr_dataframes(\n            data_load_timerange, pair, dk\n        )\n\n        unfiltered_dataframe = dk.use_strategy_to_populate_indicators(\n            strategy, corr_dataframes, base_dataframes, pair\n        )\n\n        new_trained_timerange = dk.buffer_timerange(new_trained_timerange)\n\n        unfiltered_dataframe = dk.slice_dataframe(new_trained_timerange, unfiltered_dataframe)\n\n        # find the features indicated by strategy and store in datakitchen\n        dk.find_features(unfiltered_dataframe)\n        dk.find_labels(unfiltered_dataframe)\n\n        model = self.train(unfiltered_dataframe, pair, dk)\n\n        self.dd.pair_dict[pair][\"trained_timestamp\"] = new_trained_timerange.stopts\n        dk.set_new_model_names(pair, new_trained_timerange.stopts)\n        self.dd.save_data(model, pair, dk)\n\n        if self.plot_features:\n            plot_feature_importance(model, pair, dk, self.plot_features)\n\n        self.dd.purge_old_models()\n```\n\nOriginal documentation:\n```python\nRetrieve data and train model.\n        :param new_trained_timerange: TimeRange = the timerange to train the model on\n        :param metadata: dict = strategy provided metadata\n        :param strategy: IStrategy = user defined strategy object\n        :param dk: FreqaiDataKitchen = non-persistent data container for current coin/loop\n        :param data_load_timerange: TimeRange = the amount of data to be loaded\n                                    for populate_any_indicators\n                                    (larger than new_trained_timerange so that\n                                    new_trained_timerange does not contain any NaNs)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieve data and train model.\n        :param new_trained_timerange: TimeRange = the timerange to train the model on\n        :param metadata: dict = strategy provided metadata\n        :param strategy: IStrategy = user defined strategy object\n        :param dk: FreqaiDataKitchen = non-persistent data container for current coin/loop\n        :param data_load_timerange: TimeRange = the amount of data to be loaded\n                                    for populating indicators\n                                    (larger than new_trained_timerange so that\n                                    new_trained_timerange does not contain any NaNs)", "file": "freqtrade_freqai_freqai_interface.py", "function": "IFreqaiModel.extract_data_and_train_model", "project": "freqtrade", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef build_relative_position(query_size, key_size, bucket_size=-1, max_position=-1):\n    \n    \n\n    \n    q_ids = torch.arange(0, query_size)\n    k_ids = torch.arange(0, key_size)\n    rel_pos_ids = q_ids[:, None] - k_ids[None, :]\n    if bucket_size > 0 and max_position > 0:\n        rel_pos_ids = make_log_bucket_position(rel_pos_ids, bucket_size, max_position)\n    rel_pos_ids = rel_pos_ids.to(torch.long)\n    rel_pos_ids = rel_pos_ids[:query_size, :]\n    rel_pos_ids = rel_pos_ids.unsqueeze(0)\n    return rel_pos_ids```\n\nUpdated code:\n```python\ndef build_relative_position(query_size, key_size, bucket_size=-1, max_position=-1, device=None):\n    \n    \n    \n\n    q_ids = torch.arange(0, query_size, device=device)\n    k_ids = torch.arange(0, key_size, device=device)\n    rel_pos_ids = q_ids[:, None] - k_ids[None, :]\n    if bucket_size > 0 and max_position > 0:\n        rel_pos_ids = make_log_bucket_position(rel_pos_ids, bucket_size, max_position)\n    rel_pos_ids = rel_pos_ids.to(torch.long)\n    rel_pos_ids = rel_pos_ids[:query_size, :]\n    rel_pos_ids = rel_pos_ids.unsqueeze(0)\n    return rel_pos_ids\n```\n\nOriginal documentation:\n```python\nBuild relative position according to the query and key\n\n    We assume the absolute position of query \\\\(P_q\\\\) is range from (0, query_size) and the absolute position of key\n    \\\\(P_k\\\\) is range from (0, key_size), The relative positions from query to key is \\\\(R_{q \\\\rightarrow k} = P_q -\n    P_k\\\\)\n\n    Args:\n        query_size (int): the length of query\n        key_size (int): the length of key\n        bucket_size (int): the size of position bucket\n        max_position (int): the maximum allowed absolute position\n\n    Return:\n        `torch.LongTensor`: A tensor with shape [1, query_size, key_size]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Build relative position according to the query and key\n\n    We assume the absolute position of query \\\\(P_q\\\\) is range from (0, query_size) and the absolute position of key\n    \\\\(P_k\\\\) is range from (0, key_size), The relative positions from query to key is \\\\(R_{q \\\\rightarrow k} = P_q -\n    P_k\\\\)\n\n    Args:\n        query_size (int): the length of query\n        key_size (int): the length of key\n        bucket_size (int): the size of position bucket\n        max_position (int): the maximum allowed absolute position\n        device (`torch.device`): the device on which tensors will be created.\n\n    Return:\n        `torch.LongTensor`: A tensor with shape [1, query_size, key_size]", "file": "src_transformers_models_deberta_v2_modeling_deberta_v2.py", "function": "build_relative_position", "project": "transformers", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_get_import_name_without_alias(self):\n        \n        \n        \n        import_name_with_alias = \"requests as R\"\n        expected_import_name_without_alias = \"requests\"\n        import_name_without_aliases = pipreqs.get_name_without_alias(\n            import_name_with_alias)\n        self.assertEqual(\n            import_name_without_aliases, expected_import_name_without_alias)```\n\nUpdated code:\n```python\ndef test_get_import_name_without_alias(self):\n        \n        \n        \n        import_name_with_alias = \"requests as R\"\n        expected_import_name_without_alias = \"requests\"\n        import_name_without_aliases = pipreqs.get_name_without_alias(\n            import_name_with_alias)\n        self.assertEqual(\n            import_name_without_aliases,\n            expected_import_name_without_alias\n            )\n```\n\nOriginal documentation:\n```python\nTest that function get_name_without_alias() will work on a string.\n        - Note: This isn't truly needed when pipreqs is walking the AST to find imports\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test that function get_name_without_alias()\n        will work on a string.\n        - Note: This isn't truly needed when pipreqs is walking\n          the AST to find imports", "file": "tests_test_pipreqs.py", "function": "TestPipreqs.test_get_import_name_without_alias", "project": "pipreqs", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef handle_trade(trade: Trade) -> None:\n    \n    \n    \n    try:\n        if not trade.is_open:\n            raise ValueError('attempt to handle closed trade: {}'.format(trade))\n\n        logger.debug('Handling %s ...', trade)\n\n        current_rate = exchange.get_ticker(trade.pair)['bid']\n        if should_sell(trade, current_rate, datetime.utcnow()):\n            execute_sell(trade, current_rate)\n            return\n\n    except ValueError:\n        logger.exception('Unable to handle open order')```\n\nUpdated code:\n```python\ndef handle_trade(trade: Trade) -> bool:\n    \n    \n    \n    if not trade.is_open:\n        raise ValueError('attempt to handle closed trade: {}'.format(trade))\n\n    logger.debug('Handling %s ...', trade)\n    current_rate = exchange.get_ticker(trade.pair)['bid']\n    if should_sell(trade, current_rate, datetime.utcnow()):\n        execute_sell(trade, current_rate)\n        return True\n    return False\n```\n\nOriginal documentation:\n```python\nSells the current pair if the threshold is reached and updates the trade record.\n    :return: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sells the current pair if the threshold is reached and updates the trade record.\n    :return: True if trade has been sold, False otherwise", "file": "freqtrade_main.py", "function": "handle_trade", "project": "freqtrade", "change_stats": {"code_changes": 24, "doc_changes": 4, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef using_device(*args):\n    \n\n    \n    for arg in args:\n        if isinstance(arg, GPUArray):\n            return DeviceUser(arg)\n    return DeviceUser(None)```\n\nUpdated code:\n```python\ndef using_device(*args):\n    \n\n    \n    for arg in args:\n        user = DeviceUser(arg)\n        if user.is_active:\n            return user\n    return DeviceUser(None)\n```\n\nOriginal documentation:\n```python\nReturns :class:`DeviceUser` object of the first\n    :class:`~pycuda.gpuarray.GPUArray` argument.\n\n    If none of the arguments is :class:`~pycuda.gpuarray.GPUArray`, then it\n    returns dummy :class:`DeviceUser` object which is inactive.\n\n    Args:\n        *args: Objects based on which an appropriate device should be selected.\n\n    Returns:\n        DeviceUser: Device user instance of selected argument.\n\n    .. admonition:: Example\n\n        Suppose ``arrays`` is a list of arrays of type either\n        :class:`~numpy.ndarray` or :class:`~pycuda.gpuarray.GPUArray`. Then,\n        following code invokes ``do_something_on`` with an appropriate context::\n\n            with using_device(*arrays):\n                do_something_on(arrays)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns :class:`DeviceUser` object of the first\n    :class:`~pycuda.gpuarray.GPUArray` argument.\n\n    If none of the arguments specifies a GPU device, then it returns dummy\n    :class:`DeviceUser` object which is inactive.\n\n    Args:\n        *args: Objects based on which an appropriate device should be selected.\n\n    Returns:\n        DeviceUser: Device user instance of selected argument.\n\n    .. admonition:: Example\n\n        Suppose ``arrays`` is a list of arrays of type either\n        :class:`~numpy.ndarray` or :class:`~pycuda.gpuarray.GPUArray`. Then,\n        following code invokes ``do_something_on`` with an appropriate context::\n\n            with using_device(*arrays):\n                do_something_on(arrays)", "file": "chainer_cuda.py", "function": "using_device", "project": "chainer", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cast_column(self, column: str, feature: FeatureType, new_fingerprint: str) -> \"Dataset\":\n        \n        \n        if hasattr(feature, \"decode_example\"):\n            dataset = copy.deepcopy(self)\n            dataset.features[column] = feature\n            dataset._fingerprint = new_fingerprint\n            return dataset\n        else:\n            features = self.features.copy()\n            features[column] = feature\n            return self.cast(features)```\n\nUpdated code:\n```python\ndef cast_column(self, column: str, feature: FeatureType, new_fingerprint: str) -> \"Dataset\":\n        \n        \n        if hasattr(feature, \"cast_storage\"):\n            dataset = copy.deepcopy(self)\n            dataset.features[column] = feature\n            dataset._fingerprint = new_fingerprint\n            dataset._data = dataset._data.cast(dataset.features.arrow_schema)\n            dataset._data = update_metadata_with_features(dataset._data, dataset.features)\n            return dataset\n        else:\n            features = self.features.copy()\n            features[column] = feature\n            return self.cast(features)\n```\n\nOriginal documentation:\n```python\nCast column to feature for decoding.\n\n        Args:\n            column (:obj:`str`): Column name.\n            feature (:class:`Feature`): Target feature.\n\n        Returns:\n            :class:`Dataset`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Cast column to feature for decoding.\n\n        Args:\n            column (:obj:`str`): Column name.\n            feature (:class:`FeatureType`): Target feature.\n\n        Returns:\n            :class:`Dataset`", "file": "src_datasets_arrow_dataset.py", "function": "Dataset.cast_column", "project": "datasets", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef force_string(kwargs):\n    \n    \n    \n    if isinstance(kwargs, dict):\n        return {\n            force_string(key): force_string(value) for key, value in six.iteritems(kwargs)}\n    elif isinstance(kwargs, list):\n        return [force_string(element) for element in kwargs]\n    elif six.PY2 and isinstance(kwargs, unicode):\n        return kwargs.encode('utf-8')\n    return kwargs```\n\nUpdated code:\n```python\ndef force_string(kwargs):\n    \n    \n    \n    if isinstance(kwargs, dict):\n        # Context: https://github.com/cameronmaske/celery-once/issues/58\n        # Keep equivalent to string of dict for backwards compatibility.\n        return order_dict_to_string(OrderedDict(\n            (force_string(key), force_string(value))\n            for key, value in items_sorted_by_key(kwargs)\n        ))\n    elif isinstance(kwargs, list):\n        return [force_string(element) for element in kwargs]\n    elif six.PY2 and isinstance(kwargs, unicode):\n        return kwargs.encode('utf-8')\n    return kwargs\n```\n\nOriginal documentation:\n```python\nForce key in dict or list to a string.\n    Fixes: https://github.com/TrackMaven/celery-once/issues/11\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Force key in dict or list to a string.\n    Fixes: https://github.com/cameronmaske/celery-once/issues/11", "file": "celery_once_helpers.py", "function": "force_string", "project": "celery-once", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef read_sas(\n    filepath_or_buffer: FilePath | ReadBuffer[bytes],\n    format: str | None = None,\n    index: Hashable | None = None,\n    encoding: str | None = None,\n    chunksize: int | None = None,\n    iterator: bool = False,\n) -> DataFrame | ReaderBase:\n    \n    \n    \n    if format is None:\n        buffer_error_msg = (\n            \"If this is a buffer object rather \"\n            \"than a string name, you must specify a format string\"\n        )\n        filepath_or_buffer = stringify_path(filepath_or_buffer)\n        if not isinstance(filepath_or_buffer, str):\n            raise ValueError(buffer_error_msg)\n        fname = filepath_or_buffer.lower()\n        if fname.endswith(\".xpt\"):\n            format = \"xport\"\n        elif fname.endswith(\".sas7bdat\"):\n            format = \"sas7bdat\"\n        else:\n            raise ValueError(\"unable to infer format of SAS file\")\n\n    reader: ReaderBase\n    if format.lower() == \"xport\":\n        from pandas.io.sas.sas_xport import XportReader\n\n        reader = XportReader(\n            filepath_or_buffer,\n            index=index,\n            encoding=encoding,\n            chunksize=chunksize,\n        )\n    elif format.lower() == \"sas7bdat\":\n        from pandas.io.sas.sas7bdat import SAS7BDATReader\n\n        reader = SAS7BDATReader(\n            filepath_or_buffer,\n            index=index,\n            encoding=encoding,\n            chunksize=chunksize,\n        )\n    else:\n        raise ValueError(\"unknown SAS format\")\n\n    if iterator or chunksize:\n        return reader\n\n    with reader:\n        return reader.read()```\n\nUpdated code:\n```python\ndef read_sas(\n    filepath_or_buffer: FilePath | ReadBuffer[bytes],\n    format: str | None = None,\n    index: Hashable | None = None,\n    encoding: str | None = None,\n    chunksize: int | None = None,\n    iterator: bool = False,\n    compression: CompressionOptions = \"infer\",\n) -> DataFrame | ReaderBase:\n    \n    \n    \n    if format is None:\n        buffer_error_msg = (\n            \"If this is a buffer object rather \"\n            \"than a string name, you must specify a format string\"\n        )\n        filepath_or_buffer = stringify_path(filepath_or_buffer)\n        if not isinstance(filepath_or_buffer, str):\n            raise ValueError(buffer_error_msg)\n        fname = filepath_or_buffer.lower()\n        if \".xpt\" in fname:\n            format = \"xport\"\n        elif \".sas7bdat\" in fname:\n            format = \"sas7bdat\"\n        else:\n            raise ValueError(\n                f\"unable to infer format of SAS file from filename: {repr(fname)}\"\n            )\n\n    reader: ReaderBase\n    if format.lower() == \"xport\":\n        from pandas.io.sas.sas_xport import XportReader\n\n        reader = XportReader(\n            filepath_or_buffer,\n            index=index,\n            encoding=encoding,\n            chunksize=chunksize,\n            compression=compression,\n        )\n    elif format.lower() == \"sas7bdat\":\n        from pandas.io.sas.sas7bdat import SAS7BDATReader\n\n        reader = SAS7BDATReader(\n            filepath_or_buffer,\n            index=index,\n            encoding=encoding,\n            chunksize=chunksize,\n            compression=compression,\n        )\n    else:\n        raise ValueError(\"unknown SAS format\")\n\n    if iterator or chunksize:\n        return reader\n\n    with reader:\n        return reader.read()\n```\n\nOriginal documentation:\n```python\nRead SAS files stored as either XPORT or SAS7BDAT format files.\n\n    Parameters\n    ----------\n    filepath_or_buffer : str, path object, or file-like object\n        String, path object (implementing ``os.PathLike[str]``), or file-like\n        object implementing a binary ``read()`` function. The string could be a URL.\n        Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be:\n        ``file://localhost/path/to/table.sas``.\n    format : str {'xport', 'sas7bdat'} or None\n        If None, file format is inferred from file extension. If 'xport' or\n        'sas7bdat', uses the corresponding format.\n    index : identifier of index column, defaults to None\n        Identifier of column that should be used as index of the DataFrame.\n    encoding : str, default is None\n        Encoding for text data.  If None, text data are stored as raw bytes.\n    chunksize : int\n        Read file `chunksize` lines at a time, returns iterator.\n\n        .. versionchanged:: 1.2\n\n            ``TextFileReader`` is a context manager.\n    iterator : bool, defaults to False\n        If True, returns an iterator for reading the file incrementally.\n\n        .. versionchanged:: 1.2\n\n            ``TextFileReader`` is a context manager.\n\n    Returns\n    -------\n    DataFrame if iterator=False and chunksize=None, else SAS7BDATReader\n    or XportReader\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Read SAS files stored as either XPORT or SAS7BDAT format files.\n\n    Parameters\n    ----------\n    filepath_or_buffer : str, path object, or file-like object\n        String, path object (implementing ``os.PathLike[str]``), or file-like\n        object implementing a binary ``read()`` function. The string could be a URL.\n        Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be:\n        ``file://localhost/path/to/table.sas``.\n    format : str {{'xport', 'sas7bdat'}} or None\n        If None, file format is inferred from file extension. If 'xport' or\n        'sas7bdat', uses the corresponding format.\n    index : identifier of index column, defaults to None\n        Identifier of column that should be used as index of the DataFrame.\n    encoding : str, default is None\n        Encoding for text data.  If None, text data are stored as raw bytes.\n    chunksize : int\n        Read file `chunksize` lines at a time, returns iterator.\n\n        .. versionchanged:: 1.2\n\n            ``TextFileReader`` is a context manager.\n    iterator : bool, defaults to False\n        If True, returns an iterator for reading the file incrementally.\n\n        .. versionchanged:: 1.2\n\n            ``TextFileReader`` is a context manager.\n    {decompression_options}\n\n    Returns\n    -------\n    DataFrame if iterator=False and chunksize=None, else SAS7BDATReader\n    or XportReader", "file": "pandas_io_sas_sasreader.py", "function": "read_sas", "project": "pandas", "change_stats": {"code_changes": 13, "doc_changes": 5, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef temperature_unit(self):\n        \n        if self._module.get_unit(self._channel) == TEMP_CELSIUS:\n            return TEMP_CELSIUS\n        return TEMP_FAHRENHEIT```\n\nUpdated code:\n```python\ndef temperature_unit(self):\n        \n        return TEMP_CELSIUS\n```\n\nOriginal documentation:\n```python\nReturn the unit this state is expressed in.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the unit.", "file": "homeassistant_components_velbus_climate.py", "function": "VelbusClimate.temperature_unit", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \n        \n        \n\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )```\n\nUpdated code:\n```python\ndef sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: Union[Union[bool, int], Sequence[Union[bool, int]]] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \n        \n        \n\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )\n```\n\nOriginal documentation:\n```python\nSort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            The original Series sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            The original Series sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64", "file": "pandas_core_series.py", "function": "Series.sort_index", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _reverse_inject_signature_hmac_v1_query(context: RequestContext) -> Request:\n    \n    \n    \n\n    new_headers = {}\n    new_query_string_dict = {}\n\n    for header, value in context.request.args.items():\n        header_low = header.lower()\n        if header_low not in HmacV1QueryAuthValidation.post_signature_headers:\n            new_headers[header] = value\n        elif header_low in HmacV1QueryAuthValidation.QSAOfInterest_low:\n            new_query_string_dict[header] = value\n\n    # there should not be any headers here. If there are, it means they have been added by the client\n    # We should verify them, they will fail the signature except if they were part of the original request\n    for header, value in context.request.headers.items():\n        header_low = header.lower()\n        if header_low.startswith(\"x-amz-\") or header_low in [\"content-type\", \"date\", \"content-md5\"]:\n            new_headers[header_low] = value\n\n    # rebuild the query string\n    new_query_string = percent_encode_sequence(new_query_string_dict)\n\n    # easier to recreate the request, we would have to delete every cached property otherwise\n    reversed_request = _create_new_request(\n        request=context.request,\n        headers=new_headers,\n        query_string=new_query_string,\n    )\n\n    return reversed_request```\n\nUpdated code:\n```python\ndef _reverse_inject_signature_hmac_v1_query(\n    request: Request,\n) -> tuple[urlparse.SplitResult, HTTPHeaders]:\n    \n    \n    \n    new_headers = {}\n    new_query_string_dict = {}\n\n    for header, value in request.args.items():\n        header_low = header.lower()\n        if header_low not in HmacV1QueryAuthValidation.post_signature_headers:\n            new_headers[header] = value\n        elif header_low in HmacV1QueryAuthValidation.QSAOfInterest_low:\n            new_query_string_dict[header] = value\n\n    # there should not be any headers here. If there are, it means they have been added by the client\n    # We should verify them, they will fail the signature except if they were part of the original request\n    for header, value in request.headers.items():\n        header_low = header.lower()\n        if header_low.startswith(\"x-amz-\") or header_low in [\"content-type\", \"date\", \"content-md5\"]:\n            new_headers[header_low] = value\n\n    # rebuild the query string\n    new_query_string = percent_encode_sequence(new_query_string_dict)\n\n    # we need to URL encode the path, as the key needs to be urlencoded for the signature to match\n    encoded_path = urlparse.quote(request.path)\n\n    reversed_url = f\"{request.scheme}://{request.host}{encoded_path}?{new_query_string}\"\n\n    reversed_headers = HTTPHeaders()\n    for key, value in new_headers.items():\n        reversed_headers[key] = value\n\n    return urlsplit(reversed_url), reversed_headers\n```\n\nOriginal documentation:\n```python\nReverses what does HmacV1QueryAuth._inject_signature while injecting the signature in the request.\n    Transforms the query string parameters in headers to recalculate the signature\n    see botocore.auth.HmacV1QueryAuth._inject_signature\n    :param context:\n    :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Reverses what does HmacV1QueryAuth._inject_signature while injecting the signature in the request.\n    Transforms the query string parameters in headers to recalculate the signature\n    see botocore.auth.HmacV1QueryAuth._inject_signature\n    :param request: the original request\n    :return: tuple of a split result from the reversed request, and the reversed headers", "file": "localstack_services_s3_presigned_url.py", "function": "_reverse_inject_signature_hmac_v1_query", "project": "localstack", "change_stats": {"code_changes": 27, "doc_changes": 6, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, wav, wav_len=None):\n        \n        \n        \n        STFT = self.params['compute_STFT'](wav)\n        features = self.params['compute_spectrogram'](STFT)\n\n        if self.feature_type in ['fbank', 'mfcc']:\n            features = self.params['compute_fbanks'](features)\n        if self.feature_type == 'mfcc':\n            features = self.params['compute_mfccs'](features)\n\n        if self.deltas:\n            delta1 = self.params['compute_deltas'](features)\n            delta2 = self.params['compute_deltas'](delta1)\n            features = torch.cat([features, delta1, delta2], dim=-2)\n\n        if self.context:\n            features = self.params['context_window'](features)\n\n        if self.normalize:\n            features = self.params['normalize'](features, wav_len)\n\n        return features```\n\nUpdated code:\n```python\ndef forward(self, wav):\n        \n        \n        STFT = self.params['compute_STFT'](wav)\n        features = self.params['compute_spectrogram'](STFT)\n\n        if self.feature_type in ['fbank', 'mfcc']:\n            features = self.params['compute_fbanks'](features)\n        if self.feature_type == 'mfcc':\n            features = self.params['compute_mfccs'](features)\n\n        if self.deltas:\n            delta1 = self.params['compute_deltas'](features)\n            delta2 = self.params['compute_deltas'](delta1)\n            features = torch.cat([features, delta1, delta2], dim=-2)\n\n        if self.context:\n            features = self.params['context_window'](features)\n\n        return features\n```\n\nOriginal documentation:\n```python\nArguments\n        ---------\n        wav : tensor\n            A batch of audio signals to transform to features.\n        wav_len : tensor\n            Only needed for normalization, the length of each audio signal.\n\n        Returns\n        -------\n        The generated features.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a set of features generated from the input waveforms.\n\n        Arguments\n        ---------\n        wav : tensor\n            A batch of audio signals to transform to features.", "file": "speechbrain_lobes_features.py", "function": "Features.forward", "project": "speechbrain", "change_stats": {"code_changes": 8, "doc_changes": 12, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,\n               box_code_size,\n               num_predictions_per_location,\n               conv_hyperparams,\n               kernel_size=3,\n               use_depthwise=False,\n               box_encodings_clip_range=None,\n               return_flat_predictions=True,\n               name=None):\n    \n    \n    if use_depthwise and (kernel_size == 1):\n      raise ValueError('Should not use 1x1 kernel when using depthwise conv')\n\n    super(WeightSharedConvolutionalBoxHead, self).__init__(name=name)\n    self._box_code_size = box_code_size\n    self._kernel_size = kernel_size\n    self._num_predictions_per_location = num_predictions_per_location\n    self._use_depthwise = use_depthwise\n    self._box_encodings_clip_range = box_encodings_clip_range\n    self._return_flat_predictions = return_flat_predictions\n\n    self._box_encoder_layers = []\n\n    if self._use_depthwise:\n      self._box_encoder_layers.append(\n          tf.keras.layers.SeparableConv2D(\n              num_predictions_per_location * self._box_code_size,\n              [self._kernel_size, self._kernel_size],\n              padding='SAME',\n              name='BoxPredictor',\n              **conv_hyperparams.params(use_bias=True)))\n    else:\n      self._box_encoder_layers.append(\n          tf.keras.layers.Conv2D(\n              num_predictions_per_location * self._box_code_size,\n              [self._kernel_size, self._kernel_size],\n              padding='SAME',\n              name='BoxPredictor',\n              **conv_hyperparams.params(use_bias=True)))```\n\nUpdated code:\n```python\ndef __init__(self,\n               box_code_size,\n               num_predictions_per_location,\n               conv_hyperparams,\n               kernel_size=3,\n               use_depthwise=False,\n               apply_conv_hyperparams_to_heads=False,\n               box_encodings_clip_range=None,\n               return_flat_predictions=True,\n               name=None):\n    \n    \n    if use_depthwise and (kernel_size == 1):\n      raise ValueError('Should not use 1x1 kernel when using depthwise conv')\n\n    super(WeightSharedConvolutionalBoxHead, self).__init__(name=name)\n    self._box_code_size = box_code_size\n    self._kernel_size = kernel_size\n    self._num_predictions_per_location = num_predictions_per_location\n    self._use_depthwise = use_depthwise\n    self._apply_conv_hyperparams_to_heads = apply_conv_hyperparams_to_heads\n    self._box_encodings_clip_range = box_encodings_clip_range\n    self._return_flat_predictions = return_flat_predictions\n\n    self._box_encoder_layers = []\n\n    if self._use_depthwise:\n      kwargs = conv_hyperparams.params(use_bias=True)\n      if self._apply_conv_hyperparams_to_heads:\n        kwargs['depthwise_regularizer'] = kwargs['kernel_regularizer']\n        kwargs['depthwise_initializer'] = kwargs['kernel_initializer']\n        kwargs['pointwise_regularizer'] = kwargs['kernel_regularizer']\n        kwargs['pointwise_initializer'] = kwargs['kernel_initializer']\n      self._box_encoder_layers.append(\n          tf.keras.layers.SeparableConv2D(\n              num_predictions_per_location * self._box_code_size,\n              [self._kernel_size, self._kernel_size],\n              padding='SAME',\n              name='BoxPredictor',\n              **kwargs))\n    else:\n      self._box_encoder_layers.append(\n          tf.keras.layers.Conv2D(\n              num_predictions_per_location * self._box_code_size,\n              [self._kernel_size, self._kernel_size],\n              padding='SAME',\n              name='BoxPredictor',\n              **conv_hyperparams.params(use_bias=True)))\n```\n\nOriginal documentation:\n```python\nConstructor.\n\n    Args:\n      box_code_size: Size of encoding for each box.\n      num_predictions_per_location: Number of box predictions to be made per\n        spatial location. Int specifying number of boxes per location.\n      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n        containing hyperparameters for convolution ops.\n      kernel_size: Size of final convolution kernel.\n      use_depthwise: Whether to use depthwise convolutions for prediction steps.\n        Default is False.\n      box_encodings_clip_range: Min and max values for clipping box_encodings.\n      return_flat_predictions: If true, returns flattened prediction tensor\n        of shape [batch, height * width * num_predictions_per_location,\n        box_coder]. Otherwise returns the prediction tensor before reshaping,\n        whose shape is [batch, height, width, num_predictions_per_location *\n        num_class_slots].\n      name: A string name scope to assign to the model. If `None`, Keras\n        will auto-generate one from the class name.\n\n    Raises:\n      ValueError: if use_depthwise is True and kernel_size is 1.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Constructor.\n\n    Args:\n      box_code_size: Size of encoding for each box.\n      num_predictions_per_location: Number of box predictions to be made per\n        spatial location. Int specifying number of boxes per location.\n      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n        containing hyperparameters for convolution ops.\n      kernel_size: Size of final convolution kernel.\n      use_depthwise: Whether to use depthwise convolutions for prediction steps.\n        Default is False.\n      apply_conv_hyperparams_to_heads: Whether to apply conv_hyperparams to\n        depthwise seperable convolution layers in the box and class heads. By\n        default, the conv_hyperparams are only applied to layers in the\n        predictor tower when using depthwise separable convolutions.\n      box_encodings_clip_range: Min and max values for clipping box_encodings.\n      return_flat_predictions: If true, returns flattened prediction tensor\n        of shape [batch, height * width * num_predictions_per_location,\n        box_coder]. Otherwise returns the prediction tensor before reshaping,\n        whose shape is [batch, height, width, num_predictions_per_location *\n        num_class_slots].\n      name: A string name scope to assign to the model. If `None`, Keras\n        will auto-generate one from the class name.\n\n    Raises:\n      ValueError: if use_depthwise is True and kernel_size is 1.", "file": "research_object_detection_predictors_heads_keras_box_head.py", "function": "WeightSharedConvolutionalBoxHead.__init__", "project": "models", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_image_mask(self, new_face, detected_face, predicted_mask, reference_face):\n         \n        \n        logger.trace(\"Getting mask. Image shape: %s\", new_face.shape)\n        if self._args.mask_type not in (\"none\", \"predicted\"):\n            mask_centering = detected_face.mask[self._args.mask_type].stored_centering\n        else:\n            mask_centering = \"face\"  # Unused but requires a valid value\n        crop_offset = (reference_face.pose.offset[self._centering] -\n                       reference_face.pose.offset[mask_centering])\n        mask, raw_mask = self._adjustments[\"mask\"].run(detected_face, crop_offset, self._centering,\n                                                       predicted_mask=predicted_mask)\n        if new_face.shape[2] == 4:\n            logger.trace(\"Combining mask with alpha channel box mask\")\n            new_face[:, :, -1] = np.minimum(new_face[:, :, -1], mask.squeeze())\n        else:\n            logger.trace(\"Adding mask to alpha channel\")\n            new_face = np.concatenate((new_face, mask), -1)\n        logger.trace(\"Got mask. Image shape: %s\", new_face.shape)\n        return new_face, raw_mask```\n\nUpdated code:\n```python\ndef _get_image_mask(self, new_face, detected_face, predicted_mask, reference_face):\n         \n        \n        logger.trace(\"Getting mask. Image shape: %s\", new_face.shape)\n        if self._args.mask_type not in (\"none\", \"predicted\"):\n            mask_centering = detected_face.mask[self._args.mask_type].stored_centering\n        else:\n            mask_centering = \"face\"  # Unused but requires a valid value\n        crop_offset = (reference_face.pose.offset[self._centering] -\n                       reference_face.pose.offset[mask_centering])\n        mask, raw_mask = self._adjustments[\"mask\"].run(detected_face, crop_offset, self._centering,\n                                                       predicted_mask=predicted_mask)\n        logger.trace(\"Adding mask to alpha channel\")\n        new_face = np.concatenate((new_face, mask), -1)\n        logger.trace(\"Got mask. Image shape: %s\", new_face.shape)\n        return new_face, raw_mask\n```\n\nOriginal documentation:\n```python\nReturn any selected image mask and intersect with any box mask.\n\n        Places the requested mask into the new face's Alpha channel, intersecting with any box\n        mask that has already been applied.\n\n        Parameters\n        ----------\n        new_face: :class:`numpy.ndarray`\n            The swapped face received from the faceswap model, with any box mask applied\n        detected_face: :class:`~lib.DetectedFace`\n            The detected_face object as defined in :class:`scripts.convert.Predictor`\n        predicted_mask: :class:`numpy.ndarray` or ``None``\n            The predicted mask output from the Faceswap model. ``None`` if the model\n            did not learn a mask\n        reference_face: :class:`~lib.align.AlignedFace`\n            The aligned face object sized to the model output of the original face for reference\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The swapped face with the requested mask added to the Alpha channel\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return any selected image mask\n\n        Places the requested mask into the new face's Alpha channel.\n\n        Parameters\n        ----------\n        new_face: :class:`numpy.ndarray`\n            The swapped face received from the faceswap model.\n        detected_face: :class:`~lib.DetectedFace`\n            The detected_face object as defined in :class:`scripts.convert.Predictor`\n        predicted_mask: :class:`numpy.ndarray` or ``None``\n            The predicted mask output from the Faceswap model. ``None`` if the model\n            did not learn a mask\n        reference_face: :class:`~lib.align.AlignedFace`\n            The aligned face object sized to the model output of the original face for reference\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The swapped face with the requested mask added to the Alpha channel", "file": "lib_convert.py", "function": "Converter._get_image_mask", "project": "faceswap", "change_stats": {"code_changes": 10, "doc_changes": 9, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef action_select_cursor(self) -> None:\n        \n        \n        try:\n            line = self._tree_lines[self.cursor_line]\n        except IndexError:\n            pass\n        else:\n            node = line.path[-1]\n            if self.auto_expand:\n                self._toggle_node(node)\n            self.post_message(self.NodeSelected(self, node))```\n\nUpdated code:\n```python\ndef action_select_cursor(self) -> None:\n        \n        \n        try:\n            line = self._tree_lines[self.cursor_line]\n        except IndexError:\n            pass\n        else:\n            node = line.path[-1]\n            if self.auto_expand:\n                self._toggle_node(node)\n            self.post_message(self.NodeSelected(node))\n```\n\nOriginal documentation:\n```python\nCause a select event for the target node.\n\n        Note:\n            If `auto_expand` is `True` use of this action on a non-leaf node\n            will cause both an expand/collapse event to occour, as well as a\n            selected event.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Cause a select event for the target node.\n\n        Note:\n            If `auto_expand` is `True` use of this action on a non-leaf node\n            will cause both an expand/collapse event to occur, as well as a\n            selected event.", "file": "src_textual_widgets__tree.py", "function": "Tree.action_select_cursor", "project": "textual", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_computer(self, ip, hostname, domain, os, smbv1, signing, spooler=None, zerologon=None, petitpotam=None, dc=None):\n        \n        \n        \n        domain = domain.split('.')[0].upper()\n        hosts = []\n\n        q = select(self.ComputersTable).filter(\n            self.ComputersTable.c.ip == ip\n        )\n        results = asyncio.run(self.conn.execute(q)).all()\n        logging.debug(f\"smb add_computer() - computers returned: {results}\")\n\n        host_data = {\n            \"ip\": ip,\n            \"hostname\": hostname,\n            \"domain\": domain,\n            \"os\": os,\n            \"dc\": dc,\n            \"smbv1\": smbv1,\n            \"signing\": signing,\n            \"spooler\": spooler,\n            \"zerologon\": zerologon,\n            \"petitpotam\": petitpotam\n        }\n\n        # create new computer\n        if not results:\n            hosts = [host_data]\n        # update existing hosts data\n        else:\n            for host in results:\n                computer_data = host._asdict()\n                # only update column if it is being passed in\n                if ip is not None:\n                    computer_data[\"ip\"] = ip\n                if hostname is not None:\n                    computer_data[\"hostname\"] = hostname\n                if domain is not None:\n                    computer_data[\"domain\"] = domain\n                if os is not None:\n                    computer_data[\"os\"] = os\n                if smbv1 is not None:\n                    computer_data[\"smbv1\"] = smbv1\n                if signing is not None:\n                    computer_data[\"signing\"] = signing\n                if spooler is not None:\n                    computer_data[\"spooler\"] = spooler\n                if zerologon is not None:\n                    computer_data[\"zerologon\"] = zerologon\n                if petitpotam is not None:\n                    computer_data[\"petitpotam\"] = petitpotam\n                if dc is not None:\n                    computer_data[\"dc\"] = dc\n                hosts.append(computer_data)\n        logging.debug(f\"Update Hosts: {hosts}\")\n\n        # TODO: find a way to abstract this away to a single Upsert call\n        q = Insert(self.ComputersTable)\n        q = q.on_conflict_do_update(\n            index_elements=self.ComputersTable.primary_key,\n            set_=self.ComputersTable.columns\n        )\n        asyncio.run(\n            self.conn.execute(\n                q,\n                hosts\n            )\n        )```\n\nUpdated code:\n```python\ndef add_computer(self, ip, hostname, domain, os, smbv1, signing, spooler=None, zerologon=None, petitpotam=None, dc=None):\n        \n        \n        \n        domain = domain.split('.')[0].upper()\n        hosts = []\n\n        q = select(self.ComputersTable).filter(\n            self.ComputersTable.c.ip == ip\n        )\n        results = asyncio.run(self.conn.execute(q)).all()\n        logging.debug(f\"smb add_computer() - computers returned: {results}\")\n\n        host_data = {\n            \"ip\": ip,\n            \"hostname\": hostname,\n            \"domain\": domain,\n            \"os\": os,\n            \"dc\": dc,\n            \"smbv1\": smbv1,\n            \"signing\": signing,\n            \"spooler\": spooler,\n            \"zerologon\": zerologon,\n            \"petitpotam\": petitpotam\n        }\n\n        # create new computer\n        if not results:\n            hosts = [host_data]\n        # update existing hosts data\n        else:\n            for host in results:\n                computer_data = host._asdict()\n                # only update column if it is being passed in\n                if ip is not None:\n                    computer_data[\"ip\"] = ip\n                if hostname is not None:\n                    computer_data[\"hostname\"] = hostname\n                if domain is not None:\n                    computer_data[\"domain\"] = domain\n                if os is not None:\n                    computer_data[\"os\"] = os\n                if smbv1 is not None:\n                    computer_data[\"smbv1\"] = smbv1\n                if signing is not None:\n                    computer_data[\"signing\"] = signing\n                if spooler is not None:\n                    computer_data[\"spooler\"] = spooler\n                if zerologon is not None:\n                    computer_data[\"zerologon\"] = zerologon\n                if petitpotam is not None:\n                    computer_data[\"petitpotam\"] = petitpotam\n                if dc is not None:\n                    computer_data[\"dc\"] = dc\n                # only add computer to be updated if it has changed\n                if computer_data not in hosts:\n                    hosts.append(computer_data)\n        logging.debug(f\"Update Hosts: {hosts}\")\n\n        # TODO: find a way to abstract this away to a single Upsert call\n        q = Insert(self.ComputersTable)\n        q = q.on_conflict_do_update(\n            index_elements=self.ComputersTable.primary_key,\n            set_=self.ComputersTable.columns\n        )\n        asyncio.run(\n            self.conn.execute(\n                q,\n                hosts\n            )\n        )\n```\n\nOriginal documentation:\n```python\nCheck if this host has already been added to the database, if not, add it in.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if this host has already been added to the database, if not, add it in.\n        TODO: return inserted or updated row ids as a list", "file": "cme_protocols_smb_database.py", "function": "database.add_computer", "project": "CrackMapExec", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_vars(self):\n          \n        selected_id = tk.StringVar()\n        return {\"selected_id\": selected_id}```\n\nUpdated code:\n```python\ndef set_vars(self):\n         \n        \n        return dict(selected_id=tk.StringVar(),\n                    refresh_graph=get_config().tk_vars[\"refreshgraph\"],\n                    is_training=get_config().tk_vars[\"istraining\"],\n                    analysis_folder=get_config().tk_vars[\"analysis_folder\"])\n```\n\nOriginal documentation:\n```python\nAnalysis specific vars\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the analysis specific tkinter variables to :attr:`vars`.\n\n        The tracked variables are the global variables that:\n            * Trigger when a graph refresh has been requested.\n            * Trigger training is commenced or halted\n            * The variable holding the location of the current Tensorboard log folder.\n\n        Returns\n        -------\n        dict\n            The dictionary of variable names to tkinter variables", "file": "lib_gui_display_analysis.py", "function": "Analysis.set_vars", "project": "faceswap", "change_stats": {"code_changes": 11, "doc_changes": 14, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load(\n        cls, config: Dict[Dict, Any], execution_context: ExecutionContext\n    ) -> GraphComponent:\n        \n        \n        return cls.create(config, execution_context)```\n\nUpdated code:\n```python\ndef load(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        **kwargs: Any,\n    ) -> GraphComponent:\n        \n        \n        return cls.create(config, model_storage, resource, execution_context)\n```\n\nOriginal documentation:\n```python\nCreates a component using a persisted version of itself.\n\n        If not overridden this method merely calls `create`.\n\n        Args:\n            config: This config overrides the `default_config`.\n            execution_context: Information about the current graph run.\n\n        Returns: An instantiated, loaded `GraphComponent`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates a component using a persisted version of itself.\n\n        If not overridden this method merely calls `create`.\n\n        Args:\n            config: This config overrides the `default_config`.\n            model_storage: Storage which graph components can use to persist and load\n                themselves.\n            resource: Resource locator for this component which can be used to persist\n                and load itself from the `model_storage`.\n            execution_context: Information about the current graph run.\n            kwargs: Output values from previous nodes might be passed in as `kwargs`.\n\n        Returns:\n            An instantiated, loaded `GraphComponent`.", "file": "rasa_engine_graph.py", "function": "GraphComponent.load", "project": "rasa", "change_stats": {"code_changes": 11, "doc_changes": 10, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef write_batch(\n        self,\n        batch_examples: Dict[str, List[Any]],\n        writer_batch_size: Optional[int] = None,\n    ):\n        \n        \n        schema = None if self.pa_writer is None and self.update_features else self._schema\n        try_schema = self._schema if self.pa_writer is None and self.update_features else None\n        typed_sequence_examples = {}\n        for col in sorted(batch_examples.keys()):\n            col_type = schema.field(col).type if schema is not None else None\n            col_try_type = try_schema.field(col).type if try_schema is not None and col in try_schema.names else None\n            typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\n            typed_sequence_examples[col] = typed_sequence\n        pa_table = pa.Table.from_pydict(typed_sequence_examples)\n        self.write_table(pa_table, writer_batch_size)```\n\nUpdated code:\n```python\ndef write_batch(\n        self,\n        batch_examples: Dict[str, List[Any]],\n        writer_batch_size: Optional[int] = None,\n    ):\n        \n        \n        if batch_examples and len(next(iter(batch_examples.values()))) == 0:\n            return\n        schema = None if self.pa_writer is None and self.update_features else self._schema\n        try_schema = self._schema if self.pa_writer is None and self.update_features else None\n        typed_sequence_examples = {}\n        for col in sorted(batch_examples.keys()):\n            col_type = schema.field(col).type if schema is not None else None\n            col_try_type = try_schema.field(col).type if try_schema is not None and col in try_schema.names else None\n            typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\n            typed_sequence_examples[col] = typed_sequence\n        pa_table = pa.Table.from_pydict(typed_sequence_examples)\n        self.write_table(pa_table, writer_batch_size)\n```\n\nOriginal documentation:\n```python\nWrite a batch of Example to file.\n\n        Args:\n            example: the Example to add.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write a batch of Example to file.\n        Ignores the batch if it appears to be empty,\n        preventing a potential schema update of unknown types.\n\n        Args:\n            batch_examples: the batch of examples to add.", "file": "src_datasets_arrow_writer.py", "function": "ArrowWriter.write_batch", "project": "datasets", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_available_models(api_key: str, api_base: str) -> List[str]:\n    \n    \n    \n    res = OpenAI(api_key=api_key, base_url=api_base).models.list()\n\n    return [models.id for models in res.data]```\n\nUpdated code:\n```python\ndef get_available_models(api_key: str, base_url: str) -> List[str]:\n    \n    \n    \n    res = OpenAI(api_key=api_key, base_url=base_url).models.list()\n\n    return [models.id for models in res.data]\n```\n\nOriginal documentation:\n```python\nReturns a list of available openai models for the given API key.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a list of all available OpenAI models for the given API key.\n\n    Args:\n        api_key (str): The API key used for authenticating with the OpenAI API. \n        base_url (str): The base URL of the OpenAI API.\n\n    Returns:\n        List[str]: A list of string identifiers, each representing a model that is available with the provided OpenAI key.", "file": "mindsdb_integrations_handlers_openai_handler_helpers.py", "function": "get_available_models", "project": "mindsdb", "change_stats": {"code_changes": 6, "doc_changes": 11, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef initialize(app: flask.Flask, settings):\n    \n    global _INSTALLED  # pylint: disable=global-statement\n    if not settings['server']['limiter'] and not settings['server']['public_instance']:\n        return\n    redis_client = redisdb.client()\n    if not redis_client:\n        logger.error(\n            \"The limiter requires Redis, please consult the documentation: \"\n            + \"https://docs.searxng.org/admin/searx.botdetection.html#limiter\"\n        )\n        if settings['server']['public_instance']:\n            sys.exit(1)\n        return\n    botdetection.init(get_cfg(), redis_client)\n    app.before_request(pre_request)\n    _INSTALLED = True```\n\nUpdated code:\n```python\ndef initialize(app: flask.Flask, settings):\n    \n    global _INSTALLED  # pylint: disable=global-statement\n\n    if not (settings['server']['limiter'] or settings['server']['public_instance']):\n        return\n\n    redis_client = redisdb.client()\n    if not redis_client:\n        logger.error(\n            \"The limiter requires Redis, please consult the documentation: \"\n            \"https://docs.searxng.org/admin/searx.limiter.html\"\n        )\n        if settings['server']['public_instance']:\n            sys.exit(1)\n        return\n\n    _INSTALLED = True\n\n    cfg = get_cfg()\n    if settings['server']['public_instance']:\n        # overwrite limiter.toml setting\n        cfg.set('botdetection.ip_limit.link_token', True)\n\n    botdetection.init(cfg, redis_client)\n    app.before_request(pre_request)\n```\n\nOriginal documentation:\n```python\nInstal the botlimiter aka limiter\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Install the limiter", "file": "searx_limiter.py", "function": "initialize", "project": "searxng", "change_stats": {"code_changes": 19, "doc_changes": 4, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _is_audio_message(message: Dict[Text, Any]) -> bool:\n        \n        return (\n            message.get(\"message\")\n            and message[\"message\"].get(\"attachments\")\n            and message[\"message\"][\"attachments\"][0][\"type\"] == \"audio\"\n        )```\n\nUpdated code:\n```python\ndef _is_audio_message(message: Dict[Text, Any]) -> bool:\n        \n        return (\n            \"message\" in message\n            and \"attachments\" in message[\"message\"]\n            and message[\"message\"][\"attachments\"][0][\"type\"] == \"audio\"\n        )\n```\n\nOriginal documentation:\n```python\nCheck if the users message is a recorced voice message.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if the users message is a recorded voice message.", "file": "rasa_core_channels_facebook.py", "function": "Messenger._is_audio_message", "project": "rasa", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef can_use_scim(self, organization, user):\n        \n        \n        \n        return False```\n\nUpdated code:\n```python\ndef can_use_scim(self, organization, user):\n        \n        \n        \n        return True\n```\n\nOriginal documentation:\n```python\nControls whether or not a provider can have SCIM enabled to manage users\n        and groups. default False, and only SAML2 Providers may implement SCIM\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Controls whether or not a provider can have SCIM enabled to manage users.\n        By default we have this on for all providers.", "file": "src_sentry_auth_provider.py", "function": "Provider.can_use_scim", "project": "sentry", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef csrmv(a, x, y=None, alpha=1, beta=0, transa=False):\n    \n\n    \n    if a.shape[1] != len(x):\n        raise ValueError('dimension mismatch')\n    a_shape = a.shape if not transa else a.shape[::-1]\n\n    handle = device.get_cusparse_handle()\n    m, n = a_shape\n    a, x, y = _cast_common_type(a, x, y)\n    dtype = a.dtype\n    if y is None:\n        y = cupy.zeros(m, dtype)\n    else:\n        y = cupy.asfortranarray(y)\n    alpha = numpy.array(alpha, dtype).ctypes\n    beta = numpy.array(beta, dtype).ctypes\n    _call_cusparse(\n        'csrmv', dtype,\n        handle, _transpose_flag(transa),\n        m, n, a.nnz, alpha.data, a._descr.descriptor,\n        a.data.data.ptr, a.indptr.data.ptr, a.indices.data.ptr,\n        x.data.ptr, beta.data, y.data.ptr)\n\n    return y```\n\nUpdated code:\n```python\ndef csrmv(a, x, y=None, alpha=1, beta=0, transa=False):\n    \n\n    \n    if a.shape[1] != len(x):\n        raise ValueError('dimension mismatch')\n    assert y is None or y.flags.f_contiguous\n\n    a_shape = a.shape if not transa else a.shape[::-1]\n\n    handle = device.get_cusparse_handle()\n    m, n = a_shape\n    a, x, y = _cast_common_type(a, x, y)\n    dtype = a.dtype\n    if y is None:\n        y = cupy.zeros(m, dtype)\n    alpha = numpy.array(alpha, dtype).ctypes\n    beta = numpy.array(beta, dtype).ctypes\n    _call_cusparse(\n        'csrmv', dtype,\n        handle, _transpose_flag(transa),\n        m, n, a.nnz, alpha.data, a._descr.descriptor,\n        a.data.data.ptr, a.indptr.data.ptr, a.indices.data.ptr,\n        x.data.ptr, beta.data, y.data.ptr)\n\n    return y\n```\n\nOriginal documentation:\n```python\nMatrix-vector product for a CSR-matrix and a dense vector.\n\n    .. math::\n\n       y = \\\\alpha * o_a(A) x + \\\\beta y,\n\n    where :math:`o_a` is a transpose function when ``transa`` is ``True`` and\n    is an identity function otherwise.\n\n    Args:\n        a (cupy.cusparse.csr_matrix): Matrix A.\n        x (cupy.ndarray): Vector x.\n        y (cupy.ndarray or None): Vector y.\n        alpha (float): Coefficient for x.\n        beta (float): Coefficient for y.\n        transa (bool): If ``True``, transpose of ``A`` is used.\n\n    Returns:\n        cupy.ndarray: Calculated ``y``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Matrix-vector product for a CSR-matrix and a dense vector.\n\n    .. math::\n\n       y = \\\\alpha * o_a(A) x + \\\\beta y,\n\n    where :math:`o_a` is a transpose function when ``transa`` is ``True`` and\n    is an identity function otherwise.\n\n    Args:\n        a (cupy.cusparse.csr_matrix): Matrix A.\n        x (cupy.ndarray): Vector x.\n        y (cupy.ndarray or None): Vector y. It must be contiguous.\n        alpha (float): Coefficient for x.\n        beta (float): Coefficient for y.\n        transa (bool): If ``True``, transpose of ``A`` is used.\n\n    Returns:\n        cupy.ndarray: Calculated ``y``.", "file": "cupy_cusparse.py", "function": "csrmv", "project": "cupy", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef century(\n        self,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        thai_digit: bool = False,\n        buddhist_era: bool = True,\n    ) -> str:\n        \n        \n        \n        end_century = 22\n        if buddhist_era:\n            end_century = 26\n\n        elements = [str(i) for i in range(1, end_century)]\n        text: str = self.random_element(elements, min_length, max_length)\n\n        if thai_digit:\n            text = text.translate(_HA_TH_DIGITS)\n        return text```\n\nUpdated code:\n```python\ndef century(self, thai_digit: bool = False, buddhist_era: bool = True) -> str:\n        \n        \n        \n        end_century = 22\n        if buddhist_era:\n            end_century = 26\n        text = str(self.random_element(range(1, end_century)))\n        if thai_digit:\n            text = text.translate(_HA_TH_DIGITS)\n        return text\n```\n\nOriginal documentation:\n```python\n:param min_length: minimum length of the century string (default: None)\n        :param max_length: maximum length of the century string (default: None)\n        :param thai_digit: use Thai digit or not (default: False)\n        :param buddhist_era: use Buddist era or not (default: True)\n        :example: '20'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":param thai_digi:t use Thai digit or not (default: False)\n        :param buddhist:_era use Buddist era or not (default: True)\n        :example: '20'", "file": "faker_providers_date_time_th_TH___init__.py", "function": "Provider.century", "project": "faker", "change_stats": {"code_changes": 15, "doc_changes": 8, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_headers(self, path):\n        \n        \n        abspath = self.get_absolute_path(self.settings, path)\n        modified = self.get_modified_time(path)\n\n        if modified is not None:\n            self.set_header(\"Last-Modified\", modified)\n\n        mime_type, encoding = mimetypes.guess_type(abspath)\n        if mime_type:\n            self.set_header(\"Content-Type\", mime_type)\n\n        cache_time = self.get_cache_time(path, modified, mime_type)\n\n        if cache_time > 0:\n            self.set_header(\"Expires\", datetime.datetime.utcnow() +\n                            datetime.timedelta(seconds=cache_time))\n            self.set_header(\"Cache-Control\", \"max-age=\" + str(cache_time))\n\n        self.set_extra_headers(path)\n\n        # Check the If-Modified-Since, and don't send the result if the\n        # content has not been modified\n        ims_value = self.request.headers.get(\"If-Modified-Since\")\n        if ims_value is not None:\n            date_tuple = email.utils.parsedate(ims_value)\n            if_since = datetime.datetime(*date_tuple[:6])\n            if if_since >= modified:\n                self.set_status(304)\n                return False\n        return True```\n\nUpdated code:\n```python\ndef set_headers(self):\n        \n        \n        self.set_header(\"Accept-Ranges\", \"bytes\")\n        self.set_etag_header()\n\n        if self.modified is not None:\n            self.set_header(\"Last-Modified\", self.modified)\n\n        content_type = self.get_content_type()\n        if content_type:\n            self.set_header(\"Content-Type\", content_type)\n\n        cache_time = self.get_cache_time(self.path, self.modified, content_type)\n        if cache_time > 0:\n            self.set_header(\"Expires\", datetime.datetime.utcnow() +\n                            datetime.timedelta(seconds=cache_time))\n            self.set_header(\"Cache-Control\", \"max-age=\" + str(cache_time))\n\n        self.set_extra_headers(self.path)\n```\n\nOriginal documentation:\n```python\nSet the response headers in order to ensure that client browsers\n        will cache the requested resource and not proceed to retrieve its content\n        in the case of a 304 response.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sets the content and caching headers on the response.\n\n        Returns True if the content should be returned, and False\n        if a 304 should be returned instead.", "file": "tornado_web.py", "function": "StaticFileHandler.set_headers", "project": "tornado", "change_stats": {"code_changes": 34, "doc_changes": 9, "total_changes": 43}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef log_into_subdomain(request: HttpRequest, token: str) -> HttpResponse:\n    \n    \n\n    try:\n        data = signing.loads(token, salt=_subdomain_token_salt, max_age=15)\n    except signing.SignatureExpired as e:\n        logging.warning('Subdomain cookie: {}'.format(e))\n        return HttpResponse(status=400)\n    except signing.BadSignature:\n        logging.warning('Subdomain cookie: Bad signature.')\n        return HttpResponse(status=400)\n\n    # We extract fields provided by the caller via the data object.\n    # The only fields that are required are email and subdomain (if we\n    # are simply doing login); more fields are expected if this is a\n    # new account registration flow or we're going to a specific\n    # narrow after login.\n    subdomain = get_subdomain(request)\n    if data['subdomain'] != subdomain:\n        logging.warning('Login attempt on invalid subdomain')\n        return HttpResponse(status=400)\n    email_address = data['email']\n\n    full_name = data.get('name', '')\n    is_signup = data.get('is_signup', False)\n    redirect_to = data.get('next', '')\n    full_name_validated = data.get('full_name_validated', False)\n    multiuse_object_key = data.get('multiuse_object_key', '')\n\n    # We cannot pass the actual authenticated user_profile object that\n    # was fetched by the original authentication backend and passed\n    # into redirect_and_log_into_subdomain through a signed URL token,\n    # so we need to re-fetch it from the database.\n    if is_signup:\n        # If we are creating a new user account, user_profile will\n        # always have been None, so we set that here.  In the event\n        # that a user account with this email was somehow created in a\n        # race, the eventual registration code will catch that and\n        # throw an error, so we don't need to check for that here.\n        user_profile = None\n    else:\n        # We're just trying to login.  We can be reasonably confident\n        # that this subdomain actually has a corresponding active\n        # realm, since the signed cookie proves there was one very\n        # recently.  But as part of fetching the UserProfile object\n        # for the target user, we use DummyAuthBackend, which\n        # conveniently re-validates that the realm and user account\n        # were not deactivated in the meantime.\n\n        # Note: Ideally, we'd have a nice user-facing error message\n        # for the case where this auth fails (because e.g. the realm\n        # or user was deactivated since the signed cookie was\n        # generated < 15 seconds ago), but the authentication result\n        # is correct in those cases and such a race would be very\n        # rare, so a nice error message is low priority.\n        realm = get_realm(subdomain)\n        user_profile = authenticate_remote_user(realm, email_address)\n\n    return login_or_register_remote_user(request, email_address, user_profile,\n                                         full_name,\n                                         is_signup=is_signup, redirect_to=redirect_to,\n                                         multiuse_object_key=multiuse_object_key,\n                                         full_name_validated=full_name_validated)```\n\nUpdated code:\n```python\ndef log_into_subdomain(request: HttpRequest, token: str) -> HttpResponse:\n    \n    \n    if not has_api_key_format(token):  # The tokens are intended to have the same format as API keys.\n        logging.warning(\"log_into_subdomain: Malformed token given: %s\" % (token,))\n        return HttpResponse(status=400)\n\n    data = get_login_data(token)\n    if data is None:\n        logging.warning(\"log_into_subdomain: Invalid token given: %s\" % (token,))\n        return HttpResponse(status=400)\n\n    # We extract fields provided by the caller via the data object.\n    # The only fields that are required are email and subdomain (if we\n    # are simply doing login); more fields are expected if this is a\n    # new account registration flow or we're going to a specific\n    # narrow after login.\n    subdomain = get_subdomain(request)\n    if data['subdomain'] != subdomain:\n        raise JsonableError(_(\"Invalid subdomain\"))\n    email_address = data['email']\n\n    full_name = data.get('name', '')\n    is_signup = data.get('is_signup', False)\n    redirect_to = data.get('next', '')\n    full_name_validated = data.get('full_name_validated', False)\n    multiuse_object_key = data.get('multiuse_object_key', '')\n\n    # We cannot pass the actual authenticated user_profile object that\n    # was fetched by the original authentication backend and passed\n    # into redirect_and_log_into_subdomain through a signed URL token,\n    # so we need to re-fetch it from the database.\n    if is_signup:\n        # If we are creating a new user account, user_profile will\n        # always have been None, so we set that here.  In the event\n        # that a user account with this email was somehow created in a\n        # race, the eventual registration code will catch that and\n        # throw an error, so we don't need to check for that here.\n        user_profile = None\n    else:\n        # We're just trying to login.  We can be reasonably confident\n        # that this subdomain actually has a corresponding active\n        # realm, since the signed cookie proves there was one very\n        # recently.  But as part of fetching the UserProfile object\n        # for the target user, we use DummyAuthBackend, which\n        # conveniently re-validates that the realm and user account\n        # were not deactivated in the meantime.\n\n        # Note: Ideally, we'd have a nice user-facing error message\n        # for the case where this auth fails (because e.g. the realm\n        # or user was deactivated since the signed cookie was\n        # generated < 15 seconds ago), but the authentication result\n        # is correct in those cases and such a race would be very\n        # rare, so a nice error message is low priority.\n        realm = get_realm(subdomain)\n        user_profile = authenticate_remote_user(realm, email_address)\n\n    return login_or_register_remote_user(request, email_address, user_profile,\n                                         full_name,\n                                         is_signup=is_signup, redirect_to=redirect_to,\n                                         multiuse_object_key=multiuse_object_key,\n                                         full_name_validated=full_name_validated)\n```\n\nOriginal documentation:\n```python\nGiven a valid signed authentication token (generated by\n    redirect_and_log_into_subdomain called on auth.zulip.example.com),\n    call login_or_register_remote_user, passing all the authentication\n    result data that had been encoded in the signed token.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Given a valid authentication token (generated by\n    redirect_and_log_into_subdomain called on auth.zulip.example.com),\n    call login_or_register_remote_user, passing all the authentication\n    result data that has been stored in redis, associated with this token.", "file": "zerver_views_auth.py", "function": "log_into_subdomain", "project": "zulip", "change_stats": {"code_changes": 18, "doc_changes": 6, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_closed_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' fetchClosedOrders() requires a symbol argument')\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'symbol': market['id'],\n        }\n        if since is not None:\n            request['start'] = since\n        if limit is not None:\n            request['limit'] = limit\n        response = None\n        if market['settle'] == 'USDT':\n            request['currency'] = market['settle']\n            response = self.privateGetExchangeOrderV2OrderList(self.extend(request, params))\n        elif market['swap']:\n            response = self.privateGetExchangeOrderList(self.extend(request, params))\n        else:\n            response = self.privateGetExchangeSpotOrder(self.extend(request, params))\n        #\n        # spot\n        #\n        #     {\n        #         \"code\":0,\n        #         \"msg\":\"OK\",\n        #         \"data\":{\n        #             \"total\":8,\n        #             \"rows\":[\n        #                 {\n        #                     \"orderID\":\"99232c3e-3d6a-455f-98cc-2061cdfe91bc\",\n        #                     \"stopPxEp\":0,\n        #                     \"avgPriceEp\":0,\n        #                     \"qtyType\":\"ByBase\",\n        #                     \"leavesBaseQtyEv\":0,\n        #                     \"leavesQuoteQtyEv\":0,\n        #                     \"baseQtyEv\":\"1000000000\",\n        #                     \"feeCurrency\":\"4\",\n        #                     \"stopDirection\":\"UNSPECIFIED\",\n        #                     \"symbol\":\"sETHUSDT\",\n        #                     \"side\":\"Buy\",\n        #                     \"quoteQtyEv\":250000000000,\n        #                     \"priceEp\":25000000000,\n        #                     \"ordType\":\"Limit\",\n        #                     \"timeInForce\":\"GoodTillCancel\",\n        #                     \"ordStatus\":\"Rejected\",\n        #                     \"execStatus\":\"NewRejected\",\n        #                     \"createTimeNs\":1592675305266037130,\n        #                     \"cumFeeEv\":0,\n        #                     \"cumBaseValueEv\":0,\n        #                     \"cumQuoteValueEv\":0\n        #                 },\n        #             ]\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data', {})\n        if isinstance(data, list):\n            return self.parse_orders(data, market, since, limit)\n        else:\n            rows = self.safe_value(data, 'rows', [])\n            return self.parse_orders(rows, market, since, limit)```\n\nUpdated code:\n```python\ndef fetch_closed_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:\n        \n        \n        \n        self.load_markets()\n        market = None\n        if symbol is not None:\n            market = self.market(symbol)\n        request = {\n        }\n        if market is not None:\n            request['symbol'] = market['id']\n        if since is not None:\n            request['start'] = since\n        if limit is not None:\n            request['limit'] = limit\n        response = None\n        if (symbol is None) or (self.safe_string(market, 'settle') == 'USDT'):\n            request['currency'] = self.safe_string(params, 'settle', 'USDT')\n            response = self.privateGetExchangeOrderV2OrderList(self.extend(request, params))\n        elif market['swap']:\n            response = self.privateGetExchangeOrderList(self.extend(request, params))\n        else:\n            response = self.privateGetExchangeSpotOrder(self.extend(request, params))\n        #\n        # spot\n        #\n        #     {\n        #         \"code\":0,\n        #         \"msg\":\"OK\",\n        #         \"data\":{\n        #             \"total\":8,\n        #             \"rows\":[\n        #                 {\n        #                     \"orderID\":\"99232c3e-3d6a-455f-98cc-2061cdfe91bc\",\n        #                     \"stopPxEp\":0,\n        #                     \"avgPriceEp\":0,\n        #                     \"qtyType\":\"ByBase\",\n        #                     \"leavesBaseQtyEv\":0,\n        #                     \"leavesQuoteQtyEv\":0,\n        #                     \"baseQtyEv\":\"1000000000\",\n        #                     \"feeCurrency\":\"4\",\n        #                     \"stopDirection\":\"UNSPECIFIED\",\n        #                     \"symbol\":\"sETHUSDT\",\n        #                     \"side\":\"Buy\",\n        #                     \"quoteQtyEv\":250000000000,\n        #                     \"priceEp\":25000000000,\n        #                     \"ordType\":\"Limit\",\n        #                     \"timeInForce\":\"GoodTillCancel\",\n        #                     \"ordStatus\":\"Rejected\",\n        #                     \"execStatus\":\"NewRejected\",\n        #                     \"createTimeNs\":1592675305266037130,\n        #                     \"cumFeeEv\":0,\n        #                     \"cumBaseValueEv\":0,\n        #                     \"cumQuoteValueEv\":0\n        #                 },\n        #             ]\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data', {})\n        if isinstance(data, list):\n            return self.parse_orders(data, market, since, limit)\n        else:\n            rows = self.safe_value(data, 'rows', [])\n            return self.parse_orders(rows, market, since, limit)\n```\n\nOriginal documentation:\n```python\nfetches information on multiple closed orders made by the user\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Hedged-Perpetual-API.md#queryorder\n        :param str symbol: unified market symbol of the market orders were made in\n        :param int [since]: the earliest time in ms to fetch orders for\n        :param int [limit]: the maximum number of order structures to retrieve\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :returns Order[]: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetches information on multiple closed orders made by the user\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Hedged-Perpetual-API.md#queryorder\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Contract-API-en.md#queryorder\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Hedgedd-Perpetual-API.md#query-closed-orders-by-symbol\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Spot-API-en.md#spotDataOrdersByIds\n        :param str symbol: unified market symbol of the market orders were made in\n        :param int [since]: the earliest time in ms to fetch orders for\n        :param int [limit]: the maximum number of order structures to retrieve\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.settle]: the settlement currency to fetch orders for\n        :returns Order[]: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`", "file": "python_ccxt_phemex.py", "function": "phemex.fetch_closed_orders", "project": "ccxt", "change_stats": {"code_changes": 15, "doc_changes": 6, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extract_entities(self, message: Message) -> List[Dict[Text, Any]]:\n        \n\n        if self.ent_tagger is not None:\n            text_data = self._from_text_to_crf(message)\n            features = self._sentence_to_features(text_data)\n            entities = self.ent_tagger.predict_marginals_single(features)\n            return self._from_crf_to_json(message, entities)\n        else:\n            return []```\n\nUpdated code:\n```python\ndef extract_entities(self, message: Message) -> List[Dict[Text, Any]]:\n        \n\n        if self.entity_taggers is None:\n            return []\n\n        tokens = train_utils.tokens_without_cls(message)\n        crf_tokens = self._convert_to_crf_tokens(message)\n\n        predictions = {}\n        for tag_name, entity_tagger in self.entity_taggers.items():\n            # use predicted entity tags as features for second level CRFs\n            include_tag_features = tag_name != ENTITY_ATTRIBUTE_TYPE\n            if include_tag_features:\n                self._add_tag_to_crf_token(crf_tokens, predictions)\n\n            features = self._crf_tokens_to_features(crf_tokens, include_tag_features)\n            predictions[tag_name] = entity_tagger.predict_marginals_single(features)\n\n        # convert predictions into a list of tags and a list of confidences\n        tags, confidences = self._tag_confidences(tokens, predictions)\n\n        return self.convert_predictions_into_entities(\n            message.text, tokens, tags, confidences\n        )\n```\n\nOriginal documentation:\n```python\nTake a sentence and return entities in json format\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extract entities from the given message using the trained model(s).", "file": "rasa_nlu_extractors_crf_entity_extractor.py", "function": "CRFEntityExtractor.extract_entities", "project": "rasa", "change_stats": {"code_changes": 29, "doc_changes": 4, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_updates_proxy_url(self: BuilderType, get_updates_proxy_url: str) -> BuilderType:\n        \n        \n        warn(\n            \"`ApplicationBuilder.get_updates_proxy_url` is deprecated since version \"\n            \"NEXT.VERSION. Use `ApplicationBuilder.get_updates_proxy` instead.\",\n            PTBDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.get_updates_proxy(get_updates_proxy_url)```\n\nUpdated code:\n```python\ndef get_updates_proxy_url(self: BuilderType, get_updates_proxy_url: str) -> BuilderType:\n        \n        \n        warn(\n            \"`ApplicationBuilder.get_updates_proxy_url` is deprecated since version \"\n            \"20.7. Use `ApplicationBuilder.get_updates_proxy` instead.\",\n            PTBDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.get_updates_proxy(get_updates_proxy_url)\n```\n\nOriginal documentation:\n```python\nLegacy name for :meth:`get_updates_proxy`, kept for backward compatibility.\n\n        .. seealso:: :meth:`proxy`\n\n        .. deprecated:: NEXT.VERSION\n\n        Args:\n            get_updates_proxy_url (:obj:`str` | ``httpx.Proxy`` | ``httpx.URL``): See\n                :paramref:`telegram.ext.ApplicationBuilder.get_updates_proxy.get_updates_proxy`.\n\n        Returns:\n            :class:`ApplicationBuilder`: The same builder with the updated argument.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Legacy name for :meth:`get_updates_proxy`, kept for backward compatibility.\n\n        .. seealso:: :meth:`proxy`\n\n        .. deprecated:: 20.7\n\n        Args:\n            get_updates_proxy_url (:obj:`str` | ``httpx.Proxy`` | ``httpx.URL``): See\n                :paramref:`telegram.ext.ApplicationBuilder.get_updates_proxy.get_updates_proxy`.\n\n        Returns:\n            :class:`ApplicationBuilder`: The same builder with the updated argument.", "file": "telegram_ext__applicationbuilder.py", "function": "ApplicationBuilder.get_updates_proxy_url", "project": "python-telegram-bot", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_tasks(self, tasks: list[Task]):\n        \n        \n        \n        if not tasks:\n            return\n\n        # Topologically sort the new tasks to ensure correct dependency order\n        new_tasks = self._topological_sort(tasks)\n\n        if not self.tasks:\n            # If there are no existing tasks, set the new tasks as the current tasks\n            self.tasks = new_tasks\n\n        else:\n            # Find the length of the common prefix between existing and new tasks\n            prefix_length = 0\n            for old_task, new_task in zip(self.tasks, new_tasks):\n                if old_task.task_id != new_task.task_id or old_task.instruction != new_task.instruction:\n                    break\n                prefix_length += 1\n\n            # Combine the common prefix with the remainder of the new tasks\n            final_tasks = self.tasks[:prefix_length] + new_tasks[prefix_length:]\n            self.tasks = final_tasks\n        \n        # Update current_task_id to the first unfinished task in the merged list\n        self._update_current_task()\n\n        # Update the task map for quick access to tasks by ID\n        self.task_map = {task.task_id: task for task in self.tasks}```\n\nUpdated code:\n```python\ndef add_tasks(self, tasks: list[Task]):\n        \n        \n        \n        if not tasks:\n            return\n\n        # Topologically sort the new tasks to ensure correct dependency order\n        new_tasks = self._topological_sort(tasks)\n\n        if not self.tasks:\n            # If there are no existing tasks, set the new tasks as the current tasks\n            self.tasks = new_tasks\n\n        else:\n            # Find the length of the common prefix between existing and new tasks\n            prefix_length = 0\n            for old_task, new_task in zip(self.tasks, new_tasks):\n                if old_task.task_id != new_task.task_id or old_task.instruction != new_task.instruction:\n                    break\n                prefix_length += 1\n\n            # Combine the common prefix with the remainder of the new tasks\n            final_tasks = self.tasks[:prefix_length] + new_tasks[prefix_length:]\n            self.tasks = final_tasks\n\n        # Update current_task_id to the first unfinished task in the merged list\n        self._update_current_task()\n\n        # Update the task map for quick access to tasks by ID\n        self.task_map = {task.task_id: task for task in self.tasks}\n```\n\nOriginal documentation:\n```python\nIntegrates new tasks into the existing plan, ensuring dependency order is maintained.\n        \n        This method performs two primary functions based on the current state of the task list:\n        1. If there are no existing tasks, it topologically sorts the provided tasks to ensure \n        correct execution order based on dependencies, and sets these as the current tasks.\n        2. If there are existing tasks, it merges the new tasks with the existing ones. It maintains \n        any common prefix of tasks (based on task_id and instruction) and appends the remainder \n        of the new tasks. The current task is updated to the first unfinished task in this merged list.\n\n        Args:\n            tasks (list[Task]): A list of tasks (may be unordered) to add to the plan.\n\n        Returns:\n            None: The method updates the internal state of the plan but does not return anything.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Integrates new tasks into the existing plan, ensuring dependency order is maintained.\n\n        This method performs two primary functions based on the current state of the task list:\n        1. If there are no existing tasks, it topologically sorts the provided tasks to ensure\n        correct execution order based on dependencies, and sets these as the current tasks.\n        2. If there are existing tasks, it merges the new tasks with the existing ones. It maintains\n        any common prefix of tasks (based on task_id and instruction) and appends the remainder\n        of the new tasks. The current task is updated to the first unfinished task in this merged list.\n\n        Args:\n            tasks (list[Task]): A list of tasks (may be unordered) to add to the plan.\n\n        Returns:\n            None: The method updates the internal state of the plan but does not return anything.", "file": "metagpt_schema.py", "function": "Plan.add_tasks", "project": "MetaGPT", "change_stats": {"code_changes": 4, "doc_changes": 10, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get(self, proxy_str):\n        \n        \n        \n        data = self.__conn.hget(name=self.name, key=proxy_str)\n        if data:\n            return data.decode('utf-8') if PY3 else data\n        else:\n            return None```\n\nUpdated code:\n```python\ndef get(self):\n        \n        \n        \n        proxies = self.__conn.hkeys(self.name)\n        proxy = choice(proxies) if proxies else None\n        if proxy:\n            return self.__conn.hget(self.name, proxy)\n        else:\n            return None\n```\n\nOriginal documentation:\n```python\n\u4ecehash\u4e2d\u83b7\u53d6\u5bf9\u5e94\u7684proxy, \u4f7f\u7528\u524d\u9700\u8981\u8c03\u7528changeTable()\n        :param proxy_str: proxy str\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u4ecehash\u4e2d\u968f\u673a\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\n        :return:", "file": "db_ssdbClient.py", "function": "SsdbClient.get", "project": "proxy_pool", "change_stats": {"code_changes": 11, "doc_changes": 5, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update_host_ip(self, hostIP, modified_profiles) -> str:\n        \n        \n        \n        if self.is_interface and hostIP not in modified_profiles:\n            if hostIP := self.metadata_man.get_host_ip():\n                self.db.set_host_ip(hostIP)\n        return hostIP```\n\nUpdated code:\n```python\ndef update_host_ip(self, hostIP: str, modified_profiles: Set[str]) -> str:\n        \n        \n        \n        if self.is_interface and hostIP not in modified_profiles:\n            if hostIP := self.metadata_man.get_host_ip():\n                self.db.set_host_ip(hostIP)\n        return hostIP\n```\n\nOriginal documentation:\n```python\nwhen running on an interface we keep track of the host IP. If there was no\n        # modified TWs in the host IP, we check if the network was changed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "when running on an interface we keep track of the host IP.\n        If there was no  modified TWs in the host IP, we check if the\n        network was changed.", "file": "slips_main.py", "function": "Main.update_host_ip", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \n\n    \n\n    islink, join, isdir = path.islink, path.join, path.isdir\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that listdir is global in this module due\n        # to earlier import-*.\n        names = listdir(top)\n    except OSError as err:\n        if onerror is not None:\n            onerror(err)\n        return\n\n    dirs, nondirs = [], []\n    for name in names:\n        if isdir(join(top, name)):\n            dirs.append(name)\n        else:\n            nondirs.append(name)\n\n    if topdown:\n        yield top, dirs, nondirs\n    for name in dirs:\n        new_path = join(top, name)\n        if followlinks or not islink(new_path):\n            yield from walk(new_path, topdown, onerror, followlinks)\n    if not topdown:\n        yield top, dirs, nondirs```\n\nUpdated code:\n```python\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \n\n    \n\n    dirs = []\n    nondirs = []\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that scandir is global in this module due\n        # to earlier import-*.\n        scandir_it = scandir(top)\n    except OSError as error:\n        if onerror is not None:\n            onerror(error)\n        return\n\n    while True:\n        try:\n            try:\n                entry = next(scandir_it)\n            except StopIteration:\n                break\n        except OSError as error:\n            if onerror is not None:\n                onerror(error)\n            return\n\n        try:\n            is_dir = entry.is_dir()\n        except OSError:\n            # If is_dir() raises an OSError, consider that the entry is not\n            # a directory, same behaviour than os.path.isdir().\n            is_dir = False\n\n        if is_dir:\n            dirs.append(entry.name)\n        else:\n            nondirs.append(entry.name)\n\n        if not topdown and is_dir:\n            # Bottom-up: recurse into sub-directory, but exclude symlinks to\n            # directories if followlinks is False\n            if followlinks:\n                walk_into = True\n            else:\n                try:\n                    is_symlink = entry.is_symlink()\n                except OSError:\n                    # If is_symlink() raises an OSError, consider that the\n                    # entry is not a symbolic link, same behaviour than\n                    # os.path.islink().\n                    is_symlink = False\n                walk_into = not is_symlink\n\n            if walk_into:\n                yield from walk(entry.path, topdown, onerror, followlinks)\n\n    # Yield before recursion if going top down\n    if topdown:\n        yield top, dirs, nondirs\n\n        # Recurse into sub-directories\n        islink, join = path.islink, path.join\n        for name in dirs:\n            new_path = join(top, name)\n            # Issue #23605: os.path.islink() is used instead of caching\n            # entry.is_symlink() result during the loop on os.scandir() because\n            # the caller can replace the directory entry during the \"yield\"\n            # above.\n            if followlinks or not islink(new_path):\n                yield from walk(new_path, topdown, onerror, followlinks)\n    else:\n        # Yield after recursion if going bottom up\n        yield top, dirs, nondirs\n```\n\nOriginal documentation:\n```python\nDirectory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false is ineffective, since the directories in dirnames have\n    already been generated by the time dirnames itself is generated. No matter\n    the value of topdown, the list of subdirectories is retrieved before the\n    tuples for the directory and its subdirectories are generated.\n\n    By default errors from the os.listdir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an OSError instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print(root, \"consumes\", end=\"\")\n        print(sum([getsize(join(root, name)) for name in files]), end=\"\")\n        print(\"bytes in\", len(files), \"non-directory files\")\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Directory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false is ineffective, since the directories in dirnames have\n    already been generated by the time dirnames itself is generated. No matter\n    the value of topdown, the list of subdirectories is retrieved before the\n    tuples for the directory and its subdirectories are generated.\n\n    By default errors from the os.scandir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an OSError instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print(root, \"consumes\", end=\"\")\n        print(sum([getsize(join(root, name)) for name in files]), end=\"\")\n        print(\"bytes in\", len(files), \"non-directory files\")\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories", "file": "Lib_os.py", "function": "walk", "project": "cpython", "change_stats": {"code_changes": 76, "doc_changes": 4, "total_changes": 80}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _request_handler(self, **kwargs):\n        \n        \n        \n        route = self._get_current_route(request)\n\n        try:\n            event = self._construct_event(request, self.port, route.binary_types)\n        except UnicodeDecodeError:\n            return ServiceErrorResponses.lambda_failure_response()\n\n        stdout_stream = io.BytesIO()\n\n        try:\n            self.lambda_runner.invoke(route.function_name, event, stdout=stdout_stream, stderr=self.stderr)\n        except FunctionNotFound:\n            return ServiceErrorResponses.lambda_not_found_response()\n\n        lambda_response, lambda_logs, _ = LambdaOutputParser.get_lambda_output(stdout_stream)\n\n        if self.stderr and lambda_logs:\n            # Write the logs to stderr if available.\n            self.stderr.write(lambda_logs)\n\n        try:\n            (status_code, headers, body) = self._parse_lambda_output(lambda_response,\n                                                                     route.binary_types,\n                                                                     request)\n        except (KeyError, TypeError, ValueError):\n            LOG.error(\"Function returned an invalid response (must include one of: body, headers or \"\n                      \"statusCode in the response object). Response received: %s\", lambda_response)\n            return ServiceErrorResponses.lambda_failure_response()\n\n        return self.service_response(body, headers, status_code)```\n\nUpdated code:\n```python\ndef _request_handler(self, **kwargs):\n        \n        \n        \n        route = self._get_current_route(request)\n\n        try:\n            event = self._construct_event(request, self.port, route.binary_types)\n        except UnicodeDecodeError:\n            return ServiceErrorResponses.lambda_failure_response()\n\n        stdout_stream = io.BytesIO()\n        stdout_stream_writer = StreamWriter(stdout_stream, self.is_debugging)\n\n        try:\n            self.lambda_runner.invoke(route.function_name, event, stdout=stdout_stream_writer, stderr=self.stderr)\n        except FunctionNotFound:\n            return ServiceErrorResponses.lambda_not_found_response()\n\n        lambda_response, lambda_logs, _ = LambdaOutputParser.get_lambda_output(stdout_stream)\n\n        if self.stderr and lambda_logs:\n            # Write the logs to stderr if available.\n            self.stderr.write(lambda_logs)\n\n        try:\n            (status_code, headers, body) = self._parse_lambda_output(lambda_response,\n                                                                     route.binary_types,\n                                                                     request)\n        except (KeyError, TypeError, ValueError):\n            LOG.error(\"Function returned an invalid response (must include one of: body, headers or \"\n                      \"statusCode in the response object). Response received: %s\", lambda_response)\n            return ServiceErrorResponses.lambda_failure_response()\n\n        return self.service_response(body, headers, status_code)\n```\n\nOriginal documentation:\n```python\nWe handle all requests to the host:port. The general flow of handling a request is as follows\n\n        * Fetch request from the Flask Global state. This is where Flask places the request and is per thread so\n          multiple requests are still handled correctly\n        * Find the Lambda function to invoke by doing a look up based on the request.endpoint and method\n        * If we don't find the function, we will throw a 502 (just like the 404 and 405 responses we get\n          from Flask.\n        * Since we found a Lambda function to invoke, we construct the Lambda Event from the request\n        * Then Invoke the Lambda function (docker container)\n        * We then transform the response or errors we get from the Invoke and return the data back to\n          the caller\n\n        :param kwargs dict: Keyword Args that are passed to the function from Flask. This happens when we have\n            Path Parameters.\n        :return: Response object\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "We handle all requests to the host:port. The general flow of handling a request is as follows\n\n        * Fetch request from the Flask Global state. This is where Flask places the request and is per thread so\n          multiple requests are still handled correctly\n        * Find the Lambda function to invoke by doing a look up based on the request.endpoint and method\n        * If we don't find the function, we will throw a 502 (just like the 404 and 405 responses we get\n          from Flask.\n        * Since we found a Lambda function to invoke, we construct the Lambda Event from the request\n        * Then Invoke the Lambda function (docker container)\n        * We then transform the response or errors we get from the Invoke and return the data back to\n          the caller\n\n        Parameters\n        ----------\n        kwargs dict\n            Keyword Args that are passed to the function from Flask. This happens when we have path parameters\n\n        Returns\n        -------\n        Response object", "file": "samcli_local_apigw_local_apigw_service.py", "function": "LocalApigwService._request_handler", "project": "aws-sam-cli", "change_stats": {"code_changes": 5, "doc_changes": 13, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _upsample(self, method, limit: int | None = None, fill_value=None):\n        \n        \n\n        \n        if self._from_selection:\n            raise ValueError(\n                \"Upsampling from level= or on= selection \"\n                \"is not supported, use .set_index(...) \"\n                \"to explicitly set index to datetime-like\"\n            )\n\n        ax = self.ax\n        obj = self._selected_obj\n        binner = self.binner\n        res_index = self._adjust_binner_for_upsample(binner)\n\n        # if we have the same frequency as our axis, then we are equal sampling\n        if (\n            limit is None\n            and to_offset(ax.inferred_freq) == self.freq\n            and len(obj) == len(res_index)\n        ):\n            result = obj.copy()\n            result.index = res_index\n        else:\n            if method == \"asfreq\":\n                method = None\n            result = obj.reindex(\n                res_index, method=method, limit=limit, fill_value=fill_value\n            )\n\n        return self._wrap_result(result)```\n\nUpdated code:\n```python\ndef _upsample(self, method, limit: int | None = None, fill_value=None):\n        \n        \n        \n        if self._from_selection:\n            raise ValueError(\n                \"Upsampling from level= or on= selection \"\n                \"is not supported, use .set_index(...) \"\n                \"to explicitly set index to datetime-like\"\n            )\n\n        ax = self.ax\n        obj = self._selected_obj\n        binner = self.binner\n        res_index = self._adjust_binner_for_upsample(binner)\n\n        # if we have the same frequency as our axis, then we are equal sampling\n        if (\n            limit is None\n            and to_offset(ax.inferred_freq) == self.freq\n            and len(obj) == len(res_index)\n        ):\n            result = obj.copy()\n            result.index = res_index\n        else:\n            if method == \"asfreq\":\n                method = None\n            result = obj.reindex(\n                res_index, method=method, limit=limit, fill_value=fill_value\n            )\n\n        return self._wrap_result(result)\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        method : string {'backfill', 'bfill', 'pad',\n            'ffill', 'asfreq'} method for upsampling\n        limit : int, default None\n            Maximum size gap to fill when reindexing\n        fill_value : scalar, default None\n            Value to use for missing values\n\n        See Also\n        --------\n        .fillna: Fill NA/NaN values using the specified method.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        method : string {'backfill', 'bfill', 'pad',\n            'ffill', 'asfreq'} method for upsampling\n        limit : int, default None\n            Maximum size gap to fill when reindexing\n        fill_value : scalar, default None\n            Value to use for missing values", "file": "pandas_core_resample.py", "function": "DatetimeIndexResampler._upsample", "project": "pandas", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convert_colorspace(arr, fromspace, tospace):\n    \n    \n    fromdict = {'rgb': lambda im: im, 'hsv': hsv2rgb, 'rgb cie': rgbcie2rgb,\n                'xyz': xyz2rgb, 'yuv': yuv2rgb, 'yiq': yiq2rgb,\n                'ypbpr': ypbpr2rgb, 'ycbcr': ycbcr2rgb}\n    todict = {'rgb': lambda im: im, 'hsv': rgb2hsv, 'rgb cie': rgb2rgbcie,\n              'xyz': rgb2xyz, 'yuv': rgb2yuv, 'yiq': rgb2yiq,\n              'ypbpr': rgb2ypbpr, 'ycbcr': rgb2ycbcr}\n\n    fromspace = fromspace.lower()\n    tospace = tospace.lower()\n    if fromspace not in fromdict:\n        msg = '`fromspace` has to be one of {}'.format(fromdict.keys())\n        raise ValueError(msg)\n    if tospace not in todict:\n        msg = '`tospace` has to be one of {}'.format(todict.keys())\n        raise ValueError(msg)\n\n    return todict[tospace](fromdict[fromspace](arr))```\n\nUpdated code:\n```python\ndef convert_colorspace(arr, fromspace, tospace):\n    \n    \n    fromdict = {'rgb': lambda im: im, 'hsv': hsv2rgb, 'rgb cie': rgbcie2rgb,\n                'xyz': xyz2rgb, 'yuv': yuv2rgb, 'yiq': yiq2rgb,\n                'ypbpr': ypbpr2rgb, 'ycbcr': ycbcr2rgb, 'ydbdr': ydbdr2rgb}\n    todict = {'rgb': lambda im: im, 'hsv': rgb2hsv, 'rgb cie': rgb2rgbcie,\n              'xyz': rgb2xyz, 'yuv': rgb2yuv, 'yiq': rgb2yiq,\n              'ypbpr': rgb2ypbpr, 'ycbcr': rgb2ycbcr, 'ydbdr': rgb2ydbdr}\n\n    fromspace = fromspace.lower()\n    tospace = tospace.lower()\n    if fromspace not in fromdict:\n        msg = '`fromspace` has to be one of {}'.format(fromdict.keys())\n        raise ValueError(msg)\n    if tospace not in todict:\n        msg = '`tospace` has to be one of {}'.format(todict.keys())\n        raise ValueError(msg)\n\n    return todict[tospace](fromdict[fromspace](arr))\n```\n\nOriginal documentation:\n```python\nConvert an image array to a new color space.\n\n    Parameters\n    ----------\n    arr : array_like\n        The image to convert.\n    fromspace : {'RGB', 'HSV', 'RGB CIE', 'XYZ', 'YUV', 'YIQ', 'YPbPr', 'YCbCr'}\n        The color space to convert from. Can be specified in lower case.\n    tospace : {'RGB', 'HSV', 'RGB CIE', 'XYZ', 'YUV', 'YIQ', 'YPbPr', 'YCbCr'}\n        The color space to convert to. Can be specified in lower case.\n\n    Returns\n    -------\n    out : ndarray\n        The converted image.\n\n    Notes\n    -----\n    Conversion is performed through the \"central\" RGB color space,\n    i.e. conversion from XYZ to HSV is implemented as ``XYZ -> RGB -> HSV``\n    instead of directly.\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> img = data.astronaut()\n    >>> img_hsv = convert_colorspace(img, 'RGB', 'HSV')\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert an image array to a new color space.\n\n    Parameters\n    ----------\n    arr : array_like\n        The image to convert.\n    fromspace : {'RGB', 'HSV', 'RGB CIE', 'XYZ', 'YUV',\n                 'YIQ', 'YPbPr', 'YCbCr', 'YDbDr'}\n        The color space to convert from. Can be specified in lower case.\n    tospace : {'RGB', 'HSV', 'RGB CIE', 'XYZ', 'YUV',\n               'YIQ', 'YPbPr', 'YCbCr', 'YDbDr'}\n        The color space to convert to. Can be specified in lower case.\n\n    Returns\n    -------\n    out : ndarray\n        The converted image.\n\n    Notes\n    -----\n    Conversion is performed through the \"central\" RGB color space,\n    i.e. conversion from XYZ to HSV is implemented as ``XYZ -> RGB -> HSV``\n    instead of directly.\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> img = data.astronaut()\n    >>> img_hsv = convert_colorspace(img, 'RGB', 'HSV')", "file": "skimage_color_colorconv.py", "function": "convert_colorspace", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load(cls, fname, *args, **kwargs):\n        \n        \n\n        \n        kwargs['mmap'] = kwargs.get('mmap', 'r')\n        result = super(LdaModel, cls).load(fname, *args, **kwargs)\n        try:\n            result.state = super(LdaModel, cls).load(fname + '.state', *args, **kwargs)\n        except Exception as e:\n            logging.warning(\"failed to load state from %s: %s\" % (fname + '.state', e))\n        return result```\n\nUpdated code:\n```python\ndef load(cls, fname, *args, **kwargs):\n        \n        \n\n        \n        kwargs['mmap'] = kwargs.get('mmap', None)\n        result = super(LdaModel, cls).load(fname, *args, **kwargs)\n        state_fname = utils.smart_extension(fname, '.state')\n        try:\n            result.state = super(LdaModel, cls).load(state_fname, *args, **kwargs)\n        except Exception as e:\n            logging.warning(\"failed to load state from %s: %s\" % (state_fname, e))\n        return result\n```\n\nOriginal documentation:\n```python\nLoad a previously saved object from file (also see `save`).\n\n        Large arrays are mmap'ed back as read-only (shared memory).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a previously saved object from file (also see `save`).\n\n        Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n\n            >>> LdaModel.load(fname, mmap='r')", "file": "gensim_models_ldamodel.py", "function": "LdaModel.load", "project": "gensim", "change_stats": {"code_changes": 9, "doc_changes": 6, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save_attachments(cache_key, attachments, job):\n    \n    \n    \n\n    event = job[\"event\"]\n\n    for attachment in attachments:\n        save_attachment(\n            cache_key,\n            attachment,\n            event.project,\n            event.event_id,\n            key_id=job[\"key_id\"],\n            group_id=event.group_id,\n            start_time=job[\"start_time\"],\n        )```\n\nUpdated code:\n```python\ndef save_attachments(cache_key: Optional[str], attachments: list[Attachment], job: Job) -> None:\n    \n    \n    \n\n    event = job[\"event\"]\n\n    for attachment in attachments:\n        save_attachment(\n            cache_key,\n            attachment,\n            event.project,\n            event.event_id,\n            key_id=job[\"key_id\"],\n            group_id=event.group_id,\n            start_time=job[\"start_time\"],\n        )\n```\n\nOriginal documentation:\n```python\nPersists cached event attachments into the file store.\n\n    Emits one outcome per attachment, either ACCEPTED on success or\n    INVALID(missing_chunks) if retrieving the attachment fails.\n\n    :param attachments: A filtered list of attachments to save.\n    :param job:         The job context container.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Persists cached event attachments into the file store.\n\n    Emits one outcome per attachment, either ACCEPTED on success or\n    INVALID(missing_chunks) if retrieving the attachment fails.\n    :param cache_key:  The cache key at which the attachment is stored for\n                       debugging purposes.\n    :param attachments: A filtered list of attachments to save.\n    :param job:         The job context container.", "file": "src_sentry_event_manager.py", "function": "save_attachments", "project": "sentry", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, center, r, theta1, theta2,\n                 dtheta=0.1, **kwargs):\n        \n        \n\n        \n\t# MGDTODO: Implement me\n        xc, yc = center\n        rads = (math.pi/180.)*npy.arange(theta1, theta2+0.1*dtheta, dtheta)\n        xs = r*npy.cos(rads)+xc\n        ys = r*npy.sin(rads)+yc\n        verts = [center]\n        verts.extend([(x,y) for x,y in zip(xs,ys)])\n\n        Polygon.__init__(self, verts, **kwargs)```\n\nUpdated code:\n```python\ndef __init__(self, center, r, theta1, theta2, **kwargs):\n        \n        \n\n        \n        Patch.__init__(self, **kwargs)\n        self.theta1 = theta1\n        self.theta2 = theta2\n        self._path = Path.wedge(theta1, theta2)\n        self._path_transform = transforms.Affine2D() \\\n            .scale(r).translate(*center)\n```\n\nOriginal documentation:\n```python\nDraw a wedge centered at x,y tuple center with radius r that\n        sweeps theta1 to theta2 (angles)\n\n        dtheta is the resolution in degrees\n\n        Valid kwargs are:\n        %(Patch)s\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Draw a wedge centered at x,y tuple center with radius r that\n        sweeps theta1 to theta2 (angles)\n\n        Valid kwargs are:\n        %(Patch)s", "file": "lib_matplotlib_patches.py", "function": "Wedge.__init__", "project": "matplotlib", "change_stats": {"code_changes": 20, "doc_changes": 4, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef gather_distributed_param(param: torch.Tensor, keep_vars: bool = False):\n    \n    \n    \n    param_ = param if keep_vars else param.detach()\n    if is_distributed_tensor(param_):\n        return to_global(param_)\n    elif is_customized_distributed_tensor(param_):\n        return to_global_for_customized_distributed_tensor(param_)\n    else:\n        return param_```\n\nUpdated code:\n```python\ndef gather_distributed_param(param: torch.Tensor, keep_vars: bool = False) -> torch.Tensor:\n    \n    \n    \n    param_ = param if keep_vars else param.detach()\n    if is_distributed_tensor(param_):\n        return to_global(param_)\n    elif is_customized_distributed_tensor(param_):\n        return to_global_for_customized_distributed_tensor(param_)\n    else:\n        return param_\n```\n\nOriginal documentation:\n```python\nGather the complete parameter for saving if passed in param is distributed.\n\n    Args:\n        param (torch.Tensor): A model parameter, might be d_tensor.\n        keep_vars (bool, optional): Whether to return the parameter in calculation graph. Defaults to False.\n\n    Returns:\n        torch.Tensor: the complete parameter\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gather the complete parameter for saving if passed in param is distributed under tp setting.\n\n    Args:\n        param (torch.Tensor): A model parameter, might be d_tensor.\n        keep_vars (bool, optional): Whether to return the parameter in calculation graph. Defaults to False.\n\n    Returns:\n        torch.Tensor: the complete parameter", "file": "colossalai_checkpoint_io_utils.py", "function": "gather_distributed_param", "project": "ColossalAI", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef evaluate(self):\n        \n\n        \n        iterator = self._iterators['main']\n        target = self._targets['main']\n        eval_func = self.eval_func or target\n\n        if self.eval_hook:\n            self.eval_hook(self)\n\n        if hasattr(iterator, 'reset'):\n            iterator.reset()\n            it = iterator\n        else:\n            it = copy.copy(iterator)\n\n        summary = reporter_module.DictSummary()\n\n        for batch in it:\n            observation = {}\n            with reporter_module.report_scope(observation):\n                in_arrays = self.converter(batch, self.device)\n                with function.no_backprop_mode():\n                    if isinstance(in_arrays, tuple):\n                        eval_func(*in_arrays)\n                    elif isinstance(in_arrays, dict):\n                        eval_func(**in_arrays)\n                    else:\n                        eval_func(in_arrays)\n\n            summary.add(observation)\n\n        return summary.compute_mean()```\n\nUpdated code:\n```python\ndef evaluate(self):\n        \n\n        \n        iterator = self._iterators['main']\n        eval_func = self.eval_func or self._targets['main']\n\n        if self.eval_hook:\n            self.eval_hook(self)\n\n        if hasattr(iterator, 'reset'):\n            iterator.reset()\n            it = iterator\n        else:\n            it = copy.copy(iterator)\n\n        summary = reporter_module.DictSummary()\n\n        for batch in it:\n            observation = {}\n            with reporter_module.report_scope(observation):\n                in_arrays = self.converter(batch, self.device)\n                with function.no_backprop_mode():\n                    if isinstance(in_arrays, tuple):\n                        eval_func(*in_arrays)\n                    elif isinstance(in_arrays, dict):\n                        eval_func(**in_arrays)\n                    else:\n                        eval_func(in_arrays)\n\n            summary.add(observation)\n\n        return summary.compute_mean()\n```\n\nOriginal documentation:\n```python\nEvaluates the model and returns a result dictionary.\n\n        This method runs the evaluation loop over the validation dataset. It\n        accumulates the reported values to :class:`~chainer.DictSummary` and\n        returns a dictionary whose values are means computed by the summary.\n\n        Users can override this method to customize the evaluation routine.\n\n        .. note::\n\n            This method encloses :attr:`eval_func` calls with\n            :func:`function.no_backprop_mode` context, so all calculations\n            using :class:`~chainer.Function`\\ s inside :attr:`eval_func` don't\n            make computational graphs. It is for reducing the memory\n            consumption.\n\n        Returns:\n            dict: Result dictionary. This dictionary is further reported via\n                :func:`~chainer.report` without specifying any observer.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Evaluates the model and returns a result dictionary.\n\n        This method runs the evaluation loop over the validation dataset. It\n        accumulates the reported values to :class:`~chainer.DictSummary` and\n        returns a dictionary whose values are means computed by the summary.\n\n        Users can override this method to customize the evaluation routine.\n\n        .. note::\n\n            This method encloses :attr:`eval_func` calls with\n            :func:`function.no_backprop_mode` context, so all calculations\n            using :class:`~chainer.FunctionNode`\\s inside\n            :attr:`eval_func` don't make computational graphs. It is for\n            reducing the memory consumption.\n\n        Returns:\n            dict: Result dictionary. This dictionary is further reported via\n            :func:`~chainer.report` without specifying any observer.", "file": "chainer_training_extensions_evaluator.py", "function": "Evaluator.evaluate", "project": "chainer", "change_stats": {"code_changes": 5, "doc_changes": 10, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(self) -> None:\n        \n        self.thread_id = threading.get_ident()\n        setup_result = self._setup_recorder()\n\n        if not setup_result:\n            # Give up if we could not connect\n            self.hass.add_job(self.async_connection_failed)\n            return\n\n        schema_status = migration.validate_db_schema(self.hass, self, self.get_session)\n        if schema_status is None:\n            # Give up if we could not validate the schema\n            self.hass.add_job(self.async_connection_failed)\n            return\n        self.schema_version = schema_status.current_version\n\n        if schema_status.valid:\n            self._setup_run()\n        else:\n            self.migration_in_progress = True\n            self.migration_is_live = migration.live_migration(schema_status)\n\n        self.hass.add_job(self.async_connection_success)\n        database_was_ready = self.migration_is_live or schema_status.valid\n\n        if database_was_ready:\n            # If the migrate is live or the schema is valid, we need to\n            # wait for startup to complete. If its not live, we need to continue\n            # on.\n            self._activate_and_set_db_ready()\n\n            # We wait to start a live migration until startup has finished\n            # since it can be cpu intensive and we do not want it to compete\n            # with startup which is also cpu intensive\n            if self._wait_startup_or_shutdown() is SHUTDOWN_TASK:\n                # Shutdown happened before Home Assistant finished starting\n                self.migration_in_progress = False\n                # Make sure we cleanly close the run if\n                # we restart before startup finishes\n                self._shutdown()\n                return\n\n        if not schema_status.valid:\n            if self._migrate_schema_and_setup_run(schema_status):\n                self.schema_version = SCHEMA_VERSION\n                if not self._event_listener:\n                    # If the schema migration takes so long that the end\n                    # queue watcher safety kicks in because MAX_QUEUE_BACKLOG\n                    # is reached, we need to reinitialize the listener.\n                    self.hass.add_job(self.async_initialize)\n            else:\n                persistent_notification.create(\n                    self.hass,\n                    \"The database migration failed, check [the logs](/config/logs).\",\n                    \"Database Migration Failed\",\n                    \"recorder_database_migration\",\n                )\n                self.hass.add_job(self.async_set_db_ready)\n                self._shutdown()\n                return\n\n        if not database_was_ready:\n            self._activate_and_set_db_ready()\n\n        # Catch up with missed statistics\n        self._schedule_compile_missing_statistics()\n        _LOGGER.debug(\"Recorder processing the queue\")\n        self._adjust_lru_size()\n        self.hass.add_job(self._async_set_recorder_ready_migration_done)\n        self._run_event_loop()\n        self._shutdown()```\n\nUpdated code:\n```python\ndef run(self) -> None:\n        \n        try:\n            self._run()\n        finally:\n            # Ensure shutdown happens cleanly if\n            # anything goes wrong in the run loop\n            self._shutdown()\n```\n\nOriginal documentation:\n```python\nStart processing events to save.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run the recorder thread.", "file": "homeassistant_components_recorder_core.py", "function": "Recorder.run", "project": "core", "change_stats": {"code_changes": 78, "doc_changes": 4, "total_changes": 82}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef checkCloseDo (self, event, byMenu):\n        \n        \n        \n                \n        if (self.dirty):\n            bits = os.path.splitext(self.saveDestination)\n            title = '\"' + os.path.basename(bits[0]) + '\"'\n            if title == '\"\"': title = 'your story' \n\n            message = 'Are you sure you want to close ' + title + ' without saving changes?'\n            dialog = wx.MessageDialog(self, message, 'Unsaved Changes', \\\n                                      wx.ICON_WARNING | wx.YES_NO | wx.NO_DEFAULT)\n            if (dialog.ShowModal() == wx.ID_NO):\n                event.Veto()\n                return\n            else:\n                self.dirty = False\n        \n        # ask all our widgets to close any editor windows\n        \n        for w in list(self.storyPanel.widgets):\n            if isinstance(w, PassageWidget):\n                w.closeEditor()\n\n        self.app.removeStory(self, byMenu)\n        if event != None:\n            event.Skip()\n        self.Destroy()```\n\nUpdated code:\n```python\ndef checkCloseDo (self, event, byMenu):\n        \n        \n        \n                \n        if (self.dirty):\n            bits = os.path.splitext(self.saveDestination)\n            title = '\"' + os.path.basename(bits[0]) + '\"'\n            if title == '\"\"': title = 'your story' \n\n            message = 'Do you want to save the changes to ' + title + ' before closing?'\n            dialog = wx.MessageDialog(self, message, 'Unsaved Changes', \\\n                                      wx.ICON_WARNING | wx.YES_NO | wx.CANCEL | wx.YES_DEFAULT)\n            result = dialog.ShowModal();\n            if (result == wx.ID_CANCEL):\n                event.Veto()\n                return\n            elif (result == wx.ID_NO):\n                self.dirty = False\n            else:\n                self.save(None)\n                if self.dirty:\n                    event.Veto()\n                    return\n        \n        # ask all our widgets to close any editor windows\n        \n        for w in list(self.storyPanel.widgets):\n            if isinstance(w, PassageWidget):\n                w.closeEditor()\n\n        self.app.removeStory(self, byMenu)\n        if event != None:\n            event.Skip()\n        self.Destroy()\n```\n\nOriginal documentation:\n```python\nIf this instance's dirty flag is set, asks the user to confirm that they don't want to save changes.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "If this instance's dirty flag is set, asks the user if they want to save the changes.", "file": "storyframe.py", "function": "StoryFrame.checkCloseDo", "project": "twine", "change_stats": {"code_changes": 16, "doc_changes": 4, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef complete_hosts(self, text, line):\n        \n        \n        \n        commands = [\"add\", \"remove\", \"dc\"]\n\n        mline = line.partition(' ')[2]\n        offs = len(mline) - len(text)\n        return [s[offs:] for s in commands if s.startswith(mline)]```\n\nUpdated code:\n```python\ndef complete_hosts(self, text, line, begidx, endidx):\n        \"\"\n\n        commands = [\"add\", \"remove\", \"dc\"]\n\n        mline = line.partition(' ')[2]\n        offs = len(mline) - len(text)\n        return [s[offs:] for s in commands if s.startswith(mline)]\n```\n\nOriginal documentation:\n```python\nTab-complete 'hosts' commands.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tab-complete 'creds' commands.", "file": "cme_protocols_smb_db_navigator.py", "function": "navigator.complete_hosts", "project": "CrackMapExec", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pad(\n        self,\n        images: List[np.ndarray],\n        constant_values: Union[float, Iterable[float]] = 0,\n        return_pixel_mask: bool = True,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        data_format: Optional[ChannelDimension] = None,\n    ) -> np.ndarray:\n        \n        \n        \n        pad_size = get_max_height_width(images)\n\n        padded_images = [\n            self._pad_image(image, pad_size, constant_values=constant_values, data_format=data_format)\n            for image in images\n        ]\n        data = {\"pixel_values\": padded_images}\n\n        if return_pixel_mask:\n            masks = [make_pixel_mask(image=image, output_size=pad_size) for image in images]\n            data[\"pixel_mask\"] = masks\n\n        return BatchFeature(data=data, tensor_type=return_tensors)```\n\nUpdated code:\n```python\ndef pad(\n        self,\n        images: List[np.ndarray],\n        constant_values: Union[float, Iterable[float]] = 0,\n        return_pixel_mask: bool = True,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        data_format: Optional[ChannelDimension] = None,\n    ) -> BatchFeature:\n        \n        \n        \n        pad_size = get_max_height_width(images)\n\n        padded_images = [\n            self._pad_image(image, pad_size, constant_values=constant_values, data_format=data_format)\n            for image in images\n        ]\n        data = {\"pixel_values\": padded_images}\n\n        if return_pixel_mask:\n            masks = [make_pixel_mask(image=image, output_size=pad_size) for image in images]\n            data[\"pixel_mask\"] = masks\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n```\n\nOriginal documentation:\n```python\nPads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n        in the batch and optionally returns their corresponding pixel mask.\n\n        Args:\n            image (`np.ndarray`):\n                Image to pad.\n            constant_values (`float` or `Iterable[float]`, *optional*):\n                The value to use for the padding if `mode` is `\"constant\"`.\n            return_pixel_mask (`bool`, *optional*, defaults to `True`):\n                Whether to return a pixel mask.\n            input_channel_dimension (`ChannelDimension`, *optional*):\n                The channel dimension format of the image. If not provided, it will be inferred from the input image.\n            data_format (`str` or `ChannelDimension`, *optional*):\n                The channel dimension format of the image. If not provided, it will be the same as the input image.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n        in the batch and optionally returns their corresponding pixel mask.\n\n        Args:\n            image (`np.ndarray`):\n                Image to pad.\n            constant_values (`float` or `Iterable[float]`, *optional*):\n                The value to use for the padding if `mode` is `\"constant\"`.\n            return_pixel_mask (`bool`, *optional*, defaults to `True`):\n                Whether to return a pixel mask.\n            return_tensors (`str` or `TensorType`, *optional*):\n                The type of tensors to return. Can be one of:\n                    - Unset: Return a list of `np.ndarray`.\n                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n            data_format (`str` or `ChannelDimension`, *optional*):\n                The channel dimension format of the image. If not provided, it will be the same as the input image.", "file": "src_transformers_models_oneformer_image_processing_oneformer.py", "function": "OneFormerImageProcessor.pad", "project": "transformers", "change_stats": {"code_changes": 4, "doc_changes": 11, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef interpolate_index(self,\n                          metas='', title='', css='', config='',\n                          scripts='', app_entry='', favicon=''):\n        \n        \n        \n        return _interpolate(self.index_string,\n                            metas=metas,\n                            title=title,\n                            css=css,\n                            config=config,\n                            scripts=scripts,\n                            favicon=favicon,\n                            app_entry=app_entry)```\n\nUpdated code:\n```python\ndef interpolate_index(self,\n                          metas='', title='', css='', config='',\n                          scripts='', app_entry='', favicon='', renderer=''):\n        \n        \n        \n        return _interpolate(self.index_string,\n                            metas=metas,\n                            title=title,\n                            css=css,\n                            config=config,\n                            scripts=scripts,\n                            favicon=favicon,\n                            renderer=renderer,\n                            app_entry=app_entry)\n```\n\nOriginal documentation:\n```python\nCalled to create the initial HTML string that is loaded on page.\n        Override this method to provide you own custom HTML.\n\n        :Example:\n\n            class MyDash(dash.Dash):\n                def interpolate_index(self, **kwargs):\n                    return '''\n                    <!DOCTYPE html>\n                    <html>\n                        <head>\n                            <title>My App</title>\n                        </head>\n                        <body>\n                            <div id=\"custom-header\">My custom header</div>\n                            {app_entry}\n                            {config}\n                            {scripts}\n                            <div id=\"custom-footer\">My custom footer</div>\n                        </body>\n                    </html>\n                    '''.format(\n                        app_entry=kwargs.get('app_entry'),\n                        config=kwargs.get('config'),\n                        scripts=kwargs.get('scripts'))\n\n        :param metas: Collected & formatted meta tags.\n        :param title: The title of the app.\n        :param css: Collected & formatted css dependencies as <link> tags.\n        :param config: Configs needed by dash-renderer.\n        :param scripts: Collected & formatted scripts tags.\n        :param app_entry: Where the app will render.\n        :param favicon: A favicon <link> tag if found in assets folder.\n        :return: The interpolated HTML string for the index.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Called to create the initial HTML string that is loaded on page.\n        Override this method to provide you own custom HTML.\n\n        :Example:\n\n            class MyDash(dash.Dash):\n                def interpolate_index(self, **kwargs):\n                    return '''\n                    <!DOCTYPE html>\n                    <html>\n                        <head>\n                            <title>My App</title>\n                        </head>\n                        <body>\n                            <div id=\"custom-header\">My custom header</div>\n                            {app_entry}\n                            {config}\n                            {scripts}\n                            {renderer}\n                            <div id=\"custom-footer\">My custom footer</div>\n                        </body>\n                    </html>\n                    '''.format(\n                        app_entry=kwargs.get('app_entry'),\n                        config=kwargs.get('config'),\n                        scripts=kwargs.get('scripts'),\n                        renderer=kwargs.get('renderer'))\n\n        :param metas: Collected & formatted meta tags.\n        :param title: The title of the app.\n        :param css: Collected & formatted css dependencies as <link> tags.\n        :param config: Configs needed by dash-renderer.\n        :param scripts: Collected & formatted scripts tags.\n        :param renderer: A script tag that instantiates the DashRenderer.\n        :param app_entry: Where the app will render.\n        :param favicon: A favicon <link> tag if found in assets folder.\n        :return: The interpolated HTML string for the index.", "file": "dash_dash.py", "function": "Dash.interpolate_index", "project": "dash", "change_stats": {"code_changes": 5, "doc_changes": 7, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef legalize_graph(gm: torch.fx.GraphModule):\n    \n    \n\n    \n    # Build an adjacency list representation of node dependencies in the graph. This also\n    # serves as a list of nodes that still need to be inserted into the new, topologically\n    # sorted graph.\n    dependencies = {node: node.all_input_nodes.copy() for node in gm.graph.nodes}\n\n    # Construct a new graph that will contain all nodes in topologically sorted order.\n    new_graph = torch.fx.Graph()\n    value_remap: Dict[torch.fx.Node, torch.fx.Node] = {}\n\n    # Copy over all nodes with no dependencies.\n    for node, deps in dependencies.items():\n        if not deps:\n            value_remap[node] = new_graph.node_copy(node, lambda n: value_remap[n])\n\n    # Remove the copied over nodes from the adjacency list.\n    for copied_node in value_remap.keys():\n        del dependencies[copied_node]\n\n    # While there are still nodes to insert into the new graph:\n    while dependencies:\n        copied_this_round = []\n\n        # Copy over all nodes whose dependencies already exist in the new graph.\n        for node, deps in dependencies.items():\n            all_deps_copied = True\n            for dep in deps:\n                if dep not in value_remap:\n                    all_deps_copied = False\n\n            if all_deps_copied:\n                value_remap[node] = new_graph.node_copy(node, lambda n: value_remap[n])\n                copied_this_round.append(node)\n\n        # Delete all nodes copied over in this iteration from dependencies.\n        for copied_node in copied_this_round:\n            del dependencies[copied_node]\n\n    # Replace the old graph with the new, topologically sorted one.\n    gm.graph = new_graph```\n\nUpdated code:\n```python\ndef legalize_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    \n    \n    \n    indeg = {node: 0 for node in gm.graph.nodes}\n    new_graph = torch.fx.Graph()\n    # Track how many unfulfilled dependencies each node has\n    for node in gm.graph.nodes:\n        for user in node.users:\n            indeg[user] += 1\n    queue: collections.deque = collections.deque()\n    # Add all nodes with no dependencies to the queue\n    for node in gm.graph.nodes:\n        if indeg[node] == 0:\n            queue.append(node)\n    env: Dict[torch.fx.Node, torch.fx.Node] = {}\n    # Pop nodes from the queue, and add nodes that have had all their\n    # dependencies fulfilled\n    while len(queue) > 0:\n        cur = queue.popleft()\n        env[cur] = new_graph.node_copy(cur, lambda x: env[x])\n        for user in cur.users:\n            indeg[user] -= 1\n            if indeg[user] == 0:\n                queue.append(user)\n    # If the new graph's size is not as large as the old one, then there must be\n    # a cycle (i.e. some node's dependencies were not satisfied.)\n    if len(new_graph.nodes) < len(gm.graph.nodes):\n        raise RuntimeError(f\"Input graph has cycles, unable to add {[node for node in indeg if indeg[node] != 0]}\")\n    gm.graph = new_graph\n    return gm\n```\n\nOriginal documentation:\n```python\nReplace the graph of the given GraphModule with one that contains the same nodes as the\n    original, but in topologically sorted order.\n\n    This is used by the merge_matmul transformation below, which disturbs the topologically sorted\n    order of its input GraphModule, so that this order is restored before further transformation.\n\n    Arguments:\n        gm: The graph module to topologically sort. It is modified in-place.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replace the graph of the given GraphModule with one that contains the same nodes as the\n    original, but in topologically sorted order.\n\n    This is used by the merge_matmul transformation below, which disturbs the topologically sorted\n    order of its input GraphModule, so that this order is restored before further transformation.\n\n    Arguments:\n        gm: The graph module to topologically sort. It is modified in-place.\n\n    Returns:\n        The graph module in-place sorted", "file": "torch_fx_passes_tools_common.py", "function": "legalize_graph", "project": "pytorch", "change_stats": {"code_changes": 67, "doc_changes": 5, "total_changes": 72}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef init_encoder(self, model_path: str, config_path: str) -> None:\n        \n        \n        self.encoder_config = load_config(config_path)\n        self.encoder = setup_encoder_model(self.encoder_config)\n        self.encoder_criterion = self.encoder.load_checkpoint(\n            self.encoder_config, model_path, eval=True, use_cuda=self.use_cuda\n        )\n        self.encoder_ap = AudioProcessor(**self.encoder_config.audio)```\n\nUpdated code:\n```python\ndef init_encoder(self, model_path: str, config_path: str, use_cuda=False) -> None:\n        \n        \n        self.use_cuda = use_cuda\n        self.encoder_config = load_config(config_path)\n        self.encoder = setup_encoder_model(self.encoder_config)\n        self.encoder_criterion = self.encoder.load_checkpoint(\n            self.encoder_config, model_path, eval=True, use_cuda=use_cuda\n        )\n        self.encoder_ap = AudioProcessor(**self.encoder_config.audio)\n```\n\nOriginal documentation:\n```python\nInitialize a speaker encoder model.\n\n        Args:\n            model_path (str): Model file path.\n            config_path (str): Model config file path.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize a speaker encoder model.\n\n        Args:\n            model_path (str): Model file path.\n            config_path (str): Model config file path.\n            use_cuda (bool, optional): Use CUDA. Defaults to False.", "file": "TTS_tts_utils_managers.py", "function": "EmbeddingManager.init_encoder", "project": "TTS", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extract_template(self, image):\n        \n        \n        \n        self._load_image(image)\n        prev = set(self.templates.keys())\n        new = {}\n        for item in self.items:\n            name = self.match_template(item.image)\n            if name not in prev:\n                new[name] = item.image\n\n        return new```\n\nUpdated code:\n```python\ndef extract_template(self, image, folder=None):\n        \n        \n        \n        self._load_image(image)\n        prev = set(self.templates.keys())\n        new = {}\n        for item in self.items:\n            name = self.match_template(item.image)\n            if name not in prev:\n                new[name] = item.image\n\n        if folder is not None:\n            for name, im in new.items():\n                save_image(im, os.path.join(folder, f'{name}.png'))\n\n        return new\n```\n\nOriginal documentation:\n```python\nArgs:\n            image (np.ndarray):\n\n        Returns:\n            dict: Newly found templates. Key: str, template name. Value: np.ndarray\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            image (np.ndarray):\n            folder (str): Save templates if `folder` is provided\n\n        Returns:\n            dict: Newly found templates. Key: str, template name. Value: np.ndarray", "file": "module_statistics_item.py", "function": "ItemGrid.extract_template", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef safe_asarray(X, dtype=None, order=None, copy=False, force_all_finite=True):\n    \n    if sparse.issparse(X):\n        if copy:\n            X = X.copy()\n        if force_all_finite:\n            _assert_all_finite(X.data)\n        # enforces dtype on data array (order should be kept the same).\n        X.data = np.asarray(X.data, dtype=dtype)\n    else:\n        X = np.array(X, dtype=dtype, order=order, copy=copy)\n        if force_all_finite:\n            _assert_all_finite(X)\n    return X```\n\nUpdated code:\n```python\ndef safe_asarray(X, dtype=None, order=None, copy=False, force_all_finite=True):\n    \n    \n    if sp.issparse(X):\n        if not isinstance(X, (sp.coo_matrix, sp.csc_matrix, sp.csr_matrix)):\n            X = X.tocsr()\n        elif copy:\n            X = X.copy()\n        if force_all_finite:\n            _assert_all_finite(X.data)\n        # enforces dtype on data array (order should be kept the same).\n        X.data = np.asarray(X.data, dtype=dtype)\n    else:\n        X = np.array(X, dtype=dtype, order=order, copy=copy)\n        if force_all_finite:\n            _assert_all_finite(X)\n    return X\n```\n\nOriginal documentation:\n```python\nConvert X to an array or CSC/CSR/COO sparse matrix.\n\n    Prevents copying X when possible; sparse matrices are passed through.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert X to an array or CSC/CSR/COO sparse matrix.\n\n    Prevents copying X when possible. Sparse matrices in CSR, CSC and COO\n    formats are passed through. Other sparse formats are converted to CSR\n    (somewhat arbitrarily).\n\n    If a specific compressed sparse format is required, use atleast2d_or_cs{c,r}\n    instead.", "file": "sklearn_utils_validation.py", "function": "safe_asarray", "project": "scikit-learn", "change_stats": {"code_changes": 9, "doc_changes": 9, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_pybinding(cls, argtypes: List[str], source_code: str) -> Any:\n        \n        \n        \n        parseargs = \", \".join(\n            f\"parse_arg<{argtype.replace('const ', '')}>(args, {n})\"\n            for n, argtype in enumerate(argtypes)\n        )\n        suffix = cls.suffix_template % (len(argtypes), len(argtypes), parseargs)\n        result = cls.load(source_code + suffix)\n        assert isinstance(result, ModuleType)\n        return result.kernel```\n\nUpdated code:\n```python\ndef load_pybinding(\n        cls, argtypes: List[str], source_code: str, cuda: bool = False\n    ) -> Any:\n        \n        \n        \n        parseargs = \", \".join(\n            f\"parse_arg<{argtype.replace('const ', '')}>(args, {n})\"\n            for n, argtype in enumerate(argtypes)\n        )\n        suffix = cls.suffix_template % (\n            cls.entry_function,\n            cls.extra_parse_arg,\n            cls.entry_function,\n            len(argtypes),\n            len(argtypes),\n            cls.call_entry_function % parseargs,\n            cls.entry_function,\n            cls.entry_function,\n            cls.entry_function,\n            cls.entry_function,\n        )\n        result = cls.load(source_code + suffix, cuda)\n        assert isinstance(result, ModuleType)\n        return getattr(result, cls.entry_function)\n```\n\nOriginal documentation:\n```python\nWrap a C++ function in fast Python bindings.\n\n        Args:\n            argtypes: The types of args to kernel(), e.g. [\"float*\", \"long\"]\n            source_code: C++ source code containing a kernel() function\n\n        Returns:\n            A python version of kernel()\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Wrap a C++ function in fast Python bindings.\n\n        Args:\n            argtypes: The types of args to ENTRY_FUNCTION(), e.g. [\"float*\", \"long\"]\n            source_code: C++ source code containing a ENTRY_FUNCTION() function\n\n        Returns:\n            A python version of ENTRY_FUNCTION()", "file": "torch__inductor_codecache.py", "function": "CppPythonBindingsCodeCache.load_pybinding", "project": "pytorch", "change_stats": {"code_changes": 23, "doc_changes": 8, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _AdjustSourcesAndConvertToFilterHierarchy(\n    spec, options, gyp_dir, sources, excluded_sources, list_excluded):\n  \n  \n  # Exclude excluded sources coming into the generator.\n  excluded_sources.update(set(spec.get('sources_excluded', [])))\n  # Add excluded sources into sources for good measure.\n  sources.update(excluded_sources)\n  # Convert to proper windows form.\n  # NOTE: sources goes from being a set to a list here.\n  # NOTE: excluded_sources goes from being a set to a list here.\n  sources = _FixPaths(sources)\n  # Convert to proper windows form.\n  excluded_sources = _FixPaths(excluded_sources)\n\n  excluded_idl = _IdlFilesHandledNonNatively(spec, sources)\n\n  precompiled_related = _GetPrecompileRelatedFiles(spec)\n  # Find the excluded ones, minus the precompiled header related ones.\n  fully_excluded = [i for i in excluded_sources if i not in precompiled_related]\n\n  # Convert to folders and the right slashes.\n  sources = [i.split('\\\\') for i in sources]\n  sources = _ConvertSourcesToFilterHierarchy(sources, excluded=fully_excluded,\n                                             list_excluded=list_excluded)\n\n  return sources, excluded_sources, excluded_idl```\n\nUpdated code:\n```python\ndef _AdjustSourcesAndConvertToFilterHierarchy(\n    spec, options, gyp_dir, sources, excluded_sources, list_excluded, version):\n  \n  \n  # Exclude excluded sources coming into the generator.\n  excluded_sources.update(OrderedSet(spec.get('sources_excluded', [])))\n  # Add excluded sources into sources for good measure.\n  sources.update(excluded_sources)\n  # Convert to proper windows form.\n  # NOTE: sources goes from being a set to a list here.\n  # NOTE: excluded_sources goes from being a set to a list here.\n  sources = _FixPaths(sources)\n  # Convert to proper windows form.\n  excluded_sources = _FixPaths(excluded_sources)\n\n  excluded_idl = _IdlFilesHandledNonNatively(spec, sources)\n\n  precompiled_related = _GetPrecompileRelatedFiles(spec)\n  # Find the excluded ones, minus the precompiled header related ones.\n  fully_excluded = [i for i in excluded_sources if i not in precompiled_related]\n\n  # Convert to folders and the right slashes.\n  sources = [i.split('\\\\') for i in sources]\n  sources = _ConvertSourcesToFilterHierarchy(sources, excluded=fully_excluded,\n                                             list_excluded=list_excluded,\n                                             msvs_version=version)\n\n  # Prune filters with a single child to flatten ugly directory structures\n  # such as ../../src/modules/module1 etc.\n  while len(sources) == 1 and isinstance(sources[0], MSVSProject.Filter):\n    sources = sources[0].contents\n\n  return sources, excluded_sources, excluded_idl\n```\n\nOriginal documentation:\n```python\nAdjusts the list of sources and excluded sources.\n\n  Also converts the sets to lists.\n\n  Arguments:\n    spec: The target dictionary containing the properties of the target.\n    options: Global generator options.\n    gyp_dir: The path to the gyp file being processed.\n    sources: A set of sources to be included for this project.\n    excluded_sources: A set of sources to be excluded for this project.\n  Returns:\n    A trio of (list of sources, list of excluded sources,\n               path of excluded IDL file)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adjusts the list of sources and excluded sources.\n\n  Also converts the sets to lists.\n\n  Arguments:\n    spec: The target dictionary containing the properties of the target.\n    options: Global generator options.\n    gyp_dir: The path to the gyp file being processed.\n    sources: A set of sources to be included for this project.\n    excluded_sources: A set of sources to be excluded for this project.\n    version: A MSVSVersion object.\n  Returns:\n    A trio of (list of sources, list of excluded sources,\n               path of excluded IDL file)", "file": "gyp_pylib_gyp_generator_msvs.py", "function": "_AdjustSourcesAndConvertToFilterHierarchy", "project": "node-gyp", "change_stats": {"code_changes": 14, "doc_changes": 3, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unpooling_1d(x, ksize, stride=None, pad=0, outsize=None, cover_all=True):\n    \n\n    \n    if outsize is not None and isinstance(outsize, int):\n        outsize = (outsize,)\n    return UnpoolingND(1, ksize, stride, pad, outsize, cover_all)(x)```\n\nUpdated code:\n```python\ndef unpooling_1d(x, ksize, stride=None, pad=0):\n    \n\n    \n    if len(x.shape[2:]) != 1:\n        raise ValueError(\n            'The number of dimensions under channel dimension of the input '\n            '\\'x\\' should be 1. But the actual ndim was {}.'.fromat(\n                len(x.shape[2:])))\n    return UnpoolingND(1, ksize, stride=stride, pad=pad)(x)\n```\n\nOriginal documentation:\n```python\nInverse operation of 1-dimensional spatial pooling.\n\n    .. warning::\n\n        This feature is experimental. The interface can change in the future.\n\n    This function calls :func:`~functions.unpooling_nd` with\n    ``ndim = 1``. \n    This function acts similary to :class:`~functions.Deconvolution1D`, but\n    it spreads input 1-dimensional array's value without any parameter instead\n    of computing the inner products.\n\n    Args:\n        x (~chainer.Variable): Input variable.\n        ksize (int): Size of pooling window.\n        stride (int or None): Stride of pooling applications.\n            If ``None`` is specified, then it uses same stride as the pooling\n            window size.\n        pad (int): Spatial padding width for the input array.\n        outsize (None or int): Expected output size of unpooling\n            operation :math:`out_1`. If ``None``, the size is estimated from\n            input size, stride and padding.\n        cover_all (bool): If ``True``, the pooling window is assumed to cover\n            all of the ouput array, eventually the output size may be smaller\n            than that in the case ``cover_all`` is ``False``.\n\n    Returns:\n        ~chainer.Variable: Output variable.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inverse operation of 1-dimensional spatial pooling.\n\n    .. warning::\n\n        This feature is experimental. The interface can change in the future.\n\n    .. note::\n\n        This function calls :func:`~functions.unpooling_nd` with\n        ``ndim = 1``, so see the details of the behavior in\n        :func:`~functions.unpooling_nd`.", "file": "chainer_", "function": "unpooling_1d", "project": "chainer", "change_stats": {"code_changes": 13, "doc_changes": 27, "total_changes": 40}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_lambda_output(stdout_stream: io.StringIO) -> Tuple[str, bool]:\n        \n        \n        \n        lambda_response = stdout_stream.getvalue()\n\n        # When the Lambda Function returns an Error/Exception, the output is added to the stdout of the container. From\n        # our perspective, the container returned some value, which is not always true. Since the output is the only\n        # information we have, we need to inspect this to understand if the container returned a some data or raised an\n        # error\n        is_lambda_user_error_response = LambdaOutputParser.is_lambda_error_response(lambda_response)\n\n        return lambda_response, is_lambda_user_error_response```\n\nUpdated code:\n```python\ndef get_lambda_output(\n        stdout_stream_str: io.StringIO, stdout_stream_bytes: Optional[io.BytesIO] = None\n    ) -> Tuple[Union[str, bytes], bool]:\n        \n        \n        \n        lambda_response: Union[str, bytes] = stdout_stream_str.getvalue()\n        if stdout_stream_bytes and not lambda_response:\n            lambda_response = stdout_stream_bytes.getvalue()\n\n        # When the Lambda Function returns an Error/Exception, the output is added to the stdout of the container. From\n        # our perspective, the container returned some value, which is not always true. Since the output is the only\n        # information we have, we need to inspect this to understand if the container returned a some data or raised an\n        # error\n        is_lambda_user_error_response = LambdaOutputParser.is_lambda_error_response(lambda_response)\n\n        return lambda_response, is_lambda_user_error_response\n```\n\nOriginal documentation:\n```python\nThis method will extract read the given stream and return the response from Lambda function separated out\n        from any log statements it might have outputted. Logs end up in the stdout stream if the Lambda function\n        wrote directly to stdout using System.out.println or equivalents.\n\n        Parameters\n        ----------\n        stdout_stream : io.BaseIO\n            Stream to fetch data from\n\n        Returns\n        -------\n        str\n            String data containing response from Lambda function\n        bool\n            If the response is an error/exception from the container\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method will extract read the given stream and return the response from Lambda function separated out\n        from any log statements it might have outputted. Logs end up in the stdout stream if the Lambda function\n        wrote directly to stdout using System.out.println or equivalents.\n\n        Parameters\n        ----------\n        stdout_stream_str : io.BaseIO\n            Stream to fetch data from\n\n        stdout_stream_bytes : Optional[io.BytesIO], optional\n            Stream to fetch raw bytes data from\n\n        Returns\n        -------\n        str\n            String data containing response from Lambda function\n        bool\n            If the response is an error/exception from the container", "file": "samcli_local_services_base_local_service.py", "function": "LambdaOutputParser.get_lambda_output", "project": "aws-sam-cli", "change_stats": {"code_changes": 10, "doc_changes": 7, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_config():\n        \n        \n        \n        if not ConfigurationManager._config:\n            ConfigurationManager.load()\n        return ConfigurationManager._config```\n\nUpdated code:\n```python\ndef get_config(locations=None):\n        \n        \n        \n        if not ConfigurationManager.__config:\n            ConfigurationManager.__config = ConfigurationLoader.load()\n            RemoteConfiguration().update()\n\n        if locations:\n            ConfigurationManager.load(locations)\n\n        return ConfigurationManager.__config\n```\n\nOriginal documentation:\n```python\nGet or create and get statically cached configuration.\n\n        :return: A dictionary representing config files.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get cached configuration.\n\n        :return: A dictionary representing Mycroft configuration.", "file": "mycroft_configuration_config.py", "function": "ConfigurationManager.get_config", "project": "mycroft-core", "change_stats": {"code_changes": 15, "doc_changes": 6, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef geam(transa, transb, alpha, a, beta, b, c):\n    \n    \n    assert a.ndim == b.ndim == c.ndim == 2\n    assert a.dtype == b.dtype == c.dtype\n    dtype = a.dtype.char\n    if dtype == 'f':\n        func = cublas.sgeam\n    elif dtype == 'd':\n        func = cublas.dgeam\n    elif dtype == 'F':\n        func = cublas.cgeam\n    elif dtype == 'D':\n        func = cublas.zgeam\n    else:\n        raise TypeError('invalid dtype')\n\n    transa = _trans_to_cublas_op(transa)\n    transb = _trans_to_cublas_op(transb)\n    m, n = c.shape\n    if transa == cublas.CUBLAS_OP_N:\n        assert a.shape == (m, n)\n    else:\n        assert a.shape == (n, m)\n    if transb == cublas.CUBLAS_OP_N:\n        assert b.shape == (m, n)\n    else:\n        assert b.shape == (n, m)\n\n    alpha, alpha_ptr = _get_scalar_ptr(alpha, a.dtype)\n    beta, beta_ptr = _get_scalar_ptr(beta, a.dtype)\n    handle = device.get_cublas_handle()\n    orig_mode = cublas.getPointerMode(handle)\n    if isinstance(alpha, cupy.ndarray) or isinstance(beta, cupy.ndarray):\n        if not isinstance(alpha, cupy.ndarray):\n            alpha = cupy.array(alpha)\n            alpha_ptr = alpha.data.ptr\n        if not isinstance(beta, cupy.ndarray):\n            beta = cupy.array(beta)\n            beta_ptr = beta.data.ptr\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_DEVICE)\n    else:\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_HOST)\n\n    lda, transa = _decide_ld_and_trans(a, transa)\n    ldb, transb = _decide_ld_and_trans(b, transb)\n    if not (lda is None or ldb is None):\n        if c._f_contiguous:\n            func(handle, transa, transb, m, n, alpha_ptr,\n                 a.data.ptr, lda, beta_ptr, b.data.ptr, ldb, c.data.ptr, m)\n            cublas.setPointerMode(handle, orig_mode)\n            return\n        elif c._c_contiguous:\n            # Computes c.T = alpha * a.T + beta * b.T\n            func(handle, 1 - transa, 1 - transb, n, m, alpha_ptr,\n                 a.data.ptr, lda, beta_ptr, b.data.ptr, ldb, c.data.ptr, n)\n            cublas.setPointerMode(handle, orig_mode)\n            return\n    a, lda = _change_order_if_necessary(a, lda)\n    b, ldb = _change_order_if_necessary(b, ldb)\n    cc = c\n    if not c._f_contiguous:\n        cc = c.copy(order='F')\n    func(handle, transa, transb, m, n, alpha_ptr, a.data.ptr, lda,\n         beta_ptr, b.data.ptr, ldb, cc.data.ptr, m)\n    cublas.setPointerMode(handle, orig_mode)\n    if not c._f_contiguous:\n        c[...] = cc```\n\nUpdated code:\n```python\ndef geam(transa, transb, alpha, a, beta, b, out=None):\n    \n    \n    assert a.ndim == b.ndim == 2\n    assert a.dtype == b.dtype\n    dtype = a.dtype.char\n    if dtype == 'f':\n        func = cublas.sgeam\n    elif dtype == 'd':\n        func = cublas.dgeam\n    elif dtype == 'F':\n        func = cublas.cgeam\n    elif dtype == 'D':\n        func = cublas.zgeam\n    else:\n        raise TypeError('invalid dtype')\n\n    transa = _trans_to_cublas_op(transa)\n    transb = _trans_to_cublas_op(transb)\n    if transa == cublas.CUBLAS_OP_N:\n        m, n = a.shape\n    else:\n        n, m = a.shape\n    if transb == cublas.CUBLAS_OP_N:\n        assert b.shape == (m, n)\n    else:\n        assert b.shape == (n, m)\n    if out is None:\n        out = cupy.empty((m, n), dtype=dtype, order='F')\n    else:\n        assert out.ndim == 2\n        assert out.shape == (m, n)\n        assert out.dtype == dtype\n\n    alpha, alpha_ptr = _get_scalar_ptr(alpha, a.dtype)\n    beta, beta_ptr = _get_scalar_ptr(beta, a.dtype)\n    handle = device.get_cublas_handle()\n    orig_mode = cublas.getPointerMode(handle)\n    if isinstance(alpha, cupy.ndarray) or isinstance(beta, cupy.ndarray):\n        if not isinstance(alpha, cupy.ndarray):\n            alpha = cupy.array(alpha)\n            alpha_ptr = alpha.data.ptr\n        if not isinstance(beta, cupy.ndarray):\n            beta = cupy.array(beta)\n            beta_ptr = beta.data.ptr\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_DEVICE)\n    else:\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_HOST)\n\n    lda, transa = _decide_ld_and_trans(a, transa)\n    ldb, transb = _decide_ld_and_trans(b, transb)\n    if not (lda is None or ldb is None):\n        if out._f_contiguous:\n            func(handle, transa, transb, m, n, alpha_ptr,\n                 a.data.ptr, lda, beta_ptr, b.data.ptr, ldb, out.data.ptr, m)\n            cublas.setPointerMode(handle, orig_mode)\n            return out\n        elif out._c_contiguous:\n            # Computes alpha * a.T + beta * b.T\n            func(handle, 1 - transa, 1 - transb, n, m, alpha_ptr,\n                 a.data.ptr, lda, beta_ptr, b.data.ptr, ldb, out.data.ptr, n)\n            cublas.setPointerMode(handle, orig_mode)\n            return out\n\n    a, lda = _change_order_if_necessary(a, lda)\n    b, ldb = _change_order_if_necessary(b, ldb)\n    c = out\n    if not out._f_contiguous:\n        c = out.copy(order='F')\n    func(handle, transa, transb, m, n, alpha_ptr, a.data.ptr, lda,\n         beta_ptr, b.data.ptr, ldb, c.data.ptr, m)\n    cublas.setPointerMode(handle, orig_mode)\n    if not out._f_contiguous:\n        out[...] = c\n    return out\n```\n\nOriginal documentation:\n```python\nComputes c = alpha * op(a) + beta * op(b)\n\n    op(a) = a if transa is 'N', op(a) = a.T if transa is 'T',\n    op(a) = a.T.conj() if transa is 'H'.\n    op(b) = b if transb is 'N', op(b) = b.T if transb is 'T',\n    op(b) = b.T.conj() if transb is 'H'.\n\n    Note: c will be updated.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes alpha * op(a) + beta * op(b)\n\n    op(a) = a if transa is 'N', op(a) = a.T if transa is 'T',\n    op(a) = a.T.conj() if transa is 'H'.\n    op(b) = b if transb is 'N', op(b) = b.T if transb is 'T',\n    op(b) = b.T.conj() if transb is 'H'.", "file": "cupy_cublas.py", "function": "geam", "project": "cupy", "change_stats": {"code_changes": 47, "doc_changes": 6, "total_changes": 53}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_jit_arguments(engine_kwargs: Optional[Dict[str, bool]] = None):\n    \n    \n    \n    if engine_kwargs is None:\n        engine_kwargs = {}\n\n    nopython = engine_kwargs.get(\"nopython\", True)\n    nogil = engine_kwargs.get(\"nogil\", False)\n    parallel = engine_kwargs.get(\"parallel\", False)\n    return nopython, nogil, parallel```\n\nUpdated code:\n```python\ndef get_jit_arguments(\n    engine_kwargs: Optional[Dict[str, bool]] = None\n) -> Tuple[bool, bool, bool]:\n    \n    \n    \n    if engine_kwargs is None:\n        engine_kwargs = {}\n\n    nopython = engine_kwargs.get(\"nopython\", True)\n    nogil = engine_kwargs.get(\"nogil\", False)\n    parallel = engine_kwargs.get(\"parallel\", False)\n    return nopython, nogil, parallel\n```\n\nOriginal documentation:\n```python\nReturn arguments to pass to numba.JIT, falling back on pandas default JIT settings.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return arguments to pass to numba.JIT, falling back on pandas default JIT settings.\n\n    Parameters\n    ----------\n    engine_kwargs : dict, default None\n        user passed keyword arguments for numba.JIT\n\n    Returns\n    -------\n    (bool, bool, bool)\n        nopython, nogil, parallel", "file": "pandas_core_util_numba_.py", "function": "get_jit_arguments", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 12, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_conv_gradinputs_shape_1axis(kernel_shape, top_shape, border_mode,\n                                    subsample, dilation):\n    \n    \n\n    \n    if None in [kernel_shape, top_shape, border_mode,\n                subsample, dilation]:\n        return None\n    if subsample != 1:\n        return None\n\n    # Implicit dilated kernel shape\n    dil_kernel_shape = (kernel_shape - 1) * dilation + 1\n    if border_mode == \"half\":\n        pad = dil_kernel_shape // 2\n    elif border_mode == \"full\":\n        pad = dil_kernel_shape - 1\n    elif border_mode == \"valid\":\n        pad = 0\n    else:\n        pad = border_mode\n        if pad < 0:\n            raise ValueError(\"border_mode must be >= 0\")\n\n    # In case of symbolic shape, we want to build the smallest graph\n    # image_shape = (top_shape - 1) * s - 2 * pad + dil_kernel_shape + a\n    # where 0 <= a < subsample, but we have checked that subsample == 1\n    if pad == 0:\n        image_shape = (top_shape + dil_kernel_shape - 1)\n    else:\n        image_shape = (top_shape - 2 * pad + dil_kernel_shape - 1)\n\n    return image_shape```\n\nUpdated code:\n```python\ndef get_conv_gradinputs_shape_1axis(kernel_shape, top_shape, border_mode,\n                                    subsample, dilation):\n    \n    \n\n    \n    if None in [kernel_shape, top_shape, border_mode,\n                subsample, dilation]:\n        return None\n    if subsample != 1:\n        return None\n\n    # Implicit dilated kernel shape\n    dil_kernel_shape = (kernel_shape - 1) * dilation + 1\n    if border_mode == \"half\":\n        pad_l = pad_r = dil_kernel_shape // 2\n    elif border_mode == \"full\":\n        pad_l = pad_r = dil_kernel_shape - 1\n    elif border_mode == \"valid\":\n        pad_l = pad_r = 0\n    else:\n        if isinstance(border_mode, tuple):\n            pad_l, pad_r = border_mode\n        else:\n            pad_l = pad_r = border_mode\n        if pad_l < 0 or pad_r < 0:\n            raise ValueError(\"border_mode must be >= 0\")\n\n    # In case of symbolic shape, we want to build the smallest graph\n    # image_shape = (top_shape - 1) * s - 2 * pad + dil_kernel_shape + a\n    # where 0 <= a < subsample, but we have checked that subsample == 1\n    image_shape = (top_shape + dil_kernel_shape - 1)\n    if pad_l > 0:\n        image_shape -= pad_l\n    if pad_r > 0:\n        image_shape -= pad_r\n\n    return image_shape\n```\n\nOriginal documentation:\n```python\nThis function tries to compute the image shape of convolution gradInputs.\n\n    The image shape can only be computed exactly when subsample is 1.\n    If subsample is not 1, this function will return None.\n\n    Parameters\n    ----------\n    kernel_shape: int or None. Corresponds to the kernel shape on a given\n        axis. None if undefined.\n    top_shape: int or None. Corresponds to the top shape on a given axis.\n        None if undefined.\n    border_mode: string or int. If it is a string, it must be\n        'valid', 'half' or 'full'. If it is an integer, it must correspond to\n        the padding on the considered axis.\n    subsample: int. It must correspond to the subsampling on the\n        considered axis.\n    dilation: int. It must correspond to the dilation on the\n        considered axis.\n\n    Returns\n    -------\n    image_shape: int or None. Corresponds to the input image shape on a\n        given axis. None if undefined.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This function tries to compute the image shape of convolution gradInputs.\n\n    The image shape can only be computed exactly when subsample is 1.\n    If subsample is not 1, this function will return None.\n\n    Parameters\n    ----------\n    kernel_shape: int or None. Corresponds to the kernel shape on a given\n        axis. None if undefined.\n    top_shape: int or None. Corresponds to the top shape on a given axis.\n        None if undefined.\n    border_mode: string, int or tuple of 2 ints. If it is a string, it must be\n        'valid', 'half' or 'full'. If it is an integer, it must correspond to\n        the padding on the considered axis. If it is a tuple, its two elements\n        must correspond to the asymmetric padding (e.g., left and right) on\n        the considered axis.\n    subsample: int. It must correspond to the subsampling on the\n        considered axis.\n    dilation: int. It must correspond to the dilation on the\n        considered axis.\n\n    Returns\n    -------\n    image_shape: int or None. Corresponds to the input image shape on a\n        given axis. None if undefined.", "file": "theano_tensor_nnet_abstract_conv.py", "function": "get_conv_gradinputs_shape_1axis", "project": "Theano", "change_stats": {"code_changes": 24, "doc_changes": 8, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, model_storage, metadata_storage) -> None:\n        \n        \n        \n        self.model_storage = model_storage\n        self.metadata_storage = metadata_storage```\n\nUpdated code:\n```python\ndef __init__(self, model_storage, engine_storage) -> None:\n        \n        \n        \n        self.model_storage = model_storage\n        self.engine_storage = engine_storage\n```\n\nOriginal documentation:\n```python\nInitialize any objects, fields or parameters required by the ML engine.\n\n        At least, two storage objects should be available:\n            - model_storage: stores artifacts in the file system (path specified in MindsDB config).\n            - metadata_storage: stores all model metadata in an internal MindsDB database.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize any objects, fields or parameters required by the ML engine.\n\n        At least, two storage objects should be available:\n            - model_storage: stores models in the file system (path specified in MindsDB config).\n            - engine_storage: stores model-related metadata in an internal MindsDB database.", "file": "mindsdb_integrations_libs_base.py", "function": "BaseMLEngine.__init__", "project": "mindsdb", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef source_embedding(self, xs_pad, ilens, ys_pad):\n        \n        \n        if self.replace_sos:\n            # remove source language ID in the beggining\n            tgt_lang_ids = ys_pad[:, 0:1]\n            ys_pad = ys_pad[:, 1:]  # remove target language ID in the beginning\n            xs_pad_emb = self.dropout_emb_src(self.embed_src(xs_pad[:, 1:]))\n            ilens -= 1\n        else:\n            xs_pad_emb = self.dropout_emb_src(self.embed_src(xs_pad))\n            tgt_lang_ids = None\n        return xs_pad_emb, tgt_lang_ids```\n\nUpdated code:\n```python\ndef source_embedding(self, xs_pad, ilens, ys_pad):\n        \n        \n        tgt_lang_ids = None\n        if self.replace_sos:\n            # remove language ID in the beggining\n            xs_pad = xs_pad[:, 1:]\n            ys_pad = ys_pad[:, 1:]\n            tgt_lang_ids = ys_pad[:, 0:1]\n            ilens -= 1\n        xs_pad_emb = self.dropout_emb_src(self.embed_src(xs_pad))\n        return xs_pad_emb, tgt_lang_ids\n```\n\nOriginal documentation:\n```python\nPath through the source embedding\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :return: output of the source embedding layer (B, Tmax, eunits)\n        :rtype: torch.Tensor\n        :return: loss value\n        :rtype: torch.Tensor (B, 1)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Path through the source embedding during training.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :return: output of the source embedding layer (B, Tmax, eunits)\n        :rtype: torch.Tensor\n        :return: loss value\n        :rtype: torch.Tensor (B, 1)", "file": "espnet_nets_pytorch_backend_e2e_mt.py", "function": "E2E.source_embedding", "project": "espnet", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_latest_version():\n    \n    \n    \n    try:\n        # Is installed via package manager?\n        get_package_version(\"g4f\")\n        return get_pypi_version(\"g4f\")\n    except PackageNotFoundError:\n        # Else use Github version:\n        return get_github_version(\"xtekky/gpt4free\")```\n\nUpdated code:\n```python\ndef get_latest_version() -> str:\n    \n    \n    \n    try:\n        # Is installed via package manager?\n        get_package_version(\"g4f\")\n        return get_pypi_version(\"g4f\")\n    except PackageNotFoundError:\n        # Else use Github version:\n        return get_github_version(\"xtekky/gpt4free\")\n```\n\nOriginal documentation:\n```python\nGet the latest release version from PyPI or the GitHub repository.\n\n    :return: The latest release version as a string.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieves the latest release version of the 'g4f' package from PyPI or GitHub.\n\n    Returns:\n        str: The latest release version of 'g4f'.\n\n    Note:\n        The function first tries to fetch the version from PyPI. If the package is not found, \n        it retrieves the version from the GitHub repository.", "file": "g4f_version.py", "function": "get_latest_version", "project": "gpt4free", "change_stats": {"code_changes": 4, "doc_changes": 11, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __str__(self):\n        \n        \n        \n        sql, params = self.get_compiler(DEFAULT_DB_ALIAS).as_sql()\n        return sql % params```\n\nUpdated code:\n```python\ndef __str__(self):\n        \n        \n        \n        sql, params = self.sql_with_params()\n        return sql % params\n```\n\nOriginal documentation:\n```python\nReturns the query as a string of SQL with the parameter values\n        substituted in.\n\n        Parameter values won't necessarily be quoted correctly, since that is\n        done by the database interface at execution time.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the query as a string of SQL with the parameter values\n        substituted in (use sql_with_params() to see the unsubstituted string).\n\n        Parameter values won't necessarily be quoted correctly, since that is\n        done by the database interface at execution time.", "file": "django_db_models_sql_query.py", "function": "Query.__str__", "project": "django", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef click_record_check(self, button):\n        \n        \n        \n        if sum([1 if str(prev) == str(button) else 0 for prev in self.click_record]) >= 12:\n            logger.warning(f'Too many click for a button: {button}')\n            logger.info(f'History click: {[str(prev) for prev in self.click_record]}')\n            raise GameTooManyClickError(f'Too many click for a button: {button}')\n        else:\n            self.click_record.append(str(button))\n\n        return False```\n\nUpdated code:\n```python\ndef click_record_check(self, button):\n        \n        \n        \n        self.click_record.append(str(button))\n\n        count = {}\n        for key in self.click_record:\n            count[key] = count.get(key, 0) + 1\n        count = sorted(count.items(), key=lambda item: item[1])\n        if count[0][1] >= 12:\n            logger.warning(f'Too many click for a button: {count[0][0]}')\n            logger.info(f'History click: {[str(prev) for prev in self.click_record]}')\n            raise GameTooManyClickError(f'Too many click for a button: {count[0][0]}')\n        if len(count) >= 2 and count[0][1] >= 6 and count[1][1] >= 6:\n            logger.warning(f'Too many click between 2 buttons: {count[0][0]}, {count[1][0]}')\n            logger.info(f'History click: {[str(prev) for prev in self.click_record]}')\n            raise GameTooManyClickError(f'Too many click between 2 buttons: {count[0][0]}, {count[1][0]}')\n\n        return False\n```\n\nOriginal documentation:\n```python\nArgs:\n            button (button.Button): AzurLane Button instance.\n\n        Returns:\n            bool:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            button (button.Button): AzurLane Button instance.\n\n        Returns:\n            bool:\n\n        Raises:\n            GameTooManyClickError:", "file": "module_device_control.py", "function": "Control.click_record_check", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 20, "doc_changes": 5, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef init_virtualenv(self):\n        \n        \n        if 'VIRTUAL_ENV' not in os.environ:\n            # Not in a virtualenv\n            return\n        \n        p = os.path.normcase(sys.executable)\n        p_venv = os.path.normcase(os.environ['VIRTUAL_ENV'])\n\n        # executable path should end like /bin/python or \\\\scripts\\\\python.exe\n        p_exe_up2 = os.path.dirname(os.path.dirname(p))\n        if p_exe_up2 and os.path.exists(p_venv) and os.path.samefile(p_exe_up2, p_venv):\n            # Our exe is inside the virtualenv, don't need to do anything.\n            return\n\n        # fallback venv detection:\n        # stdlib venv may symlink sys.executable, so we can't use realpath.\n        # but others can symlink *to* the venv Python, so we can't just use sys.executable.\n        # So we just check every item in the symlink tree (generally <= 3)\n        paths = [p]\n        while os.path.islink(p):\n            p = os.path.normcase(os.path.join(os.path.dirname(p), os.readlink(p)))\n            paths.append(p)\n        \n        # In Cygwin paths like \"c:\\...\" and '\\cygdrive\\c\\...' are possible\n        if p_venv.startswith('\\\\cygdrive'):\n            p_venv = p_venv[11:]\n        elif len(p_venv) >= 2 and p_venv[1] == ':':\n            p_venv = p_venv[2:]\n\n        if any(p_venv in p for p in paths):\n            # Running properly in the virtualenv, don't need to do anything\n            return\n        \n        warn(\"Attempting to work in a virtualenv. If you encounter problems, please \"\n             \"install IPython inside the virtualenv.\")\n        if sys.platform == \"win32\":\n            virtual_env = os.path.join(os.environ['VIRTUAL_ENV'], 'Lib', 'site-packages') \n        else:\n            virtual_env = os.path.join(os.environ['VIRTUAL_ENV'], 'lib',\n                       'python%d.%d' % sys.version_info[:2], 'site-packages')\n        \n        import site\n        sys.path.insert(0, virtual_env)\n        site.addsitedir(virtual_env)```\n\nUpdated code:\n```python\ndef init_virtualenv(self):\n        \n        \n        if 'VIRTUAL_ENV' not in os.environ:\n            # Not in a virtualenv\n            return\n        elif os.environ['VIRTUAL_ENV'] == '':\n            warn(\"Virtual env path set to '', please check if this is intended.\")\n            return\n\n        p = Path(sys.executable)\n        p_venv = Path(os.environ['VIRTUAL_ENV'])\n\n        # fallback venv detection:\n        # stdlib venv may symlink sys.executable, so we can't use realpath.\n        # but others can symlink *to* the venv Python, so we can't just use sys.executable.\n        # So we just check every item in the symlink tree (generally <= 3)\n        paths = [p]\n        while p.is_symlink():\n            p = Path(os.readlink(p))\n            paths.append(p.resolve())\n\n        # In Cygwin paths like \"c:\\...\" and '\\cygdrive\\c\\...' are possible\n        if p_venv.startswith('\\\\cygdrive'):\n            p_venv = p_venv[11:]\n        elif len(p_venv) >= 2 and p_venv[1] == ':':\n            p_venv = p_venv[2:]\n\n        if any(os.fspath(p_venv) in os.fspath(p) for p in paths):\n            # Our exe is inside or has access to the virtualenv, don't need to do anything.\n            return\n\n        warn(\"Attempting to work in a virtualenv. If you encounter problems, please \"\n             \"install IPython inside the virtualenv.\")\n        if sys.platform == \"win32\":\n            virtual_env = Path(os.environ['VIRTUAL_ENV']).joinpath(\n                'Lib',\n                'site-packages'\n            )\n        else:\n            virtual_env = Path(os.environ['VIRTUAL_ENV']).joinpath(\n                'lib',\n                \"python{}.{}\".format(*sys.version_info[:2]),\n                'site-packages'\n            )\n        \n        import site\n        sys.path.insert(0, virtual_env)\n        site.addsitedir(virtual_env)\n```\n\nOriginal documentation:\n```python\nAdd a virtualenv to sys.path so the user can import modules from it.\n        This isn't perfect: it doesn't use the Python interpreter with which the\n        virtualenv was built, and it ignores the --no-site-packages option. A\n        warning will appear suggesting the user installs IPython in the\n        virtualenv, but for many cases, it probably works well enough.\n\n        Adapted from code snippets online.\n\n        http://blog.ufsoft.org/2009/1/29/ipython-and-virtualenv\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add the current virtualenv to sys.path so the user can import modules from it.\n        This isn't perfect: it doesn't use the Python interpreter with which the\n        virtualenv was built, and it ignores the --no-site-packages option. A\n        warning will appear suggesting the user installs IPython in the\n        virtualenv, but for many cases, it probably works well enough.\n\n        Adapted from code snippets online.\n\n        http://blog.ufsoft.org/2009/1/29/ipython-and-virtualenv", "file": "IPython_core_interactiveshell.py", "function": "InteractiveShell.init_virtualenv", "project": "ipython", "change_stats": {"code_changes": 41, "doc_changes": 4, "total_changes": 45}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef configure_inline_support(shell, backend):\n    \n    \n    \n    warnings.warn(\n        \"`configure_inline_support` is deprecated, directly use \"\n        \"`matplotlib_inline.backend_inline.configure_inline_support()`\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    from matplotlib_inline.backend_inline import configure_inline_support_orig\n\n    configure_inline_support_orig(shell, backend)```\n\nUpdated code:\n```python\ndef configure_inline_support(shell, backend):\n    \n    \n    \n    warnings.warn(\n        \"`configure_inline_support` is deprecated since IPython 7.23, directly \"\n        \"use `matplotlib_inline.backend_inline.configure_inline_support()`\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    from matplotlib_inline.backend_inline import configure_inline_support_orig\n\n    configure_inline_support_orig(shell, backend)\n```\n\nOriginal documentation:\n```python\nDEPRECATED\n\n    Configure an IPython shell object for matplotlib use.\n\n    Parameters\n    ----------\n    shell : InteractiveShell instance\n\n    backend : matplotlib backend\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ".. deprecated: 7.23\n\n        use `matplotlib_inline.backend_inline.configure_inline_support()`\n\n    Configure an IPython shell object for matplotlib use.\n\n    Parameters\n    ----------\n    shell : InteractiveShell instance\n\n    backend : matplotlib backend", "file": "IPython_core_pylabtools.py", "function": "configure_inline_support", "project": "ipython", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef calc_grid_position(self, index, minp):\n        \n        \n        \n        pos = index * self.reso + minp\n        return pos```\n\nUpdated code:\n```python\ndef calc_grid_position(self, index, min_position):\n        \n        \n        \n        pos = index * self.resolution + min_position\n        return pos\n```\n\nOriginal documentation:\n```python\ncalc grid position\n\n        :param index:\n        :param minp:\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "calc grid position\n\n        :param index:\n        :param min_position:\n        :return:", "file": "PathPlanning_BidirectionalAStar_bidirectional_a_star.py", "function": "BidirectionalAStarPlanner.calc_grid_position", "project": "PythonRobotics", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_draggable(self, state, use_blit=False, update='loc'):\n        \n        \n        \n        if state:\n            if self._draggable is None:\n                self._draggable = DraggableLegend(self,\n                                                  use_blit,\n                                                  update=update)\n        else:\n            if self._draggable is not None:\n                self._draggable.disconnect()\n            self._draggable = None```\n\nUpdated code:\n```python\ndef set_draggable(self, state, use_blit=False, update='loc'):\n        \n        \n        \n        if state:\n            if self._draggable is None:\n                self._draggable = DraggableLegend(self,\n                                                  use_blit,\n                                                  update=update)\n        else:\n            if self._draggable is not None:\n                self._draggable.disconnect()\n            self._draggable = None\n        return self._draggable\n```\n\nOriginal documentation:\n```python\nEnable or disable mouse dragging support of the legend.\n\n        Parameters\n        ----------\n        state : bool\n            ``True`` / ``False`` enables / disables mouse dragging.\n        use_blit : bool, optional\n\n        update_loc : ['loc' | 'bbox'], optional\n            The legend parameter to be changed when dragged:\n\n            - 'loc': update the *loc* parameter of the legend\n            - 'bbox': update the *bbox_to_anchor* parameter of the legend\n\n        Returns\n        -------\n        If *state* is ``True`` this returns the `~.DraggableLegend` helper\n        instance. Otherwise this returns ``None``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Enable or disable mouse dragging support of the legend.\n\n        Parameters\n        ----------\n        state : bool\n            ``True`` / ``False`` enables / disables mouse dragging.\n        use_blit : bool, optional\n            Use blitting for faster image composition. For details see\n            :ref:`func-animation`.\n        update : ['loc' | 'bbox'], optional\n            The legend parameter to be changed when dragged:\n\n            - 'loc': update the *loc* parameter of the legend\n            - 'bbox': update the *bbox_to_anchor* parameter of the legend\n\n        Returns\n        -------\n        If *state* is ``True`` this returns the `~.DraggableLegend` helper\n        instance. Otherwise this returns ``None``.", "file": "lib_matplotlib_legend.py", "function": "Legend.set_draggable", "project": "matplotlib", "change_stats": {"code_changes": 3, "doc_changes": 7, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_query(self, query: str) -> (GraphQLDocument, ExecutionResult):\n        \n        \n        if not query:\n            return (\n                None,\n                ExecutionResult(\n                    errors=[ValueError(\"Must provide a query string.\")], invalid=True\n                ),\n            )\n\n        # Attempt to parse the query, if it fails, return the error\n        try:\n            return self.backend.document_from_string(self.schema, query), None\n        except (ValueError, GraphQLSyntaxError) as e:\n            return None, ExecutionResult(errors=[e], invalid=True)```\n\nUpdated code:\n```python\ndef parse_query(self, query: str) -> (GraphQLDocument, ExecutionResult):\n        \n        \n        if not query or not isinstance(query, str):\n            return (\n                None,\n                ExecutionResult(\n                    errors=[ValueError(\"Must provide a query string.\")], invalid=True\n                ),\n            )\n\n        # Attempt to parse the query, if it fails, return the error\n        try:\n            return self.backend.document_from_string(self.schema, query), None\n        except (ValueError, GraphQLSyntaxError) as e:\n            return None, ExecutionResult(errors=[e], invalid=True)\n```\n\nOriginal documentation:\n```python\nAttempt to parse a query (mandatory) to a gql document object.\n\n        If no query was given, it returns an error.\n        If the query is invalid, it returns an error as well.\n        Otherwise, it returns the parsed gql document.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Attempt to parse a query (mandatory) to a gql document object.\n\n        If no query was given or query is not a string, it returns an error.\n        If the query is invalid, it returns an error as well.\n        Otherwise, it returns the parsed gql document.", "file": "saleor_graphql_views.py", "function": "GraphQLView.parse_query", "project": "saleor", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_dqn_compilation(self):\n        \n        num_iterations = 1\n        config = dqn.dqn.DQNConfig().rollouts(num_rollout_workers=2)\n\n        for _ in framework_iterator(config, with_eager_tracing=True):\n            # Double-dueling DQN.\n            print(\"Double-dueling\")\n            plain_config = deepcopy(config)\n            trainer = dqn.DQNTrainer(config=plain_config, env=\"CartPole-v0\")\n            for i in range(num_iterations):\n                results = trainer.train()\n                check_train_results(results)\n                print(results)\n\n            check_compute_single_action(trainer)\n            trainer.stop()\n\n            # Rainbow.\n            print(\"Rainbow\")\n            rainbow_config = deepcopy(config).training(\n                num_atoms=10, noisy=True, double_q=True, dueling=True, n_step=5\n            )\n            trainer = dqn.DQNTrainer(config=rainbow_config, env=\"CartPole-v0\")\n            for i in range(num_iterations):\n                results = trainer.train()\n                check_train_results(results)\n                print(results)\n\n            check_compute_single_action(trainer)\n\n            trainer.stop()```\n\nUpdated code:\n```python\ndef test_dqn_compilation(self):\n        \n        num_iterations = 1\n        config = dqn.dqn.DQNConfig().rollouts(num_rollout_workers=2)\n\n        for _ in framework_iterator(config, with_eager_tracing=True):\n            # Double-dueling DQN.\n            print(\"Double-dueling\")\n            plain_config = deepcopy(config)\n            trainer = dqn.DQN(config=plain_config, env=\"CartPole-v0\")\n            for i in range(num_iterations):\n                results = trainer.train()\n                check_train_results(results)\n                print(results)\n\n            check_compute_single_action(trainer)\n            trainer.stop()\n\n            # Rainbow.\n            print(\"Rainbow\")\n            rainbow_config = deepcopy(config).training(\n                num_atoms=10, noisy=True, double_q=True, dueling=True, n_step=5\n            )\n            trainer = dqn.DQN(config=rainbow_config, env=\"CartPole-v0\")\n            for i in range(num_iterations):\n                results = trainer.train()\n                check_train_results(results)\n                print(results)\n\n            check_compute_single_action(trainer)\n\n            trainer.stop()\n```\n\nOriginal documentation:\n```python\nTest whether a DQNTrainer can be built on all frameworks.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test whether DQN can be built on all frameworks.", "file": "rllib_algorithms_dqn_tests_test_dqn.py", "function": "TestDQN.test_dqn_compilation", "project": "ray", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, measured, ideals, sloppy_input=False,\n        is_reciprocal=True,name=None, self_calibration=False,*args, **kwargs):\n        '''\n        \n\n\n        '''\n\n        # allow them to pass di\n        # lets make an ideal flush thru for them :\n        if hasattr(measured, 'keys'):\n            measured = measured.values()\n            if sloppy_input == False:\n                warn('dictionary passed, sloppy_input automatically activated')\n                sloppy_input = True\n\n        if hasattr(ideals, 'keys'):\n            ideals = ideals.values()\n            if sloppy_input == False:\n                warn('dictionary passed, sloppy_input automatically activated')\n                sloppy_input = True\n\n        # fill measured and ideals with copied lists of input\n        self.measured = [ntwk.copy() for ntwk in measured]\n        self.ideals = [ntwk.copy() for ntwk in ideals]\n\n        self.sloppy_input=sloppy_input\n        if sloppy_input:\n            self.measured, self.ideals = \\\n                align_measured_ideals(self.measured, self.ideals)\n\n        self.self_calibration = self_calibration\n        if self_calibration == False and len(self.measured) != len(self.ideals):\n            raise(IndexError('The length of measured and ideals lists are different. Number of ideals must equal the number of measured. If you are using `sloppy_input` ensure the names are uniquely alignable.'))\n\n\n        # ensure all the measured Networks' frequency's are the same\n        for measure in self.measured:\n            if self.measured[0].frequency != measure.frequency:\n                raise(ValueError('measured Networks dont have matching frequencies.'))\n        # ensure that all ideals have same frequency of the measured\n        # if not, then attempt to interpolate\n        for k in list(range(len(self.ideals))):\n            if self.ideals[k].frequency != self.measured[0].frequency:\n                print('Warning: Frequency information doesn\\'t match on ideals[{}], attempting to interpolate the ideal[{}] Network ..'.format(k,k))\n                try:\n                    # try to resample our ideals network to match\n                    # the measurement frequency\n                    self.ideals[k].interpolate_self(\\\n                        self.measured[0].frequency)\n                    print('Success')\n\n                except:\n                    raise(IndexError('Failed to interpolate. Check frequency of ideals[{}].'.format(k)))\n\n\n        # passed to calibration algorithm in run()\n        self.kwargs = kwargs\n        self.name = name\n        self.is_reciprocal = is_reciprocal\n\n        # initialized internal properties to None\n        self._residual_ntwks = None\n        self._caled_ntwks =None\n        self._caled_ntwk_sets = None```\n\nUpdated code:\n```python\ndef __init__(self, measured, ideals, sloppy_input=False,\n        is_reciprocal=True,name=None, self_calibration=False,*args, **kwargs):\n        \n\n\n        \n\n        # allow them to pass di\n        # lets make an ideal flush thru for them :\n        if hasattr(measured, 'keys'):\n            measured = measured.values()\n            if sloppy_input == False:\n                warn('dictionary passed, sloppy_input automatically activated')\n                sloppy_input = True\n\n        if hasattr(ideals, 'keys'):\n            ideals = ideals.values()\n            if sloppy_input == False:\n                warn('dictionary passed, sloppy_input automatically activated')\n                sloppy_input = True\n\n        # fill measured and ideals with copied lists of input\n        self.measured = [ntwk.copy() for ntwk in measured]\n        self.ideals = [ntwk.copy() for ntwk in ideals]\n\n        self.sloppy_input=sloppy_input\n        if sloppy_input:\n            self.measured, self.ideals = \\\n                align_measured_ideals(self.measured, self.ideals)\n\n        self.self_calibration = self_calibration\n        if self_calibration == False and len(self.measured) != len(self.ideals):\n            raise(IndexError('The length of measured and ideals lists are different. Number of ideals must equal the number of measured. If you are using `sloppy_input` ensure the names are uniquely alignable.'))\n\n\n        # ensure all the measured Networks' frequency's are the same\n        for measure in self.measured:\n            if self.measured[0].frequency != measure.frequency:\n                raise(ValueError('measured Networks dont have matching frequencies.'))\n        # ensure that all ideals have same frequency of the measured\n        # if not, then attempt to interpolate\n        for k in list(range(len(self.ideals))):\n            if self.ideals[k].frequency != self.measured[0].frequency:\n                print('Warning: Frequency information doesn\\'t match on ideals[{}], attempting to interpolate the ideal[{}] Network ..'.format(k,k))\n                try:\n                    # try to resample our ideals network to match\n                    # the measurement frequency\n                    self.ideals[k].interpolate_self(\\\n                        self.measured[0].frequency)\n                    print('Success')\n\n                except:\n                    raise(IndexError('Failed to interpolate. Check frequency of ideals[{}].'.format(k)))\n\n\n        # passed to calibration algorithm in run()\n        self.kwargs = kwargs\n        self.name = name\n        self.is_reciprocal = is_reciprocal\n\n        # initialized internal properties to None\n        self._residual_ntwks = None\n        self._caled_ntwks =None\n        self._caled_ntwk_sets = None\n```\n\nOriginal documentation:\n```python\nCalibration initializer.\n\n\n        Notes\n        -------\n        About the order of supplied standards,\n\n        If the measured and ideals parameters are lists of Networks and\n        `sloppy_input=False`, then their elements must align. However,\n        if the measured and ideals are dictionaries, or\n        `sloppy_input=True`, then we will try to align them for you\n        based on the names of the networks (see `func:`align_measured_ideals`).\n\n        You do not want to use this `sloppy_input` feature if the\n        calibration depends on the standard order (like TRL).\n\n\n        Parameters\n        ----------\n        measured : list/dict  of :class:`~skrf.network.Network` objects\n            Raw measurements of the calibration standards. The order\n            must align with the `ideals` parameter ( or use `sloppy_input`)\n\n        ideals : list/dict of :class:`~skrf.network.Network` objects\n            Predicted ideal response of the calibration standards.\n            The order must align with `ideals` list ( or use `sloppy_input`)\n\n        sloppy_input :  Boolean.\n            Allows ideals and measured lists to be 'aligned' based on\n            the network names.\n\n        is_reciprocal : Boolean\n            enables the reciprocity assumption on the calculation of the\n            error_network, which is only relevant for one-port\n            calibrations.\n\n        name: string\n            the name of this calibration instance, like 'waveguide cal'\n            this is just for convenience [None].\n\n        self_calibration: Boolean\n            True if there are less ideals than measurements.\n            Used in self-calibration such as LMR, LRRM, where some of the\n            standards can be unknown.\n\n        \\*args, \\*\\*kwargs : key-word arguments\n            stored in self.kwargs, which may be used by sub-classes\n            most likely in `run`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Calibration initializer.\n\n\n        Notes\n        -------\n        About the order of supplied standards,\n\n        If the measured and ideals parameters are lists of Networks and\n        `sloppy_input=False`, then their elements must align. However,\n        if the measured and ideals are dictionaries, or\n        `sloppy_input=True`, then we will try to align them for you\n        based on the names of the networks (see `func:`align_measured_ideals`).\n\n        You do not want to use this `sloppy_input` feature if the\n        calibration depends on the standard order (like TRL).\n\n\n        Parameters\n        ----------\n        measured : list/dict  of :class:`~skrf.network.Network` objects\n            Raw measurements of the calibration standards. The order\n            must align with the `ideals` parameter ( or use `sloppy_input`)\n\n        ideals : list/dict of :class:`~skrf.network.Network` objects\n            Predicted ideal response of the calibration standards.\n            The order must align with `ideals` list ( or use `sloppy_input`)\n\n        sloppy_input :  Boolean.\n            Allows ideals and measured lists to be 'aligned' based on\n            the network names.\n\n        is_reciprocal : Boolean\n            enables the reciprocity assumption on the calculation of the\n            error_network, which is only relevant for one-port\n            calibrations.\n\n        name: string\n            the name of this calibration instance, like 'waveguide cal'\n            this is just for convenience [None].\n\n        self_calibration: Boolean\n            True if there are less ideals than measurements.\n            Used in self-calibration such as LMR, LRRM, where some of the\n            standards can be unknown.\n\n        \\*args, \\*\\*kwargs : key-word arguments\n            stored in self.kwargs, which may be used by sub-classes\n            most likely in `run`.", "file": "skrf_calibration_calibration.py", "function": "Calibration.__init__", "project": "scikit-rf", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_multilabel(y):\n     \n\n    \n    return is_label_indicator_matrix(y) or is_sequence_of_sequences(y)```\n\nUpdated code:\n```python\ndef is_multilabel(y):\n     \n\n    \n    return is_label_indicator_matrix(y)\n```\n\nOriginal documentation:\n```python\nCheck if ``y`` is in a multilabel format.\n\n    Parameters\n    ----------\n    y : numpy array of shape [n_samples] or sequence of sequences\n        Target values. In the multilabel case the nested sequences can\n        have variable lengths.\n\n    Returns\n    -------\n    out : bool,\n        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.multiclass import is_multilabel\n    >>> is_multilabel([0, 1, 0, 1])\n    False\n    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n    True\n    >>> is_multilabel(np.array([[1], [0], [0]]))\n    False\n    >>> is_multilabel(np.array([[1, 0, 0]]))\n    True\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if ``y`` is in a multilabel format.\n\n    Parameters\n    ----------\n    y : numpy array of shape [n_samples]\n        Target values.\n\n    Returns\n    -------\n    out : bool,\n        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.multiclass import is_multilabel\n    >>> is_multilabel([0, 1, 0, 1])\n    False\n    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n    True\n    >>> is_multilabel(np.array([[1], [0], [0]]))\n    False\n    >>> is_multilabel(np.array([[1, 0, 0]]))\n    True", "file": "sklearn_utils_multiclass.py", "function": "is_multilabel", "project": "scikit-learn", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _validate_scalar(\n        self,\n        value,\n        *,\n        allow_listlike: bool = False,\n        setitem: bool = True,\n        unbox: bool = True,\n    ):\n        \n        \n        \n        if isinstance(value, self._scalar_type):\n            pass\n\n        elif isinstance(value, str):\n            # NB: Careful about tzawareness\n            try:\n                value = self._scalar_from_string(value)\n            except ValueError as err:\n                msg = self._validation_error_message(value, allow_listlike)\n                raise TypeError(msg) from err\n\n        elif is_valid_na_for_dtype(value, self.dtype):\n            # GH#18295\n            value = NaT\n\n        elif isna(value):\n            # if we are dt64tz and value is dt64(\"NaT\"), dont cast to NaT,\n            #  or else we'll fail to raise in _unbox_scalar\n            msg = self._validation_error_message(value, allow_listlike)\n            raise TypeError(msg)\n\n        elif isinstance(value, self._recognized_scalars):\n            value = self._scalar_type(value)\n\n        else:\n            msg = self._validation_error_message(value, allow_listlike)\n            raise TypeError(msg)\n\n        if not unbox:\n            # NB: In general NDArrayBackedExtensionArray will unbox here;\n            #  this option exists to prevent a performance hit in\n            #  TimedeltaIndex.get_loc\n            return value\n        return self._unbox_scalar(value)```\n\nUpdated code:\n```python\ndef _validate_scalar(\n        self,\n        value,\n        *,\n        allow_listlike: bool = False,\n        unbox: bool = True,\n    ):\n        \n        \n        \n        if isinstance(value, self._scalar_type):\n            pass\n\n        elif isinstance(value, str):\n            # NB: Careful about tzawareness\n            try:\n                value = self._scalar_from_string(value)\n            except ValueError as err:\n                msg = self._validation_error_message(value, allow_listlike)\n                raise TypeError(msg) from err\n\n        elif is_valid_na_for_dtype(value, self.dtype):\n            # GH#18295\n            value = NaT\n\n        elif isna(value):\n            # if we are dt64tz and value is dt64(\"NaT\"), dont cast to NaT,\n            #  or else we'll fail to raise in _unbox_scalar\n            msg = self._validation_error_message(value, allow_listlike)\n            raise TypeError(msg)\n\n        elif isinstance(value, self._recognized_scalars):\n            value = self._scalar_type(value)\n\n        else:\n            msg = self._validation_error_message(value, allow_listlike)\n            raise TypeError(msg)\n\n        if not unbox:\n            # NB: In general NDArrayBackedExtensionArray will unbox here;\n            #  this option exists to prevent a performance hit in\n            #  TimedeltaIndex.get_loc\n            return value\n        return self._unbox_scalar(value)\n```\n\nOriginal documentation:\n```python\nValidate that the input value can be cast to our scalar_type.\n\n        Parameters\n        ----------\n        value : object\n        allow_listlike: bool, default False\n            When raising an exception, whether the message should say\n            listlike inputs are allowed.\n        setitem : bool, default True\n            Whether to check compatibility with setitem strictness.\n        unbox : bool, default True\n            Whether to unbox the result before returning.  Note: unbox=False\n            skips the setitem compatibility check.\n\n        Returns\n        -------\n        self._scalar_type or NaT\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Validate that the input value can be cast to our scalar_type.\n\n        Parameters\n        ----------\n        value : object\n        allow_listlike: bool, default False\n            When raising an exception, whether the message should say\n            listlike inputs are allowed.\n        unbox : bool, default True\n            Whether to unbox the result before returning.  Note: unbox=False\n            skips the setitem compatibility check.\n\n        Returns\n        -------\n        self._scalar_type or NaT", "file": "pandas_core_arrays_datetimelike.py", "function": "DatetimeLikeArrayMixin._validate_scalar", "project": "pandas", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(self, workflow_id: Optional[str] = None) -> Any:\n        \n        \n        return ray.get(self.run_async(workflow_id))```\n\nUpdated code:\n```python\ndef run(self,\n            workflow_id: Optional[str] = None,\n            metadata: Optional[Dict[str, Any]] = None) -> Any:\n        \n        \n        return ray.get(self.run_async(workflow_id, metadata))\n```\n\nOriginal documentation:\n```python\nRun a workflow.\n\n        If the workflow with the given id already exists, it will be resumed.\n\n        Examples:\n            >>> @workflow.step\n            ... def book_flight(origin: str, dest: str) -> Flight:\n            ...    return Flight(...)\n\n            >>> @workflow.step\n            ... def book_hotel(location: str) -> Reservation:\n            ...    return Reservation(...)\n\n            >>> @workflow.step\n            ... def finalize_trip(bookings: List[Any]) -> Trip:\n            ...    return Trip(...)\n\n            >>> flight1 = book_flight.step(\"OAK\", \"SAN\")\n            >>> flight2 = book_flight.step(\"SAN\", \"OAK\")\n            >>> hotel = book_hotel.step(\"SAN\")\n            >>> trip = finalize_trip.step([flight1, flight2, hotel])\n            >>> result = trip.run()\n\n        Args:\n            workflow_id: A unique identifier that can be used to resume the\n                workflow. If not specified, a random id will be generated.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run a workflow.\n\n        If the workflow with the given id already exists, it will be resumed.\n\n        Examples:\n            >>> @workflow.step\n            ... def book_flight(origin: str, dest: str) -> Flight:\n            ...    return Flight(...)\n\n            >>> @workflow.step\n            ... def book_hotel(location: str) -> Reservation:\n            ...    return Reservation(...)\n\n            >>> @workflow.step\n            ... def finalize_trip(bookings: List[Any]) -> Trip:\n            ...    return Trip(...)\n\n            >>> flight1 = book_flight.step(\"OAK\", \"SAN\")\n            >>> flight2 = book_flight.step(\"SAN\", \"OAK\")\n            >>> hotel = book_hotel.step(\"SAN\")\n            >>> trip = finalize_trip.step([flight1, flight2, hotel])\n            >>> result = trip.run()\n\n        Args:\n            workflow_id: A unique identifier that can be used to resume the\n                workflow. If not specified, a random id will be generated.\n            metadata: The metadata to add to the workflow. It has to be able\n                to serialize to json.\n\n        Returns:\n            The running result.", "file": "python_ray_workflow_common.py", "function": "Workflow.run", "project": "ray", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_rgba_array(c, alpha=None):\n    \n    \n\n    \n    # Special-case inputs that are already arrays, for performance.  (If the\n    # array has the wrong kind or shape, raise the error during one-at-a-time\n    # conversion.)\n    if np.iterable(alpha):\n        alpha = np.asarray(alpha).ravel()\n    if (isinstance(c, np.ndarray) and c.dtype.kind in \"if\"\n            and c.ndim == 2 and c.shape[1] in [3, 4]):\n        mask = c.mask.any(axis=1) if np.ma.is_masked(c) else None\n        c = np.ma.getdata(c)\n        if np.iterable(alpha):\n            if c.shape[0] == 1 and alpha.shape[0] > 1:\n                c = np.tile(c, (alpha.shape[0], 1))\n            elif c.shape[0] != alpha.shape[0]:\n                raise ValueError(\"The number of colors must match the number\"\n                                 \" of alpha values if there are more than one\"\n                                 \" of each.\")\n        if c.shape[1] == 3:\n            result = np.column_stack([c, np.zeros(len(c))])\n            result[:, -1] = alpha if alpha is not None else 1.\n        elif c.shape[1] == 4:\n            result = c.copy()\n            if alpha is not None:\n                result[:, -1] = alpha\n        if mask is not None:\n            result[mask] = 0\n        if np.any((result < 0) | (result > 1)):\n            raise ValueError(\"RGBA values should be within 0-1 range\")\n        return result\n    # Handle single values.\n    # Note that this occurs *after* handling inputs that are already arrays, as\n    # `to_rgba(c, alpha)` (below) is expensive for such inputs, due to the need\n    # to format the array in the ValueError message(!).\n    if cbook._str_lower_equal(c, \"none\"):\n        return np.zeros((0, 4), float)\n    try:\n        if np.iterable(alpha):\n            return np.array([to_rgba(c, a) for a in alpha], float)\n        else:\n            return np.array([to_rgba(c, alpha)], float)\n    except (ValueError, TypeError):\n        pass\n\n    if isinstance(c, str):\n        raise ValueError(\"Using a string of single character colors as \"\n                         \"a color sequence is not supported. The colors can \"\n                         \"be passed as an explicit list instead.\")\n\n    if len(c) == 0:\n        return np.zeros((0, 4), float)\n\n    # Quick path if the whole sequence can be directly converted to a numpy\n    # array in one shot.\n    lens = {len(cc) if isinstance(cc, (list, tuple)) else -1 for cc in c}\n    if lens == {3}:\n        rgba = np.column_stack([c, np.ones(len(c))])\n    elif lens == {4}:\n        rgba = np.array(c)\n    else:\n        rgba = np.array([to_rgba(cc) for cc in c])\n\n    if alpha is not None:\n        rgba[:, 3] = alpha\n    return rgba```\n\nUpdated code:\n```python\ndef to_rgba_array(c, alpha=None):\n    \n    \n    \n    # Special-case inputs that are already arrays, for performance.  (If the\n    # array has the wrong kind or shape, raise the error during one-at-a-time\n    # conversion.)\n    if np.iterable(alpha):\n        alpha = np.asarray(alpha).ravel()\n    if (isinstance(c, np.ndarray) and c.dtype.kind in \"if\"\n            and c.ndim == 2 and c.shape[1] in [3, 4]):\n        mask = c.mask.any(axis=1) if np.ma.is_masked(c) else None\n        c = np.ma.getdata(c)\n        if np.iterable(alpha):\n            if c.shape[0] == 1 and alpha.shape[0] > 1:\n                c = np.tile(c, (alpha.shape[0], 1))\n            elif c.shape[0] != alpha.shape[0]:\n                raise ValueError(\"The number of colors must match the number\"\n                                 \" of alpha values if there are more than one\"\n                                 \" of each.\")\n        if c.shape[1] == 3:\n            result = np.column_stack([c, np.zeros(len(c))])\n            result[:, -1] = alpha if alpha is not None else 1.\n        elif c.shape[1] == 4:\n            result = c.copy()\n            if alpha is not None:\n                result[:, -1] = alpha\n        if mask is not None:\n            result[mask] = 0\n        if np.any((result < 0) | (result > 1)):\n            raise ValueError(\"RGBA values should be within 0-1 range\")\n        return result\n    # Handle single values.\n    # Note that this occurs *after* handling inputs that are already arrays, as\n    # `to_rgba(c, alpha)` (below) is expensive for such inputs, due to the need\n    # to format the array in the ValueError message(!).\n    if cbook._str_lower_equal(c, \"none\"):\n        return np.zeros((0, 4), float)\n    try:\n        if np.iterable(alpha):\n            return np.array([to_rgba(c, a) for a in alpha], float)\n        else:\n            return np.array([to_rgba(c, alpha)], float)\n    except (ValueError, TypeError):\n        pass\n\n    if isinstance(c, str):\n        raise ValueError(\"Using a string of single character colors as \"\n                         \"a color sequence is not supported. The colors can \"\n                         \"be passed as an explicit list instead.\")\n\n    if len(c) == 0:\n        return np.zeros((0, 4), float)\n\n    # Quick path if the whole sequence can be directly converted to a numpy\n    # array in one shot.\n    if isinstance(c, Sequence):\n        lens = {len(cc) if isinstance(cc, (list, tuple)) else -1 for cc in c}\n        if lens == {3}:\n            rgba = np.column_stack([c, np.ones(len(c))])\n        elif lens == {4}:\n            rgba = np.array(c)\n        else:\n            rgba = np.array([to_rgba(cc) for cc in c])\n    else:\n        rgba = np.array([to_rgba(cc) for cc in c])\n\n    if alpha is not None:\n        rgba[:, 3] = alpha\n    return rgba\n```\n\nOriginal documentation:\n```python\nConvert *c* to a (n, 4) array of RGBA colors.\n\n    Parameters\n    ----------\n    c : Matplotlib color or array of colors\n        If *c* is a masked array, an ndarray is returned with a (0, 0, 0, 0)\n        row for each masked value or row in *c*.\n\n    alpha : float or sequence of floats, optional\n        If *alpha* is not ``None``, it forces the alpha value, except if *c* is\n        ``\"none\"`` (case-insensitive), which always maps to ``(0, 0, 0, 0)``.\n        If *alpha* is a sequence and *c* is a single color, *c* will be\n        repeated to match the length of *alpha*.\n\n    Returns\n    -------\n    array\n        (n, 4) array of RGBA colors.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert *c* to a (n, 4) array of RGBA colors.\n\n    Parameters\n    ----------\n    c : Matplotlib color or array of colors\n        If *c* is a masked array, an ndarray is returned with a (0, 0, 0, 0)\n        row for each masked value or row in *c*.\n\n    alpha : float or sequence of floats, optional\n        If *alpha* is given, force the alpha value of the returned RGBA tuple\n        to *alpha*.\n\n        If None, the alpha value from *c* is used. If *c* does not have an\n        alpha channel, then alpha defaults to 1.\n\n        *alpha* is ignored for the color value ``\"none\"`` (case-insensitive),\n        which always maps to ``(0, 0, 0, 0)``.\n\n        If *alpha* is a sequence and *c* is a single color, *c* will be\n        repeated to match the length of *alpha*.\n\n    Returns\n    -------\n    array\n        (n, 4) array of RGBA colors,  where each channel (red, green, blue,\n        alpha) can assume values between 0 and 1.", "file": "lib_matplotlib_colors.py", "function": "to_rgba_array", "project": "matplotlib", "change_stats": {"code_changes": 16, "doc_changes": 16, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef finalize(self, params, state):\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef finalize(self, params, prev, curr):\n        \n        \n        \n        pass\n```\n\nOriginal documentation:\n```python\nOptional method for likelihoods that depend on the entire time series.\n\n        This should be used only for non-factorizable likelihoods that couple\n        states across time. Factorizable likelihoods should instead be added to\n        the :meth:`transition` method, thereby enabling their use in\n        :meth:`heuristic` initialization. Since this method is called only\n        after the last time step, it is not used in :meth:`heuristic`\n        initialization.\n\n        .. warning:: This currently does not support latent variables.\n\n        :param params: The global params returned by :meth:`global_model`.\n        :param dict state: A dictionary mapping compartment name to tensor of\n            entire time series. For quantized inference, this uses the\n            approximate point estimates, so users must request any needed time\n            series in :meth:`__init__`, e.g. by calling\n            ``super().__init__(..., approximate=(\"I\", \"E\"))`` if likelihood\n            depends on the ``I`` and ``E`` time series.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Optional method for likelihoods that depend on entire time series.\n\n        This should be used only for non-factorizable likelihoods that couple\n        states across time. Factorizable likelihoods should instead be added to\n        the :meth:`transition` method, thereby enabling their use in\n        :meth:`heuristic` initialization. Since this method is called only\n        after the last time step, it is not used in :meth:`heuristic`\n        initialization.\n\n        .. warning:: This currently does not support latent variables.\n\n        :param params: The global params returned by :meth:`global_model`.\n        :param dict prev:\n        :param dict curr: Dictionaries mapping compartment name to tensor of\n            entire time series. These two parameters are offset by 1 step,\n            thereby making it easy to compute time series of fluxes. For\n            quantized inference, this uses the approximate point estimates, so\n            users must request any needed time series in :meth:`__init__`, e.g.\n            by calling ``super().__init__(..., approximate=(\"I\", \"E\"))`` if\n            likelihood depends on the ``I`` and ``E`` time series.", "file": "pyro_contrib_epidemiology_compartmental.py", "function": "CompartmentalModel.finalize", "project": "pyro", "change_stats": {"code_changes": 4, "doc_changes": 18, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef best_trial(self) -> FrozenTrial:\n        \n        \n\n        if self._is_multi_objective():\n            raise RuntimeError(\n                \"A single best trial cannot be retrieved from a multi-objective study. Consider \"\n                \"using Study.best_trials to retrieve a list containing the best trials.\"\n            )\n\n        return copy.deepcopy(self._storage.get_best_trial(self._study_id))```\n\nUpdated code:\n```python\ndef best_trial(self) -> FrozenTrial:\n        \n\n        \n\n        if self._is_multi_objective():\n            raise RuntimeError(\n                \"A single best trial cannot be retrieved from a multi-objective study. Consider \"\n                \"using Study.best_trials to retrieve a list containing the best trials.\"\n            )\n\n        return copy.deepcopy(self._storage.get_best_trial(self._study_id))\n```\n\nOriginal documentation:\n```python\nReturn the best trial in the study.\n\n        Returns:\n            A :class:`~optuna.trial.FrozenTrial` object of the best trial.\n\n        Raises:\n            :exc:`RuntimeError`:\n                If the study has more than one direction.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the best trial in the study.\n\n        Returns:\n            A :class:`~optuna.trial.FrozenTrial` object of the best trial.\n\n        Raises:\n            :exc:`RuntimeError`:\n                If the study has more than one direction.\n\n        .. seealso::\n            The :ref:`reuse_best_trial` tutorial provides a detailed example of how to use this\n            method.", "file": "optuna_study_study.py", "function": "Study.best_trial", "project": "optuna", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, x, y, dx, dy, width=0.001, length_includes_head=False,\n                 head_width=None, head_length=None, shape='full', overhang=0,\n                 head_starts_at_zero=False, **kwargs):\n        \n        \n        \n        if head_width is None:\n            head_width = 3 * width\n        if head_length is None:\n            head_length = 1.5 * head_width\n\n        distance = np.hypot(dx, dy)\n\n        if length_includes_head:\n            length = distance\n        else:\n            length = distance + head_length\n        if not length:\n            verts = np.empty([0, 2])  # display nothing if empty\n        else:\n            # start by drawing horizontal arrow, point at (0, 0)\n            hw, hl, hs, lw = head_width, head_length, overhang, width\n            left_half_arrow = np.array([\n                [0.0, 0.0],                 # tip\n                [-hl, -hw / 2],             # leftmost\n                [-hl * (1 - hs), -lw / 2],  # meets stem\n                [-length, -lw / 2],         # bottom left\n                [-length, 0],\n            ])\n            # if we're not including the head, shift up by head length\n            if not length_includes_head:\n                left_half_arrow += [head_length, 0]\n            # if the head starts at 0, shift up by another head length\n            if head_starts_at_zero:\n                left_half_arrow += [head_length / 2, 0]\n            # figure out the shape, and complete accordingly\n            if shape == 'left':\n                coords = left_half_arrow\n            else:\n                right_half_arrow = left_half_arrow * [1, -1]\n                if shape == 'right':\n                    coords = right_half_arrow\n                elif shape == 'full':\n                    # The half-arrows contain the midpoint of the stem,\n                    # which we can omit from the full arrow. Including it\n                    # twice caused a problem with xpdf.\n                    coords = np.concatenate([left_half_arrow[:-1],\n                                             right_half_arrow[-2::-1]])\n                else:\n                    raise ValueError(\"Got unknown shape: %s\" % shape)\n            if distance != 0:\n                cx = dx / distance\n                sx = dy / distance\n            else:\n                # Account for division by zero\n                cx, sx = 0, 1\n            M = [[cx, sx], [-sx, cx]]\n            verts = np.dot(coords, M) + (x + dx, y + dy)\n\n        super().__init__(verts, closed=True, **kwargs)```\n\nUpdated code:\n```python\ndef __init__(self, x, y, dx, dy, width=0.001, length_includes_head=False,\n                 head_width=None, head_length=None, shape='full', overhang=0,\n                 head_starts_at_zero=False, **kwargs):\n        \n        \n        \n        self._x = x\n        self._y = y\n        self._dx = dx\n        self._dy = dy\n        self._width = width\n        self._length_includes_head = length_includes_head\n        self._head_width = head_width\n        self._head_length = head_length\n        self._shape = shape\n        self._overhang = overhang\n        self._head_starts_at_zero = head_starts_at_zero\n        self._make_verts()\n        super().__init__(self.verts, closed=True, **kwargs)\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        width : float, default: 0.001\n            Width of full arrow tail.\n\n        length_includes_head : bool, default: False\n            True if head is to be counted in calculating the length.\n\n        head_width : float or None, default: 3*width\n            Total width of the full arrow head.\n\n        head_length : float or None, default: 1.5*head_width\n            Length of arrow head.\n\n        shape : {'full', 'left', 'right'}, default: 'full'\n            Draw the left-half, right-half, or full arrow.\n\n        overhang : float, default: 0\n            Fraction that the arrow is swept back (0 overhang means\n            triangular shape). Can be negative or greater than one.\n\n        head_starts_at_zero : bool, default: False\n            If True, the head starts being drawn at coordinate 0\n            instead of ending at coordinate 0.\n\n        **kwargs\n            `.Patch` properties:\n\n            %(Patch_kwdoc)s\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        x, y : float\n            The x and y coordinates of the arrow base.\n\n        dx, dy : float\n            The length of the arrow along x and y direction.\n\n        width : float, default: 0.001\n            Width of full arrow tail.\n\n        length_includes_head : bool, default: False\n            True if head is to be counted in calculating the length.\n\n        head_width : float or None, default: 3*width\n            Total width of the full arrow head.\n\n        head_length : float or None, default: 1.5*head_width\n            Length of arrow head.\n\n        shape : {'full', 'left', 'right'}, default: 'full'\n            Draw the left-half, right-half, or full arrow.\n\n        overhang : float, default: 0\n            Fraction that the arrow is swept back (0 overhang means\n            triangular shape). Can be negative or greater than one.\n\n        head_starts_at_zero : bool, default: False\n            If True, the head starts being drawn at coordinate 0\n            instead of ending at coordinate 0.\n\n        **kwargs\n            `.Patch` properties:\n\n            %(Patch:kwdoc)s", "file": "lib_matplotlib_patches.py", "function": "FancyArrow.__init__", "project": "matplotlib", "change_stats": {"code_changes": 69, "doc_changes": 10, "total_changes": 79}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef build_model(self, inputs):\n         \n        \n        input_a = inputs[0]\n        input_b = inputs[1]\n\n        encoder = self.encoder()\n        encoder_a = [encoder(input_a)]\n        encoder_b = [encoder(input_b)]\n\n        outputs = [self.decoder(\"a\")(encoder_a), self.decoder(\"b\")(encoder_b)]\n\n        autoencoder = KerasModel(inputs, outputs, name=self.model_name)\n        return autoencoder```\n\nUpdated code:\n```python\ndef build_model(self, inputs):\n         \n        \n        input_a = inputs[0]\n        input_b = inputs[1]\n\n        encoder = self.encoder()\n        encoder_a = [encoder(input_a)]\n        encoder_b = [encoder(input_b)]\n\n        outputs = [self.decoder(\"a\")(encoder_a), self.decoder(\"b\")(encoder_b)]\n\n        autoencoder = KModel(inputs, outputs, name=self.model_name)\n        return autoencoder\n```\n\nOriginal documentation:\n```python\nCreate the model's structure.\n\n        This function is automatically called immediately after :func:`__init__` has been called if\n        a new model is being created. It is ignored if an existing model is being loaded from disk\n        as the model structure will be defined in the saved model file.\n\n        The model's final structure is defined here.\n\n        For the original model, An encoder instance is defined, then the same instance is\n        referenced twice, one for each input \"A\" and \"B\" so that the same model is used for\n        both inputs.\n\n        2 Decoders are then defined (one for each side) with the encoder instances passed in as\n        input to the corresponding decoders.\n\n        It is important to note that any models and sub-models should not call\n        :class:`keras.models.Model` directly, but rather call\n        :class:`plugins.train.model._base.KerasModel`. This acts as a wrapper for Keras' Model\n        class, but handles some minor differences which need to be handled between Nvidia and AMD\n        backends.\n\n        The final output of the model should always call :class:`lib.model.nn_blocks.Conv2DOutput`\n        so that the correct data type is set for the final activation, to support Mixed Precision\n        Training. Failure to do so is likely to lead to issues when Mixed Precision is enabled.\n\n        Parameters\n        ----------\n        inputs: list\n            A list of input tensors for the model. This will be a list of 2 tensors of\n            shape :attr:`input_shape`, the first for side \"a\", the second for side \"b\".\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The output of this function must be a keras model generated from\n            :class:`plugins.train.model._base.KerasModel`. See Keras documentation for the correct\n            structure, but note that parameter :attr:`name` is a required rather than an optional\n            argument in Faceswap. You should assign this to the attribute ``self.name`` that is\n            automatically generated from the plugin's filename.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create the model's structure.\n\n        This function is automatically called immediately after :func:`__init__` has been called if\n        a new model is being created. It is ignored if an existing model is being loaded from disk\n        as the model structure will be defined in the saved model file.\n\n        The model's final structure is defined here.\n\n        For the original model, An encoder instance is defined, then the same instance is\n        referenced twice, one for each input \"A\" and \"B\" so that the same model is used for\n        both inputs.\n\n        2 Decoders are then defined (one for each side) with the encoder instances passed in as\n        input to the corresponding decoders.\n\n        The final output of the model should always call :class:`lib.model.nn_blocks.Conv2DOutput`\n        so that the correct data type is set for the final activation, to support Mixed Precision\n        Training. Failure to do so is likely to lead to issues when Mixed Precision is enabled.\n\n        Parameters\n        ----------\n        inputs: list\n            A list of input tensors for the model. This will be a list of 2 tensors of\n            shape :attr:`input_shape`, the first for side \"a\", the second for side \"b\".\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            See Keras documentation for the correct\n            structure, but note that parameter :attr:`name` is a required rather than an optional\n            argument in Faceswap. You should assign this to the attribute ``self.name`` that is\n            automatically generated from the plugin's filename.", "file": "plugins_train_model_original.py", "function": "Model.build_model", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 11, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pre_fill(self, filenames: list[str], side: T.Literal[\"a\", \"b\"]) -> None:\n         \n        \n        with self._lock:\n            for filename, meta in tqdm(read_image_meta_batch(filenames),\n                                       desc=f\"WTL: Caching Landmarks ({side.upper()})\",\n                                       total=len(filenames),\n                                       leave=False):\n                if \"itxt\" not in meta or \"alignments\" not in meta[\"itxt\"]:\n                    raise FaceswapError(f\"Invalid face image found. Aborting: '{filename}'\")\n\n                meta = meta[\"itxt\"]\n                key = os.path.basename(filename)\n                # Version Check\n                self._validate_version(meta, filename)\n                detected_face = self._load_detected_face(filename, meta[\"alignments\"])\n                self._cache[key] = detected_face\n                self._partially_loaded.append(key)```\n\nUpdated code:\n```python\ndef pre_fill(self, filenames: list[str], side: T.Literal[\"a\", \"b\"]) -> None:\n         \n        \n        with self._lock:\n            for filename, meta in tqdm(read_image_meta_batch(filenames),\n                                       desc=f\"WTL: Caching Landmarks ({side.upper()})\",\n                                       total=len(filenames),\n                                       leave=False):\n                if \"itxt\" not in meta or \"alignments\" not in meta[\"itxt\"]:\n                    raise FaceswapError(f\"Invalid face image found. Aborting: '{filename}'\")\n\n                meta = meta[\"itxt\"]\n                key = os.path.basename(filename)\n                # Version Check\n                self._validate_version(meta, filename)\n                detected_face = self._load_detected_face(filename, meta[\"alignments\"])\n\n                aligned = detected_face.aligned\n                assert aligned is not None\n                if aligned.landmark_type != LandmarkType.LM_2D_68:\n                    raise FaceswapError(\"68 Point facial Landmarks are required for Warp-to-\"\n                                        f\"landmarks. The face that failed was: '{filename}'\")\n\n                self._cache[key] = detected_face\n                self._partially_loaded.append(key)\n```\n\nOriginal documentation:\n```python\nWhen warp to landmarks is enabled, the cache must be pre-filled, as each side needs\n        access to the other side's alignments.\n\n        Parameters\n        ----------\n        filenames: list\n            The list of full paths to the images to load the metadata from\n        side: str\n            `\"a\"` or `\"b\"`. The side of the model being cached. Used for info output\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "When warp to landmarks is enabled, the cache must be pre-filled, as each side needs\n        access to the other side's alignments.\n\n        Parameters\n        ----------\n        filenames: list\n            The list of full paths to the images to load the metadata from\n        side: str\n            `\"a\"` or `\"b\"`. The side of the model being cached. Used for info output\n\n        Raises\n        ------\n        FaceSwapError\n            If unsupported landmark type exists", "file": "lib_training_cache.py", "function": "_Cache.pre_fill", "project": "faceswap", "change_stats": {"code_changes": 9, "doc_changes": 7, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mock_coro(return_value=None):\n    \n    return mock_coro_func(return_value)()```\n\nUpdated code:\n```python\ndef mock_coro(return_value=None, exception=None):\n    \n    return mock_coro_func(return_value, exception)()\n```\n\nOriginal documentation:\n```python\nReturn a coro that returns a value.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a coro that returns a value or raise an exception.", "file": "tests_common.py", "function": "mock_coro", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef register_env(name, env_creator):\n    \n    \n\n    if not isinstance(env_creator, FunctionType):\n        raise TypeError(\"Second argument must be a function.\", env_creator)\n    _global_registry.register(ENV_CREATOR, name, env_creator)```\n\nUpdated code:\n```python\ndef register_env(name, env_creator):\n    \n    \n\n    if not callable(env_creator):\n        raise TypeError(\"Second argument must be callable.\", env_creator)\n    _global_registry.register(ENV_CREATOR, name, env_creator)\n```\n\nOriginal documentation:\n```python\nRegister a custom environment for use with RLlib.\n\n    This enables the environment to be accessed on every Ray process\n    in the cluster.\n\n    Args:\n        name (str): Name to register.\n        env_creator (obj): Function that creates an env.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Register a custom environment for use with RLlib.\n\n    This enables the environment to be accessed on every Ray process\n    in the cluster.\n\n    Args:\n        name (str): Name to register.\n        env_creator (obj): Callable that creates an env.", "file": "python_ray_tune_registry.py", "function": "register_env", "project": "ray", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_quant_method(self, layer: torch.nn.Module) -> QuantizeMethodBase:\n        \n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef get_quant_method(\n            self, layer: torch.nn.Module) -> Optional[QuantizeMethodBase]:\n        \n        \n        raise NotImplementedError\n```\n\nOriginal documentation:\n```python\nGet the quantize method to use for the quantized layer.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the quantize method to use for the quantized layer.\n        \n        Args:\n            layer: The layer for the quant method.\n        Returns:\n            The quantize method. None if the given layer doesn't support quant\n            method.", "file": "vllm_model_executor_layers_quantization_base_config.py", "function": "QuantizationConfig.get_quant_method", "project": "vllm", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sensor_callback(self, var, value):\n        \n        _LOGGER.debug(\"Got value from bridge: %d = %d\", var, value)\n\n        if var in [SENSOR_TEMPERATURE_EXTRACT, SENSOR_TEMPERATURE_OUTDOOR]:\n            self.data[var] = value / 10\n        else:\n            self.data[var] = value\n\n        # Notify listeners that we have received an update\n        dispatcher_send(self.hass, SIGNAL_COMFOCONNECT_UPDATE_RECEIVED, var)```\n\nUpdated code:\n```python\ndef sensor_callback(self, var, value):\n        \n        _LOGGER.debug(\"Received update for %s: %s\", var, value)\n        dispatcher_send(\n            self.hass, SIGNAL_COMFOCONNECT_UPDATE_RECEIVED.format(var), value\n        )\n```\n\nOriginal documentation:\n```python\nCall function for sensor updates.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Notify listeners that we have received an update.", "file": "homeassistant_components_comfoconnect___init__.py", "function": "ComfoConnectBridge.sensor_callback", "project": "core", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef auto_zoom(self, mobjects, margin=0, only_mobjects_in_frame=False):\n        \n\n        \n        scene_critical_x_left = None\n        scene_critical_x_right = None\n        scene_critical_y_up = None\n        scene_critical_y_down = None\n\n        for m in mobjects:\n            if (m == self.frame) or (\n                only_mobjects_in_frame and not self.is_in_frame(m)\n            ):\n                # detected camera frame, should not be used to calculate final position of camera\n                continue\n\n            # initialize scene critical points with first mobjects critical points\n            if scene_critical_x_left is None:\n                scene_critical_x_left = m.get_critical_point(LEFT)[0]\n                scene_critical_x_right = m.get_critical_point(RIGHT)[0]\n                scene_critical_y_up = m.get_critical_point(UP)[1]\n                scene_critical_y_down = m.get_critical_point(DOWN)[1]\n\n            else:\n                if m.get_critical_point(LEFT)[0] < scene_critical_x_left:\n                    scene_critical_x_left = m.get_critical_point(LEFT)[0]\n\n                if m.get_critical_point(RIGHT)[0] > scene_critical_x_right:\n                    scene_critical_x_right = m.get_critical_point(RIGHT)[0]\n\n                if m.get_critical_point(UP)[1] > scene_critical_y_up:\n                    scene_critical_y_up = m.get_critical_point(UP)[1]\n\n                if m.get_critical_point(DOWN)[1] < scene_critical_y_down:\n                    scene_critical_y_down = m.get_critical_point(DOWN)[1]\n\n        # calculate center x and y\n        x = (scene_critical_x_left + scene_critical_x_right) / 2\n        y = (scene_critical_y_up + scene_critical_y_down) / 2\n\n        # calculate proposed width and height of zoomed scene\n        new_width = abs(scene_critical_x_left - scene_critical_x_right)\n        new_height = abs(scene_critical_y_up - scene_critical_y_down)\n\n        # zoom to fit all mobjects along the side that has the largest size\n        if new_width / self.frame.width > new_height / self.frame.height:\n            return self.frame.animate.set_x(x).set_y(y).set(width=new_width + margin)\n        else:\n            return self.frame.animate.set_x(x).set_y(y).set(height=new_height + margin)```\n\nUpdated code:\n```python\ndef auto_zoom(\n        self,\n        mobjects: list[Mobject],\n        margin: float = 0,\n        only_mobjects_in_frame: bool = False,\n        animate: bool = True,\n    ):\n        \n\n        \n        scene_critical_x_left = None\n        scene_critical_x_right = None\n        scene_critical_y_up = None\n        scene_critical_y_down = None\n\n        for m in mobjects:\n            if (m == self.frame) or (\n                only_mobjects_in_frame and not self.is_in_frame(m)\n            ):\n                # detected camera frame, should not be used to calculate final position of camera\n                continue\n\n            # initialize scene critical points with first mobjects critical points\n            if scene_critical_x_left is None:\n                scene_critical_x_left = m.get_critical_point(LEFT)[0]\n                scene_critical_x_right = m.get_critical_point(RIGHT)[0]\n                scene_critical_y_up = m.get_critical_point(UP)[1]\n                scene_critical_y_down = m.get_critical_point(DOWN)[1]\n\n            else:\n                if m.get_critical_point(LEFT)[0] < scene_critical_x_left:\n                    scene_critical_x_left = m.get_critical_point(LEFT)[0]\n\n                if m.get_critical_point(RIGHT)[0] > scene_critical_x_right:\n                    scene_critical_x_right = m.get_critical_point(RIGHT)[0]\n\n                if m.get_critical_point(UP)[1] > scene_critical_y_up:\n                    scene_critical_y_up = m.get_critical_point(UP)[1]\n\n                if m.get_critical_point(DOWN)[1] < scene_critical_y_down:\n                    scene_critical_y_down = m.get_critical_point(DOWN)[1]\n\n        # calculate center x and y\n        x = (scene_critical_x_left + scene_critical_x_right) / 2\n        y = (scene_critical_y_up + scene_critical_y_down) / 2\n\n        # calculate proposed width and height of zoomed scene\n        new_width = abs(scene_critical_x_left - scene_critical_x_right)\n        new_height = abs(scene_critical_y_up - scene_critical_y_down)\n\n        m_target = self.frame.animate if animate else self.frame\n        # zoom to fit all mobjects along the side that has the largest size\n        if new_width / self.frame.width > new_height / self.frame.height:\n            return m_target.set_x(x).set_y(y).set(width=new_width + margin)\n        else:\n            return m_target.set_x(x).set_y(y).set(height=new_height + margin)\n```\n\nOriginal documentation:\n```python\nZooms on to a given array of mobjects (or a singular mobject)\n        and automatically resizes to frame all the mobjects.\n\n        .. NOTE::\n\n            This method only works when 2D-objects in the XY-plane are considered, it\n            will not work correctly when the camera has been rotated.\n\n        Parameters\n        ----------\n        mobjects\n            The mobject or array of mobjects that the camera will focus on.\n\n        margin\n            The width of the margin that is added to the frame (optional, 0 by default).\n\n        only_mobjects_in_frame\n            If set to ``True``, only allows focusing on mobjects that are already in frame.\n\n        Returns\n        -------\n        _AnimationBuilder\n            Returns an animation that zooms the camera view to a given\n            list of mobjects.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Zooms on to a given array of mobjects (or a singular mobject)\n        and automatically resizes to frame all the mobjects.\n\n        .. NOTE::\n\n            This method only works when 2D-objects in the XY-plane are considered, it\n            will not work correctly when the camera has been rotated.\n\n        Parameters\n        ----------\n        mobjects\n            The mobject or array of mobjects that the camera will focus on.\n\n        margin\n            The width of the margin that is added to the frame (optional, 0 by default).\n\n        only_mobjects_in_frame\n            If set to ``True``, only allows focusing on mobjects that are already in frame.\n\n        animate\n            If set to ``False``, applies the changes instead of returning the corresponding animation\n\n        Returns\n        -------\n        Union[_AnimationBuilder, ScreenRectangle]\n            _AnimationBuilder that zooms the camera view to a given list of mobjects\n            or ScreenRectangle with position and size updated to zoomed position.", "file": "manim_camera_moving_camera.py", "function": "MovingCamera.auto_zoom", "project": "manim", "change_stats": {"code_changes": 15, "doc_changes": 11, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_zipfile(filename):\n    \n    try:\n        fpin = open(filename, \"rb\")\n        endrec = _EndRecData(fpin)\n        fpin.close()\n        if endrec:\n            return True                 # file has correct magic number\n    except IOError:\n        pass\n    return False```\n\nUpdated code:\n```python\ndef is_zipfile(filename):\n    \n    \n    result = False\n    try:\n        if hasattr(filename, \"read\"):\n            result = _check_zipfile(filename)\n        else:\n            with open(filename, \"rb\") as fp:\n                result = _check_zipfile(fp)\n    except IOError:\n        pass\n    return result\n```\n\nOriginal documentation:\n```python\nQuickly see if file is a ZIP file by checking the magic number.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Quickly see if a file is a ZIP file by checking the magic number.\n\n    The filename argument may be a file or file-like object too.", "file": "src_calibre_utils_zipfile.py", "function": "is_zipfile", "project": "calibre", "change_stats": {"code_changes": 16, "doc_changes": 6, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _update_plot_range(self) -> None:\n        \n        \n        \n        max_ix = self._max_ix\n        min_ix = self._max_ix - self._bar_count\n\n        # Update limit and range for x-axis\n        for plot in self._plots:\n            plot.setLimits(\n                xMin=-self.MIN_BAR_COUNT,\n                xMax=self._manager.get_count()\n            )\n            plot.setRange(\n                xRange=(min_ix, max_ix),\n                padding=0\n            )\n\n        # Update limit for y-axis\n        min_price, max_price = self._manager.get_price_range()\n        self._candle_plot.setLimits(yMin=min_price, yMax=max_price)\n\n        min_volume, max_volume = self._manager.get_volume_range()\n        self._volume_plot.setLimits(yMin=min_volume, yMax=max_volume)```\n\nUpdated code:\n```python\ndef _update_plot_range(self) -> None:\n        \n        \n        \n        view = self._first_plot.getViewBox()\n        view_range = view.viewRange()\n\n        min_ix = max(0, int(view_range[0][0]))\n        max_ix = min(self._manager.get_count(), int(view_range[0][1]))\n\n        # Update limit for y-axis\n        for item, plot in self._item_plot_map.items():\n            y_range = item.get_y_range(min_ix, max_ix)\n            plot.setRange(yRange=y_range)\n```\n\nOriginal documentation:\n```python\nUpdate the range of plots.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Reset the y-axis range of plots.", "file": "vnpy_chart_widget.py", "function": "ChartWidget._update_plot_range", "project": "vnpy", "change_stats": {"code_changes": 26, "doc_changes": 4, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_xlabel(self, xlabel, fontdict=None, labelpad=None, *,\n                   loc=None, **kwargs):\n        \n        \n        \n        if labelpad is not None:\n            self.xaxis.labelpad = labelpad\n        protected_kw = ['x', 'horizontalalignment', 'ha']\n        if {*kwargs} & {*protected_kw}:\n            if loc is not None:\n                raise TypeError(f\"Specifying 'loc' is disallowed when any of \"\n                                f\"its corresponding low level kwargs \"\n                                f\"({protected_kw}) are supplied as well\")\n            loc = 'center'\n        else:\n            loc = loc if loc is not None else rcParams['xaxis.labellocation']\n        cbook._check_in_list(('left', 'center', 'right'), loc=loc)\n        if loc == 'left':\n            kwargs.update(x=0, horizontalalignment='left')\n        elif loc == 'right':\n            kwargs.update(x=1, horizontalalignment='right')\n        return self.xaxis.set_label_text(xlabel, fontdict, **kwargs)```\n\nUpdated code:\n```python\ndef set_xlabel(self, xlabel, fontdict=None, labelpad=None, *,\n                   loc=None, **kwargs):\n        \n        \n        \n        if labelpad is not None:\n            self.xaxis.labelpad = labelpad\n        protected_kw = ['x', 'horizontalalignment', 'ha']\n        if {*kwargs} & {*protected_kw}:\n            if loc is not None:\n                raise TypeError(f\"Specifying 'loc' is disallowed when any of \"\n                                f\"its corresponding low level keyword \"\n                                f\"arguments ({protected_kw}) are also \"\n                                f\"supplied\")\n            loc = 'center'\n        else:\n            loc = loc if loc is not None else rcParams['xaxis.labellocation']\n        cbook._check_in_list(('left', 'center', 'right'), loc=loc)\n        if loc == 'left':\n            kwargs.update(x=0, horizontalalignment='left')\n        elif loc == 'right':\n            kwargs.update(x=1, horizontalalignment='right')\n        return self.xaxis.set_label_text(xlabel, fontdict, **kwargs)\n```\n\nOriginal documentation:\n```python\nSet the label for the x-axis.\n\n        Parameters\n        ----------\n        xlabel : str\n            The label text.\n\n        labelpad : float, default: None\n            Spacing in points from the axes bounding box including ticks\n            and tick labels.\n\n        loc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n            The label position. This is a high-level alternative for passing\n            parameters *x* and *horizonatalalignment*.\n\n        Other Parameters\n        ----------------\n        **kwargs : `.Text` properties\n            `.Text` properties control the appearance of the label.\n\n        See also\n        --------\n        text : for information on how override and the optional args work\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the label for the x-axis.\n\n        Parameters\n        ----------\n        xlabel : str\n            The label text.\n\n        labelpad : float, default: None\n            Spacing in points from the axes bounding box including ticks\n            and tick labels.\n\n        loc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n            The label position. This is a high-level alternative for passing\n            parameters *x* and *horizontalalignment*.\n\n        Other Parameters\n        ----------------\n        **kwargs : `.Text` properties\n            `.Text` properties control the appearance of the label.\n\n        See also\n        --------\n        text : for information on how override and the optional arguments work", "file": "lib_matplotlib_axes__axes.py", "function": "Axes.set_xlabel", "project": "matplotlib", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef perform_sc(self, X, n_neighbors=10):\n        \n        \n\n        # Computation of affinity matrix\n        connectivity = kneighbors_graph(\n            X, n_neighbors=n_neighbors, include_self=True,\n        )\n        self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n\n        # Perform spectral clustering on affinity matrix\n        self.labels_ = spectral_clustering_sb(\n            self.affinity_matrix_,\n            n_clusters=self.n_clusters,\n            assign_labels=self.assign_labels,\n        )\n        return self```\n\nUpdated code:\n```python\ndef perform_sc(self, X, n_neighbors=10):\n        \n        \n        \n\n        # Computation of affinity matrix\n        connectivity = kneighbors_graph(\n            X, n_neighbors=n_neighbors, include_self=True,\n        )\n        self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n\n        # Perform spectral clustering on affinity matrix\n        self.labels_ = spectral_clustering_sb(\n            self.affinity_matrix_, n_clusters=self.n_clusters,\n        )\n        return self\n```\n\nOriginal documentation:\n```python\nPerforms spectral clustering using sklearn on embeddings (X).\n\n        Arguments\n        ---------\n        X : array (n_samples, n_features)\n            Embeddings to be clustered\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Performs spectral clustering using sklearn on embeddings.\n\n        Arguments\n        ---------\n        X : array (n_samples, n_features)\n            Embeddings to be clustered\n        n_neighbors : int\n            Number of neighbors in estimating affinity matrix.\n\n        Reference\n        ---------\n        https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/cluster/_spectral.py", "file": "speechbrain_processing_diarization.py", "function": "Spec_Cluster.perform_sc", "project": "speechbrain", "change_stats": {"code_changes": 7, "doc_changes": 10, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_cross_segment_issues_of_single_machine(source_subnet_range, target_subnet_range):\n        \n        \n        \n        cross_segment_issues = []\n\n        for monkey in mongo.db.monkey.find({}, {\"ip_addresses\": 1, \"hostname\": 1}):\n            ip_in_src = None\n            ip_in_dst = None\n            for ip_addr in monkey[\"ip_addresses\"]:\n                if source_subnet_range.is_in_range(str(ip_addr)):\n                    ip_in_src = ip_addr\n                    break\n\n            # No point searching the dst subnet if there are no IPs in src subnet.\n            if not ip_in_src:\n                continue\n\n            for ip_addr in monkey[\"ip_addresses\"]:\n                if target_subnet_range.is_in_range(str(ip_addr)):\n                    ip_in_dst = ip_addr\n                    break\n\n            if ip_in_dst:\n                cross_segment_issues.append(\n                    {\n                        \"source\": ip_in_src,\n                        \"hostname\": monkey[\"hostname\"],\n                        \"target\": ip_in_dst,\n                        \"services\": None,\n                        \"is_self\": True,\n                    }\n                )\n\n        return cross_segment_issues```\n\nUpdated code:\n```python\ndef get_cross_segment_issues_of_single_machine(\n        cls, source_subnet_range: NetworkRange, target_subnet_range: NetworkRange\n    ) -> List[Dict[str, Any]]:\n        \n        \n        \n        if cls._agent_repository is None:\n            raise RuntimeError()\n        if cls._machine_repository is None:\n            raise RuntimeError()\n\n        cross_segment_issues = []\n\n        # Get IP addresses and hostname for each agent\n        # agents = cls._agent_repository.get_agents()\n        # agent_machines = (cls._machine_repository.get_machine_by_id(a.machine_id) for a in agents)\n        for agent in cls._agent_repository.get_agents():\n            machine = cls._machine_repository.get_machine_by_id(agent.machine_id)\n\n            ip_in_src: Optional[ipaddress.IPv4Address] = None\n            for iface in machine.network_interfaces:\n                if source_subnet_range.is_in_range(str(iface.ip)):\n                    ip_in_src = iface.ip\n                    break\n\n            if not ip_in_src:\n                continue\n\n            ip_in_dst = None\n            for iface in machine.network_interfaces:\n                if target_subnet_range.is_in_range(str(iface.ip)):\n                    ip_in_dst = iface.ip\n                    break\n\n            if ip_in_dst:\n                cross_segment_issues.append(\n                    {\n                        \"source\": ip_in_src,\n                        \"hostname\": machine.hostname,\n                        \"target\": ip_in_dst,\n                        \"services\": None,\n                        \"is_self\": True,\n                    }\n                )\n\n        return cross_segment_issues\n```\n\nOriginal documentation:\n```python\nGets list of cross segment issues of a single machine.\n        Meaning a machine has an interface for each of the subnets\n\n        :param source_subnet_range: The subnet range which shouldn't be able to access target_subnet\n        :param target_subnet_range: The subnet range which shouldn't be accessible fromsource_subnet\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets list of cross segment issues of a single machine.\n        Meaning a machine has an interface for each of the subnets\n\n        :param source_subnet_range: The subnet range which shouldn't be able to access target_subnet\n        :param target_subnet_range: The subnet range which shouldn't be accessible from\n            source_subnet\n        :return:", "file": "monkey_monkey_island_cc_services_reporting_report.py", "function": "ReportService.get_cross_segment_issues_of_single_machine", "project": "monkey", "change_stats": {"code_changes": 37, "doc_changes": 5, "total_changes": 42}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_or_create_table(self, table_name: str):\n        \n        if self.table_name in self.client.list_tables()[\"TableNames\"]:\n            return boto3.resource('dynamodb').Table(table_name)\n        else:\n            table = self.client.create_table(\n                TableName=self.table_name,\n                KeySchema=[\n                    {\n                        'AttributeName': 'sender_id',\n                        'KeyType': 'HASH'\n                    },\n                    {\n                        'AttributeName': 'session_date',\n                        'KeyType': 'RANGE'\n                    }\n                ],\n                AttributeDefinitions=[\n                    {\n                        'AttributeName': 'sender_id',\n                        'AttributeType': 'S'\n                    },\n                    {\n                        'AttributeName': 'session_date',\n                        'AttributeType': 'N'\n                    },\n                ],\n                ProvisionedThroughput={\n                    'ReadCapacityUnits': 5,\n                    'WriteCapacityUnits': 5\n                }\n            )\n\n            # Wait until the table exists.\n            table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n            return table```\n\nUpdated code:\n```python\ndef get_or_create_table(self, table_name: str) -> \"boto3.resources.factory.dynamodb.Table\":\n        \n        if self.table_name not in self.client.list_tables()[\"TableNames\"]:\n            table = self.client.create_table(\n                TableName=self.table_name,\n                KeySchema=[\n                    {\n                        'AttributeName': 'sender_id',\n                        'KeyType': 'HASH'\n                    },\n                    {\n                        'AttributeName': 'session_date',\n                        'KeyType': 'RANGE'\n                    }\n                ],\n                AttributeDefinitions=[\n                    {\n                        'AttributeName': 'sender_id',\n                        'AttributeType': 'S'\n                    },\n                    {\n                        'AttributeName': 'session_date',\n                        'AttributeType': 'N'\n                    },\n                ],\n                ProvisionedThroughput={\n                    'ReadCapacityUnits': 5,\n                    'WriteCapacityUnits': 5\n                }\n            )\n\n            # Wait until the table exists.\n            table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n        return boto3.resource('dynamodb').Table(table_name)\n```\n\nOriginal documentation:\n```python\nreturns table or creates one if the table name is not in the table list\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns table or creates one if the table name is not in the table list", "file": "rasa_core_tracker_store.py", "function": "DynamoTrackerStore.get_or_create_table", "project": "rasa", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_env(self, env: \"Environment\"):\n        \n        self._rc.env = env```\n\nUpdated code:\n```python\ndef set_env(self, env: 'Environment'):\n        \n        self._rc.env = env\n```\n\nOriginal documentation:\n```python\n\u8bbe\u7f6e\u89d2\u8272\u5de5\u4f5c\u6240\u5904\u7684\u73af\u5883\uff0c\u89d2\u8272\u53ef\u4ee5\u5411\u73af\u5883\u8bf4\u8bdd\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u89c2\u5bdf\u63a5\u53d7\u73af\u5883\u6d88\u606f\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the environment in which the role works. The role can talk to the environment and can also receive messages by observing.", "file": "metagpt_roles_role.py", "function": "set_env", "project": "MetaGPT", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, ax, label, initial='',\n                 color='.95', hovercolor='1', label_pad=.01):\n        \n        \n        \n        super().__init__(ax)\n\n        self.DIST_FROM_LEFT = .05\n\n        self.label = ax.text(\n            -label_pad, 0.5, label, transform=ax.transAxes,\n            verticalalignment='center', horizontalalignment='right')\n\n        # TextBox's text object should not parse mathtext at all.\n        self.text_disp = self.ax.text(\n            self.DIST_FROM_LEFT, 0.5, initial,\n            transform=self.ax.transAxes, verticalalignment='center',\n            horizontalalignment='left', parse_math=False)\n\n        self._observers = cbook.CallbackRegistry()\n\n        ax.set(\n            xlim=(0, 1), ylim=(0, 1),  # s.t. cursor appears from first click.\n            navigate=False, facecolor=color,\n            xticks=[], yticks=[])\n\n        self.cursor_index = 0\n\n        self.cursor = ax.vlines(0, 0, 0, visible=False, color=\"k\", lw=1,\n                                transform=mpl.transforms.IdentityTransform())\n\n        self.connect_event('button_press_event', self._click)\n        self.connect_event('button_release_event', self._release)\n        self.connect_event('motion_notify_event', self._motion)\n        self.connect_event('key_press_event', self._keypress)\n        self.connect_event('resize_event', self._resize)\n\n        self.color = color\n        self.hovercolor = hovercolor\n\n        self.capturekeystrokes = False```\n\nUpdated code:\n```python\ndef __init__(self, ax, label, initial='',\n                 color='.95', hovercolor='1', label_pad=.01,\n                 textalignment=\"left\"):\n        \n        \n        \n        super().__init__(ax)\n\n        self._DIST_FROM_LEFT = .05\n\n        self._text_position = _api.check_getitem(\n            {\"left\": 0.05, \"center\": 0.5, \"right\": 0.95},\n            textalignment=textalignment)\n\n        self.label = ax.text(\n            -label_pad, 0.5, label, transform=ax.transAxes,\n            verticalalignment='center', horizontalalignment='right')\n\n        # TextBox's text object should not parse mathtext at all.\n        self.text_disp = self.ax.text(\n            self._text_position, 0.5, initial, transform=self.ax.transAxes,\n            verticalalignment='center', horizontalalignment=textalignment,\n            parse_math=False)\n\n        self._observers = cbook.CallbackRegistry()\n\n        ax.set(\n            xlim=(0, 1), ylim=(0, 1),  # s.t. cursor appears from first click.\n            navigate=False, facecolor=color,\n            xticks=[], yticks=[])\n\n        self.cursor_index = 0\n\n        self.cursor = ax.vlines(0, 0, 0, visible=False, color=\"k\", lw=1,\n                                transform=mpl.transforms.IdentityTransform())\n\n        self.connect_event('button_press_event', self._click)\n        self.connect_event('button_release_event', self._release)\n        self.connect_event('motion_notify_event', self._motion)\n        self.connect_event('key_press_event', self._keypress)\n        self.connect_event('resize_event', self._resize)\n\n        self.color = color\n        self.hovercolor = hovercolor\n\n        self.capturekeystrokes = False\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            The `~.axes.Axes` instance the button will be placed into.\n        label : str\n            Label for this text box.\n        initial : str\n            Initial value in the text box.\n        color : color\n            The color of the box.\n        hovercolor : color\n            The color of the box when the mouse is over it.\n        label_pad : float\n            The distance between the label and the right side of the textbox.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            The `~.axes.Axes` instance the button will be placed into.\n        label : str\n            Label for this text box.\n        initial : str\n            Initial value in the text box.\n        color : color\n            The color of the box.\n        hovercolor : color\n            The color of the box when the mouse is over it.\n        label_pad : float\n            The distance between the label and the right side of the textbox.\n        textalignment : {'left', 'center', 'right'}\n            The horizontal location of the text.", "file": "lib_matplotlib_widgets.py", "function": "TextBox.__init__", "project": "matplotlib", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_on(self):\n        \n        parent_is_on = super().is_on\n        from homematicip.base.enums import SmokeDetectorAlarmType\n        if parent_is_on or \\\n                self._device.powerMainsFailure or \\\n                self._device.moistureDetected or \\\n                self._device.waterlevelDetected:\n            return True\n        if self._device.smokeDetectorAlarmType is not None and \\\n                self._device.smokeDetectorAlarmType != \\\n                SmokeDetectorAlarmType.IDLE_OFF:\n            return True\n        return False```\n\nUpdated code:\n```python\ndef is_on(self):\n        \n        parent_is_on = super().is_on\n        from homematicip.base.enums import SmokeDetectorAlarmType\n        if parent_is_on or \\\n                self._device.powerMainsFailure or \\\n                self._device.moistureDetected or \\\n                self._device.waterlevelDetected or \\\n                self._device.lowBat:\n            return True\n        if self._device.smokeDetectorAlarmType is not None and \\\n                self._device.smokeDetectorAlarmType != \\\n                SmokeDetectorAlarmType.IDLE_OFF:\n            return True\n        return False\n```\n\nOriginal documentation:\n```python\nReturn true if security issue detected.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return true if safety issue detected.", "file": "homeassistant_components_homematicip_cloud_binary_sensor.py", "function": "HomematicipSecuritySensorGroup.is_on", "project": "core", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef voltage_current_propagation(v1, i1, z0, gamma, d):\n    \n    \n    \n    # outer product by broadcasting of the electrical length\n    theta = gamma[:, npy.newaxis] * d  # (nbfreqs x nbd)\n    # ABCD parameters of a transmission line (gamma, z0)\n    A = npy.cosh(theta)\n    B = z0*npy.sinh(theta)\n    C = npy.sinh(theta)/z0\n    D = npy.cosh(theta)\n    # transpose and de-transpose operations are necessary \n    # for linalg.inv to inverse square matrices\n    ABCD = npy.array([[A, B],[C, D]]).transpose()   \n    inv_ABCD = npy.linalg.inv(ABCD).transpose()\n    \n    v2 = inv_ABCD[0,0] * v1 + inv_ABCD[0,1] * i1\n    i2 = inv_ABCD[1,0] * v1 + inv_ABCD[1,1] * i1\n    return v2, i2```\n\nUpdated code:\n```python\ndef voltage_current_propagation(v1, i1, z0, theta):\n    \n    \n    \n    # outer product by broadcasting of the electrical length\n    #theta = gamma[:, npy.newaxis] * d  # (nbfreqs x nbd)\n    # ABCD parameters of a transmission line (gamma, z0)\n    A = npy.cosh(theta)\n    B = z0*npy.sinh(theta)\n    C = npy.sinh(theta)/z0\n    D = npy.cosh(theta)\n    # transpose and de-transpose operations are necessary \n    # for linalg.inv to inverse square matrices\n    ABCD = npy.array([[A, B],[C, D]]).transpose()   \n    inv_ABCD = npy.linalg.inv(ABCD).transpose()\n    \n    v2 = inv_ABCD[0,0] * v1 + inv_ABCD[0,1] * i1\n    i2 = inv_ABCD[1,0] * v1 + inv_ABCD[1,1] * i1\n    return v2, i2\n```\n\nOriginal documentation:\n```python\nPropagate voltage and current on length d of a transmissio line.\n\n    Give voltage v2 and current i1 at z=-L, given voltage v1 \n    and current i1 (at z=0) and given characteristic parameters gamma and Z0.\n    \n    \n      i1                     i2\n    \u25cb-->---------------------->--\u25cb\n    \n    v1         gamma,z0         v2\n    \n    \u25cb----------------------------\u25cb\n    \n    <------------ d ------------->\n    \n    z=0                         z=d\n    \n    Use (inverse) ABCD parameters of a transmission line. \n    \n    Parameters\n    ----------\n    v1 : array-like (nfreqs,)\n        total voltage at z=0\n    i1 : array-like (nfreqs,)\n        total current at z=0, directed toward the transmission line\n    z0: array-like (nfreqs,)\n        characteristic impedance\n    gamma : array-like (nfreqs,)\n        propagation constant\n    d : array-like (nd,)\n        transmission line length\n    \n    Return\n    ------\n    v2 : array-like (nfreqs, nd)\n        total voltage at z=d\n    i2 : array-like (nfreqs, nd)\n        total current at z=d, directed outtoward the transmission line\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Propagate voltage and current on electrical length theta of a transmission line.\n\n    Give voltage v2 and current i1 at theta, given voltage v1 \n    and current i1 (at theta=0) and given characteristic parameters gamma and z0.\n    \n    \n      i1                     i2\n    \u25cb-->---------------------->--\u25cb\n    \n    v1         gamma,z0         v2\n    \n    \u25cb----------------------------\u25cb\n    \n    <------------ d ------------->\n    \n    0                         theta\n    \n    Use (inverse) ABCD parameters of a transmission line. \n    \n    Parameters\n    ----------\n    v1 : array-like (nfreqs,)\n        total voltage at z=0\n    i1 : array-like (nfreqs,)\n        total current at z=0, directed toward the transmission line\n    z0: array-like (nfreqs,)\n        characteristic impedance\n    theta : number or array-like (nfreq, ntheta)\n        electrical length of the line (may be complex).\n    \n    Return\n    ------\n    v2 : array-like (nfreqs, ntheta)\n        total voltage at z=d\n    i2 : array-like (nfreqs, ndtheta\n        total current at z=d, directed outtoward the transmission line", "file": "skrf_tlineFunctions.py", "function": "voltage_current_propagation", "project": "scikit-rf", "change_stats": {"code_changes": 6, "doc_changes": 20, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef from_checkpoint(\n        cls, checkpoint: Checkpoint, model: Optional[torch.nn.Module] = None\n    ) -> \"TorchPredictor\":\n        \n        \n        model, preprocessor = load_checkpoint(checkpoint, model)\n        return TorchPredictor(model=model, preprocessor=preprocessor)```\n\nUpdated code:\n```python\ndef from_checkpoint(\n        cls,\n        checkpoint: Checkpoint,\n        model: Optional[torch.nn.Module] = None,\n        use_gpu: bool = False,\n    ) -> \"TorchPredictor\":\n        \n        \n        model, preprocessor = load_checkpoint(checkpoint, model)\n        return TorchPredictor(model=model, preprocessor=preprocessor, use_gpu=use_gpu)\n```\n\nOriginal documentation:\n```python\nInstantiate the predictor from a Checkpoint.\n\n        The checkpoint is expected to be a result of ``TorchTrainer``.\n\n        Args:\n            checkpoint: The checkpoint to load the model and\n                preprocessor from. It is expected to be from the result of a\n                ``TorchTrainer`` run.\n            model: If the checkpoint contains a model state dict, and not\n                the model itself, then the state dict will be loaded to this\n                ``model``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Instantiate the predictor from a Checkpoint.\n\n        The checkpoint is expected to be a result of ``TorchTrainer``.\n\n        Args:\n            checkpoint: The checkpoint to load the model and\n                preprocessor from. It is expected to be from the result of a\n                ``TorchTrainer`` run.\n            model: If the checkpoint contains a model state dict, and not\n                the model itself, then the state dict will be loaded to this\n                ``model``.\n            use_gpu: If set, the model will be moved to GPU on instantiation and\n                prediction happens on GPU.", "file": "python_ray_train_torch_torch_predictor.py", "function": "TorchPredictor.from_checkpoint", "project": "ray", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef execution_order(self, refcounts):\n        \n        \n        \n        return iter(nx.topological_sort(\n            self.graph.subgraph(\n                {term for term, refcount in refcounts.items() if refcount > 0},\n            ),\n        ))```\n\nUpdated code:\n```python\ndef execution_order(self, workspace, refcounts):\n        \n        \n        \n        return list(nx.topological_sort(\n            self.graph.subgraph(\n                {\n                    term for term, refcount in refcounts.items()\n                    if refcount > 0 and term not in workspace\n                },\n            ),\n        ))\n```\n\nOriginal documentation:\n```python\nReturn a topologically-sorted iterator over the terms in ``self`` which\n        need to be computed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a topologically-sorted list of the terms in ``self`` which\n        need to be computed.\n\n        Filters out any terms that are already present in ``workspace``, as\n        well as any terms with refcounts of 0.\n\n        Parameters\n        ----------\n        workspace : dict[Term, np.ndarray]\n            Initial state of workspace for a pipeline execution. May contain\n            pre-computed values provided by ``populate_initial_workspace``.\n        refcounts : dict[Term, int]\n            Reference counts for terms to be computed. Terms with reference\n            counts of 0 do not need to be computed.", "file": "zipline_pipeline_graph.py", "function": "TermGraph.execution_order", "project": "zipline", "change_stats": {"code_changes": 11, "doc_changes": 16, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef has_add_permission(self, request):\n        \"\"\n        opts = self.opts\n        return request.user.has_perm(opts.app_label + '.' + opts.get_add_permission())```\n\nUpdated code:\n```python\ndef has_add_permission(self, request):\n        \n        \n        \n        opts = self.opts\n        return request.user.has_perm(opts.app_label + '.' + opts.get_add_permission())\n```\n\nOriginal documentation:\n```python\nReturns True if the given request has permission to add an object.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns True if the given request has permission to add an object.\n        Can be overriden by the user in subclasses.", "file": "django_contrib_admin_options.py", "function": "ModelAdmin.has_add_permission", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef main():\n    \n    \n    \n    global _CONF\n    with open('config.json') as file:\n        _CONF = json.load(file)\n        validate(_CONF, CONF_SCHEMA)\n        app(_CONF)```\n\nUpdated code:\n```python\ndef main():\n    \n    \n    \n    logger.info('Starting freqtrade %s', __version__)\n\n    global _CONF\n    with open('config.json') as file:\n        _CONF = json.load(file)\n\n    logger.info('Validating configuration ...')\n    validate(_CONF, CONF_SCHEMA)\n\n    init(_CONF)\n    old_state = get_state()\n    logger.info('Initial State: %s', old_state)\n    telegram.send_msg('*Status:* `{}`'.format(old_state.name.lower()))\n    while True:\n        new_state = get_state()\n        # Log state transition\n        if new_state != old_state:\n            telegram.send_msg('*Status:* `{}`'.format(new_state.name.lower()))\n            logging.info('Changing state to: %s', new_state.name)\n\n        if new_state == State.STOPPED:\n            time.sleep(1)\n        elif new_state == State.RUNNING:\n            _process()\n            # We need to sleep here because otherwise we would run into bittrex rate limit\n            time.sleep(exchange.get_sleep_time())\n        old_state = new_state\n```\n\nOriginal documentation:\n```python\nLoads and validates the config and starts the main loop\n    :return: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads and validates the config and handles the main loop\n    :return: None", "file": "freqtrade_main.py", "function": "main", "project": "freqtrade", "change_stats": {"code_changes": 28, "doc_changes": 4, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef QA_fetch_ctp_tick(code, format='pd', collections=DATABASE.ctp_tick):\n    \n    \n\n    code = QA_util_code_tolist(code, auto_fill=False)\n    cursor = collections.find({\n        'InstrumentID': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, batch_size=10000)\n\n    hq = pd.DataFrame([data for data in cursor]).replace(1.7976931348623157e+308,\n                                                         np.nan).replace('', np.nan).dropna(axis=1)\n    p1 = hq.loc[:, ['ActionDay', 'AskPrice1', 'AskVolume1', 'AveragePrice', 'BidPrice1',\n                    'BidVolume1', 'HighestPrice', 'InstrumentID', 'LastPrice',\n                    'OpenInterest', 'TradingDay', 'UpdateMillisec',\n                    'UpdateTime', 'Volume']]\n    p1 = p1.assign(datetime=p1.ActionDay.apply(QA.QAUtil.QADate.QA_util_date_int2str)+' '+p1.UpdateTime + (p1.UpdateMillisec/1000000).apply(lambda x: str('%.6f' % x)[1:]),\n                   code=p1.InstrumentID)\n    p1.datetime = pd.to_datetime(p1.datetime)\n    return p1.set_index(p1.datetime)```\n\nUpdated code:\n```python\ndef QA_fetch_ctp_tick(code, format='pd', collections=DATABASE.ctp_tick):\n    \n    \n\n    code = QA_util_code_tolist(code, auto_fill=False)\n    cursor = collections.find({\n        'InstrumentID': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, {\"_id\": 0}, batch_size=10000)\n\n    hq = pd.DataFrame([data for data in cursor]).replace(1.7976931348623157e+308,\n                                                         np.nan).replace('', np.nan).dropna(axis=1)\n    p1 = hq.loc[:, ['ActionDay', 'AskPrice1', 'AskVolume1', 'AveragePrice', 'BidPrice1',\n                    'BidVolume1', 'HighestPrice', 'InstrumentID', 'LastPrice',\n                    'OpenInterest', 'TradingDay', 'UpdateMillisec',\n                    'UpdateTime', 'Volume']]\n    p1 = p1.assign(datetime=p1.ActionDay.apply(QA.QAUtil.QADate.QA_util_date_int2str)+' '+p1.UpdateTime + (p1.UpdateMillisec/1000000).apply(lambda x: str('%.6f' % x)[1:]),\n                   code=p1.InstrumentID)\n    p1.datetime = pd.to_datetime(p1.datetime)\n    return p1.set_index(p1.datetime)\n```\n\nOriginal documentation:\n```python\n\u4ec5\u4f9b\u5b58\u50a8\u7684ctp tick\u4f7f\u7528\n    \n    Arguments:\n        code {[type]} -- [description]\n    \n    Keyword Arguments:\n        format {str} -- [description] (default: {'pd'})\n        collections {[type]} -- [description] (default: {DATABASE.ctp_tick})\n    \n    Returns:\n        [type] -- [description]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u4ec5\u4f9b\u5b58\u50a8\u7684ctp tick\u4f7f\u7528\n\n    Arguments:\n        code {[type]} -- [description]\n\n    Keyword Arguments:\n        format {str} -- [description] (default: {'pd'})\n        collections {[type]} -- [description] (default: {DATABASE.ctp_tick})\n\n    Returns:\n        [type] -- [description]", "file": "QUANTAXIS_QAFetch_QAQuery.py", "function": "QA_fetch_ctp_tick", "project": "QUANTAXIS", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef vol_circular_cylinder(radius: float, height: float) -> float:\n    \n    \n    return pi * pow(radius, 2) * height```\n\nUpdated code:\n```python\ndef vol_circular_cylinder(radius: float, height: float) -> float:\n    \n    \n    if height < 0 or radius < 0:\n        raise ValueError(\"vol_circular_cylinder() only accepts non-negative values\")\n    return pi * pow(radius, 2) * height\n```\n\nOriginal documentation:\n```python\nCalculate the Volume of a Circular Cylinder.\n    Wikipedia reference: https://en.wikipedia.org/wiki/Cylinder\n    :return pi * radius^2 * height\n\n    >>> vol_circular_cylinder(1, 1)\n    3.141592653589793\n    >>> vol_circular_cylinder(4, 3)\n    150.79644737231007\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculate the Volume of a Circular Cylinder.\n    Wikipedia reference: https://en.wikipedia.org/wiki/Cylinder\n    :return pi * radius^2 * height\n    >>> vol_circular_cylinder(1, 1)\n    3.141592653589793\n    >>> vol_circular_cylinder(4, 3)\n    150.79644737231007\n    >>> vol_circular_cylinder(1.6, 1.6)\n    12.867963509103795\n    >>> vol_circular_cylinder(0, 0)\n    0.0\n    >>> vol_circular_cylinder(-1, 1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_circular_cylinder() only accepts non-negative values\n    >>> vol_circular_cylinder(1, -1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_circular_cylinder() only accepts non-negative values", "file": "maths_volume.py", "function": "vol_circular_cylinder", "project": "Python", "change_stats": {"code_changes": 4, "doc_changes": 15, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ask(\n        self, fixed_distributions: Optional[Dict[str, BaseDistribution]] = None\n    ) -> trial_module.Trial:\n        \n        \n\n        if not self._optimize_lock.locked():\n            if self._storage.is_heartbeat_enabled():\n                warnings.warn(\"Heartbeat of storage is supposed to be used with Study.optimize.\")\n\n        fixed_distributions = fixed_distributions or {}\n\n        # Sync storage once every trial.\n        self._storage.read_trials_from_remote_storage(self._study_id)\n\n        trial_id = self._pop_waiting_trial_id()\n        if trial_id is None:\n            trial_id = self._storage.create_new_trial(self._study_id)\n        trial = trial_module.Trial(self, trial_id)\n\n        for name, param in fixed_distributions.items():\n            trial._suggest(name, param)\n\n        return trial```\n\nUpdated code:\n```python\ndef ask(\n        self, fixed_distributions: Optional[Dict[str, BaseDistribution]] = None\n    ) -> trial_module.Trial:\n        \n        \n\n        if not self._optimize_lock.locked():\n            if self._storage.is_heartbeat_enabled():\n                warnings.warn(\"Heartbeat of storage is supposed to be used with Study.optimize.\")\n\n        fixed_distributions = fixed_distributions or {}\n        fixed_distributions = {\n            key: _convert_old_distribution_to_new_distribution(dist)\n            for key, dist in fixed_distributions.items()\n        }\n\n        # Sync storage once every trial.\n        self._storage.read_trials_from_remote_storage(self._study_id)\n\n        trial_id = self._pop_waiting_trial_id()\n        if trial_id is None:\n            trial_id = self._storage.create_new_trial(self._study_id)\n        trial = trial_module.Trial(self, trial_id)\n\n        for name, param in fixed_distributions.items():\n            trial._suggest(name, param)\n\n        return trial\n```\n\nOriginal documentation:\n```python\nCreate a new trial from which hyperparameters can be suggested.\n\n        This method is part of an alternative to :func:`~optuna.study.Study.optimize` that allows\n        controlling the lifetime of a trial outside the scope of ``func``. Each call to this\n        method should be followed by a call to :func:`~optuna.study.Study.tell` to finish the\n        created trial.\n\n        .. seealso::\n\n            The :ref:`ask_and_tell` tutorial provides use-cases with examples.\n\n        Example:\n\n            Getting the trial object with the :func:`~optuna.study.Study.ask` method.\n\n            .. testcode::\n\n                import optuna\n\n\n                study = optuna.create_study()\n\n                trial = study.ask()\n\n                x = trial.suggest_float(\"x\", -1, 1)\n\n                study.tell(trial, x**2)\n\n        Example:\n\n            Passing previously defined distributions to the :func:`~optuna.study.Study.ask`\n            method.\n\n            .. testcode::\n\n                import optuna\n\n\n                study = optuna.create_study()\n\n                distributions = {\n                    \"optimizer\": optuna.distributions.CategoricalDistribution([\"adam\", \"sgd\"]),\n                    \"lr\": optuna.distributions.LogUniformDistribution(0.0001, 0.1),\n                }\n\n                # You can pass the distributions previously defined.\n                trial = study.ask(fixed_distributions=distributions)\n\n                # `optimizer` and `lr` are already suggested and accessible with `trial.params`.\n                assert \"optimizer\" in trial.params\n                assert \"lr\" in trial.params\n\n        Args:\n            fixed_distributions:\n                A dictionary containing the parameter names and parameter's distributions. Each\n                parameter in this dictionary is automatically suggested for the returned trial,\n                even when the suggest method is not explicitly invoked by the user. If this\n                argument is set to :obj:`None`, no parameter is automatically suggested.\n\n        Returns:\n            A :class:`~optuna.trial.Trial`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a new trial from which hyperparameters can be suggested.\n\n        This method is part of an alternative to :func:`~optuna.study.Study.optimize` that allows\n        controlling the lifetime of a trial outside the scope of ``func``. Each call to this\n        method should be followed by a call to :func:`~optuna.study.Study.tell` to finish the\n        created trial.\n\n        .. seealso::\n\n            The :ref:`ask_and_tell` tutorial provides use-cases with examples.\n\n        Example:\n\n            Getting the trial object with the :func:`~optuna.study.Study.ask` method.\n\n            .. testcode::\n\n                import optuna\n\n\n                study = optuna.create_study()\n\n                trial = study.ask()\n\n                x = trial.suggest_float(\"x\", -1, 1)\n\n                study.tell(trial, x**2)\n\n        Example:\n\n            Passing previously defined distributions to the :func:`~optuna.study.Study.ask`\n            method.\n\n            .. testcode::\n\n                import optuna\n\n\n                study = optuna.create_study()\n\n                distributions = {\n                    \"optimizer\": optuna.distributions.CategoricalDistribution([\"adam\", \"sgd\"]),\n                    \"lr\": optuna.distributions.FloatDistribution(0.0001, 0.1, log=True),\n                }\n\n                # You can pass the distributions previously defined.\n                trial = study.ask(fixed_distributions=distributions)\n\n                # `optimizer` and `lr` are already suggested and accessible with `trial.params`.\n                assert \"optimizer\" in trial.params\n                assert \"lr\" in trial.params\n\n        Args:\n            fixed_distributions:\n                A dictionary containing the parameter names and parameter's distributions. Each\n                parameter in this dictionary is automatically suggested for the returned trial,\n                even when the suggest method is not explicitly invoked by the user. If this\n                argument is set to :obj:`None`, no parameter is automatically suggested.\n\n        Returns:\n            A :class:`~optuna.trial.Trial`.", "file": "optuna_study_study.py", "function": "Study.ask", "project": "optuna", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convert_predictions_into_entities(\n        self,\n        text: Text,\n        tokens: List[Token],\n        tags: Dict[Text, List[Text]],\n        confidences: Optional[Dict[Text, List[float]]] = None,\n        split_entities_config: Optional[Dict[Text, bool]] = None,\n    ) -> List[Dict[Text, Any]]:\n        \n        \n        \n        import rasa.nlu.utils.bilou_utils as bilou_utils\n\n        entities = []\n\n        last_entity_tag = NO_ENTITY_TAG\n        last_role_tag = NO_ENTITY_TAG\n        last_group_tag = NO_ENTITY_TAG\n        last_token_end = -1\n\n        for idx, token in enumerate(tokens):\n            current_entity_tag = self.get_tag_for(tags, ENTITY_ATTRIBUTE_TYPE, idx)\n\n            if current_entity_tag == NO_ENTITY_TAG:\n                last_entity_tag = NO_ENTITY_TAG\n                last_token_end = token.end\n                continue\n\n            current_group_tag = self.get_tag_for(tags, ENTITY_ATTRIBUTE_GROUP, idx)\n            current_group_tag = bilou_utils.tag_without_prefix(current_group_tag)\n            current_role_tag = self.get_tag_for(tags, ENTITY_ATTRIBUTE_ROLE, idx)\n            current_role_tag = bilou_utils.tag_without_prefix(current_role_tag)\n\n            group_or_role_changed = (\n                last_group_tag != current_group_tag or last_role_tag != current_role_tag\n            )\n\n            if bilou_utils.bilou_prefix_from_tag(current_entity_tag):\n                # checks for new bilou tag\n                # new bilou tag begins are not with I- , L- tags\n                new_bilou_tag_starts = last_entity_tag != current_entity_tag and (\n                    bilou_utils.LAST\n                    != bilou_utils.bilou_prefix_from_tag(current_entity_tag)\n                    and bilou_utils.INSIDE\n                    != bilou_utils.bilou_prefix_from_tag(current_entity_tag)\n                )\n\n                # to handle bilou tags such as only I-, L- tags without B-tag\n                # and handle multiple U-tags consecutively\n                new_unigram_bilou_tag_starts = (\n                    last_entity_tag == NO_ENTITY_TAG\n                    or bilou_utils.UNIT\n                    == bilou_utils.bilou_prefix_from_tag(current_entity_tag)\n                )\n\n                new_tag_found = (\n                    new_bilou_tag_starts\n                    or new_unigram_bilou_tag_starts\n                    or group_or_role_changed\n                )\n                last_entity_tag = current_entity_tag\n                current_entity_tag = bilou_utils.tag_without_prefix(current_entity_tag)\n            else:\n                new_tag_found = (\n                    last_entity_tag != current_entity_tag or group_or_role_changed\n                )\n                last_entity_tag = current_entity_tag\n\n            if new_tag_found:\n                # new entity found\n                entity = self._create_new_entity(\n                    list(tags.keys()),\n                    current_entity_tag,\n                    current_group_tag,\n                    current_role_tag,\n                    token,\n                    idx,\n                    confidences,\n                )\n                entities.append(entity)\n            elif token.start - last_token_end <= 1:\n                # current token has the same entity tag as the token before and\n                # the two tokens are only separated by at most one symbol (e.g. space,\n                # dash, etc.)\n                entities[-1][ENTITY_ATTRIBUTE_END] = token.end\n                if confidences is not None:\n                    self._update_confidence_values(entities, confidences, idx)\n\n            elif self._check_is_single_entity(\n                text, token, last_token_end, split_entities_config, current_entity_tag\n            ):\n                # current token has the same entity tag as the token before and\n                # the two tokens are separated by at most 3 symbols, where each\n                # of the symbols has to be either punctuation (e.g. \".\" or \",\")\n                # and a whitespace.\n                entities[-1][ENTITY_ATTRIBUTE_END] = token.end\n                if confidences is not None:\n                    self._update_confidence_values(entities, confidences, idx)\n\n            else:\n                # the token has the same entity tag as the token before but the two\n                # tokens are separated by at least 2 symbols (e.g. multiple spaces,\n                # a comma and a space, etc.)\n                entity = self._create_new_entity(\n                    list(tags.keys()),\n                    current_entity_tag,\n                    current_group_tag,\n                    current_role_tag,\n                    token,\n                    idx,\n                    confidences,\n                )\n                entities.append(entity)\n\n            last_group_tag = current_group_tag\n            last_role_tag = current_role_tag\n            last_token_end = token.end\n\n        for entity in entities:\n            entity[ENTITY_ATTRIBUTE_VALUE] = text[\n                entity[ENTITY_ATTRIBUTE_START] : entity[ENTITY_ATTRIBUTE_END]\n            ]\n\n        return entities```\n\nUpdated code:\n```python\ndef convert_predictions_into_entities(self, text: Text,\n                                          tokens: List[Token], tags: Dict[Text, List[Text]],\n                                          split_entities_config: Dict[Text, bool] = None,\n                                          confidences: Optional[Dict[Text, List[float]]] = None\n                                ) -> List[Dict[Text, Any]]:\n        \n        \n        \n        import rasa.nlu.utils.bilou_utils as bilou_utils\n\n        entities = []\n\n        last_entity_tag = NO_ENTITY_TAG\n        last_role_tag = NO_ENTITY_TAG\n        last_group_tag = NO_ENTITY_TAG\n        last_token_end = -1\n\n        for idx, token in enumerate(tokens):\n            current_entity_tag = self.get_tag_for(tags, ENTITY_ATTRIBUTE_TYPE, idx)\n\n            if current_entity_tag == NO_ENTITY_TAG:\n                last_entity_tag = NO_ENTITY_TAG\n                last_token_end = token.end\n                continue\n\n            current_group_tag = self.get_tag_for(tags, ENTITY_ATTRIBUTE_GROUP, idx)\n            current_group_tag = bilou_utils.tag_without_prefix(current_group_tag)\n            current_role_tag = self.get_tag_for(tags, ENTITY_ATTRIBUTE_ROLE, idx)\n            current_role_tag = bilou_utils.tag_without_prefix(current_role_tag)\n\n            group_or_role_changed = (\n                last_group_tag != current_group_tag or last_role_tag != current_role_tag\n            )\n\n            if bilou_utils.bilou_prefix_from_tag(current_entity_tag):\n                # checks for new bilou tag\n                # new bilou tag begins are not with I- , L- tags\n                new_bilou_tag_starts = last_entity_tag != current_entity_tag and (\n                    bilou_utils.LAST\n                    != bilou_utils.bilou_prefix_from_tag(current_entity_tag)\n                    and bilou_utils.INSIDE\n                    != bilou_utils.bilou_prefix_from_tag(current_entity_tag)\n                )\n\n                # to handle bilou tags such as only I-, L- tags without B-tag\n                # and handle multiple U-tags consecutively\n                new_unigram_bilou_tag_starts = (\n                    last_entity_tag == NO_ENTITY_TAG\n                    or bilou_utils.UNIT\n                    == bilou_utils.bilou_prefix_from_tag(current_entity_tag)\n                )\n\n                new_tag_found = (\n                    new_bilou_tag_starts\n                    or new_unigram_bilou_tag_starts\n                    or group_or_role_changed\n                )\n                last_entity_tag = current_entity_tag\n                current_entity_tag = bilou_utils.tag_without_prefix(current_entity_tag)\n            else:\n                new_tag_found = (\n                    last_entity_tag != current_entity_tag or group_or_role_changed\n                )\n                last_entity_tag = current_entity_tag\n\n            if new_tag_found:\n                # new entity found\n                entity = self._create_new_entity(\n                    list(tags.keys()),\n                    current_entity_tag,\n                    current_group_tag,\n                    current_role_tag,\n                    token,\n                    idx,\n                    confidences,\n                )\n                entities.append(entity)\n            elif token.start - last_token_end <= 1:\n                # current token has the same entity tag as the token before and\n                # the two tokens are only separated by at most one symbol (e.g. space,\n                # dash, etc.)\n                entities[-1][ENTITY_ATTRIBUTE_END] = token.end\n                if confidences is not None:\n                    self._update_confidence_values(entities, confidences, idx)\n\n            elif self._check_is_single_entity(\n                text, token, last_token_end, split_entities_config, current_entity_tag\n            ):\n                # current token has the same entity tag as the token before and\n                # the two tokens are separated by at most 3 symbols, where each\n                # of the symbols has to be either punctuation (e.g. \".\" or \",\")\n                # and a whitespace.\n                entities[-1][ENTITY_ATTRIBUTE_END] = token.end\n                if confidences is not None:\n                    self._update_confidence_values(entities, confidences, idx)\n\n            else:\n                # the token has the same entity tag as the token before but the two\n                # tokens are separated by at least 2 symbols (e.g. multiple spaces,\n                # a comma and a space, etc.) and also shouldn't be represented as a\n                # single entity\n                entity = self._create_new_entity(\n                    list(tags.keys()),\n                    current_entity_tag,\n                    current_group_tag,\n                    current_role_tag,\n                    token,\n                    idx,\n                    confidences,\n                )\n                entities.append(entity)\n\n            last_group_tag = current_group_tag\n            last_role_tag = current_role_tag\n            last_token_end = token.end\n\n        for entity in entities:\n            entity[ENTITY_ATTRIBUTE_VALUE] = text[\n                entity[ENTITY_ATTRIBUTE_START] : entity[ENTITY_ATTRIBUTE_END]\n            ]\n\n        return entities\n```\n\nOriginal documentation:\n```python\nConvert predictions into entities.\n\n        Args:\n            text: The text message.\n            tokens: Message tokens without CLS token.\n            tags: Predicted tags.\n            confidences: Confidences of predicted tags.\n\n        Returns:\n            Entities.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert predictions into entities.\n\n        Args:\n            text: The text message.\n            tokens: Message tokens without CLS token.\n            tags: Predicted tags.\n            split_entities_config: config for handling splitting a list of entities\n            confidences: Confidences of predicted tags.\n\n        Returns:\n            Entities.", "file": "rasa_nlu_extractors_extractor.py", "function": "EntityExtractor.convert_predictions_into_entities", "project": "rasa", "change_stats": {"code_changes": 18, "doc_changes": 3, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef construct_from_string(cls, string: str):\n        \n        \n        if not isinstance(string, str):\n            raise TypeError(\"Expects a string, got {typ}\".format(typ=type(string)))\n        if string != cls.name:\n            raise TypeError(\n                \"Cannot construct a '{cls}' from '{string}'\".format(\n                    cls=cls.__name__, string=string\n                )\n            )\n        return cls()```\n\nUpdated code:\n```python\ndef construct_from_string(cls, string: str):\n        \n        \n        if not isinstance(string, str):\n            raise TypeError(f\"Expects a string, got {type(string).__name__}\")\n        if string != cls.name:\n            raise TypeError(f\"Cannot construct a '{cls.__name__}' from '{string}'\")\n        return cls()\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Construct this type from a string.\n\n        This is useful mainly for data types that accept parameters.\n        For example, a period dtype accepts a frequency parameter that\n        can be set as ``period[H]`` (where H means hourly frequency).\n\n        By default, in the abstract class, just the name of the type is\n        expected. But subclasses can overwrite this method to accept\n        parameters.\n\n        Parameters\n        ----------\n        string : str\n            The name of the type, for example ``category``.\n\n        Returns\n        -------\n        ExtensionDtype\n            Instance of the dtype.\n\n        Raises\n        ------\n        TypeError\n            If a class cannot be constructed from this 'string'.\n\n        Examples\n        --------\n        For extension dtypes with arguments the following may be an\n        adequate implementation.\n\n        >>> @classmethod\n        ... def construct_from_string(cls, string):\n        ...     pattern = re.compile(r\"^my_type\\[(?P<arg_name>.+)\\]$\")\n        ...     match = pattern.match(string)\n        ...     if match:\n        ...         return cls(**match.groupdict())\n        ...     else:\n        ...         raise TypeError(\"Cannot construct a '{}' from \"\n        ...                         \"'{}'\".format(cls.__name__, string))\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Construct this type from a string.\n\n        This is useful mainly for data types that accept parameters.\n        For example, a period dtype accepts a frequency parameter that\n        can be set as ``period[H]`` (where H means hourly frequency).\n\n        By default, in the abstract class, just the name of the type is\n        expected. But subclasses can overwrite this method to accept\n        parameters.\n\n        Parameters\n        ----------\n        string : str\n            The name of the type, for example ``category``.\n\n        Returns\n        -------\n        ExtensionDtype\n            Instance of the dtype.\n\n        Raises\n        ------\n        TypeError\n            If a class cannot be constructed from this 'string'.\n\n        Examples\n        --------\n        For extension dtypes with arguments the following may be an\n        adequate implementation.\n\n        >>> @classmethod\n        ... def construct_from_string(cls, string):\n        ...     pattern = re.compile(r\"^my_type\\[(?P<arg_name>.+)\\]$\")\n        ...     match = pattern.match(string)\n        ...     if match:\n        ...         return cls(**match.groupdict())\n        ...     else:\n        ...         raise TypeError(f\"Cannot construct a '{cls.__name__}' from\n        ...             \" \"'{string}'\")", "file": "pandas_core_dtypes_base.py", "function": "ExtensionDtype.construct_from_string", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate_square_subsequent_mask(self, sz):\n        \n        \n        mask = paddle.zeros([sz, sz], dtype='float32')\n        mask_inf = paddle.triu(\n            paddle.full(\n                shape=[sz, sz], dtype='float32', fill_value='-inf'),\n            diagonal=1)\n        mask = mask + mask_inf\n        return mask.unsqueeze([0, 1])```\n\nUpdated code:\n```python\ndef generate_square_subsequent_mask(self, sz):\n        \n        \n        mask = paddle.zeros([sz, sz], dtype=\"float32\")\n        mask_inf = paddle.triu(\n            paddle.full(shape=[sz, sz], dtype=\"float32\", fill_value=\"-inf\"), diagonal=1\n        )\n        mask = mask + mask_inf\n        return mask.unsqueeze([0, 1])\n```\n\nOriginal documentation:\n```python\nGenerate a square mask for the sequence. The masked positions are filled with float('-inf').\n            Unmasked positions are filled with float(0.0).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n        Unmasked positions are filled with float(0.0).", "file": "ppocr_modeling_heads_rec_nrtr_head.py", "function": "Transformer.generate_square_subsequent_mask", "project": "PaddleOCR", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef encode_tensors(self, input_tensors):\n    \n    \n    if not self._config.hparams.conditional:\n      raise RuntimeError('Cannot encode with a non-conditional model.')\n\n    n = len(input_tensors)\n    input_depth = self._config.note_sequence_converter.input_depth\n    batch_size = self._config.hparams.batch_size\n\n    batch_pad_amt = -n % batch_size\n    input_tensors += [np.zeros([0, input_depth])] * batch_pad_amt\n\n    inputs_length = np.array([len(t) for t in input_tensors], np.int32)\n    inputs_array = np.zeros(\n        [len(input_tensors), max(inputs_length), input_depth])\n    for i in range(len(input_tensors)):\n      inputs_array[i, :inputs_length[i]] = input_tensors[i]\n\n    outputs = []\n    for i in range(len(inputs_array) // batch_size):\n      batch_begin = i * batch_size\n      batch_end = (i+1) * batch_size\n      feed_dict = {self._inputs: inputs_array[batch_begin:batch_end],\n                   self._inputs_length: inputs_length[batch_begin:batch_end]}\n      outputs.append(\n          self._sess.run([self._z, self._mu, self._sigma], feed_dict))\n    assert outputs\n    return tuple(np.vstack(v)[:n] for v in zip(*outputs))```\n\nUpdated code:\n```python\ndef encode_tensors(self, input_tensors, lengths):\n    \n    \n    if not self._config.hparams.conditional:\n      raise RuntimeError('Cannot encode with a non-conditional model.')\n\n    n = len(input_tensors)\n    input_depth = self._config.data_converter.input_depth\n    batch_size = self._config.hparams.batch_size\n\n    batch_pad_amt = -n % batch_size\n    input_tensors += [np.zeros([0, input_depth])] * batch_pad_amt\n    length_array = np.array(lengths, np.int32)\n    length_array = np.pad(\n        length_array,\n        [(0, batch_pad_amt)] + [(0, 0)] * (length_array.ndim - 1),\n        'constant')\n\n    max_length = max([len(t) for t in input_tensors])\n    inputs_array = np.zeros(\n        [len(input_tensors), max_length, input_depth])\n    for i, t in enumerate(input_tensors):\n      inputs_array[i, :len(t)] = t\n\n    outputs = []\n    for i in range(len(inputs_array) // batch_size):\n      batch_begin = i * batch_size\n      batch_end = (i+1) * batch_size\n      feed_dict = {self._inputs: inputs_array[batch_begin:batch_end],\n                   self._inputs_length: length_array[batch_begin:batch_end]}\n      outputs.append(\n          self._sess.run([self._z, self._mu, self._sigma], feed_dict))\n    assert outputs\n    return tuple(np.vstack(v)[:n] for v in zip(*outputs))\n```\n\nOriginal documentation:\n```python\nEncodes a collection of input tensors into latent vectors.\n\n    Args:\n      input_tensors: Collection of input tensors to encode.\n    Returns:\n      The encoded `z`, `mu`, and `sigma` values.\n    Raises:\n       RuntimeError: If called for a non-conditional model.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Encodes a collection of input tensors into latent vectors.\n\n    Args:\n      input_tensors: Collection of input tensors to encode.\n      lengths: Collection of lengths of input tensors.\n    Returns:\n      The encoded `z`, `mu`, and `sigma` values.\n    Raises:\n       RuntimeError: If called for a non-conditional model.", "file": "magenta_models_music_vae_trained_model.py", "function": "TrainedModel.encode_tensors", "project": "magenta", "change_stats": {"code_changes": 21, "doc_changes": 3, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_ordering(cls):\n          \n\n        from django.db.models import FieldDoesNotExist\n\n        if not cls._meta.ordering:\n            return []\n\n        if not isinstance(cls._meta.ordering, (list, tuple)):\n            return [\n                checks.Error(\n                    (\"'ordering' must be a tuple or list \"\n                     \"(even if you want to order by only one field).\"),\n                    hint=None,\n                    obj=cls,\n                    id='models.E014',\n                )\n            ]\n\n        errors = []\n\n        fields = cls._meta.ordering\n\n        # Skip '?' fields.\n        fields = (f for f in fields if f != '?')\n\n        # Convert \"-field\" to \"field\".\n        fields = ((f[1:] if f.startswith('-') else f) for f in fields)\n\n        fields = (f for f in fields if\n            f != '_order' or not cls._meta.order_with_respect_to)\n\n        # Skip ordering in the format field1__field2 (FIXME: checking\n        # this format would be nice, but it's a little fiddly).\n        fields = (f for f in fields if '__' not in f)\n\n        # Skip ordering on pk. This is always a valid order_by field\n        # but is an alias and therefore won't be found by opts.get_field.\n        fields = (f for f in fields if f != 'pk')\n\n        for field_name in fields:\n            try:\n                cls._meta.get_field(field_name, many_to_many=False)\n            except FieldDoesNotExist:\n                errors.append(\n                    checks.Error(\n                        \"'ordering' refers to the non-existent field '%s'.\" % field_name,\n                        hint=None,\n                        obj=cls,\n                        id='models.E015',\n                    )\n                )\n        return errors```\n\nUpdated code:\n```python\ndef _check_ordering(cls):\n          \n\n        from django.db.models import FieldDoesNotExist\n\n        if not cls._meta.ordering:\n            return []\n\n        if not isinstance(cls._meta.ordering, (list, tuple)):\n            return [\n                checks.Error(\n                    (\"'ordering' must be a tuple or list \"\n                     \"(even if you want to order by only one field).\"),\n                    hint=None,\n                    obj=cls,\n                    id='models.E014',\n                )\n            ]\n\n        errors = []\n\n        fields = cls._meta.ordering\n\n        # Skip '?' fields.\n        fields = (f for f in fields if f != '?')\n\n        # Convert \"-field\" to \"field\".\n        fields = ((f[1:] if f.startswith('-') else f) for f in fields)\n\n        fields = (f for f in fields if\n            f != '_order' or not cls._meta.order_with_respect_to)\n\n        # Skip ordering in the format field1__field2 (FIXME: checking\n        # this format would be nice, but it's a little fiddly).\n        fields = (f for f in fields if '__' not in f)\n\n        # Skip ordering on pk. This is always a valid order_by field\n        # but is an alias and therefore won't be found by opts.get_field.\n        fields = (f for f in fields if f != 'pk')\n\n        for field_name in fields:\n            try:\n                cls._meta.get_field(field_name, many_to_many=False)\n            except FieldDoesNotExist:\n                if field_name.endswith('_id'):\n                    try:\n                        field = cls._meta.get_field(field_name[:-3], many_to_many=False)\n                    except FieldDoesNotExist:\n                        pass\n                    else:\n                        if field.attname == field_name:\n                            continue\n                errors.append(\n                    checks.Error(\n                        \"'ordering' refers to the non-existent field '%s'.\" % field_name,\n                        hint=None,\n                        obj=cls,\n                        id='models.E015',\n                    )\n                )\n        return errors\n```\n\nOriginal documentation:\n```python\nCheck \"ordering\" option -- is it a list of lists and do all fields\n        exist?\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check \"ordering\" option -- is it a list of strings and do all fields\n        exist?", "file": "django_db_models_base.py", "function": "Model._check_ordering", "project": "django", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef arrow(self, x, y, dx, dy, **kwargs):\n        \n        \n\n        \n        # Strip away units for the underlying patch since units\n        # do not make sense to most patch-like code\n        x = self.convert_xunits(x)\n        y = self.convert_yunits(y)\n        dx = self.convert_xunits(dx)\n        dy = self.convert_yunits(dy)\n\n        a = mpatches.FancyArrow(x, y, dx, dy, **kwargs)\n        self.add_artist(a)\n        return a```\n\nUpdated code:\n```python\ndef arrow(self, x, y, dx, dy, **kwargs):\n        \n        \n\n        \n        # Strip away units for the underlying patch since units\n        # do not make sense to most patch-like code\n        x = self.convert_xunits(x)\n        y = self.convert_yunits(y)\n        dx = self.convert_xunits(dx)\n        dy = self.convert_yunits(dy)\n\n        a = mpatches.FancyArrow(x, y, dx, dy, **kwargs)\n        self.add_patch(a)\n        self._request_autoscale_view()\n        return a\n```\n\nOriginal documentation:\n```python\nAdd an arrow to the axes.\n\n        This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n\n        Parameters\n        ----------\n        x, y : float\n            The x and y coordinates of the arrow base.\n\n        dx, dy : float\n            The length of the arrow along x and y direction.\n\n        %(FancyArrow)s\n\n        Returns\n        -------\n        arrow : `.FancyArrow`\n            The created `.FancyArrow` object.\n\n        Notes\n        -----\n        The resulting arrow is affected by the axes aspect ratio and limits.\n        This may produce an arrow whose head is not square with its stem. To\n        create an arrow whose head is square with its stem,\n        use :meth:`annotate` for example:\n\n        >>> ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),\n        ...             arrowprops=dict(arrowstyle=\"->\"))\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add an arrow to the axes.\n\n        This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n\n        Parameters\n        ----------\n        x, y : float\n            The x and y coordinates of the arrow base.\n\n        dx, dy : float\n            The length of the arrow along x and y direction.\n\n        %(FancyArrow)s\n\n        Returns\n        -------\n        `.FancyArrow`\n            The created `.FancyArrow` object.\n\n        Notes\n        -----\n        The resulting arrow is affected by the axes aspect ratio and limits.\n        This may produce an arrow whose head is not square with its stem. To\n        create an arrow whose head is square with its stem,\n        use :meth:`annotate` for example:\n\n        >>> ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),\n        ...             arrowprops=dict(arrowstyle=\"->\"))", "file": "lib_matplotlib_axes__axes.py", "function": "Axes.arrow", "project": "matplotlib", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __getslice__(self, i, j):\n        \n        \n        \n        newArr = self.values[i:j].copy()\n        newIndex = self.index[i:j]\n\n        return Series(newArr, index=newIndex)```\n\nUpdated code:\n```python\ndef __getslice__(self, i, j):\n        \n        \n        \n        return Series(self.values[i:j].copy(), index=self.index[i:j])\n```\n\nOriginal documentation:\n```python\nReturns a slice of the Series.\n\n        Note that the underlying values are COPIES.\n\n        The reason that the getslice returns copies is that otherwise you\n        will have a reference to the original series which could be\n        inadvertently changed if the slice were altered (made mutable).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a slice of the Series.\n\n        Note that the underlying values are COPIES.\n\n        The reason that the getslice returns copies is that otherwise you\n        will have a reference to the original series which could be\n        inadvertently changed", "file": "pandas_core_series.py", "function": "Series.__getslice__", "project": "pandas", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_tensor_constants(self):\n        \n        \n        \n        data = FxGraphCachePickler.dumps(torch.tensor(list(range(9))))\n        self.assertIsInstance(pickle.loads(data), TensorMetadata)\n\n        data = FxGraphCachePickler.dumps(torch.tensor(list(range(8))))\n        self.assertIsInstance(pickle.loads(data), TensorMetadataAndValues)```\n\nUpdated code:\n```python\ndef test_tensor_constants(self):\n        \n        \n        \n        data = FxGraphCachePickler.dumps(torch.tensor(list(range(9))))\n        self.assertIsInstance(pickle.loads(data), TensorMetadataAndValues)\n```\n\nOriginal documentation:\n```python\nTest the handling of small vs. large tensor constants.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test the hashing of tensor constants.", "file": "test_inductor_test_codecache.py", "function": "TestFxGraphCacheHashing.test_tensor_constants", "project": "pytorch", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, query: Tensor, value: Tensor, key_padding_mask: Tensor,\n                self_attn_mask: Tensor, reference_points: Tensor,\n                spatial_shapes: Tensor, level_start_index: Tensor,\n                valid_ratios: Tensor, reg_branches: nn.ModuleList,\n                **kwargs) -> Tensor:\n        \n        \n        intermediate = []\n        intermediate_reference_points = [reference_points]\n        for lid, layer in enumerate(self.layers):\n            if reference_points.shape[-1] == 4:\n                reference_points_input = \\\n                    reference_points[:, :, None] * torch.cat(\n                        [valid_ratios, valid_ratios], -1)[:, None]\n            else:\n                assert reference_points.shape[-1] == 2\n                reference_points_input = \\\n                    reference_points[:, :, None] * valid_ratios[:, None]\n\n            query_sine_embed = coordinate_to_encoding(\n                reference_points_input[:, :, 0, :])\n            query_pos = self.ref_point_head(query_sine_embed)\n\n            query = layer(\n                query,\n                query_pos=query_pos,\n                value=value,\n                key_padding_mask=key_padding_mask,\n                self_attn_mask=self_attn_mask,\n                spatial_shapes=spatial_shapes,\n                level_start_index=level_start_index,\n                valid_ratios=valid_ratios,\n                reference_points=reference_points_input,\n                **kwargs)\n\n            if reg_branches is not None:\n                tmp = reg_branches[lid](query)\n                assert reference_points.shape[-1] == 4\n                new_reference_points = tmp + inverse_sigmoid(\n                    reference_points, eps=1e-3)\n                new_reference_points = new_reference_points.sigmoid()\n                reference_points = new_reference_points.detach()\n\n            if self.return_intermediate:\n                intermediate.append(self.norm(query))\n                intermediate_reference_points.append(new_reference_points)\n                # NOTE this is for the \"Look Forward Twice\" module,\n                # in the DeformDETR, reference_points was appended.\n\n        if self.return_intermediate:\n            return torch.stack(intermediate), torch.stack(\n                intermediate_reference_points)\n\n        return query, reference_points```\n\nUpdated code:\n```python\ndef forward(self, query: Tensor, value: Tensor, key_padding_mask: Tensor,\n                self_attn_mask: Tensor, reference_points: Tensor,\n                spatial_shapes: Tensor, level_start_index: Tensor,\n                valid_ratios: Tensor, reg_branches: nn.ModuleList,\n                **kwargs) -> Tuple[Tensor]:\n        \n        \n        intermediate = []\n        intermediate_reference_points = [reference_points]\n        for lid, layer in enumerate(self.layers):\n            if reference_points.shape[-1] == 4:\n                reference_points_input = \\\n                    reference_points[:, :, None] * torch.cat(\n                        [valid_ratios, valid_ratios], -1)[:, None]\n            else:\n                assert reference_points.shape[-1] == 2\n                reference_points_input = \\\n                    reference_points[:, :, None] * valid_ratios[:, None]\n\n            query_sine_embed = coordinate_to_encoding(\n                reference_points_input[:, :, 0, :])\n            query_pos = self.ref_point_head(query_sine_embed)\n\n            query = layer(\n                query,\n                query_pos=query_pos,\n                value=value,\n                key_padding_mask=key_padding_mask,\n                self_attn_mask=self_attn_mask,\n                spatial_shapes=spatial_shapes,\n                level_start_index=level_start_index,\n                valid_ratios=valid_ratios,\n                reference_points=reference_points_input,\n                **kwargs)\n\n            if reg_branches is not None:\n                tmp = reg_branches[lid](query)\n                assert reference_points.shape[-1] == 4\n                new_reference_points = tmp + inverse_sigmoid(\n                    reference_points, eps=1e-3)\n                new_reference_points = new_reference_points.sigmoid()\n                reference_points = new_reference_points.detach()\n\n            if self.return_intermediate:\n                intermediate.append(self.norm(query))\n                intermediate_reference_points.append(new_reference_points)\n                # NOTE this is for the \"Look Forward Twice\" module,\n                # in the DeformDETR, reference_points was appended.\n\n        if self.return_intermediate:\n            return torch.stack(intermediate), torch.stack(\n                intermediate_reference_points)\n\n        return query, reference_points\n```\n\nOriginal documentation:\n```python\nForward function of Transformer encoder.\n\n        Args:\n            query (Tensor): The input query, has shape (num_queries, bs, dim).\n            value (Tensor): The input values, has shape (num_value, bs, dim).\n            key_padding_mask (Tensor): The `key_padding_mask` of `self_attn`\n                input. ByteTensor, has shape (num_queries, bs).\n            self_attn_mask (Tensor): The attention mask to prevent information\n                leakage from different denoising groups and matching parts, has\n                shape (num_queries_total, num_queries_total). It is `None` when\n                `self.training` is `False`.\n            reference_points (Tensor): The initial reference, has shape\n                (bs, num_queries, 4) with the last dimension arranged as\n                (cx, cy, w, h).\n            spatial_shapes (Tensor): Spatial shapes of features in all levels,\n                has shape (num_levels, 2), last dimension represents (h, w).\n            level_start_index (Tensor): The start index of each level.\n                A tensor has shape (num_levels, ) and can be represented\n                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].\n            valid_ratios (Tensor): The ratios of the valid width and the valid\n                height relative to the width and the height of features in all\n                levels, has shape (bs, num_levels, 2).\n            reg_branches: (obj:`nn.ModuleList`): Used for refining the\n                regression results.\n\n        Returns:\n            Tensor: Output queries of Transformer encoder, which is also\n            called 'encoder output embeddings' or 'memory', has shape\n            (num_queries, bs, dim)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward function of Transformer decoder.\n\n        Args:\n            query (Tensor): The input query, has shape (num_queries, bs, dim).\n            value (Tensor): The input values, has shape (num_value, bs, dim).\n            key_padding_mask (Tensor): The `key_padding_mask` of `self_attn`\n                input. ByteTensor, has shape (num_queries, bs).\n            self_attn_mask (Tensor): The attention mask to prevent information\n                leakage from different denoising groups and matching parts, has\n                shape (num_queries_total, num_queries_total). It is `None` when\n                `self.training` is `False`.\n            reference_points (Tensor): The initial reference, has shape\n                (bs, num_queries, 4) with the last dimension arranged as\n                (cx, cy, w, h).\n            spatial_shapes (Tensor): Spatial shapes of features in all levels,\n                has shape (num_levels, 2), last dimension represents (h, w).\n            level_start_index (Tensor): The start index of each level.\n                A tensor has shape (num_levels, ) and can be represented\n                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].\n            valid_ratios (Tensor): The ratios of the valid width and the valid\n                height relative to the width and the height of features in all\n                levels, has shape (bs, num_levels, 2).\n            reg_branches: (obj:`nn.ModuleList`): Used for refining the\n                regression results.\n\n        Returns:\n            tuple[Tensor]: Output queries and references of Transformer\n                decoder\n\n            - query (Tensor): Output embeddings of the last decoder, has\n              shape (num_queries, bs, embed_dims) when `return_intermediate`\n              is `False`. Otherwise, Intermediate output embeddings of all\n              decoder layers, has shape (num_decoder_layers, num_queries, bs,\n              embed_dims).\n            - reference_points (Tensor): The reference of the last decoder\n              layer, has shape (bs, num_queries, 4)  when `return_intermediate`\n              is `False`. Otherwise, Intermediate references of all decoder\n              layers, has shape (num_decoder_layers, bs, num_queries, 4). The\n              coordinates are arranged as (cx, cy, w, h)", "file": "mmdet_models_layers_transformer_dino_layers.py", "function": "DinoTransformerDecoder.forward", "project": "mmdetection", "change_stats": {"code_changes": 4, "doc_changes": 20, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_secure_cookie(self, name, include_name=True, value=None,\n                          max_age_days=31):\n        \n        \n        if value is None: value = self.get_cookie(name)\n        if not value: return None\n        parts = utf8(value).split(b(\"|\"))\n        if len(parts) != 3: return None\n        if include_name:\n            signature = self._cookie_signature(name, parts[0], parts[1])\n        else:\n            signature = self._cookie_signature(parts[0], parts[1])\n        if not _time_independent_equals(parts[2], signature):\n            logging.warning(\"Invalid cookie signature %r\", value)\n            return None\n        timestamp = int(parts[1])\n        if timestamp < time.time() - max_age_days * 86400:\n            logging.warning(\"Expired cookie %r\", value)\n            return None\n        if timestamp > time.time() + 31 * 86400:\n            # _cookie_signature does not hash a delimiter between the\n            # parts of the cookie, so an attacker could transfer trailing\n            # digits from the payload to the timestamp without altering the\n            # signature.  For backwards compatibility, sanity-check timestamp\n            # here instead of modifying _cookie_signature.\n            logging.warning(\"Cookie timestamp in future; possible tampering %r\", value)\n            return None\n        if parts[1].startswith(b(\"0\")):\n            logging.warning(\"Tampered cookie %r\", value)\n        try:\n            return base64.b64decode(parts[0])\n        except:\n            return None```\n\nUpdated code:\n```python\ndef get_secure_cookie(self, name, value=None, max_age_days=31):\n        \n        if value is None: value = self.get_cookie(name)\n        if not value: return None\n        parts = utf8(value).split(b(\"|\"))\n        if len(parts) != 3: return None\n        signature = self._cookie_signature(name, parts[0], parts[1])\n        if not _time_independent_equals(parts[2], signature):\n            logging.warning(\"Invalid cookie signature %r\", value)\n            return None\n        timestamp = int(parts[1])\n        if timestamp < time.time() - max_age_days * 86400:\n            logging.warning(\"Expired cookie %r\", value)\n            return None\n        if timestamp > time.time() + 31 * 86400:\n            # _cookie_signature does not hash a delimiter between the\n            # parts of the cookie, so an attacker could transfer trailing\n            # digits from the payload to the timestamp without altering the\n            # signature.  For backwards compatibility, sanity-check timestamp\n            # here instead of modifying _cookie_signature.\n            logging.warning(\"Cookie timestamp in future; possible tampering %r\", value)\n            return None\n        if parts[1].startswith(b(\"0\")):\n            logging.warning(\"Tampered cookie %r\", value)\n        try:\n            return base64.b64decode(parts[0])\n        except:\n            return None\n```\n\nOriginal documentation:\n```python\nReturns the given signed cookie if it validates, or None.\n\n        In older versions of Tornado (0.1 and 0.2), we did not include the\n        name of the cookie in the cookie signature. To read these old-style\n        cookies, pass include_name=False to this method. Otherwise, all\n        attempts to read old-style cookies will fail (and you may log all\n        your users out whose cookies were written with a previous Tornado\n        version).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the given signed cookie if it validates, or None.", "file": "tornado_web.py", "function": "RequestHandler.get_secure_cookie", "project": "tornado", "change_stats": {"code_changes": 11, "doc_changes": 9, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _mesh(self):\n        '''\n        \n        '''\n        # if boundaries and values are None, then we can go ahead and\n        # scale this up for Auto tick location.  Otherwise we\n        # want to keep normalized between 0 and 1 and use manual tick\n        # locations.\n\n        x = np.array([0.0, 1.0])\n        if self.spacing == 'uniform':\n            y = self._uniform_y(self._central_N())\n        else:\n            y = self._proportional_y()\n        if self._use_auto_colorbar_locator():\n            y = self.norm.inverse(y)\n            x = self.norm.inverse(x)\n        self._y = y\n        X, Y = np.meshgrid(x, y)\n        if self._use_auto_colorbar_locator():\n            xmid = self.norm.inverse(0.5)\n        else:\n            xmid = 0.5\n        if self._extend_lower() and not self.extendrect:\n            X[0, :] = xmid\n        if self._extend_upper() and not self.extendrect:\n            X[-1, :] = xmid\n        return X, Y```\n\nUpdated code:\n```python\ndef _mesh(self):\n        '''\n        \n        '''\n        # copy the norm and change the vmin and vmax to the vmin and\n        # vmax of the colorbar, not the norm.  This allows the situation\n        # where the colormap has a narrower range than the colorbar, to\n        # accomodate extra contours:\n        norm = copy.copy(self.norm)\n        norm.vmin = self.vmin\n        norm.vmax = self.vmax\n        x = np.array([0.0, 1.0])\n        if self.spacing == 'uniform':\n            y = self._uniform_y(self._central_N())\n        else:\n            y = self._proportional_y()\n        xmid = np.array([0.5])\n        try:\n            y = norm.inverse(y)\n            x = norm.inverse(x)\n            xmid = norm.inverse(xmid)\n        except ValueError:\n            # occurs for norms that don't have an inverse, in\n            # which case manually scale:\n            dv = self.vmax - self.vmin\n            x = x * dv + self.vmin\n            y = y * dv + self.vmin\n            xmid = xmid * dv + self.vmin\n        self._y = y\n        X, Y = np.meshgrid(x, y)\n        if self._extend_lower() and not self.extendrect:\n            X[0, :] = xmid\n        if self._extend_upper() and not self.extendrect:\n            X[-1, :] = xmid\n        return X, Y\n```\n\nOriginal documentation:\n```python\nReturn X,Y, the coordinate arrays for the colorbar pcolormesh.\n        These are suitable for a vertical colorbar; swapping and\n        transposition for a horizontal colorbar are done outside\n        this function.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return X,Y, the coordinate arrays for the colorbar pcolormesh.\n        These are suitable for a vertical colorbar; swapping and\n        transposition for a horizontal colorbar are done outside\n        this function.\n\n        These are scaled between vmin and vmax", "file": "lib_matplotlib_colorbar.py", "function": "ColorbarBase._mesh", "project": "matplotlib", "change_stats": {"code_changes": 33, "doc_changes": 4, "total_changes": 37}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get(\n        self,\n        indices: Optional[Union[int, slice, List[int]]] = None,\n        *,\n        neg_indices_left_of_zero: bool = False,\n        fill: Optional[float] = None,\n        one_hot_discrete: bool = False,\n        _ignore_last_ts: bool = False,\n        _add_last_ts_value: Optional[Any] = None,\n    ):\n        \n        \n        if indices is None:\n            data = self._get_all_data(\n                one_hot_discrete=one_hot_discrete,\n                _ignore_last_ts=_ignore_last_ts,\n            )\n        elif isinstance(indices, slice):\n            data = self._get_slice(\n                indices,\n                fill=fill,\n                neg_indices_left_of_zero=neg_indices_left_of_zero,\n                one_hot_discrete=one_hot_discrete,\n                _ignore_last_ts=_ignore_last_ts,\n                _add_last_ts_value=_add_last_ts_value,\n            )\n        elif isinstance(indices, list):\n            data = [\n                self._get_int_index(\n                    idx,\n                    fill=fill,\n                    neg_indices_left_of_zero=neg_indices_left_of_zero,\n                    one_hot_discrete=one_hot_discrete,\n                    _ignore_last_ts=_ignore_last_ts,\n                    _add_last_ts_value=_add_last_ts_value,\n                )\n                for idx in indices\n            ]\n            if self.finalized:\n                data = batch(data)\n        else:\n            assert isinstance(indices, int)\n            data = self._get_int_index(\n                indices,\n                fill=fill,\n                neg_indices_left_of_zero=neg_indices_left_of_zero,\n                one_hot_discrete=one_hot_discrete,\n                _ignore_last_ts=_ignore_last_ts,\n                _add_last_ts_value=_add_last_ts_value,\n            )\n\n        return data```\n\nUpdated code:\n```python\ndef get(\n        self,\n        indices: Optional[Union[int, slice, List[int]]] = None,\n        *,\n        neg_indices_left_of_zero: bool = False,\n        fill: Optional[Any] = None,\n        one_hot_discrete: bool = False,\n        _ignore_last_ts: bool = False,\n        _add_last_ts_value: Optional[Any] = None,\n    ):\n        \n        \n        if indices is None:\n            data = self._get_all_data(\n                one_hot_discrete=one_hot_discrete,\n                _ignore_last_ts=_ignore_last_ts,\n            )\n        elif isinstance(indices, slice):\n            data = self._get_slice(\n                indices,\n                fill=fill,\n                neg_indices_left_of_zero=neg_indices_left_of_zero,\n                one_hot_discrete=one_hot_discrete,\n                _ignore_last_ts=_ignore_last_ts,\n                _add_last_ts_value=_add_last_ts_value,\n            )\n        elif isinstance(indices, list):\n            data = [\n                self._get_int_index(\n                    idx,\n                    fill=fill,\n                    neg_indices_left_of_zero=neg_indices_left_of_zero,\n                    one_hot_discrete=one_hot_discrete,\n                    _ignore_last_ts=_ignore_last_ts,\n                    _add_last_ts_value=_add_last_ts_value,\n                )\n                for idx in indices\n            ]\n            if self.finalized:\n                data = batch(data)\n        else:\n            assert isinstance(indices, int)\n            data = self._get_int_index(\n                indices,\n                fill=fill,\n                neg_indices_left_of_zero=neg_indices_left_of_zero,\n                one_hot_discrete=one_hot_discrete,\n                _ignore_last_ts=_ignore_last_ts,\n                _add_last_ts_value=_add_last_ts_value,\n            )\n\n        return data\n```\n\nOriginal documentation:\n```python\nReturns data, based on the given args, from this buffer.\n\n        Args:\n            indices: A single int is interpreted as an index, from which to return the\n                individual data stored at this index.\n                A list of ints is interpreted as a list of indices from which to gather\n                individual data in a batch of size len(indices).\n                A slice object is interpreted as a range of data to be returned.\n                Thereby, negative indices by default are interpreted as \"before the end\"\n                unless the `neg_indices_left_of_zero=True` option is used, in which case\n                negative indices are interpreted as \"before ts=0\", meaning going back\n                into the lookback buffer.\n            neg_indices_left_of_zero: If True, negative values in `indices` are\n                interpreted as \"before ts=0\", meaning going back into the lookback\n                buffer. For example, an buffer with data [4, 5, 6,  7, 8, 9],\n                where [4, 5, 6] is the lookback buffer range (ts=0 item is 7), will\n                respond to `get(-1, neg_indices_left_of_zero=True)` with `6` and to\n                `get(slice(-2, 1), neg_indices_left_of_zero=True)` with `[5, 6,  7]`.\n            fill: An optional float value to use for filling up the returned results at\n                the boundaries. This filling only happens if the requested index range's\n                start/stop boundaries exceed the buffer's boundaries (including the\n                lookback buffer on the left side). This comes in very handy, if users\n                don't want to worry about reaching such boundaries and want to zero-pad.\n                For example, a buffer with data [10, 11,  12, 13, 14] and lookback\n                buffer size of 2 (meaning `10` and `11` are part of the lookback buffer)\n                will respond to `get(slice(-7, -2), fill=0.0)`\n                with `[0.0, 0.0, 10, 11, 12]`.\n            one_hot_discrete: If True, will return one-hot vectors (instead of\n                int-values) for those sub-components of a (possibly complex) space\n                that are Discrete or MultiDiscrete. Note that if `fill=0` and the\n                requested `indices` are out of the range of our data, the returned\n                one-hot vectors will actually be zero-hot (all slots zero).\n            _ignore_last_ts: Whether to ignore the last record in our internal\n                `self.data` when getting the provided indices.\n            _add_last_ts_value: Whether to add the value of this arg to the end of\n                the internal `self.data` buffer (just for the duration of this get\n                operation, not permanently).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns data, based on the given args, from this buffer.\n\n        Args:\n            indices: A single int is interpreted as an index, from which to return the\n                individual data stored at this index.\n                A list of ints is interpreted as a list of indices from which to gather\n                individual data in a batch of size len(indices).\n                A slice object is interpreted as a range of data to be returned.\n                Thereby, negative indices by default are interpreted as \"before the end\"\n                unless the `neg_indices_left_of_zero=True` option is used, in which case\n                negative indices are interpreted as \"before ts=0\", meaning going back\n                into the lookback buffer.\n            neg_indices_left_of_zero: If True, negative values in `indices` are\n                interpreted as \"before ts=0\", meaning going back into the lookback\n                buffer. For example, a buffer with data [4, 5, 6,  7, 8, 9],\n                where [4, 5, 6] is the lookback buffer range (ts=0 item is 7), will\n                respond to `get(-1, neg_indices_left_of_zero=True)` with `6` and to\n                `get(slice(-2, 1), neg_indices_left_of_zero=True)` with `[5, 6,  7]`.\n            fill: An optional float value to use for filling up the returned results at\n                the boundaries. This filling only happens if the requested index range's\n                start/stop boundaries exceed the buffer's boundaries (including the\n                lookback buffer on the left side). This comes in very handy, if users\n                don't want to worry about reaching such boundaries and want to zero-pad.\n                For example, a buffer with data [10, 11,  12, 13, 14] and lookback\n                buffer size of 2 (meaning `10` and `11` are part of the lookback buffer)\n                will respond to `get(slice(-7, -2), fill=0.0)`\n                with `[0.0, 0.0, 10, 11, 12]`.\n            one_hot_discrete: If True, will return one-hot vectors (instead of\n                int-values) for those sub-components of a (possibly complex) space\n                that are Discrete or MultiDiscrete. Note that if `fill=0` and the\n                requested `indices` are out of the range of our data, the returned\n                one-hot vectors will actually be zero-hot (all slots zero).\n            _ignore_last_ts: Whether to ignore the last record in our internal\n                `self.data` when getting the provided indices.\n            _add_last_ts_value: Whether to add the value of this arg to the end of\n                the internal `self.data` buffer (just for the duration of this get\n                operation, not permanently).", "file": "rllib_env_utils_infinite_lookback_buffer.py", "function": "InfiniteLookbackBuffer.get", "project": "ray", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef genops(pickle):\n    \n    \n\n    if isinstance(pickle, bytes_types):\n        import io\n        pickle = io.BytesIO(pickle)\n\n    if hasattr(pickle, \"tell\"):\n        getpos = pickle.tell\n    else:\n        getpos = lambda: None\n\n    while True:\n        pos = getpos()\n        code = pickle.read(1)\n        opcode = code2op.get(code.decode(\"latin-1\"))\n        if opcode is None:\n            if code == b\"\":\n                raise ValueError(\"pickle exhausted before seeing STOP\")\n            else:\n                raise ValueError(\"at position %s, opcode %r unknown\" % (\n                                 pos is None and \"<unknown>\" or pos,\n                                 code))\n        if opcode.arg is None:\n            arg = None\n        else:\n            arg = opcode.arg.reader(pickle)\n        yield opcode, arg, pos\n        if code == b'.':\n            assert opcode.name == 'STOP'\n            break```\n\nUpdated code:\n```python\ndef genops(pickle):\n    \n    \n\n    import cStringIO as StringIO\n\n    if isinstance(pickle, str):\n        pickle = StringIO.StringIO(pickle)\n\n    if hasattr(pickle, \"tell\"):\n        getpos = pickle.tell\n    else:\n        getpos = lambda: None\n\n    while True:\n        pos = getpos()\n        code = pickle.read(1)\n        opcode = code2op.get(code)\n        if opcode is None:\n            if code == \"\":\n                raise ValueError(\"pickle exhausted before seeing STOP\")\n            else:\n                raise ValueError(\"at position %s, opcode %r unknown\" % (\n                                 pos is None and \"<unknown>\" or pos,\n                                 code))\n        if opcode.arg is None:\n            arg = None\n        else:\n            arg = opcode.arg.reader(pickle)\n        yield opcode, arg, pos\n        if code == '.':\n            assert opcode.name == 'STOP'\n            break\n```\n\nOriginal documentation:\n```python\nGenerate all the opcodes in a pickle.\n\n    'pickle' is a file-like object, or string, containing the pickle.\n\n    Each opcode in the pickle is generated, from the current pickle position,\n    stopping after a STOP opcode is delivered.  A triple is generated for\n    each opcode:\n\n        opcode, arg, pos\n\n    opcode is an OpcodeInfo record, describing the current opcode.\n\n    If the opcode has an argument embedded in the pickle, arg is its decoded\n    value, as a Python object.  If the opcode doesn't have an argument, arg\n    is None.\n\n    If the pickle has a tell() method, pos was the value of pickle.tell()\n    before reading the current opcode.  If the pickle is a bytes object,\n    it's wrapped in a BytesIO object, and the latter's tell() result is\n    used.  Else (the pickle doesn't have a tell(), and it's not obvious how\n    to query its current position) pos is None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate all the opcodes in a pickle.\n\n    'pickle' is a file-like object, or string, containing the pickle.\n\n    Each opcode in the pickle is generated, from the current pickle position,\n    stopping after a STOP opcode is delivered.  A triple is generated for\n    each opcode:\n\n        opcode, arg, pos\n\n    opcode is an OpcodeInfo record, describing the current opcode.\n\n    If the opcode has an argument embedded in the pickle, arg is its decoded\n    value, as a Python object.  If the opcode doesn't have an argument, arg\n    is None.\n\n    If the pickle has a tell() method, pos was the value of pickle.tell()\n    before reading the current opcode.  If the pickle is a string object,\n    it's wrapped in a StringIO object, and the latter's tell() result is\n    used.  Else (the pickle doesn't have a tell(), and it's not obvious how\n    to query its current position) pos is None.", "file": "Lib_pickletools.py", "function": "genops", "project": "cpython", "change_stats": {"code_changes": 15, "doc_changes": 6, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_label_map_dict(label_map_path):\n  \n  \n  label_map = load_labelmap(label_map_path)\n  label_map_dict = {}\n  for item in label_map.item:\n    label_map_dict[item.name] = item.id\n  return label_map_dict```\n\nUpdated code:\n```python\ndef get_label_map_dict(label_map_path, use_display_name=False):\n  \n  \n  label_map = load_labelmap(label_map_path)\n  label_map_dict = {}\n  for item in label_map.item:\n    if use_display_name:\n      label_map_dict[item.display_name] = item.id\n    else:\n      label_map_dict[item.name] = item.id\n  return label_map_dict\n```\n\nOriginal documentation:\n```python\nReads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n\n  Returns:\n    A dictionary mapping label names to id.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Reads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n    use_display_name: whether to use the label map items' display names as keys.\n\n  Returns:\n    A dictionary mapping label names to id.", "file": "research_object_detection_utils_label_map_util.py", "function": "get_label_map_dict", "project": "models", "change_stats": {"code_changes": 9, "doc_changes": 3, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_config_update_handlers(bus):\n        \n        \n        bus.on(\"configuration.updated\", Configuration.updated)\n        bus.on(\"configuration.patch\", Configuration.patch)```\n\nUpdated code:\n```python\ndef set_config_update_handlers(bus):\n        \n        \n        bus.on(\"configuration.updated\", Configuration.updated)\n        bus.on(\"configuration.patch\", Configuration.patch)\n        bus.on(\"configuration.patch.clear\", Configuration.patch_clear)\n```\n\nOriginal documentation:\n```python\nSetup websocket handlers to update config.\n\n        Args:\n            bus: Message bus client instance\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Setup websocket handlers to update config.\n\n        Arguments:\n            bus: Message bus client instance", "file": "mycroft_configuration_config.py", "function": "Configuration.set_config_update_handlers", "project": "mycroft-core", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef app_uninstall(self, pkg_name):\n          \n        self.shell([\"pm\", \"uninstall\", pkg_name])```\n\nUpdated code:\n```python\ndef app_uninstall(self, pkg_name) -> bool:\n         \n        \n        ret = self.shell([\"pm\", \"uninstall\", pkg_name])\n        return ret.exit_code == 0\n```\n\nOriginal documentation:\n```python\nUninstall an app\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Uninstall an app \n        \n        Returns:\n            bool: success", "file": "uiautomator2___init__.py", "function": "UIAutomatorServer.app_uninstall", "project": "uiautomator2", "change_stats": {"code_changes": 10, "doc_changes": 7, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_ssh_struct(name: str, home_dir: str) -> Dict:\n    \n    \n    \n    # TODO: There may be multiple public keys for a single user\n    # TODO: Authorized keys are missing.\n    return {\n        \"name\": name,\n        \"home_dir\": home_dir,\n        \"public_key\": None,\n        \"private_key\": None,\n        \"known_hosts\": None,\n    }```\n\nUpdated code:\n```python\ndef _get_ssh_struct(name: str, home_dir: str) -> Dict:\n    \n    \n    \n    # TODO: There may be multiple public keys for a single user\n    # TODO: Authorized keys are missing.\n    return {\n        \"name\": name,\n        \"home_dir\": home_dir,\n        \"public_key\": None,\n        \"private_key\": None,\n    }\n```\n\nOriginal documentation:\n```python\nConstruct the SSH info. It consisted of: name, home_dir,\n    public_key, private_key and known_hosts.\n\n    public_key: contents of *.pub file (public key)\n    private_key: contents of * file (private key)\n    known_hosts: contents of known_hosts file(all the servers keys are good for,\n    possibly hashed)\n\n    :param name: username of user, for whom the keys belong\n    :param home_dir: users home directory\n    :return: SSH info struct\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Construct the SSH info. It consisted of: name, home_dir,\n    public_key and private_key.\n\n    public_key: contents of *.pub file (public key)\n    private_key: contents of * file (private key)\n\n    :param name: username of user, for whom the keys belong\n    :param home_dir: users home directory\n    :return: SSH info struct", "file": "monkey_infection_monkey_credential_collectors_ssh_collector_ssh_handler.py", "function": "_get_ssh_struct", "project": "monkey", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef paragraph(cls, nb_sentences=3, variable_nb_sentences=True, ext_word_list=None):\n        \n        \n        \n        if nb_sentences <= 0:\n            return ''\n\n        if variable_nb_sentences:\n            nb_sentences = cls.randomize_nb_elements(nb_sentences)\n\n        para =  cls.word_connector.join(cls.sentences(\n                nb_sentences, ext_word_list=ext_word_list)\n                )\n\n        return para```\n\nUpdated code:\n```python\ndef paragraph(self, nb_sentences=3, variable_nb_sentences=True, ext_word_list=None):\n        \n        \n        \n        if nb_sentences <= 0:\n            return ''\n\n        if variable_nb_sentences:\n            nb_sentences = self.randomize_nb_elements(nb_sentences)\n\n        para = self.word_connector.join(self.sentences(\n            nb_sentences, ext_word_list=ext_word_list\n        ))\n\n        return para\n```\n\nOriginal documentation:\n```python\nGenerate a single paragraph\n        :example 'Sapiente sunt omnis. Ut pariatur ad autem ducimus et. Voluptas rem voluptas sint modi dolorem amet.'\n\n        Keyword arguments:\n        :param 'nb_sentences' --- around how many sentences the paragraph should contain\n        :param 'variable_nb_sentences' --- set to false if you want exactly $nbSentences returned,\n            otherwise $nbSentences may vary by +/-40% with a minimum of 1\n        :param 'ext_word_list' --- a list of word you would like to have \n                 instead of 'Lorem ipsum'\n\n        :return string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":returns: A single paragraph. For example: 'Sapiente sunt omnis. Ut\n            pariatur ad autem ducimus et. Voluptas rem voluptas sint modi dolorem amet.'\n\n        Keyword arguments:\n        :param nb_sentences: around how many sentences the paragraph should contain\n        :param variable_nb_sentences: set to false if you want exactly ``nb``\n            sentences returned, otherwise the result may include a number of\n            sentences of ``nb`` +/-40% (with a minimum of 1)\n        :param ext_word_list: a list of words you would like to have instead of\n            'Lorem ipsum'.\n\n        :rtype: str", "file": "faker_providers_lorem___init__.py", "function": "Provider.paragraph", "project": "faker", "change_stats": {"code_changes": 12, "doc_changes": 19, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_children(self):\n        ''\n        children = []\n        if self._legend_box:\n            children.append(self._legend_box)\n        children.append(self.get_frame())\n\n        return children```\n\nUpdated code:\n```python\ndef get_children(self):\n        \n        children = []\n        if self._legend_box:\n            children.append(self._legend_box)\n        children.append(self.get_frame())\n\n        return children\n```\n\nOriginal documentation:\n```python\nReturn a list of child artists.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the list of child artists.", "file": "lib_matplotlib_legend.py", "function": "Legend.get_children", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef prompt_choice_for_config(cookiecutter_dict, env, key, options, no_input):\n    \n    \n    rendered_options = [render_variable(env, raw, cookiecutter_dict) for raw in options]\n\n    if no_input:\n        return rendered_options[0]\n    return read_user_choice(key, rendered_options)```\n\nUpdated code:\n```python\ndef prompt_choice_for_config(cookiecutter_dict, env, key, options, no_input):\n    \n    \n    rendered_options = [render_variable(env, raw, cookiecutter_dict) for raw in options]\n    if no_input:\n        return rendered_options[0]\n    return read_user_choice(key, rendered_options)\n```\n\nOriginal documentation:\n```python\nPrompt user with a set of options to choose from.\n\n    Each of the possible choices is rendered beforehand.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Prompt user with a set of options to choose from.\n\n    :param no_input: Do not prompt for user input and return the first available option.", "file": "cookiecutter_prompt.py", "function": "prompt_choice_for_config", "project": "cookiecutter", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        filepath_or_buffer: FilePath | ReadBuffer[bytes],\n        storage_options: StorageOptions = None,\n    ) -> None:\n        \n        \n        \n        import_optional_dependency(\"openpyxl\")\n        super().__init__(filepath_or_buffer, storage_options=storage_options)```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        filepath_or_buffer: FilePath | ReadBuffer[bytes],\n        storage_options: StorageOptions = None,\n        engine_kwargs: dict | None = None,\n    ) -> None:\n        \n        \n        \n        import_optional_dependency(\"openpyxl\")\n        super().__init__(\n            filepath_or_buffer,\n            storage_options=storage_options,\n            engine_kwargs=engine_kwargs,\n        )\n```\n\nOriginal documentation:\n```python\nReader using openpyxl engine.\n\n        Parameters\n        ----------\n        filepath_or_buffer : str, path object or Workbook\n            Object to be parsed.\n        {storage_options}\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Reader using openpyxl engine.\n\n        Parameters\n        ----------\n        filepath_or_buffer : str, path object or Workbook\n            Object to be parsed.\n        {storage_options}\n        engine_kwargs : dict, optional\n            Arbitrary keyword arguments passed to excel engine.", "file": "pandas_io_excel__openpyxl.py", "function": "OpenpyxlReader.__init__", "project": "pandas", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_secure_cookie(\n        self,\n        name: str,\n        value: Union[str, bytes],\n        expires_days: int = 30,\n        version: int = None,\n        **kwargs: Any\n    ) -> None:\n        \n        \n        self.set_cookie(\n            name,\n            self.create_signed_value(name, value, version=version),\n            expires_days=expires_days,\n            **kwargs\n        )```\n\nUpdated code:\n```python\ndef set_secure_cookie(\n        self,\n        name: str,\n        value: Union[str, bytes],\n        expires_days: Optional[int] = 30,\n        version: Optional[int] = None,\n        **kwargs: Any\n    ) -> None:\n        \n        \n        self.set_cookie(\n            name,\n            self.create_signed_value(name, value, version=version),\n            expires_days=expires_days,\n            **kwargs\n        )\n```\n\nOriginal documentation:\n```python\nSigns and timestamps a cookie so it cannot be forged.\n\n        You must specify the ``cookie_secret`` setting in your Application\n        to use this method. It should be a long, random sequence of bytes\n        to be used as the HMAC secret for the signature.\n\n        To read a cookie set with this method, use `get_secure_cookie()`.\n\n        Note that the ``expires_days`` parameter sets the lifetime of the\n        cookie in the browser, but is independent of the ``max_age_days``\n        parameter to `get_secure_cookie`.\n\n        Secure cookies may contain arbitrary byte values, not just unicode\n        strings (unlike regular cookies)\n\n        Similar to `set_cookie`, the effect of this method will not be\n        seen until the following request.\n\n        .. versionchanged:: 3.2.1\n\n           Added the ``version`` argument.  Introduced cookie version 2\n           and made it the default.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Signs and timestamps a cookie so it cannot be forged.\n\n        You must specify the ``cookie_secret`` setting in your Application\n        to use this method. It should be a long, random sequence of bytes\n        to be used as the HMAC secret for the signature.\n\n        To read a cookie set with this method, use `get_secure_cookie()`.\n\n        Note that the ``expires_days`` parameter sets the lifetime of the\n        cookie in the browser, but is independent of the ``max_age_days``\n        parameter to `get_secure_cookie`.\n        A value of None limits the lifetime to the current browser session.\n\n        Secure cookies may contain arbitrary byte values, not just unicode\n        strings (unlike regular cookies)\n\n        Similar to `set_cookie`, the effect of this method will not be\n        seen until the following request.\n\n        .. versionchanged:: 3.2.1\n\n           Added the ``version`` argument.  Introduced cookie version 2\n           and made it the default.", "file": "tornado_web.py", "function": "RequestHandler.set_secure_cookie", "project": "tornado", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef backward(ctx: Any, *grad_outputs: Any) -> Any:\n        \n        \n        raise NotImplementedError(\"You must implement the backward function for custom\"\n                                  \" autograd.Function.\")```\n\nUpdated code:\n```python\ndef backward(ctx: Any, *grad_outputs: Any) -> Any:\n        \n        \n        raise NotImplementedError(\"You must implement either the backward or vjp method for \"\n                                  \"your custom autograd.Function to use it with backward \"\n                                  \"mode AD.\")\n```\n\nOriginal documentation:\n```python\nr\"\"\"Defines a formula for differentiating the operation.\n\n        This function is to be overridden by all subclasses.\n\n        It must accept a context :attr:`ctx` as the first argument, followed by\n        as many outputs as the :func:`forward` returned (None will be passed in\n        for non tensor outputs of the forward function),\n        and it should return as many tensors, as there were inputs to\n        :func:`forward`. Each argument is the gradient w.r.t the given output,\n        and each returned value should be the gradient w.r.t. the\n        corresponding input. If an input is not a Tensor or is a Tensor not\n        requiring grads, you can just pass None as a gradient for that input.\n\n        The context can be used to retrieve tensors saved during the forward\n        pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n        of booleans representing whether each input needs gradient. E.g.,\n        :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n        first input to :func:`forward` needs gradient computated w.r.t. the\n        output.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Defines a formula for differentiating the operation with backward mode\n        automatic differentiation.\n\n        This function is to be overridden by all subclasses.\n\n        It must accept a context :attr:`ctx` as the first argument, followed by\n        as many outputs as the :func:`forward` returned (None will be passed in\n        for non tensor outputs of the forward function),\n        and it should return as many tensors, as there were inputs to\n        :func:`forward`. Each argument is the gradient w.r.t the given output,\n        and each returned value should be the gradient w.r.t. the\n        corresponding input. If an input is not a Tensor or is a Tensor not\n        requiring grads, you can just pass None as a gradient for that input.\n\n        The context can be used to retrieve tensors saved during the forward\n        pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n        of booleans representing whether each input needs gradient. E.g.,\n        :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n        first input to :func:`forward` needs gradient computated w.r.t. the\n        output.", "file": "torch_autograd_function.py", "function": "Function.backward", "project": "pytorch", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef prepare_filter(cursor, sorting_fields, sorting_direction):\n    \n    \n    \n    filter_kwargs = Q()\n    for field_id in range(len(sorting_fields)):\n        field_expression = {}\n        for val_id in range(len(cursor[:field_id])):\n            field_expression[sorting_fields[val_id]] = cursor[val_id]\n        field_expression[f\"{sorting_fields[field_id]}__{sorting_direction}\"] = cursor[\n            field_id\n        ]\n        filter_kwargs |= Q(**field_expression)\n    return filter_kwargs```\n\nUpdated code:\n```python\ndef prepare_filter(cursor, sorting_fields, sorting_direction):\n    \n    \n    filter_kwargs = Q()\n    for index, field_name in enumerate(sorting_fields):\n        field_expression = {}\n        for cursor_id, cursor_value in enumerate(cursor[:index]):\n            field_expression[sorting_fields[cursor_id]] = cursor_value\n        field_expression[f\"{field_name}__{sorting_direction}\"] = cursor[index]\n        filter_kwargs |= Q(**field_expression)\n    return filter_kwargs\n```\n\nOriginal documentation:\n```python\n:param cursor: list of values that are passed from page_info, used for filtering.\n    :param sorting_fields: list of fields that were used for sorting.\n    :param sorting_direction: keyword direction ('lt', gt').\n    :return: Q() in following format\n        (OR: ('first_field__gt', 'first_value_form_cursor'),\n            (AND: ('second_field__gt', 'second_value_form_cursor'),\n                ('first_field', 'first_value_form_cursor')),\n            (AND: ('thrid_field__gt', 'third_value_form_cursor'),\n                ('second_field__gt', 'second_value_form_cursor'),\n                ('first_field', 'first_value_form_cursor'))\n        )\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create filter arguments based on sorting fields.\n\n    :param cursor: list of values that are passed from page_info, used for filtering.\n    :param sorting_fields: list of fields that were used for sorting.\n    :param sorting_direction: keyword direction ('lt', gt').\n    :return: Q() in following format\n        (OR: ('first_field__gt', 'first_value_form_cursor'),\n            (AND: ('second_field__gt', 'second_value_form_cursor'),\n                ('first_field', 'first_value_form_cursor')),\n            (AND: ('third_field__gt', 'third_value_form_cursor'),\n                ('second_field', 'second_value_form_cursor'),\n                ('first_field', 'first_value_form_cursor'))\n        )", "file": "saleor_graphql_core_connection.py", "function": "prepare_filter", "project": "saleor", "change_stats": {"code_changes": 13, "doc_changes": 10, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE):\n    \n    \n    image = _decode_and_center_crop(image_bytes, image_size)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image```\n\nUpdated code:\n```python\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \n    \n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n```\n\nOriginal documentation:\n```python\nPreprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n\n    Returns:\n      A preprocessed image `Tensor`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.", "file": "timm_data_tf_preprocessing.py", "function": "preprocess_for_eval", "project": "pytorch-image-models", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _monitor(self, thread):\n         \n        \n        logger.debug(\"Launching Monitor\")\n        logger.info(\"===================================================\")\n        logger.info(\"  Starting\")\n        if self._args.preview:\n            logger.info(\"  Using live preview\")\n        if sys.stdout.isatty():\n            logger.info(\"  Press '%s' to save and quit\",\n                        \"Stop\" if self._args.redirect_gui or self._args.colab else \"ENTER\")\n        if not self._args.redirect_gui and not self._args.colab and sys.stdout.isatty():\n            logger.info(\"  Press 'S' to save model weights immediately\")\n        logger.info(\"===================================================\")\n\n        keypress = KBHit(is_gui=self._args.redirect_gui)\n        window_created = False\n        err = False\n        while True:\n            try:\n                if self._args.preview:\n                    with self._lock:\n                        for name, image in self._preview_buffer.items():\n                            if not window_created:\n                                self._create_resizable_window(name, image.shape)\n                            cv2.imshow(name, image)  # pylint: disable=no-member\n                        if not window_created:\n                            window_created = bool(self._preview_buffer)\n                    cv_key = cv2.waitKey(1000)  # pylint: disable=no-member\n                else:\n                    cv_key = None\n\n                if thread.has_error:\n                    logger.debug(\"Thread error detected\")\n                    err = True\n                    break\n                if self._stop:\n                    logger.debug(\"Stop received\")\n                    break\n\n                # Preview Monitor\n                if not self._preview_monitor(cv_key):\n                    break\n\n                # Console Monitor\n                if keypress.kbhit():\n                    console_key = keypress.getch()\n                    if console_key in (\"\\n\", \"\\r\"):\n                        logger.debug(\"Exit requested\")\n                        break\n                    if console_key in (\"s\", \"S\"):\n                        logger.info(\"Save requested\")\n                        self._save_now = True\n\n                # GUI Preview trigger update monitor\n                self._process_gui_triggers()\n\n                sleep(1)\n            except KeyboardInterrupt:\n                logger.debug(\"Keyboard Interrupt received\")\n                break\n        keypress.set_normal_term()\n        logger.debug(\"Closed Monitor\")\n        return err```\n\nUpdated code:\n```python\ndef _monitor(self, thread: MultiThread) -> bool:\n         \n        \n        self._output_startup_info()\n        keypress = KBHit(is_gui=self._args.redirect_gui)\n        window_created = False\n        err = False\n        while True:\n            try:\n                if self._args.preview:\n                    self._do_preview(window_created)\n                    cv_key = cv2.waitKey(1000)  # pylint: disable=no-member\n                else:\n                    cv_key = None\n\n                if thread.has_error:\n                    logger.debug(\"Thread error detected\")\n                    err = True\n                    break\n                if self._stop:\n                    logger.debug(\"Stop received\")\n                    break\n\n                # Preview Monitor\n                if not self._preview_monitor(cv_key):\n                    break\n\n                # Console Monitor\n                if self._check_keypress(keypress):\n                    break  # Exit requested\n\n                # GUI Preview trigger update monitor\n                self._process_gui_triggers()\n\n                sleep(1)\n            except KeyboardInterrupt:\n                logger.debug(\"Keyboard Interrupt received\")\n                break\n        keypress.set_normal_term()\n        logger.debug(\"Closed Monitor\")\n        return err\n```\n\nOriginal documentation:\n```python\nMonitor the background :func:`_training` thread for key presses and errors.\n\n        Returns\n        -------\n        bool\n            ``True`` if there has been an error in the background thread otherwise ``False``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Monitor the background :func:`_training` thread for key presses and errors.\n\n        Parameters\n        ----------\n        thread: :class:~`lib.multithreading.MultiThread`\n            The thread containing the training loop\n\n        Returns\n        -------\n        bool\n            ``True`` if there has been an error in the background thread otherwise ``False``", "file": "scripts_train.py", "function": "Train._monitor", "project": "faceswap", "change_stats": {"code_changes": 35, "doc_changes": 7, "total_changes": 42}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef read_image_batch(filenames):\n     \n    \n    logger.trace(\"Requested batch: '%s'\", filenames)\n    executor = futures.ThreadPoolExecutor()\n    with executor:\n        images = [executor.submit(read_image, filename, raise_error=True)\n                  for filename in filenames]\n        batch = np.array([future.result() for future in futures.as_completed(images)])\n    logger.trace(\"Returning images: %s\", batch.shape)\n    return batch```\n\nUpdated code:\n```python\ndef read_image_batch(filenames):\n     \n    \n    logger.trace(\"Requested batch: '%s'\", filenames)\n    executor = futures.ThreadPoolExecutor()\n    with executor:\n        images = {executor.submit(read_image, filename, raise_error=True): filename\n                  for filename in filenames}\n        batch = []\n        filenames = []\n        for future in futures.as_completed(images):\n            batch.append(future.result())\n            filenames.append(images[future])\n        batch = np.array(batch)\n    logger.trace(\"Returning images: (filenames: %s, batch shape: %s)\", filenames, batch.shape)\n    return filenames, batch\n```\n\nOriginal documentation:\n```python\nLoad a batch of images from the given file locations.\n\n    Leverages multi-threading to load multiple images from disk at the same time\n    leading to vastly reduced image read times.\n\n    Parameters\n    ----------\n    filenames: list\n        A list of ``str`` full paths to the images to be loaded.\n\n    Returns\n    -------\n    numpy.ndarray\n        The batch of images in `BGR` channel order.\n\n    Notes\n    -----\n    As the images are compiled into a batch, they must be all of the same dimensions.\n\n    Example\n    -------\n    >>> image_filenames = [\"/path/to/image_1.png\", \"/path/to/image_2.png\", \"/path/to/image_3.png\"]\n    >>> images = read_image_batch(image_filenames)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a batch of images from the given file locations.\n\n    Leverages multi-threading to load multiple images from disk at the same time\n    leading to vastly reduced image read times.\n\n    Notes\n    -----\n    Images are loaded concurrently, so the order of the returned batch will likely not be the same\n    as the order of the input filenames. Filenames are returned with the batch in the correct order\n    corresponding to the returned batch.\n\n    Parameters\n    ----------\n    filenames: list\n        A list of ``str`` full paths to the images to be loaded.\n\n    Returns\n    -------\n    list\n        Filenames in the correct order as they are returned\n    numpy.ndarray\n        The batch of images in `BGR` channel order.\n\n    Notes\n    -----\n    As the images are compiled into a batch, they must be all of the same dimensions.\n\n    Example\n    -------\n    >>> image_filenames = [\"/path/to/image_1.png\", \"/path/to/image_2.png\", \"/path/to/image_3.png\"]\n    >>> filenames, images = read_image_batch(image_filenames)", "file": "lib_image.py", "function": "read_image_batch", "project": "faceswap", "change_stats": {"code_changes": 17, "doc_changes": 12, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_benchmark_data():\n    \n    \n    \n    start_date = datetime(year=1950, month=1, day=3)\n    end_date = datetime.utcnow()\n\n    raw_benchmark_data = get_raw_benchmark_data(start_date, end_date)\n    # Reverse data so we can load it in reverse chron order.\n    benchmarks_source = reversed(list(raw_benchmark_data))\n\n    mappings = benchmark_mappings()\n\n    return source_to_records(mappings, benchmarks_source)```\n\nUpdated code:\n```python\ndef get_benchmark_data(symbol):\n    \n    \n    \n    start_date = datetime(year=1950, month=1, day=3)\n    end_date = datetime.utcnow()\n\n    raw_benchmark_data = get_raw_benchmark_data(start_date, end_date, symbol)\n    # Reverse data so we can load it in reverse chron order.\n    benchmarks_source = reversed(list(raw_benchmark_data))\n\n    mappings = benchmark_mappings()\n\n    return source_to_records(mappings, benchmarks_source)\n```\n\nOriginal documentation:\n```python\nBenchmarks from Yahoo's GSPC source.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Benchmarks from Yahoo.", "file": "zipline_data_benchmarks.py", "function": "get_benchmark_data", "project": "zipline", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __iter__(self):\n        \n        \n        return self._iter()```\n\nUpdated code:\n```python\ndef __iter__(self):\n        \n        \n        if self._indices is None and config.PYARROW_VERSION.major >= 8:\n            # Fast iteration\n            # Benchmark: https://gist.github.com/mariosasko/0248288a2e3a7556873969717c1fe52b (fast_iter_batch)\n            format_kwargs = self._format_kwargs if self._format_kwargs is not None else {}\n            formatter = get_formatter(self._format_type, features=self.features, **format_kwargs)\n            batch_size = config.ARROW_READER_BATCH_SIZE_IN_DATASET_ITER\n            for pa_subtable in table_iter(self.data, batch_size=batch_size):\n                for i in range(pa_subtable.num_rows):\n                    pa_subtable_ex = pa_subtable.slice(i, 1)\n                    formatted_output = format_table(\n                        pa_subtable_ex,\n                        0,\n                        formatter=formatter,\n                        format_columns=self._format_columns,\n                        output_all_columns=self._output_all_columns,\n                    )\n                    yield formatted_output\n        else:\n            for i in range(self.num_rows):\n                yield self._getitem(\n                    i,\n                )\n```\n\nOriginal documentation:\n```python\nIterate through the examples.\n\n        If a formatting is set with [`~datasets.Dataset.set_format`] rows will be returned with the\n        selected format.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Iterate through the examples.\n\n        If a formatting is set with :meth:`Dataset.set_format` rows will be returned with the\n        selected format.", "file": "src_datasets_arrow_dataset.py", "function": "Dataset.__iter__", "project": "datasets", "change_stats": {"code_changes": 25, "doc_changes": 4, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate(self, app=None, display_num_errors=False):\n        \n        \n\n        \n        from django.core.management.validation import get_validation_errors\n        s = StringIO()\n        num_errors = get_validation_errors(s, app)\n        if num_errors:\n            s.seek(0)\n            error_text = s.read()\n            raise CommandError(\"One or more models did not validate:\\n%s\" % error_text)\n        if display_num_errors:\n            self.stdout.write(\"%s error%s found\" % (num_errors, '' if num_errors == 1 else 's'))```\n\nUpdated code:\n```python\ndef validate(self, app_config=None, display_num_errors=False):\n        \n        \n\n        \n        from django.core.management.validation import get_validation_errors\n        s = StringIO()\n        num_errors = get_validation_errors(s, app_config)\n        if num_errors:\n            s.seek(0)\n            error_text = s.read()\n            raise CommandError(\"One or more models did not validate:\\n%s\" % error_text)\n        if display_num_errors:\n            self.stdout.write(\"%s error%s found\" % (num_errors, '' if num_errors == 1 else 's'))\n```\n\nOriginal documentation:\n```python\nValidates the given app, raising CommandError for any errors.\n\n        If app is None, then this will validate all installed apps.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Validates the given app, raising CommandError for any errors.\n\n        If app_config is None, then this will validate all installed apps.", "file": "django_core_management_base.py", "function": "BaseCommand.validate", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef bezier_curve(y0, x0, y1, x1, y2, x2, weight, shape=None):\n    \n    \n    return _bezier_curve(y0, x0, y1, x1, y2, x2, weight, shape)```\n\nUpdated code:\n```python\ndef bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape=None):\n    \n    \n    return _bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape)\n```\n\nOriginal documentation:\n```python\nGenerate Bezier curve coordinates.\n\n    Parameters\n    ----------\n    y0, x0 : int\n        Coordinates of the first control point.\n    y1, x1 : int\n        Coordinates of the middle control point.\n    y2, x2 : int\n        Coordinates of the last control point.\n    weight : double\n        Middle control point weight, it describes the line tension.\n    shape : tuple, optional\n        Image shape which is used to determine the maximum extent of output\n        pixel coordinates. This is useful for curves which exceed the image\n        size. If None, the full extent of the curve are used.\n\n    Returns\n    -------\n    rr, cc : (N,) ndarray of int\n        Indices of pixels that belong to the Bezier curve.\n        May be used to directly index into an array, e.g.\n        ``img[rr, cc] = 1``.\n\n    Notes\n    -----\n    The algorithm is the rational quadratic algorithm presented in\n    reference [1]_.\n\n    References\n    ----------\n    .. [1] A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\n           http://members.chello.at/easyfilter/Bresenham.pdf\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.draw import bezier_curve\n    >>> img = np.zeros((10, 10), dtype=np.uint8)\n    >>> rr, cc = bezier_curve(1, 5, 5, -2, 8, 8, 2)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate Bezier curve coordinates.\n\n    Parameters\n    ----------\n    r0, c0 : int\n        Coordinates of the first control point.\n    r1, c1 : int\n        Coordinates of the middle control point.\n    r2, c2 : int\n        Coordinates of the last control point.\n    weight : double\n        Middle control point weight, it describes the line tension.\n    shape : tuple, optional\n        Image shape which is used to determine the maximum extent of output\n        pixel coordinates. This is useful for curves that exceed the image\n        size. If None, the full extent of the curve is used.\n\n    Returns\n    -------\n    rr, cc : (N,) ndarray of int\n        Indices of pixels that belong to the Bezier curve.\n        May be used to directly index into an array, e.g.\n        ``img[rr, cc] = 1``.\n\n    Notes\n    -----\n    The algorithm is the rational quadratic algorithm presented in\n    reference [1]_.\n\n    References\n    ----------\n    .. [1] A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\n           http://members.chello.at/easyfilter/Bresenham.pdf\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.draw import bezier_curve\n    >>> img = np.zeros((10, 10), dtype=np.uint8)\n    >>> rr, cc = bezier_curve(1, 5, 5, -2, 8, 8, 2)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)", "file": "skimage_draw_draw.py", "function": "bezier_curve", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 12, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_new_pos_enc(self, pos_enc: torch.Tensor) -> torch.Tensor:\n        \n\n        \n        pos_enc_positive = pos_enc[:, : pos_enc.size(1) // 2 + 1, :]\n        pos_enc_negative = pos_enc[:, pos_enc.size(1) // 2 :, :]\n\n        if self.padding != 0:\n            pos_enc_positive = pos_enc_positive[:, : -self.padding, :]\n            pos_enc_negative = pos_enc_negative[:, : -self.padding, :]\n\n        pos_enc_positive = pos_enc_positive[:, :: self.stride, :]\n        pos_enc_negative = pos_enc_negative[:, :: self.stride, :]\n\n        pos_enc = torch.cat([pos_enc_positive, pos_enc_negative[:, 1:, :]], dim=1)\n\n        return self.out_pos(pos_enc)```\n\nUpdated code:\n```python\ndef create_new_pos_enc(self, pos_enc: torch.Tensor) -> torch.Tensor:\n        \n\n        \n        pos_enc_positive = pos_enc[:, : pos_enc.size(1) // 2 + 1, :]\n        pos_enc_negative = pos_enc[:, pos_enc.size(1) // 2 :, :]\n\n        if self.padding != 0:\n            pos_enc_positive = pos_enc_positive[:, : -self.padding, :]\n            pos_enc_negative = pos_enc_negative[:, : -self.padding, :]\n\n        pos_enc_positive = pos_enc_positive[:, :: self.stride, :]\n        pos_enc_negative = pos_enc_negative[:, :: self.stride, :]\n\n        pos_enc = torch.cat([pos_enc_positive, pos_enc_negative[:, 1:, :]], dim=1)\n\n        return pos_enc\n```\n\nOriginal documentation:\n```python\nCreate new positional embedding vector.\n\n        Args:\n            pos_enc: Input sequences positional embedding.\n                     (B, 2 * (T - 1), D_att)\n\n        Returns:\n            pos_enc: Output sequences positional embedding.\n                     (B, 2 * (sub(T) - 1), D_att)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create new positional embedding vector.\n\n        Args:\n            pos_enc: Input sequences positional embedding.\n                     (B, 2 * (T - 1), D_in)\n\n        Returns:\n            pos_enc: Output sequences positional embedding.\n                     (B, 2 * (sub(T) - 1), D_in)", "file": "espnet2_asr_transducer_encoder_blocks_conv1d.py", "function": "Conv1d.create_new_pos_enc", "project": "espnet", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef annotation_select(self):\n        \n        \n        \n        if self._annotation_select_cache is not None:\n            return self._annotation_select_cache\n        elif not self._annotations:\n            return {}\n        elif self.annotation_select_mask is not None:\n            self._annotation_select_cache = OrderedDict(\n                (k, v) for k, v in self.annotations.items()\n                if k in self.annotation_select_mask\n            )\n            return self._annotation_select_cache\n        else:\n            return self.annotations```\n\nUpdated code:\n```python\ndef annotation_select(self):\n        \n        \n        \n        if self._annotation_select_cache is not None:\n            return self._annotation_select_cache\n        elif not self.annotations:\n            return {}\n        elif self.annotation_select_mask is not None:\n            self._annotation_select_cache = {\n                k: v for k, v in self.annotations.items()\n                if k in self.annotation_select_mask\n            }\n            return self._annotation_select_cache\n        else:\n            return self.annotations\n```\n\nOriginal documentation:\n```python\nReturn the OrderedDict of aggregate columns that are not masked and\n        should be used in the SELECT clause. Cache this result for performance.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the dictionary of aggregate columns that are not masked and\n        should be used in the SELECT clause. Cache this result for performance.", "file": "django_db_models_sql_query.py", "function": "Query.annotation_select", "project": "django", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _from_folder(self):\n         \n        \n        logger.debug(\"Loading images from folder: '%s'\", self.location)\n        for idx, filename in enumerate(self.file_list):\n            if idx in self._skip_list:\n                logger.trace(\"Skipping face %s due to skip list\")\n                continue\n            image_read = read_image(filename, raise_error=False, with_hash=True)\n            retval = filename, *image_read\n            if retval[1] is None:\n                logger.warning(\"Face not loaded: '%s'\", filename)\n                continue\n            yield retval```\n\nUpdated code:\n```python\ndef _from_folder(self):\n         \n        \n        logger.debug(\"Loading images from folder: '%s'\", self.location)\n        for idx, filename in enumerate(self.file_list):\n            if idx in self._skip_list:\n                logger.trace(\"Skipping face %s due to skip list\")\n                continue\n            image_read = read_image(filename, raise_error=False, with_metadata=True)\n            retval = filename, *image_read\n            if retval[1] is None:\n                logger.warning(\"Face not loaded: '%s'\", filename)\n                continue\n            yield retval\n```\n\nOriginal documentation:\n```python\nGenerator for loading images from a folder\n        Faces will only ever be loaded from a folder, so this is the only function requiring\n        an override\n\n        Yields\n        ------\n        filename: str\n            The filename of the loaded image.\n        image: numpy.ndarray\n            The loaded image.\n        sha1_hash: str\n            The sha1 hash of the loaded image.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generator for loading images from a folder\n        Faces will only ever be loaded from a folder, so this is the only function requiring\n        an override\n\n        Yields\n        ------\n        filename: str\n            The filename of the loaded image.\n        image: numpy.ndarray\n            The loaded image.\n        metadata: dict\n            The Faceswap metadata associated with the loaded image.", "file": "lib_image.py", "function": "FacesLoader._from_folder", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef xyxy2xywh(x):\n    \n    \n    \n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n    y[..., 2] = x[..., 2] - x[..., 0]  # width\n    y[..., 3] = x[..., 3] - x[..., 1]  # height\n    return y```\n\nUpdated code:\n```python\ndef xyxy2xywh(x):\n    \n    \n    \n    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n    y[..., 2] = x[..., 2] - x[..., 0]  # width\n    y[..., 3] = x[..., 3] - x[..., 1]  # height\n    return y\n```\n\nOriginal documentation:\n```python\nConvert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.\n\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n       y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.", "file": "ultralytics_utils_ops.py", "function": "xyxy2xywh", "project": "ultralytics", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_if_feature_list_matches_strategy(self, dataframe: DataFrame,\n                                               dh: FreqaiDataKitchen) -> None:\n        \n        \n        \n        strategy_provided_features = dh.find_features(dataframe)\n        if 'training_features_list_raw' in dh.data:\n            feature_list = dh.data['training_features_list_raw']\n        else:\n            feature_list = dh.training_features_list\n        if strategy_provided_features != feature_list:\n            raise OperationalException(\"Trying to access pretrained model with `identifier` \"\n                                       \"but found different features furnished by current strategy.\"\n                                       \"Change `identifer` to train from scratch, or ensure the\"\n                                       \"strategy is furnishing the same features as the pretrained\"\n                                       \"model\")```\n\nUpdated code:\n```python\ndef check_if_feature_list_matches_strategy(self, dataframe: DataFrame,\n                                               dk: FreqaiDataKitchen) -> None:\n        \n        \n        \n        dk.find_features(dataframe)\n        if 'training_features_list_raw' in dk.data:\n            feature_list = dk.data['training_features_list_raw']\n        else:\n            feature_list = dk.training_features_list\n        if dk.training_features_list != feature_list:\n            raise OperationalException(\"Trying to access pretrained model with `identifier` \"\n                                       \"but found different features furnished by current strategy.\"\n                                       \"Change `identifer` to train from scratch, or ensure the\"\n                                       \"strategy is furnishing the same features as the pretrained\"\n                                       \"model\")\n```\n\nOriginal documentation:\n```python\nEnsure user is passing the proper feature set if they are reusing an `identifier` pointing\n        to a folder holding existing models.\n        :params:\n        dataframe: DataFrame = strategy provided dataframe\n        dh: FreqaiDataKitchen = non-persistent data container/analyzer for current coin/bot loop\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure user is passing the proper feature set if they are reusing an `identifier` pointing\n        to a folder holding existing models.\n        :params:\n        dataframe: DataFrame = strategy provided dataframe\n        dk: FreqaiDataKitchen = non-persistent data container/analyzer for current coin/bot loop", "file": "freqtrade_freqai_freqai_interface.py", "function": "IFreqaiModel.check_if_feature_list_matches_strategy", "project": "freqtrade", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_code_object(x):\n    \n    if hasattr(x, '__func__'): # Method\n        x = x.__func__\n    if hasattr(x, '__code__'): # Function\n        x = x.__code__\n    if hasattr(x, 'gi_code'):  # Generator\n        x = x.gi_code\n    if isinstance(x, str):     # Source code\n        x = _try_compile(x, \"<disassembly>\")\n    if hasattr(x, 'co_code'):  # Code object\n        return x\n    raise TypeError(\"don't know how to disassemble %s objects\" %\n                    type(x).__name__)```\n\nUpdated code:\n```python\ndef _get_code_object(x):\n    \n    # Extract functions from methods.\n    if hasattr(x, '__func__'):\n        x = x.__func__\n    # Extract compiled code objects from...\n    if hasattr(x, '__code__'):  # ...a function, or\n        x = x.__code__\n    elif hasattr(x, 'gi_code'):  #...a generator object, or\n        x = x.gi_code\n    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or\n        x = x.ag_code\n    elif hasattr(x, 'cr_code'):  #...a coroutine.\n        x = x.cr_code\n    # Handle source code.\n    if isinstance(x, str):\n        x = _try_compile(x, \"<disassembly>\")\n    # By now, if we don't have a code object, we can't disassemble x.\n    if hasattr(x, 'co_code'):\n        return x\n    raise TypeError(\"don't know how to disassemble %s objects\" %\n                    type(x).__name__)\n```\n\nOriginal documentation:\n```python\nHelper to handle methods, functions, generators, strings and raw code objects\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Helper to handle methods, compiled or raw code objects, and strings.", "file": "Lib_dis.py", "function": "_get_code_object", "project": "cpython", "change_stats": {"code_changes": 20, "doc_changes": 4, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(\n        self,\n        past_values: torch.Tensor,\n        future_values: torch.Tensor = None,\n        output_hidden_states: Optional[bool] = False,\n        return_loss: bool = True,\n        return_dict: Optional[bool] = None,\n    ) -> PatchTSMixerForRegressionOutput:\n        \n\n        \n\n        if self.loss == \"mse\":\n            loss = nn.MSELoss(reduction=\"mean\")\n        elif self.loss == \"nll\":\n            loss = nll\n        else:\n            raise ValueError(\"Invalid loss function: Allowed values: mse and nll\")\n\n        return_dict = return_dict if return_dict is not None else self.use_return_dict\n        model_output = self.model(\n            past_values,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )  # model_output: [batch_size x nvars x num_patch x d_model]\n        if isinstance(model_output, tuple):\n            model_output = PatchTSMixerModelOutput(*model_output)\n\n        if self.inject_scale is not None:\n            model_output.last_hidden_state = self.inject_scale(\n                model_output.last_hidden_state,\n                loc=model_output.loc,\n                scale=model_output.scale,\n            )  # x: [batch_size x nvars x num_patch x d_model]\n\n        y_hat = self.head(model_output.last_hidden_state)  # [batch_size x num_targets]\n\n        if future_values is not None and return_loss is True:\n            if self.distribution_output:\n                if self.distribution_output == \"negative_binomial\" and torch.any(future_values < 0):\n                    raise Exception(\"future_values cannot be negative for negative_binomial distribution.\")\n                distribution = self.distribution_output.distribution(y_hat)\n                loss_val = loss(distribution, future_values)\n                # take average of the loss\n                loss_val = weighted_average(loss_val)\n            else:\n                loss_val = loss(y_hat, future_values)\n        else:\n            loss_val = None\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    loss_val,\n                    y_hat,\n                    model_output.last_hidden_state,\n                    model_output.hidden_states,\n                ]\n            )\n\n        return PatchTSMixerForRegressionOutput(\n            loss=loss_val,\n            prediction_outputs=y_hat,  # tensor [batch_size x num_targets]\n            last_hidden_state=model_output.last_hidden_state,  # [batch_size x nvars x num_patch x d_model]\n            hidden_states=model_output.hidden_states,\n        )```\n\nUpdated code:\n```python\ndef forward(\n        self,\n        past_values: torch.Tensor,\n        target_values: torch.Tensor = None,\n        output_hidden_states: Optional[bool] = False,\n        return_loss: bool = True,\n        return_dict: Optional[bool] = None,\n    ) -> PatchTSMixerForRegressionOutput:\n        \n\n        \n\n        if self.loss == \"mse\":\n            loss = nn.MSELoss(reduction=\"mean\")\n        elif self.loss == \"nll\":\n            loss = nll\n        else:\n            raise ValueError(\"Invalid loss function: Allowed values: mse and nll\")\n\n        return_dict = return_dict if return_dict is not None else self.use_return_dict\n        model_output = self.model(\n            past_values,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )  # model_output: [batch_size x nvars x num_patch x d_model]\n        if isinstance(model_output, tuple):\n            model_output = PatchTSMixerModelOutput(*model_output)\n\n        if self.inject_scale is not None:\n            model_output.last_hidden_state = self.inject_scale(\n                model_output.last_hidden_state,\n                loc=model_output.loc,\n                scale=model_output.scale,\n            )  # x: [batch_size x nvars x num_patch x d_model]\n\n        y_hat = self.head(model_output.last_hidden_state)  # [batch_size x num_targets]\n\n        if target_values is not None and return_loss is True:\n            if self.distribution_output:\n                if self.distribution_output == \"negative_binomial\" and torch.any(target_values < 0):\n                    raise Exception(\"target_values cannot be negative for negative_binomial distribution.\")\n                distribution = self.distribution_output.distribution(y_hat)\n                # y_hat should be a 2-tuple, each with dimension [bs, num_targets]\n                y_hat = tuple([item.view(-1, self.config.num_targets) for item in y_hat])\n                loss_val = loss(distribution, target_values)\n                # take average of the loss\n                loss_val = weighted_average(loss_val)\n            else:\n                loss_val = loss(y_hat, target_values)\n        else:\n            loss_val = None\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    loss_val,\n                    y_hat,\n                    model_output.last_hidden_state,\n                    model_output.hidden_states,\n                ]\n            )\n\n        return PatchTSMixerForRegressionOutput(\n            loss=loss_val,\n            regression_outputs=y_hat,  # tensor [batch_size x num_targets]\n            last_hidden_state=model_output.last_hidden_state,  # [batch_size x nvars x num_patch x d_model]\n            hidden_states=model_output.hidden_states,\n        )\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        future_values (`torch.FloatTensor` of shape `(batch_size, target_len, num_input_channels)` for forecasting,\n            `(batch_size, num_targets)` for regression, or `(batch_size,)` for classification, *optional*): Target\n            values of the time series, that serve as labels for the model. The `future_values` is what the\n            Transformer needs during training to learn to output, given the `past_values`. Note that, this is NOT\n            required for a pretraining task.\n\n            For a forecasting task, the shape is be `(batch_size, target_len, num_input_channels)`. Even if we want\n            to forecast only specific channels by setting the indices in `prediction_channel_indices` parameter,\n            pass the target data with all channels, as channel Filtering for both prediction and target will be\n            manually applied before the loss computation.\n\n            For a classification task, it has a shape of `(batch_size,)`.\n\n            For a regression task, it has a shape of `(batch_size, num_targets)`.\n        return_loss (`bool`, *optional*):\n            Whether to return the loss in the `forward` call.\n\n        Returns:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        target_values (`torch.FloatTensor` of shape `(batch_size, target_len, num_input_channels)` for forecasting,\n            `(batch_size, num_targets)` for regression, or `(batch_size,)` for classification, *optional*): Target\n            values of the time series, that serve as labels for the model. The `target_values` is what the\n            Transformer needs during training to learn to output, given the `past_values`. Note that, this is NOT\n            required for a pretraining task.\n\n            For a forecasting task, the shape is be `(batch_size, target_len, num_input_channels)`. Even if we want\n            to forecast only specific channels by setting the indices in `prediction_channel_indices` parameter,\n            pass the target data with all channels, as channel Filtering for both prediction and target will be\n            manually applied before the loss computation.\n\n            For a classification task, it has a shape of `(batch_size,)`.\n\n            For a regression task, it has a shape of `(batch_size, num_targets)`.\n        return_loss (`bool`, *optional*):\n            Whether to return the loss in the `forward` call.\n\n        Returns:", "file": "src_transformers_models_patchtsmixer_modeling_patchtsmixer.py", "function": "PatchTSMixerForRegression.forward", "project": "transformers", "change_stats": {"code_changes": 18, "doc_changes": 6, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _config_axis(self):\n        \n        ax = self.ax\n\n        if self.orientation == 'vertical':\n            long_axis, short_axis = ax.yaxis, ax.xaxis\n            if mpl.rcParams['ytick.minor.visible']:\n                self.minorticks_on()\n        else:\n            long_axis, short_axis = ax.xaxis, ax.yaxis\n            if mpl.rcParams['xtick.minor.visible']:\n                self.minorticks_on()\n\n        long_axis.set_label_position(self.ticklocation)\n        long_axis.set_ticks_position(self.ticklocation)\n        short_axis.set_ticks([])\n        short_axis.set_ticks([], minor=True)\n        self._set_label()```\n\nUpdated code:\n```python\ndef _config_axis(self):\n        \n        ax = self.ax\n        if self.orientation == 'vertical':\n            long_axis, short_axis = ax.yaxis, ax.xaxis\n            if mpl.rcParams['ytick.minor.visible']:\n                self.minorticks_on()\n        else:\n            long_axis, short_axis = ax.xaxis, ax.yaxis\n            if mpl.rcParams['xtick.minor.visible']:\n                self.minorticks_on()\n        long_axis.set(label_position=self.ticklocation,\n                      ticks_position=self.ticklocation)\n        short_axis.set_ticks([])\n        short_axis.set_ticks([], minor=True)\n        self._set_label()\n```\n\nOriginal documentation:\n```python\nConfigure the ticks, ticklabels and label of the underlying Axes.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set up long and short axis.", "file": "lib_matplotlib_colorbar.py", "function": "ColorbarBase._config_axis", "project": "matplotlib", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef about():\n    \n    return render('about.html', help=user_help.HELP)```\n\nUpdated code:\n```python\ndef about():\n    \n    return redirect(url_for('help_page', pagename='about'))\n```\n\nOriginal documentation:\n```python\nRender about page\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Redirect to about page", "file": "searx_webapp.py", "function": "about", "project": "searxng", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _is_relevant_marker(self, marker: BaseMarker) -> bool:\n        \n        \n        \n        return (\n            not marker.is_empty()\n            and self._python_constraint.allows_any(\n                get_python_constraint_from_marker(marker)\n            )\n            and (not self._env or marker.validate(self._env.marker_env))\n        )```\n\nUpdated code:\n```python\ndef _is_relevant_marker(\n        self, marker: BaseMarker, active_extras: Collection[NormalizedName] | None\n    ) -> bool:\n        \n        \n        \n        return (\n            not marker.is_empty()\n            and self._python_constraint.allows_any(\n                get_python_constraint_from_marker(marker)\n            )\n            and (active_extras is None or marker.validate({\"extra\": active_extras}))\n            and (not self._env or marker.validate(self._env.marker_env))\n        )\n```\n\nOriginal documentation:\n```python\nA marker is relevant if\n        - it is not empty\n        - allowed by the project's python constraint\n        - allowed by the environment (only during installation)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A marker is relevant if\n        - it is not empty\n        - allowed by the project's python constraint\n        - allowed by active extras of the dependency (not relevant for root package)\n        - allowed by the environment (only during installation)", "file": "src_poetry_puzzle_provider.py", "function": "Provider._is_relevant_marker", "project": "poetry", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef async_store_trace(hass, trace, stored_traces):\n    \n    key = trace.key\n    if key[1]:\n        traces = hass.data[DATA_TRACE]\n        if key not in traces:\n            traces[key] = LimitedSizeDict(size_limit=stored_traces)\n        else:\n            traces[key].size_limit = stored_traces\n        traces[key][trace.run_id] = trace```\n\nUpdated code:\n```python\ndef async_store_trace(hass, trace, stored_traces):\n    \n    key = trace.key\n    if key:\n        traces = hass.data[DATA_TRACE]\n        if key not in traces:\n            traces[key] = LimitedSizeDict(size_limit=stored_traces)\n        else:\n            traces[key].size_limit = stored_traces\n        traces[key][trace.run_id] = trace\n```\n\nOriginal documentation:\n```python\nStore a trace if its item_id is valid.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Store a trace if its key is valid.", "file": "homeassistant_components_trace___init__.py", "function": "async_store_trace", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_text_list(list_, last_word=ugettext_lazy(u'or')):\n    \n    \n    \n    if len(list_) == 0: return u''\n    if len(list_) == 1: return force_unicode(list_[0])\n    return u'%s %s %s' % (\n        # Translators: This string is used as a separator between list elements\n        _(', ').join([force_unicode(i) for i in list_][:-1]),\n        force_unicode(last_word), force_unicode(list_[-1]))```\n\nUpdated code:\n```python\ndef get_text_list(list_, last_word=ugettext_lazy('or')):\n    \n    \n    \n    if len(list_) == 0: return ''\n    if len(list_) == 1: return force_unicode(list_[0])\n    return '%s %s %s' % (\n        # Translators: This string is used as a separator between list elements\n        _(', ').join([force_unicode(i) for i in list_][:-1]),\n        force_unicode(last_word), force_unicode(list_[-1]))\n```\n\nOriginal documentation:\n```python\n>>> get_text_list(['a', 'b', 'c', 'd'])\n    u'a, b, c or d'\n    >>> get_text_list(['a', 'b', 'c'], 'and')\n    u'a, b and c'\n    >>> get_text_list(['a', 'b'], 'and')\n    u'a and b'\n    >>> get_text_list(['a'])\n    u'a'\n    >>> get_text_list([])\n    u''\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ">>> get_text_list(['a', 'b', 'c', 'd'])\n    'a, b, c or d'\n    >>> get_text_list(['a', 'b', 'c'], 'and')\n    'a, b and c'\n    >>> get_text_list(['a', 'b'], 'and')\n    'a and b'\n    >>> get_text_list(['a'])\n    'a'\n    >>> get_text_list([])\n    ''", "file": "django_utils_text.py", "function": "get_text_list", "project": "django", "change_stats": {"code_changes": 8, "doc_changes": 12, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_deposit_address(self, code: str, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        request = {\n            'currency': currency['id'],\n        }\n        response = self.privateWalletGetDepositAddress(self.extend(request, params))\n        #\n        #    {\n        #        \"currency\": \"XRP\",\n        #        \"address\": \"rHcFoo6a9qT5NHiVn1THQRhsEGcxtYCV4d 391331007\",\n        #        \"multichain_addresses\": [\n        #            {\n        #                \"chain\": \"XRP\",\n        #                \"address\": \"rHcFoo6a9qT5NHiVn1THQRhsEGcxtYCV4d\",\n        #                \"payment_id\": \"391331007\",\n        #                \"payment_name\": \"Tag\",\n        #                \"obtain_failed\": 0\n        #            }\n        #        ]\n        #    }\n        #\n        currencyId = self.safe_string(response, 'currency')\n        code = self.safe_currency_code(currencyId)\n        addressField = self.safe_string(response, 'address')\n        tag = None\n        address = None\n        if addressField is not None:\n            if addressField.find('New address is being generated for you, please wait') >= 0:\n                raise BadResponse(self.id + ' ' + 'New address is being generated for you, please wait a few seconds and try again to get the address.')\n            if addressField.find(' ') >= 0:\n                splitted = addressField.split(' ')\n                address = splitted[0]\n                tag = splitted[1]\n            else:\n                address = addressField\n        self.check_address(address)\n        return {\n            'info': response,\n            'code': code,  # kept here for backward-compatibility, but will be removed soon\n            'currency': code,\n            'address': address,\n            'tag': tag,\n            'network': None,\n        }```\n\nUpdated code:\n```python\ndef fetch_deposit_address(self, code: str, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        rawNetwork = self.safe_string_upper(params, 'network')\n        params = self.omit(params, 'network')\n        request = {\n            'currency': currency['id'],\n        }\n        response = self.privateWalletGetDepositAddress(self.extend(request, params))\n        #\n        #    {\n        #        \"currency\": \"XRP\",\n        #        \"address\": \"rHcFoo6a9qT5NHiVn1THQRhsEGcxtYCV4d 391331007\",\n        #        \"multichain_addresses\": [\n        #            {\n        #                \"chain\": \"XRP\",\n        #                \"address\": \"rHcFoo6a9qT5NHiVn1THQRhsEGcxtYCV4d\",\n        #                \"payment_id\": \"391331007\",\n        #                \"payment_name\": \"Tag\",\n        #                \"obtain_failed\": 0\n        #            }\n        #        ]\n        #    }\n        #\n        currencyId = self.safe_string(response, 'currency')\n        code = self.safe_currency_code(currencyId)\n        networkId = self.network_code_to_id(rawNetwork, code)\n        network = None\n        tag = None\n        address = None\n        if networkId is not None:\n            addresses = self.safe_value(response, 'multichain_addresses')\n            for i in range(0, len(addresses)):\n                entry = addresses[i]\n                entryNetwork = self.safe_string(entry, 'chain')\n                if networkId == entryNetwork:\n                    obtainFailed = self.safe_integer(entry, 'obtain_failed')\n                    if obtainFailed:\n                        break\n                    address = self.safe_string(entry, 'address')\n                    tag = self.safe_string(entry, 'payment_id')\n                    network = self.network_id_to_code(networkId, code)\n                    break\n        else:\n            addressField = self.safe_string(response, 'address')\n            if addressField is not None:\n                if addressField.find('New address is being generated for you, please wait') >= 0:\n                    raise BadResponse(self.id + ' ' + 'New address is being generated for you, please wait a few seconds and try again to get the address.')\n                if addressField.find(' ') >= 0:\n                    splitted = addressField.split(' ')\n                    address = splitted[0]\n                    tag = splitted[1]\n                else:\n                    address = addressField\n        self.check_address(address)\n        return {\n            'info': response,\n            'code': code,  # kept here for backward-compatibility, but will be removed soon\n            'currency': code,\n            'address': address,\n            'tag': tag,\n            'network': network,\n        }\n```\n\nOriginal documentation:\n```python\nfetch the deposit address for a currency associated with self account\n        :see: https://www.gate.io/docs/developers/apiv4/en/#generate-currency-deposit-address\n        :param str code: unified currency code\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :returns dict: an `address structure <https://docs.ccxt.com/#/?id=address-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch the deposit address for a currency associated with self account\n        :see: https://www.gate.io/docs/developers/apiv4/en/#generate-currency-deposit-address\n        :param str code: unified currency code\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.network]: unified network code(not used directly by gate.io but used by ccxt to filter the response)\n        :returns dict: an `address structure <https://docs.ccxt.com/#/?id=address-structure>`", "file": "python_ccxt_gate.py", "function": "gate.fetch_deposit_address", "project": "ccxt", "change_stats": {"code_changes": 42, "doc_changes": 3, "total_changes": 45}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, marker_pad=0.3, numpoints=None, **kw):\n        \n        \n        \n        super().__init__(marker_pad=marker_pad, numpoints=numpoints, **kw)```\n\nUpdated code:\n```python\ndef __init__(self, marker_pad=0.3, numpoints=None, **kwargs):\n        \n        \n        \n        super().__init__(marker_pad=marker_pad, numpoints=numpoints, **kwargs)\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        marker_pad : float\n            Padding between points in legend entry.\n\n        numpoints : int\n            Number of points to show in legend entry.\n\n        Notes\n        -----\n        Any other keyword arguments are given to `HandlerNpoints`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        marker_pad : float\n            Padding between points in legend entry.\n        numpoints : int\n            Number of points to show in legend entry.\n        **kwargs\n            Keyword arguments forwarded to `.HandlerNpoints`.", "file": "lib_matplotlib_legend_handler.py", "function": "HandlerLine2D.__init__", "project": "matplotlib", "change_stats": {"code_changes": 6, "doc_changes": 9, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef bert_embeddings_post_processor(\n    sequence_embeddings: np.array,\n) -> Tuple[np.array, np.array]:\n    \n\n    sentence_embedding = sequence_embeddings[0]\n    post_processed_embedding = sequence_embeddings[1:-1]\n\n    return sentence_embedding, post_processed_embedding```\n\nUpdated code:\n```python\ndef bert_embeddings_post_processor(\n    sequence_embeddings: np.ndarray,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \n\n    sentence_embedding = sequence_embeddings[0]\n    post_processed_embedding = sequence_embeddings[1:-1]\n\n    return sentence_embedding, post_processed_embedding\n```\n\nOriginal documentation:\n```python\nPost process embeddings from BERT by removing CLS and SEP embeddings and returning CLS\n\n    token embedding as sentence representation\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Post process embeddings from BERT by removing CLS and SEP embeddings and\n    returning CLS\n\n    token embedding as sentence representation", "file": "rasa_nlu_utils_hugging_face_transformers_pre_post_processors.py", "function": "bert_embeddings_post_processor", "project": "rasa", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef setIcon(filePath, icnsPath):\n    \n    \n    \n    ref, isDirectory = Carbon.File.FSPathMakeRef(icnsPath)\n    icon = Carbon.Icn.ReadIconFile(ref)\n    del ref\n\n    #\n    # Open the resource fork of the target, to add the icon later on.\n    # For directories we use the file 'Icon\\r' inside the directory.\n    #\n\n    ref, isDirectory = Carbon.File.FSPathMakeRef(filePath)\n\n    if isDirectory:\n        # There is a problem with getting this into the pax(1) archive,\n        # just ignore directory icons for now.\n        return\n\n        tmpPath = os.path.join(filePath, \"Icon\\r\")\n        if not os.path.exists(tmpPath):\n            fp = open(tmpPath, 'w')\n            fp.close()\n\n        tmpRef, _ = Carbon.File.FSPathMakeRef(tmpPath)\n        spec = Carbon.File.FSSpec(tmpRef)\n\n    else:\n        spec = Carbon.File.FSSpec(ref)\n\n    try:\n        Carbon.Res.HCreateResFile(*spec.as_tuple())\n    except MacOS.Error:\n        pass\n\n    # Try to create the resource fork again, this will avoid problems\n    # when adding an icon to a directory. I have no idea why this helps,\n    # but without this adding the icon to a directory will fail sometimes.\n    try:\n        Carbon.Res.HCreateResFile(*spec.as_tuple())\n    except MacOS.Error:\n        pass\n\n    refNum = Carbon.Res.FSpOpenResFile(spec, fsRdWrPerm)\n\n    Carbon.Res.UseResFile(refNum)\n\n    # Check if there already is an icon, remove it if there is.\n    try:\n        h = Carbon.Res.Get1Resource('icns', kCustomIconResource)\n    except MacOS.Error:\n        pass\n\n    else:\n        h.RemoveResource()\n        del h\n\n    # Add the icon to the resource for of the target\n    res = Carbon.Res.Resource(icon)\n    res.AddResource('icns', kCustomIconResource, '')\n    res.WriteResource()\n    res.DetachResource()\n    Carbon.Res.CloseResFile(refNum)\n\n    # And now set the kHasCustomIcon property for the target. Annoyingly,\n    # python doesn't seem to have bindings for the API that is needed for\n    # this. Cop out and call SetFile\n    os.system(\"/Developer/Tools/SetFile -a C %s\"%(\n            shellQuote(filePath),))\n\n    if isDirectory:\n        os.system('/Developer/Tools/SetFile -a V %s'%(\n            shellQuote(tmpPath),\n        ))```\n\nUpdated code:\n```python\ndef setIcon(filePath, icnsPath):\n    \n    \n    \n\n    toolPath = os.path.join(os.path.dirname(__file__), \"seticon.app/Contents/MacOS/seticon\")\n    dirPath = os.path.dirname(__file__)\n    if not os.path.exists(toolPath) or os.stat(toolPath).st_mtime < os.stat(dirPath + '/seticon.m').st_mtime:\n        # NOTE: The tool is created inside an .app bundle, otherwise it won't work due\n        # to connections to the window server.\n        if not os.path.exists('seticon.app/Contents/MacOS'):\n            os.makedirs('seticon.app/Contents/MacOS')\n        runCommand(\"cc -o %s %s/seticon.m -framework Cocoa\"%(\n            shellQuote(toolPath), shellQuote(dirPath)))\n\n    runCommand(\"%s %s %s\"%(shellQuote(os.path.abspath(toolPath)), shellQuote(icnsPath),\n        shellQuote(filePath)))\n```\n\nOriginal documentation:\n```python\nSet the custom icon for the specified file or directory.\n\n    For a directory the icon data is written in a file named 'Icon\\r' inside\n    the directory. For both files and directories write the icon as an 'icns'\n    resource. Furthermore set kHasCustomIcon in the finder flags for filePath.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the custom icon for the specified file or directory.", "file": "Mac_BuildScript_build-installer.py", "function": "setIcon", "project": "cpython", "change_stats": {"code_changes": 82, "doc_changes": 6, "total_changes": 88}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef GetGPUMemoryUsageStats():\n    \n    from caffe2.python import workspace, core\n    workspace.RunOperatorOnce(\n        core.CreateOperator(\n            \"GetGPUMemoryUsage\",\n            [],\n            [\"____mem____\"],\n            device_option=core.DeviceOption(caffe2_pb2.CUDA, 0),\n        ),\n    )\n    b = workspace.FetchBlob(\"____mem____\")\n    return {\n        'total_by_gpu': b[0, :],\n        'max_by_gpu': b[1, :],\n        'total': np.sum(b[0, :]),\n        'max_total': np.sum(b[1, :])\n    }```\n\nUpdated code:\n```python\ndef GetGPUMemoryUsageStats():\n    \n    from caffe2.python import workspace, core\n    workspace.RunOperatorOnce(\n        core.CreateOperator(\n            \"GetGPUMemoryUsage\",\n            [],\n            [\"____mem____\"],\n            device_option=core.DeviceOption(workspace.GpuDeviceType, 0),\n        ),\n    )\n    b = workspace.FetchBlob(\"____mem____\")\n    return {\n        'total_by_gpu': b[0, :],\n        'max_by_gpu': b[1, :],\n        'total': np.sum(b[0, :]),\n        'max_total': np.sum(b[1, :])\n    }\n```\n\nOriginal documentation:\n```python\nGet GPU memory usage stats from CUDAContext. This requires flag\n       --caffe2_gpu_memory_tracking to be enabled\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get GPU memory usage stats from CUDAContext/HIPContext. This requires flag\n       --caffe2_gpu_memory_tracking to be enabled", "file": "caffe2_python_utils.py", "function": "GetGPUMemoryUsageStats", "project": "pytorch", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _from_pretrained(\n        cls,\n        model_id: Union[str, Path],\n        use_auth_token: Optional[Union[bool, str, None]] = None,\n        revision: Optional[Union[str, None]] = None,\n        force_download: bool = False,\n        cache_dir: Optional[str] = None,\n        file_name: Optional[str] = None,\n        provider: Optional[str] = None,\n        sess_options: Optional[\"ort.SessionOptions\"] = None,\n        **kwargs,\n    ):\n        \n        \n        \n        model_file_name = file_name if file_name is not None else ONNX_WEIGHTS_NAME\n        # load model from local directory\n        if os.path.isdir(model_id):\n            model = OnnxRuntimeModel.load_model(\n                os.path.join(model_id, model_file_name), provider=provider, sess_options=sess_options\n            )\n            kwargs[\"model_save_dir\"] = Path(model_id)\n        # load model from hub\n        else:\n            # download model\n            model_cache_path = hf_hub_download(\n                repo_id=model_id,\n                filename=model_file_name,\n                use_auth_token=use_auth_token,\n                revision=revision,\n                cache_dir=cache_dir,\n                force_download=force_download,\n            )\n            kwargs[\"model_save_dir\"] = Path(model_cache_path).parent\n            kwargs[\"latest_model_name\"] = Path(model_cache_path).name\n            model = OnnxRuntimeModel.load_model(model_cache_path, provider=provider, sess_options=sess_options)\n        return cls(model=model, **kwargs)```\n\nUpdated code:\n```python\ndef _from_pretrained(\n        cls,\n        model_id: Union[str, Path],\n        token: Optional[Union[bool, str, None]] = None,\n        revision: Optional[Union[str, None]] = None,\n        force_download: bool = False,\n        cache_dir: Optional[str] = None,\n        file_name: Optional[str] = None,\n        provider: Optional[str] = None,\n        sess_options: Optional[\"ort.SessionOptions\"] = None,\n        **kwargs,\n    ):\n        \n        \n        \n        model_file_name = file_name if file_name is not None else ONNX_WEIGHTS_NAME\n        # load model from local directory\n        if os.path.isdir(model_id):\n            model = OnnxRuntimeModel.load_model(\n                os.path.join(model_id, model_file_name), provider=provider, sess_options=sess_options\n            )\n            kwargs[\"model_save_dir\"] = Path(model_id)\n        # load model from hub\n        else:\n            # download model\n            model_cache_path = hf_hub_download(\n                repo_id=model_id,\n                filename=model_file_name,\n                token=token,\n                revision=revision,\n                cache_dir=cache_dir,\n                force_download=force_download,\n            )\n            kwargs[\"model_save_dir\"] = Path(model_cache_path).parent\n            kwargs[\"latest_model_name\"] = Path(model_cache_path).name\n            model = OnnxRuntimeModel.load_model(model_cache_path, provider=provider, sess_options=sess_options)\n        return cls(model=model, **kwargs)\n```\n\nOriginal documentation:\n```python\nLoad a model from a directory or the HF Hub.\n\n        Arguments:\n            model_id (`str` or `Path`):\n                Directory from which to load\n            use_auth_token (`str` or `bool`):\n                Is needed to load models from a private or gated repository\n            revision (`str`):\n                Revision is the specific model version to use. It can be a branch name, a tag name, or a commit id\n            cache_dir (`Union[str, Path]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            file_name(`str`):\n                Overwrites the default model file name from `\"model.onnx\"` to `file_name`. This allows you to load\n                different model files from the same repository or directory.\n            provider(`str`):\n                The ONNX runtime provider, e.g. `CPUExecutionProvider` or `CUDAExecutionProvider`.\n            kwargs (`Dict`, *optional*):\n                kwargs will be passed to the model during initialization\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a model from a directory or the HF Hub.\n\n        Arguments:\n            model_id (`str` or `Path`):\n                Directory from which to load\n            token (`str` or `bool`):\n                Is needed to load models from a private or gated repository\n            revision (`str`):\n                Revision is the specific model version to use. It can be a branch name, a tag name, or a commit id\n            cache_dir (`Union[str, Path]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            file_name(`str`):\n                Overwrites the default model file name from `\"model.onnx\"` to `file_name`. This allows you to load\n                different model files from the same repository or directory.\n            provider(`str`):\n                The ONNX runtime provider, e.g. `CPUExecutionProvider` or `CUDAExecutionProvider`.\n            kwargs (`Dict`, *optional*):\n                kwargs will be passed to the model during initialization", "file": "src_diffusers_pipelines_onnx_utils.py", "function": "OnnxRuntimeModel._from_pretrained", "project": "diffusers", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __reduce__(self) -> NoReturn:\n        \n        raise pickle.PicklingError(\"Bot objects cannot be pickled!\")```\n\nUpdated code:\n```python\ndef __reduce__(self) -> NoReturn:\n        \n        \n        raise pickle.PicklingError(\"Bot objects cannot be pickled!\")\n```\n\nOriginal documentation:\n```python\nCalled by pickle.dumps(). Serializing bots is unadvisable, so we forbid pickling.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Customizes how :func:`copy.deepcopy` processes objects of this type. Bots can not\n        be pickled and this method will always raise an exception.\n\n        .. versionadded:: 20.0\n\n        Raises:\n            :exc:`pickle.PicklingError`", "file": "telegram__bot.py", "function": "Bot.__reduce__", "project": "python-telegram-bot", "change_stats": {"code_changes": 3, "doc_changes": 10, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_date_list(self, queryset, date_type=None, ordering='ASC'):\n        \n        \n        \n        date_field = self.get_date_field()\n        allow_empty = self.get_allow_empty()\n        if date_type is None:\n            date_type = self.get_date_list_period()\n\n        date_list = queryset.dates(date_field, date_type, ordering)\n        if date_list is not None and not date_list and not allow_empty:\n            name = force_text(queryset.model._meta.verbose_name_plural)\n            raise Http404(_(\"No %(verbose_name_plural)s available\") %\n                          {'verbose_name_plural': name})\n\n        return date_list```\n\nUpdated code:\n```python\ndef get_date_list(self, queryset, date_type=None, ordering='ASC'):\n        \n        \n        \n        date_field = self.get_date_field()\n        allow_empty = self.get_allow_empty()\n        if date_type is None:\n            date_type = self.get_date_list_period()\n\n        if self.uses_datetime_field:\n            date_list = queryset.datetimes(date_field, date_type, ordering)\n        else:\n            date_list = queryset.dates(date_field, date_type, ordering)\n        if date_list is not None and not date_list and not allow_empty:\n            name = force_text(queryset.model._meta.verbose_name_plural)\n            raise Http404(_(\"No %(verbose_name_plural)s available\") %\n                          {'verbose_name_plural': name})\n\n        return date_list\n```\n\nOriginal documentation:\n```python\nGet a date list by calling `queryset.dates()`, checking along the way\n        for empty lists that aren't allowed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get a date list by calling `queryset.dates/datetimes()`, checking\n        along the way for empty lists that aren't allowed.", "file": "django_views_generic_dates.py", "function": "BaseDateListView.get_date_list", "project": "django", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef read_frame(sql, con, index_col=None, coerce_float=True):\n    \n    \n    \n    cur = execute(sql, con)\n    rows = _safe_fetch(cur)\n    columns = [col_desc[0] for col_desc in cur.description]\n\n    cur.close()\n    con.commit()\n\n    result = DataFrame.from_records(rows, columns=columns,\n                                    coerce_float=coerce_float)\n\n    if index_col is not None:\n        result = result.set_index(index_col)\n\n    return result```\n\nUpdated code:\n```python\ndef read_frame(sql, con, index_col=None, coerce_float=True, params=None):\n    \n    \n    \n    cur = execute(sql, con, params=params)\n    rows = _safe_fetch(cur)\n    columns = [col_desc[0] for col_desc in cur.description]\n\n    cur.close()\n    con.commit()\n\n    result = DataFrame.from_records(rows, columns=columns,\n                                    coerce_float=coerce_float)\n\n    if index_col is not None:\n        result = result.set_index(index_col)\n\n    return result\n```\n\nOriginal documentation:\n```python\nReturns a DataFrame corresponding to the result set of the query\n    string.\n\n    Optionally provide an index_col parameter to use one of the\n    columns as the index. Otherwise will be 0 to len(results) - 1.\n\n    Parameters\n    ----------\n    sql: string\n        SQL query to be executed\n    con: DB connection object, optional\n    index_col: string, optional\n        column name to use for the returned DataFrame object.\n    coerce_float : boolean, default True\n        Attempt to convert values to non-string, non-numeric objects (like\n        decimal.Decimal) to floating point, useful for SQL result sets\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a DataFrame corresponding to the result set of the query\n    string.\n\n    Optionally provide an index_col parameter to use one of the\n    columns as the index. Otherwise will be 0 to len(results) - 1.\n\n    Parameters\n    ----------\n    sql: string\n        SQL query to be executed\n    con: DB connection object, optional\n    index_col: string, optional\n        column name to use for the returned DataFrame object.\n    params: list or tuple, optional\n        List of parameters to pass to execute method.", "file": "pandas_io_sql.py", "function": "read_frame", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 7, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _is_potential_multi_index(\n    columns, index_col: Optional[Union[bool, Sequence[int]]] = None\n):\n    \n    \n    \n    if index_col is None or isinstance(index_col, bool):\n        index_col = []\n\n    return (\n        len(columns)\n        and not isinstance(columns, MultiIndex)\n        and all(isinstance(c, tuple) for c in columns if c not in list(index_col))\n    )```\n\nUpdated code:\n```python\ndef _is_potential_multi_index(\n    columns, index_col: Optional[Union[bool, Sequence[int]]] = None\n) -> bool:\n    \n    \n    \n    if index_col is None or isinstance(index_col, bool):\n        index_col = []\n\n    return bool(\n        len(columns)\n        and not isinstance(columns, MultiIndex)\n        and all(isinstance(c, tuple) for c in columns if c not in list(index_col))\n    )\n```\n\nOriginal documentation:\n```python\nCheck whether or not the `columns` parameter\n    could be converted into a MultiIndex.\n\n    Parameters\n    ----------\n    columns : array-like\n        Object which may or may not be convertible into a MultiIndex\n    index_col : None, bool or list, optional\n        Column or columns to use as the (possibly hierarchical) index\n\n    Returns\n    -------\n    boolean : Whether or not columns could become a MultiIndex\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check whether or not the `columns` parameter\n    could be converted into a MultiIndex.\n\n    Parameters\n    ----------\n    columns : array-like\n        Object which may or may not be convertible into a MultiIndex\n    index_col : None, bool or list, optional\n        Column or columns to use as the (possibly hierarchical) index\n\n    Returns\n    -------\n    bool : Whether or not columns could become a MultiIndex", "file": "pandas_io_parsers_base_parser.py", "function": "_is_potential_multi_index", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef normalize_function(\n        target: Callable, args: Tuple[Any], kwargs : Optional[Dict[str, Any]] = None, arg_types : Optional[Tuple[Any]] = None,\n        kwarg_types : Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:\n    \n    \n    \n    if kwargs is None:\n        kwargs = {}\n    new_kwargs = None\n    if target in boolean_dispatched or target.__module__ in ['torch.nn.functional', 'torch.functional']:\n        target_for_analysis = target\n        if target in boolean_dispatched:\n            # HACK: `boolean_dispatch` as used in `torch.nn.functional` makes it so that we have\n            # a 2-way dispatch based on a boolean value. Here we check that the `true` and `false`\n            # branches of the dispatch have exactly the same signature. If they do, use the `true`\n            # branch signature for analysis. Otherwise, leave this un-normalized\n            assert not isinstance(target, str)\n            dispatched = boolean_dispatched[target]\n            if_true, if_false = dispatched['if_true'], dispatched['if_false']\n            if inspect.signature(if_true).parameters != inspect.signature(if_false).parameters:\n                return None\n            target_for_analysis = if_true\n\n        assert callable(target_for_analysis)\n        sig = inspect.signature(inspect.unwrap(target_for_analysis))\n        new_kwargs = _args_kwargs_to_normalized_kwargs(sig, args, kwargs)\n    else:\n        assert callable(target)\n        torch_op_schemas = get_signature_for_torch_op(target)\n        matched_schemas = []\n        if torch_op_schemas:\n            # Iterate through all of the schema until we find one that matches\n            # If one matches, populate `new_kwargs` with the combined args/kwargs\n            # values. If none matches, `new_kwargs` will be None\n            for candidate_signature in torch_op_schemas:\n                try:\n                    candidate_signature.bind(*args, **kwargs)\n                    matched_schemas.append(candidate_signature)\n                except TypeError as e:\n                    continue\n\n            if len(matched_schemas) == 0:\n                # Did not match any schema. Cannot normalize\n                pass\n            elif len(matched_schemas) == 1:\n                # Matched exactly one schema, unambiguous\n                new_kwargs = _args_kwargs_to_normalized_kwargs(matched_schemas[0], args, kwargs)\n            else:\n                if arg_types is not None or kwarg_types is not None:\n                    arg_types = arg_types if arg_types else cast(Tuple[Any], ())\n                    kwarg_types = kwarg_types if kwarg_types else {}\n                    for candidate_signature in torch_op_schemas:\n                        sig_matches = True\n                        try:\n                            bound_types = candidate_signature.bind(*arg_types, **kwarg_types)\n                            for arg_name, arg_type in bound_types.arguments.items():\n                                param = candidate_signature.parameters[arg_name]\n                                sig_matches = sig_matches and type_matches(param.annotation, arg_type)\n                        except TypeError as e:\n                            sig_matches = False\n                        if sig_matches:\n                            new_kwargs = _args_kwargs_to_normalized_kwargs(candidate_signature, args, kwargs)\n                            break\n                else:\n                    # Matched more than one schema. In this situation, the caller must provide the types of\n                    # the arguments of the overload they expect.\n                    schema_printouts = '\\n'.join(str(schema) for schema in matched_schemas)\n                    raise RuntimeError(f'Tried to normalize arguments to {torch.typename(target)} but '\n                                       f'the schema match was ambiguous! Please provide argument types to '\n                                       f'the normalize_arguments() call. Available schemas:\\n{schema_printouts}')\n\n    return new_kwargs```\n\nUpdated code:\n```python\ndef normalize_function(\n        target: Callable, args: Tuple[Any], kwargs : Optional[Dict[str, Any]] = None, arg_types : Optional[Tuple[Any]] = None,\n        kwarg_types : Optional[Dict[str, Any]] = None,\n        normalize_to_only_use_kwargs : bool = False) -> Optional[ArgsKwargsPair]:\n    \n    \n    \n    if kwargs is None:\n        kwargs = {}\n    new_args_and_kwargs = None\n    if target in boolean_dispatched or target.__module__ in ['torch.nn.functional', 'torch.functional']:\n        target_for_analysis = target\n        if target in boolean_dispatched:\n            # HACK: `boolean_dispatch` as used in `torch.nn.functional` makes it so that we have\n            # a 2-way dispatch based on a boolean value. Here we check that the `true` and `false`\n            # branches of the dispatch have exactly the same signature. If they do, use the `true`\n            # branch signature for analysis. Otherwise, leave this un-normalized\n            assert not isinstance(target, str)\n            dispatched = boolean_dispatched[target]\n            if_true, if_false = dispatched['if_true'], dispatched['if_false']\n            if inspect.signature(if_true).parameters != inspect.signature(if_false).parameters:\n                return None\n            target_for_analysis = if_true\n\n        assert callable(target_for_analysis)\n        sig = inspect.signature(inspect.unwrap(target_for_analysis))\n        new_args_and_kwargs = _args_kwargs_to_normalized_args_kwargs(sig, args, kwargs, normalize_to_only_use_kwargs)\n    else:\n        assert callable(target)\n        torch_op_schemas = get_signature_for_torch_op(target)\n        matched_schemas = []\n        if torch_op_schemas:\n            # Iterate through all of the schema until we find one that matches\n            # If one matches, populate `new_args_and_kwargs` with the new args/kwargs\n            # values. If none matches, `new_args_and_kwargs` will be None\n            for candidate_signature in torch_op_schemas:\n                try:\n                    candidate_signature.bind(*args, **kwargs)\n                    matched_schemas.append(candidate_signature)\n                except TypeError as e:\n                    continue\n\n            if len(matched_schemas) == 0:\n                # Did not match any schema. Cannot normalize\n                pass\n            elif len(matched_schemas) == 1:\n                # Matched exactly one schema, unambiguous\n                new_args_and_kwargs = _args_kwargs_to_normalized_args_kwargs(matched_schemas[0], args, kwargs,\n                                                                             normalize_to_only_use_kwargs)\n            else:\n                if arg_types is not None or kwarg_types is not None:\n                    arg_types = arg_types if arg_types else cast(Tuple[Any], ())\n                    kwarg_types = kwarg_types if kwarg_types else {}\n                    for candidate_signature in torch_op_schemas:\n                        sig_matches = True\n                        try:\n                            bound_types = candidate_signature.bind(*arg_types, **kwarg_types)\n                            for arg_name, arg_type in bound_types.arguments.items():\n                                param = candidate_signature.parameters[arg_name]\n                                sig_matches = sig_matches and type_matches(param.annotation, arg_type)\n                        except TypeError as e:\n                            sig_matches = False\n                        if sig_matches:\n                            new_args_and_kwargs = _args_kwargs_to_normalized_args_kwargs(candidate_signature, args, kwargs,\n                                                                                         normalize_to_only_use_kwargs)\n                            break\n                else:\n                    # Matched more than one schema. In this situation, the caller must provide the types of\n                    # the arguments of the overload they expect.\n                    schema_printouts = '\\n'.join(str(schema) for schema in matched_schemas)\n                    raise RuntimeError(f'Tried to normalize arguments to {torch.typename(target)} but '\n                                       f'the schema match was ambiguous! Please provide argument types to '\n                                       f'the normalize_arguments() call. Available schemas:\\n{schema_printouts}')\n\n    return new_args_and_kwargs\n```\n\nOriginal documentation:\n```python\nReturns normalized arguments to PyTorch functions. This means that\n    `args/kwargs` will be matched up to the functional's\n    signature and return exclusively kwargs in positional order.\n    Also populates default values. Does not support positional-only\n    parameters or varargs parameters (*args, **kwargs). Does not support modules.\n\n    May require `arg_types` and `kwarg_types` in order to disambiguate overloads.\n\n    Args:\n        target (Callable): Function that we are normalizing\n        args (Tuple[Any]): Tuple of args to the function\n        kwargs (Optional[Dict[str, Any]]): Dict of kwargs to the function\n        arg_types (Optional[Tuple[Any]]): Tuple of arg types for the args\n        kwarg_types (Optional[Dict[str, Any]]): Dict of arg types for the kwargs\n\n    Returns:\n\n        Returns normalized_kwargs, or `None` if not successful.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns normalized arguments to PyTorch functions. This means that\n    `args/kwargs` will be matched up to the functional's\n    signature and return exclusively kwargs in positional order if\n    `normalize_to_only_use_kwargs` is True.\n    Also populates default values. Does not support positional-only\n    parameters or varargs parameters (*args, **kwargs). Does not support modules.\n\n    May require `arg_types` and `kwarg_types` in order to disambiguate overloads.\n\n    Args:\n        target (Callable): Function that we are normalizing\n        args (Tuple[Any]): Tuple of args to the function\n        kwargs (Optional[Dict[str, Any]]): Dict of kwargs to the function\n        arg_types (Optional[Tuple[Any]]): Tuple of arg types for the args\n        kwarg_types (Optional[Dict[str, Any]]): Dict of arg types for the kwargs\n        normalize_to_only_use_kwargs (bool): Whether to normalize to only use kwargs.\n\n    Returns:\n\n        Returns normalized_args_and_kwargs, or `None` if not successful.", "file": "torch_fx_operator_schemas.py", "function": "normalize_function", "project": "pytorch", "change_stats": {"code_changes": 21, "doc_changes": 8, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef publish_sqs_metric(\n    region: str, queue_name: str, metric: str, value: float = 1, unit: str = \"Count\"\n):\n    \n    \n    \n    cw_client = connect_to(region_name=region).cloudwatch\n    try:\n        cw_client.put_metric_data(\n            Namespace=\"AWS/SQS\",\n            MetricData=[\n                {\n                    \"MetricName\": metric,\n                    \"Dimensions\": [{\"Name\": \"QueueName\", \"Value\": queue_name}],\n                    \"Unit\": unit,\n                    \"Timestamp\": datetime.utcnow().replace(tzinfo=timezone.utc),\n                    \"Value\": value,\n                }\n            ],\n        )\n    except Exception as e:\n        LOG.info(f'Unable to put metric data for metric \"{metric}\" to CloudWatch: {e}')```\n\nUpdated code:\n```python\ndef publish_sqs_metric(\n    account_id: str,\n    region: str,\n    queue_name: str,\n    metric: str,\n    value: float = 1,\n    unit: str = \"Count\",\n):\n    \n    \n    \n    cw_client = connect_to(region_name=region, aws_access_key_id=account_id).cloudwatch\n    try:\n        cw_client.put_metric_data(\n            Namespace=\"AWS/SQS\",\n            MetricData=[\n                {\n                    \"MetricName\": metric,\n                    \"Dimensions\": [{\"Name\": \"QueueName\", \"Value\": queue_name}],\n                    \"Unit\": unit,\n                    \"Timestamp\": datetime.utcnow().replace(tzinfo=timezone.utc),\n                    \"Value\": value,\n                }\n            ],\n        )\n    except Exception as e:\n        LOG.info(f'Unable to put metric data for metric \"{metric}\" to CloudWatch: {e}')\n```\n\nOriginal documentation:\n```python\nPublishes the metrics for SQS to CloudWatch using the namespace \"AWS/SQS\"\n    See also: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-cloudwatch-metrics.html\n    :param region The region that should be used for CloudWatch\n    :param queue_name The name of the queue\n    :param metric The metric name to be used\n    :param value The value of the metric data, default: 1\n    :param unit The unit of the metric data, default: \"Count\"\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Publishes the metrics for SQS to CloudWatch using the namespace \"AWS/SQS\"\n    See also: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-cloudwatch-metrics.html\n    :param account_id The account id that should be used for CloudWatch\n    :param region The region that should be used for CloudWatch\n    :param queue_name The name of the queue\n    :param metric The metric name to be used\n    :param value The value of the metric data, default: 1\n    :param unit The unit of the metric data, default: \"Count\"", "file": "localstack_utils_cloudwatch_cloudwatch_util.py", "function": "publish_sqs_metric", "project": "localstack", "change_stats": {"code_changes": 11, "doc_changes": 3, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef QA_util_calc_time(func, *args, **kwargs):\n    ''\n    _time = datetime.datetime.now()\n    func(*args, **kwargs)\n    print(datetime.datetime.now() - _time)```\n\nUpdated code:\n```python\ndef QA_util_calc_time(func, *args, **kwargs):\n    \n    \n    \n    _time = datetime.datetime.now()\n    func(*args, **kwargs)\n    print(datetime.datetime.now() - _time)\n```\n\nOriginal documentation:\n```python\n\u8017\u65f6\u957f\u5ea6\u7684\u88c5\u9970\u5668\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "'\u8017\u65f6\u957f\u5ea6\u7684\u88c5\u9970\u5668'\n    :param func:\n    :param args:\n    :param kwargs:\n    :return:", "file": "QUANTAXIS_QAUtil_QADate.py", "function": "QA_util_calc_time", "project": "QUANTAXIS", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef metadata_dict_from_readme(path: Path) -> Optional[Dict[str, List[str]]]:\n     \n    yaml_block = yaml_block_from_readme(path=path)\n    if yaml_block is None:\n        return None\n    metada_dict = yaml.load(yaml_block, Loader=NoDuplicateSafeLoader) or dict()\n    return metada_dict```\n\nUpdated code:\n```python\ndef metadata_dict_from_readme(path: Path) -> Optional[Dict[str, List[str]]]:\n    \n    yaml_block = yaml_block_from_readme(path=path)\n    if yaml_block is None:\n        return None\n    metada_dict = yaml.load(yaml_block, Loader=NoDuplicateSafeLoader) or dict()\n    return metada_dict\n```\n\nOriginal documentation:\n```python\n\"Loads a dataset's metadata from the dataset card (REAMDE.md), as a Python dict\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads a dataset's metadata from the dataset card (REAMDE.md), as a Python dict", "file": "src_datasets_utils_metadata.py", "function": "metadata_dict_from_readme", "project": "datasets", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unsubscribe_scenario(callback: Callable[[Scenario, Job], None], scenario: Optional[Scenario] = None):\n    \n    \n    return _ScenarioManagerFactory._build_manager()._unsubscribe(callback, scenario)```\n\nUpdated code:\n```python\ndef unsubscribe_scenario(\n    callback: Callable[[Scenario, Job], None], params: Optional[List[Any]] = None, scenario: Optional[Scenario] = None\n):\n    \n    \n    return _ScenarioManagerFactory._build_manager()._unsubscribe(callback, params, scenario)\n```\n\nOriginal documentation:\n```python\nUnsubscribe a function that is called when the status of a `Job^` changes.\n\n    If _scenario_ is not provided, the subscription is removed for all scenarios.\n\n    Parameters:\n        callback (Callable[[Scenario^, Job^], None]): The function to unsubscribe to.\n        scenario (Optional[Scenario^]): The scenario to unsubscribe to. If None, all scenarios\n            unsubscribe to _callback_.\n    Note:\n        The function will continue to be called for ongoing jobs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Unsubscribe a function that is called when the status of a `Job^` changes.\n\n    If _scenario_ is not provided, the subscription is removed for all scenarios.\n\n    Parameters:\n        callback (Callable[[Scenario^, Job^], None]): The function to unsubscribe to.\n        params (Optional[List[Any]]): The parameters to be passed to the _callback_.\n        scenario (Optional[Scenario^]): The scenario to unsubscribe to. If None, all scenarios\n            unsubscribe to _callback_.\n    Note:\n        The function will continue to be called for ongoing jobs.", "file": "taipy_core_taipy.py", "function": "unsubscribe_scenario", "project": "taipy", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef campaign_ensure_mode(self, mode='normal'):\n        \n        \n        \n        if self.campaign_predict_mode() == mode:\n            return False\n        else:\n            # Poor click. May unstable.\n            self.device.click(MODE_CHANGE)\n            self.handle_stage_icon_spawn()\n            return True```\n\nUpdated code:\n```python\ndef campaign_ensure_mode(self, mode='normal'):\n        \n        \n        \n        switch_2 = MODE_SWITCH_2.get(main=self)\n\n        if switch_2 == 'unknown':\n            if mode == 'ex':\n                logger.warning('Trying to goto EX, but no EX mode switch')\n            elif mode == 'normal':\n                MODE_SWITCH_1.set('hard', main=self)\n            elif mode == 'hard':\n                MODE_SWITCH_1.set('normal', main=self)\n            else:\n                logger.warning(f'Unknown campaign mode: {mode}')\n        else:\n            if mode == 'ex':\n                MODE_SWITCH_2.set('hard', main=self)\n            elif mode == 'normal':\n                MODE_SWITCH_2.set('ex', main=self)\n                MODE_SWITCH_1.set('hard', main=self)\n            elif mode == 'hard':\n                MODE_SWITCH_2.set('ex', main=self)\n                MODE_SWITCH_1.set('normal', main=self)\n            else:\n                logger.warning(f'Unknown campaign mode: {mode}')\n```\n\nOriginal documentation:\n```python\nArgs:\n            mode (str): 'normal', 'hard'.\n\n        Returns:\n            bool: If mode changed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            mode (str): 'normal', 'hard', 'ex'\n\n        Returns:\n            bool: If mode changed.", "file": "module_campaign_campaign_ui.py", "function": "CampaignUI.campaign_ensure_mode", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 29, "doc_changes": 4, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update(self, parameters):\n        \n        \n        if not isinstance(parameters, container_abcs.Iterable):\n            raise TypeError(\"ParametersDict.update should be called with an \"\n                            \"iterable of key/value pairs, but got \" +\n                            type(parameters).__name__)\n\n        if isinstance(parameters, container_abcs.Mapping):\n            if isinstance(parameters, OrderedDict):\n                for key, parameter in parameters.items():\n                    self[key] = parameter\n            else:\n                for key, parameter in sorted(parameters.items()):\n                    self[key] = parameter\n        else:\n            for j, p in enumerate(parameters):\n                if not isinstance(p, container_abcs.Iterable):\n                    raise TypeError(\"ParameterDict update sequence element \"\n                                    \"#\" + str(j) + \" should be Iterable; is\" +\n                                    type(p).__name__)\n                if not len(p) == 2:\n                    raise ValueError(\"ParameterDict update sequence element \"\n                                     \"#\" + str(j) + \" has length \" + str(len(p)) +\n                                     \"; 2 is required\")\n                self[p[0]] = p[1]```\n\nUpdated code:\n```python\ndef update(self, parameters):\n        \n        \n        if not isinstance(parameters, container_abcs.Iterable):\n            raise TypeError(\"ParametersDict.update should be called with an \"\n                            \"iterable of key/value pairs, but got \" +\n                            type(parameters).__name__)\n\n        if isinstance(parameters, container_abcs.Mapping):\n            if isinstance(parameters, (OrderedDict, ParameterDict)):\n                for key, parameter in parameters.items():\n                    self[key] = parameter\n            else:\n                for key, parameter in sorted(parameters.items()):\n                    self[key] = parameter\n        else:\n            for j, p in enumerate(parameters):\n                if not isinstance(p, container_abcs.Iterable):\n                    raise TypeError(\"ParameterDict update sequence element \"\n                                    \"#\" + str(j) + \" should be Iterable; is\" +\n                                    type(p).__name__)\n                if not len(p) == 2:\n                    raise ValueError(\"ParameterDict update sequence element \"\n                                     \"#\" + str(j) + \" has length \" + str(len(p)) +\n                                     \"; 2 is required\")\n                self[p[0]] = p[1]\n```\n\nOriginal documentation:\n```python\nr\"\"\"Update the ParameterDict with the key/value pairs from a mapping or\n        an iterable, overwriting existing keys.\n\n        Arguments:\n            parameters (iterable): a mapping (dictionary) of\n                (string : :class:`~torch.nn.Parameter`) or an iterable of\n                key/value pairs of type (string, :class:`~torch.nn.Parameter`)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Update the :class:`~torch.nn.ParameterDict` with the key-value pairs from a\n        mapping or an iterable, overwriting existing keys.\n\n        .. note::\n            If :attr:`parameters` is an ``OrderedDict``, a :class:`~torch.nn.ParameterDict`, or\n            an iterable of key-value pairs, the order of new elements in it is preserved.\n\n        Arguments:\n            parameters (iterable): a mapping (dictionary) from string to\n                :class:`~torch.nn.Parameter`, or an iterable of\n                key-value pairs of type (string, :class:`~torch.nn.Parameter`)", "file": "torch_nn_modules_container.py", "function": "ParameterDict.update", "project": "pytorch", "change_stats": {"code_changes": 4, "doc_changes": 16, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load(cls, path: Text) -> \"Policy\":\n        \n        \n\n        raise NotImplementedError(\"Policy must have the capacity to load itself.\")```\n\nUpdated code:\n```python\ndef load(cls, path: Union[Text, Path]) -> \"Policy\":\n        \n        \n        metadata_file = Path(path) / cls._metadata_filename()\n\n        if metadata_file.is_file():\n            data = json.loads(rasa.shared.utils.io.read_file(metadata_file))\n\n            if (Path(path) / FEATURIZER_FILE).is_file():\n                featurizer = TrackerFeaturizer.load(path)\n                data[\"featurizer\"] = featurizer\n\n            return cls(**data)\n\n        logger.info(\n            f\"Couldn't load metadata for policy '{cls.__name__}'. \"\n            f\"File '{metadata_file}' doesn't exist.\"\n        )\n        return cls()\n```\n\nOriginal documentation:\n```python\nLoads a policy from the storage.\n\n        Needs to load its featurizer.\n\n        Args:\n            path: the path from where to load the policy\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads a policy from path.\n\n        Args:\n            path: Path to load policy from.\n\n        Returns:\n            An instance of `Policy`.", "file": "rasa_core_policies_policy.py", "function": "Policy.load", "project": "rasa", "change_stats": {"code_changes": 20, "doc_changes": 11, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_runtime_env(self, runtime_env: Optional[Dict[str, Any]]) -> None:\n        \n        \n        self.runtime_env = runtime_env if runtime_env is not None else {}\n        self._cached_pb = None```\n\nUpdated code:\n```python\ndef set_runtime_env(self,\n                        runtime_env: Optional[Dict[str, Any]],\n                        validate: bool = False) -> None:\n        \n        \n        self.runtime_env = runtime_env if runtime_env is not None else {}\n        if validate:\n            self.runtime_env = self._validate_runtime_env()[0]\n        self._cached_pb = None\n```\n\nOriginal documentation:\n```python\nModify the runtime_env of the JobConfig.\n\n        We don't validate the runtime_env here because it may go through some\n        translation before actually being passed to C++ (e.g., working_dir\n        translated from a local directory to a URI).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Modify the runtime_env of the JobConfig.\n\n        We don't validate the runtime_env by default here because it may go\n        through some translation before actually being passed to C++ (e.g.,\n        working_dir translated from a local directory to a URI.", "file": "python_ray_job_config.py", "function": "JobConfig.set_runtime_env", "project": "ray", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef copy(self,\n             source_project_id,\n             source_dataset_id,\n             source_table_id,\n             dest_project_id,\n             dest_dataset_id,\n             dest_table_id,\n             create_disposition=CreateDisposition.CREATE_IF_NEEDED,\n             write_disposition=WriteDisposition.WRITE_TRUNCATE):\n        \n\n        job = {\n            \"projectId\": dest_project_id,\n            \"configuration\": {\n                \"copy\": {\n                    \"sourceTable\": {\n                        \"projectId\": source_project_id,\n                        \"datasetId\": source_dataset_id,\n                        \"tableId\": source_table_id,\n                    },\n                    \"destinationTable\": {\n                        \"projectId\": dest_project_id,\n                        \"datasetId\": dest_dataset_id,\n                        \"tableId\": dest_table_id,\n                    },\n                    \"createDisposition\": create_disposition,\n                    \"writeDisposition\": write_disposition,\n                }\n            }\n        }\n\n        self.run_job(dest_project_id, job, dataset_id=dest_dataset_id)```\n\nUpdated code:\n```python\ndef copy(self,\n             source_table,\n             dest_table,\n             create_disposition=CreateDisposition.CREATE_IF_NEEDED,\n             write_disposition=WriteDisposition.WRITE_TRUNCATE):\n        \n        \n\n        job = {\n            \"projectId\": dest_table.project_id,\n            \"configuration\": {\n                \"copy\": {\n                    \"sourceTable\": {\n                        \"projectId\": source_table.project_id,\n                        \"datasetId\": source_table.dataset_id,\n                        \"tableId\": source_table.table_id,\n                    },\n                    \"destinationTable\": {\n                        \"projectId\": dest_table.project_id,\n                        \"datasetId\": dest_table.dataset_id,\n                        \"tableId\": dest_table.table_id,\n                    },\n                    \"createDisposition\": create_disposition,\n                    \"writeDisposition\": write_disposition,\n                }\n            }\n        }\n\n        self.run_job(dest_table.project_id, job, dataset=dest_table.dataset)\n```\n\nOriginal documentation:\n```python\nCopies (or appends) a table to another table.\n\n           :param create_disposition whether to create the table if needed\n           :param write_disposition whether to append/truncate/fail if the table exists\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Copies (or appends) a table to another table.\n\n            :param source_table:\n            :type source_table: BQTable\n            :param dest_table:\n            :type dest_table: BQTable\n            :param create_disposition: whether to create the table if needed\n            :type create_disposition: CreateDisposition\n            :param write_disposition: whether to append/truncate/fail if the table exists\n            :type write_disposition: WriteDisposition", "file": "luigi_contrib_bigquery.py", "function": "BigqueryClient.copy", "project": "luigi", "change_stats": {"code_changes": 27, "doc_changes": 12, "total_changes": 39}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_closed_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:\n        \n        \n        \n        self.check_required_symbol('fetchClosedOrders', symbol)\n        self.load_markets()\n        method = 'privatePostArchivedOrdersPair'\n        market = self.market(symbol)\n        request = {'pair': market['id']}\n        response = getattr(self, method)(self.extend(request, params))\n        return self.parse_orders(response, market, since, limit)```\n\nUpdated code:\n```python\ndef fetch_closed_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' fetchClosedOrders() requires a symbol argument')\n        self.load_markets()\n        method = 'privatePostArchivedOrdersPair'\n        market = self.market(symbol)\n        request = {'pair': market['id']}\n        response = getattr(self, method)(self.extend(request, params))\n        return self.parse_orders(response, market, since, limit)\n```\n\nOriginal documentation:\n```python\nfetches information on multiple closed orders made by the user\n        :param str symbol: unified market symbol of the market orders were made in\n        :param int [since]: the earliest time in ms to fetch orders for\n        :param int [limit]: the maximum number of order structures to retrieve\n        :param dict [params]: extra parameters specific to the cex api endpoint\n        :returns Order[]: a list of `order structures <https://github.com/ccxt/ccxt/wiki/Manual#order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetches information on multiple closed orders made by the user\n        :param str symbol: unified market symbol of the market orders were made in\n        :param int [since]: the earliest time in ms to fetch orders for\n        :param int [limit]: the maximum number of order structures to retrieve\n        :param dict [params]: extra parameters specific to the cex api endpoint\n        :returns Order[]: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`", "file": "python_ccxt_cex.py", "function": "cex.fetch_closed_orders", "project": "ccxt", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cancel_all_orders(self, symbol: Str = None, params={}):\n        \n        \n        \n        self.check_required_symbol('cancelAllOrders', symbol)\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'product_id': market['numericId'],\n            # 'cancel_limit_orders': 'true',\n            # 'cancel_stop_orders': 'true',\n        }\n        response = self.privateDeleteOrdersAll(self.extend(request, params))\n        #\n        #     {\n        #         \"result\":{},\n        #         \"success\":true\n        #     }\n        #\n        return response```\n\nUpdated code:\n```python\ndef cancel_all_orders(self, symbol: Str = None, params={}):\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' cancelAllOrders() requires a symbol argument')\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'product_id': market['numericId'],\n            # 'cancel_limit_orders': 'true',\n            # 'cancel_stop_orders': 'true',\n        }\n        response = self.privateDeleteOrdersAll(self.extend(request, params))\n        #\n        #     {\n        #         \"result\":{},\n        #         \"success\":true\n        #     }\n        #\n        return response\n```\n\nOriginal documentation:\n```python\ncancel all open orders in a market\n        :see: https://docs.delta.exchange/#cancel-all-open-orders\n        :param str symbol: unified market symbol of the market to cancel orders in\n        :param dict [params]: extra parameters specific to the delta api endpoint\n        :returns dict[]: a list of `order structures <https://github.com/ccxt/ccxt/wiki/Manual#order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "cancel all open orders in a market\n        :see: https://docs.delta.exchange/#cancel-all-open-orders\n        :param str symbol: unified market symbol of the market to cancel orders in\n        :param dict [params]: extra parameters specific to the delta api endpoint\n        :returns dict[]: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`", "file": "python_ccxt_delta.py", "function": "delta.cancel_all_orders", "project": "ccxt", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_hashes(self, no_fingerprint=False):\n        \n        \n        \n        from sentry.event_hashing import get_event_hashes\n        return get_event_hashes(self, no_fingerprint)```\n\nUpdated code:\n```python\ndef get_hashes(self):\n        \n        \n        \n        from sentry.event_hashing import calculate_event_hashes\n        # If we have hashes stored in the data we use them, otherwise we\n        # fall back to generating new ones from the data\n        hashes = self.data.get('hashes')\n        if hashes is not None:\n            return hashes\n        return calculate_event_hashes(self)\n```\n\nOriginal documentation:\n```python\nReturns the calculated hashes for the event.\n\n        The `no_fingerprint` parameter disables using the fingerprint value.\n        This is a legacy code path that should be removed but matches old\n        event ingestion behavior.  This will make the code consider the\n        legacy checksum as well before falling back to the default fingerprint.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the calculated hashes for the event.", "file": "src_sentry_models_event.py", "function": "Event.get_hashes", "project": "sentry", "change_stats": {"code_changes": 13, "doc_changes": 7, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef init_module(self, empty_init: Optional[bool] = None) -> ContextManager:\n        \n\n        \n        self._validate_launched()\n        if not _TORCH_GREATER_EQUAL_2_0 and self.device.type != \"cpu\":\n            rank_zero_warn(\n                \"`Fabric.init_module()` can't place the model parameters on the device directly\"\n                \" with PyTorch < 2.0. Parameters will remain on CPU until `Fabric.setup()` is called.\"\n                \" Upgrade to PyTorch >= 2.0 to fully utilize this feature.\",\n                category=PossibleUserWarning,\n            )\n        return self._strategy.module_init_context(empty_init=empty_init)```\n\nUpdated code:\n```python\ndef init_module(self, empty_init: Optional[bool] = None) -> ContextManager:\n        \n\n        \n        self._validate_launched()\n        return self._strategy.module_init_context(empty_init=empty_init)\n```\n\nOriginal documentation:\n```python\nInstantiate the model and its parameters under this context manager to reduce peak memory usage.\n\n        The parameters get created on the device and with the right data type right away without wasting memory being\n        allocated unnecessarily. The automatic device placement under this context manager is only supported with\n        PyTorch 2.0 and newer.\n\n        Args:\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\n                If ``None``, the strategy will decide. Some strategies may not support all options.\n                Set this to ``True`` if you are loading a checkpoint into a large model.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Instantiate the model and its parameters under this context manager to reduce peak memory usage.\n\n        The parameters get created on the device and with the right data type right away without wasting memory being\n        allocated unnecessarily.\n\n        Args:\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\n                If ``None``, the strategy will decide. Some strategies may not support all options.\n                Set this to ``True`` if you are loading a checkpoint into a large model.", "file": "src_lightning_fabric_fabric.py", "function": "Fabric.init_module", "project": "pytorch-lightning", "change_stats": {"code_changes": 9, "doc_changes": 5, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_store(path, **kwargs):\n    \n    \n    \n    store = None\n    try:\n        store = HDFStore(path, **kwargs)\n        yield store\n    finally:\n        if store is not None:\n            store.close()```\n\nUpdated code:\n```python\ndef get_store(path, **kwargs):\n     \n    \n    return HDFStore(path, **kwargs)\n```\n\nOriginal documentation:\n```python\nCreates an HDFStore instance. This function can be used in a with statement\n\n    Parameters\n    ----------\n    same as HDFStore\n\n    Examples\n    --------\n    >>> from pandas import DataFrame\n    >>> from numpy.random import randn\n    >>> bar = DataFrame(randn(10, 4))\n    >>> with get_store('test.h5') as store:\n    ...     store['foo'] = bar   # write to HDF5\n    ...     bar = store['foo']   # retrieve\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Backwards compatible alias for ``HDFStore``", "file": "pandas_io_pytables.py", "function": "get_store", "project": "pandas", "change_stats": {"code_changes": 13, "doc_changes": 17, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef gabor_filter(image, frequency, theta=0, bandwidth=1, sigma_x=None,\n                 sigma_y=None, n_stds=3, offset=0, mode='reflect', cval=0):\n    \n    \n    assert_nD(image, 2)\n    g = gabor_kernel(frequency, theta, bandwidth, sigma_x, sigma_y, n_stds,\n                     offset)\n\n    filtered_real = ndimage.convolve(image, np.real(g), mode=mode, cval=cval)\n    filtered_imag = ndimage.convolve(image, np.imag(g), mode=mode, cval=cval)\n\n    return filtered_real, filtered_imag```\n\nUpdated code:\n```python\ndef gabor_filter(image, frequency, theta=0, bandwidth=1, sigma_x=None,\n                 sigma_y=None, n_stds=3, offset=0, mode='reflect', cval=0):\n    \n    \n    assert_nD(image, 2)\n    g = gabor_kernel(frequency, theta, bandwidth, sigma_x, sigma_y, n_stds,\n                     offset)\n\n    filtered_real = ndi.convolve(image, np.real(g), mode=mode, cval=cval)\n    filtered_imag = ndi.convolve(image, np.imag(g), mode=mode, cval=cval)\n\n    return filtered_real, filtered_imag\n```\n\nOriginal documentation:\n```python\nReturn real and imaginary responses to Gabor filter.\n\n    The real and imaginary parts of the Gabor filter kernel are applied to the\n    image and the response is returned as a pair of arrays.\n\n    Gabor filter is a linear filter with a Gaussian kernel which is modulated\n    by a sinusoidal plane wave. Frequency and orientation representations of\n    the Gabor filter are similar to those of the human visual system.\n    Gabor filter banks are commonly used in computer vision and image\n    processing. They are especially suitable for edge detection and texture\n    classification.\n\n    Parameters\n    ----------\n    image : 2-D array\n        Input image.\n    frequency : float\n        Spatial frequency of the harmonic function. Specified in pixels.\n    theta : float, optional\n        Orientation in radians. If 0, the harmonic is in the x-direction.\n    bandwidth : float, optional\n        The bandwidth captured by the filter. For fixed bandwidth, `sigma_x`\n        and `sigma_y` will decrease with increasing frequency. This value is\n        ignored if `sigma_x` and `sigma_y` are set by the user.\n    sigma_x, sigma_y : float, optional\n        Standard deviation in x- and y-directions. These directions apply to\n        the kernel *before* rotation. If `theta = pi/2`, then the kernel is\n        rotated 90 degrees so that `sigma_x` controls the *vertical* direction.\n    n_stds : scalar, optional\n        The linear size of the kernel is n_stds (3 by default) standard\n        deviations.\n    offset : float, optional\n        Phase offset of harmonic function in radians.\n    mode : string, optional\n        Mode used to convolve image with a kernel, passed to `ndimage.convolve`\n    cval : scalar, optional\n        Value to fill past edges of input if `mode` of convolution is\n        'constant'. The parameter is passed to `ndimage.convolve`.\n\n    Returns\n    -------\n    real, imag : arrays\n        Filtered images using the real and imaginary parts of the Gabor filter\n        kernel. Images are of the same dimensions as the input one.\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Gabor_filter\n    .. [2] http://mplab.ucsd.edu/tutorials/gabor.pdf\n\n    Examples\n    --------\n    >>> from skimage.filter import gabor_filter\n    >>> from skimage import data, io\n    >>> from matplotlib import pyplot as plt  # doctest: +SKIP\n\n    >>> image = data.coins()\n    >>> # detecting edges in a coin image\n    >>> filt_real, filt_imag = gabor_filter(image, frequency=0.6)\n    >>> plt.figure()            # doctest: +SKIP\n    >>> io.imshow(filt_real)    # doctest: +SKIP\n    >>> io.show()               # doctest: +SKIP\n\n    >>> # less sensitivity to finer details with the lower frequency kernel\n    >>> filt_real, filt_imag = gabor_filter(image, frequency=0.1)\n    >>> plt.figure()            # doctest: +SKIP\n    >>> io.imshow(filt_real)    # doctest: +SKIP\n    >>> io.show()               # doctest: +SKIP\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return real and imaginary responses to Gabor filter.\n\n    The real and imaginary parts of the Gabor filter kernel are applied to the\n    image and the response is returned as a pair of arrays.\n\n    Gabor filter is a linear filter with a Gaussian kernel which is modulated\n    by a sinusoidal plane wave. Frequency and orientation representations of\n    the Gabor filter are similar to those of the human visual system.\n    Gabor filter banks are commonly used in computer vision and image\n    processing. They are especially suitable for edge detection and texture\n    classification.\n\n    Parameters\n    ----------\n    image : 2-D array\n        Input image.\n    frequency : float\n        Spatial frequency of the harmonic function. Specified in pixels.\n    theta : float, optional\n        Orientation in radians. If 0, the harmonic is in the x-direction.\n    bandwidth : float, optional\n        The bandwidth captured by the filter. For fixed bandwidth, `sigma_x`\n        and `sigma_y` will decrease with increasing frequency. This value is\n        ignored if `sigma_x` and `sigma_y` are set by the user.\n    sigma_x, sigma_y : float, optional\n        Standard deviation in x- and y-directions. These directions apply to\n        the kernel *before* rotation. If `theta = pi/2`, then the kernel is\n        rotated 90 degrees so that `sigma_x` controls the *vertical* direction.\n    n_stds : scalar, optional\n        The linear size of the kernel is n_stds (3 by default) standard\n        deviations.\n    offset : float, optional\n        Phase offset of harmonic function in radians.\n    mode : string, optional\n        Mode used to convolve image with a kernel, passed to `ndi.convolve`\n    cval : scalar, optional\n        Value to fill past edges of input if `mode` of convolution is\n        'constant'. The parameter is passed to `ndi.convolve`.\n\n    Returns\n    -------\n    real, imag : arrays\n        Filtered images using the real and imaginary parts of the Gabor filter\n        kernel. Images are of the same dimensions as the input one.\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Gabor_filter\n    .. [2] http://mplab.ucsd.edu/tutorials/gabor.pdf\n\n    Examples\n    --------\n    >>> from skimage.filter import gabor_filter\n    >>> from skimage import data, io\n    >>> from matplotlib import pyplot as plt  # doctest: +SKIP\n\n    >>> image = data.coins()\n    >>> # detecting edges in a coin image\n    >>> filt_real, filt_imag = gabor_filter(image, frequency=0.6)\n    >>> plt.figure()            # doctest: +SKIP\n    >>> io.imshow(filt_real)    # doctest: +SKIP\n    >>> io.show()               # doctest: +SKIP\n\n    >>> # less sensitivity to finer details with the lower frequency kernel\n    >>> filt_real, filt_imag = gabor_filter(image, frequency=0.1)\n    >>> plt.figure()            # doctest: +SKIP\n    >>> io.imshow(filt_real)    # doctest: +SKIP\n    >>> io.show()               # doctest: +SKIP", "file": "skimage_filters__gabor.py", "function": "gabor_filter", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_public_streams(self):\n        \n        \n        \n        email = 'hamlet@zulip.com'\n        self.login(email)\n\n        result = self.client.post(\"/json/get_public_streams\")\n\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n\n        self.assertIn(\"streams\", json)\n        self.assertIsInstance(json[\"streams\"], list)```\n\nUpdated code:\n```python\ndef test_public_streams(self):\n        \n        \n        \n        email = 'hamlet@zulip.com'\n        self.login(email)\n\n        result = self.client.get(\"/json/streams?include_subscribed=false\")\n\n        self.assert_json_success(result)\n        json = ujson.loads(result.content)\n\n        self.assertIn(\"streams\", json)\n        self.assertIsInstance(json[\"streams\"], list)\n```\n\nOriginal documentation:\n```python\nEnsure that get_public_streams successfully returns a list of streams\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure that streams successfully returns a list of streams", "file": "zerver_tests_test_subs.py", "function": "GetPublicStreamsTest.test_public_streams", "project": "zulip", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef retrieval_intent_templates(self) -> Dict[Text, List[Dict[Text, Any]]]:\n        \n        \n        return dict(filter(lambda x: \"/\" in x[0], self.templates.items()))```\n\nUpdated code:\n```python\ndef retrieval_intent_templates(self) -> Dict[Text, List[Dict[Text, Any]]]:\n        \n\n        return dict(\n            filter(\n                lambda x: self.is_retrieval_intent_template(x), self.templates.items()\n            )\n        )\n```\n\nOriginal documentation:\n```python\nReturn only the templates which are defined for retrieval intents\n\n        These templates have a `/` symbol in their name. Use that to filter them from the rest.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return only the templates which are defined for retrieval intents", "file": "rasa_shared_core_domain.py", "function": "Domain.retrieval_intent_templates", "project": "rasa", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef from_tensors(\n        tensors: List[torch.Tensor],\n        size_divisibility: int = 0,\n        pad_value: float = 0.0,\n        padding_constraints: Optional[Dict[str, int]] = None,\n    ) -> \"ImageList\":\n        \n        \n        \n        assert len(tensors) > 0\n        assert isinstance(tensors, (tuple, list))\n        for t in tensors:\n            assert isinstance(t, torch.Tensor), type(t)\n            assert t.shape[:-2] == tensors[0].shape[:-2], t.shape\n\n        image_sizes = [(im.shape[-2], im.shape[-1]) for im in tensors]\n        image_sizes_tensor = [shapes_to_tensor(x) for x in image_sizes]\n        max_size = torch.stack(image_sizes_tensor).max(0).values\n\n        if padding_constraints is not None:\n            if padding_constraints.get(\"square\", 0) > 0:\n                # pad to square.\n                max_size[0] = max_size[1] = max_size.max()\n            if \"size_divisibility\" in padding_constraints:\n                size_divisibility = padding_constraints[\"size_divisibility\"]\n        if size_divisibility > 1:\n            stride = size_divisibility\n            # the last two dims are H,W, both subject to divisibility requirement\n            max_size = (max_size + (stride - 1)).div(stride, rounding_mode=\"floor\") * stride\n\n        # handle weirdness of scripting and tracing ...\n        if torch.jit.is_scripting():\n            max_size: List[int] = max_size.to(dtype=torch.long).tolist()\n        else:\n            if torch.jit.is_tracing():\n                image_sizes = image_sizes_tensor\n\n        if len(tensors) == 1:\n            # This seems slightly (2%) faster.\n            # TODO: check whether it's faster for multiple images as well\n            image_size = image_sizes[0]\n            padding_size = [0, max_size[-1] - image_size[1], 0, max_size[-2] - image_size[0]]\n            batched_imgs = F.pad(tensors[0], padding_size, value=pad_value).unsqueeze_(0)\n        else:\n            # max_size can be a tensor in tracing mode, therefore convert to list\n            batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + list(max_size)\n            device = (\n                None if torch.jit.is_scripting() else (\"cpu\" if torch.jit.is_tracing() else None)\n            )\n            batched_imgs = tensors[0].new_full(batch_shape, pad_value, device=device)\n            batched_imgs = move_device_like(batched_imgs, tensors[0])\n            for img, pad_img in zip(tensors, batched_imgs):\n                pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)\n\n        return ImageList(batched_imgs.contiguous(), image_sizes)```\n\nUpdated code:\n```python\ndef from_tensors(\n        tensors: List[torch.Tensor],\n        size_divisibility: int = 0,\n        pad_value: float = 0.0,\n        padding_constraints: Optional[Dict[str, int]] = None,\n    ) -> \"ImageList\":\n        \n        \n        \n        assert len(tensors) > 0\n        assert isinstance(tensors, (tuple, list))\n        for t in tensors:\n            assert isinstance(t, torch.Tensor), type(t)\n            assert t.shape[:-2] == tensors[0].shape[:-2], t.shape\n\n        image_sizes = [(im.shape[-2], im.shape[-1]) for im in tensors]\n        image_sizes_tensor = [shapes_to_tensor(x) for x in image_sizes]\n        max_size = torch.stack(image_sizes_tensor).max(0).values\n\n        if padding_constraints is not None:\n            square_size = padding_constraints.get(\"square_size\", 0)\n            if square_size > 0:\n                # pad to square.\n                max_size[0] = max_size[1] = square_size\n            if \"size_divisibility\" in padding_constraints:\n                size_divisibility = padding_constraints[\"size_divisibility\"]\n        if size_divisibility > 1:\n            stride = size_divisibility\n            # the last two dims are H,W, both subject to divisibility requirement\n            max_size = (max_size + (stride - 1)).div(stride, rounding_mode=\"floor\") * stride\n\n        # handle weirdness of scripting and tracing ...\n        if torch.jit.is_scripting():\n            max_size: List[int] = max_size.to(dtype=torch.long).tolist()\n        else:\n            if torch.jit.is_tracing():\n                image_sizes = image_sizes_tensor\n\n        if len(tensors) == 1:\n            # This seems slightly (2%) faster.\n            # TODO: check whether it's faster for multiple images as well\n            image_size = image_sizes[0]\n            padding_size = [0, max_size[-1] - image_size[1], 0, max_size[-2] - image_size[0]]\n            batched_imgs = F.pad(tensors[0], padding_size, value=pad_value).unsqueeze_(0)\n        else:\n            # max_size can be a tensor in tracing mode, therefore convert to list\n            batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + list(max_size)\n            device = (\n                None if torch.jit.is_scripting() else (\"cpu\" if torch.jit.is_tracing() else None)\n            )\n            batched_imgs = tensors[0].new_full(batch_shape, pad_value, device=device)\n            batched_imgs = move_device_like(batched_imgs, tensors[0])\n            for img, pad_img in zip(tensors, batched_imgs):\n                pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)\n\n        return ImageList(batched_imgs.contiguous(), image_sizes)\n```\n\nOriginal documentation:\n```python\nArgs:\n            tensors: a tuple or list of `torch.Tensor`, each of shape (Hi, Wi) or\n                (C_1, ..., C_K, Hi, Wi) where K >= 1. The Tensors will be padded\n                to the same shape with `pad_value`.\n            size_divisibility (int): If `size_divisibility > 0`, add padding to ensure\n                the common height and width is divisible by `size_divisibility`.\n                This depends on the model and many models need a divisibility of 32.\n            pad_value (float): value to pad.\n            padding_constraints (optional[Dict]): If given, it would follow the format as\n                {\"size_divisibility\": int, \"square\": int}, where `size_divisibility` will overwrite\n                the above one if presented and `square` indicates if require inputs to be padded to\n                square.\n\n        Returns:\n            an `ImageList`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            tensors: a tuple or list of `torch.Tensor`, each of shape (Hi, Wi) or\n                (C_1, ..., C_K, Hi, Wi) where K >= 1. The Tensors will be padded\n                to the same shape with `pad_value`.\n            size_divisibility (int): If `size_divisibility > 0`, add padding to ensure\n                the common height and width is divisible by `size_divisibility`.\n                This depends on the model and many models need a divisibility of 32.\n            pad_value (float): value to pad.\n            padding_constraints (optional[Dict]): If given, it would follow the format as\n                {\"size_divisibility\": int, \"square_size\": int}, where `size_divisibility` will\n                overwrite the above one if presented and `square_size` indicates the\n                square padding size if `square_size` > 0.\n        Returns:\n            an `ImageList`.", "file": "detectron2_structures_image_list.py", "function": "ImageList.from_tensors", "project": "detectron2", "change_stats": {"code_changes": 7, "doc_changes": 9, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef padded(self, p):\n        \n        points = self.get_points()\n        return Bbox(points + [[-p, -p], [p, p]])```\n\nUpdated code:\n```python\ndef padded(self, w_pad, h_pad=None):\n        \n        \n\n        \n        points = self.get_points()\n        if h_pad is None:\n            h_pad = w_pad\n        return Bbox(points + [[-w_pad, -h_pad], [w_pad, h_pad]])\n```\n\nOriginal documentation:\n```python\nConstruct a `Bbox` by padding this one on all four sides by *p*.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Construct a `Bbox` by padding this one on all four sides.\n\n        Parameters\n        ----------\n        w_pad : float\n            Width pad\n        h_pad: float, optional\n            Height pad.  Defaults to *w_pad*.", "file": "lib_matplotlib_transforms.py", "function": "BboxBase.padded", "project": "matplotlib", "change_stats": {"code_changes": 11, "doc_changes": 11, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pack(img_list):\n    \n    \n    \n    img_list = [np.array(i) for i in img_list]\n    image = cv2.vconcat(img_list)\n    return image```\n\nUpdated code:\n```python\ndef pack(img_list):\n    \n    \n    \n    image = cv2.vconcat(img_list)\n    return image\n```\n\nOriginal documentation:\n```python\nStack images vertically.\n\n    Args:\n        img_list (list): List of pillow image\n\n    Returns:\n        Pillow image\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Stack images vertically.\n\n    Args:\n        img_list (list): List of image\n\n    Returns:\n        np.ndarray:", "file": "module_statistics_utils.py", "function": "pack", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sudoku(grid: Matrix) -> Union[Matrix, bool]:\n    \n    \n    \n\n    if is_completed(grid):\n        return grid\n\n    row, column = find_empty_location(grid)\n\n    for digit in range(1, 10):\n        if is_safe(grid, row, column, digit):\n            grid[row][column] = digit\n\n            if sudoku(grid):\n                return grid\n\n            grid[row][column] = 0\n\n    return False```\n\nUpdated code:\n```python\ndef sudoku(grid: Matrix) -> Optional[Matrix]:\n    \n    \n    \n\n    if is_completed(grid):\n        return grid\n\n    location = find_empty_location(grid)\n    if location is not None:\n        row, column = location\n    else:\n        # If the location is ``None``, then the grid is solved.\n        return grid\n\n    for digit in range(1, 10):\n        if is_safe(grid, row, column, digit):\n            grid[row][column] = digit\n\n            if sudoku(grid) is not None:\n                return grid\n\n            grid[row][column] = 0\n\n    return None\n```\n\nOriginal documentation:\n```python\nTakes a partially filled-in grid and attempts to assign values to\n    all unassigned locations in such a way to meet the requirements\n    for Sudoku solution (non-duplication across rows, columns, and boxes)\n\n    >>> sudoku(initial_grid)  # doctest: +NORMALIZE_WHITESPACE\n    [[3, 1, 6, 5, 7, 8, 4, 9, 2],\n     [5, 2, 9, 1, 3, 4, 7, 6, 8],\n     [4, 8, 7, 6, 2, 9, 5, 3, 1],\n     [2, 6, 3, 4, 1, 5, 9, 8, 7],\n     [9, 7, 4, 8, 6, 3, 1, 2, 5],\n     [8, 5, 1, 7, 9, 2, 6, 4, 3],\n     [1, 3, 8, 9, 4, 7, 2, 5, 6],\n     [6, 9, 2, 3, 5, 1, 8, 7, 4],\n     [7, 4, 5, 2, 8, 6, 3, 1, 9]]\n     >>> sudoku(no_solution)\n     False\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Takes a partially filled-in grid and attempts to assign values to\n    all unassigned locations in such a way to meet the requirements\n    for Sudoku solution (non-duplication across rows, columns, and boxes)\n\n    >>> sudoku(initial_grid)  # doctest: +NORMALIZE_WHITESPACE\n    [[3, 1, 6, 5, 7, 8, 4, 9, 2],\n     [5, 2, 9, 1, 3, 4, 7, 6, 8],\n     [4, 8, 7, 6, 2, 9, 5, 3, 1],\n     [2, 6, 3, 4, 1, 5, 9, 8, 7],\n     [9, 7, 4, 8, 6, 3, 1, 2, 5],\n     [8, 5, 1, 7, 9, 2, 6, 4, 3],\n     [1, 3, 8, 9, 4, 7, 2, 5, 6],\n     [6, 9, 2, 3, 5, 1, 8, 7, 4],\n     [7, 4, 5, 2, 8, 6, 3, 1, 9]]\n     >>> sudoku(no_solution) is None\n     True", "file": "backtracking_sudoku.py", "function": "sudoku", "project": "Python", "change_stats": {"code_changes": 15, "doc_changes": 6, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef axis(self, *args, **kwargs):\n        \n        \n        \n\n        if len(args) == len(kwargs) == 0:\n            xmin, xmax = self.get_xlim()\n            ymin, ymax = self.get_ylim()\n            return xmin, xmax, ymin, ymax\n\n        emit = kwargs.get('emit', True)\n\n        if len(args) == 1 and isinstance(args[0], str):\n            s = args[0].lower()\n            if s == 'on':\n                self.set_axis_on()\n            elif s == 'off':\n                self.set_axis_off()\n            elif s in ('equal', 'tight', 'scaled', 'normal',\n                       'auto', 'image', 'square'):\n                if s == 'normal':\n                    cbook.warn_deprecated(\n                        \"3.1\", message=\"Passing 'normal' to axis() is \"\n                        \"deprecated since %(since)s; use 'auto' instead.\")\n                self.set_autoscale_on(True)\n                self.set_aspect('auto')\n                self.autoscale_view(tight=False)\n                # self.apply_aspect()\n                if s == 'equal':\n                    self.set_aspect('equal', adjustable='datalim')\n                elif s == 'scaled':\n                    self.set_aspect('equal', adjustable='box', anchor='C')\n                    self.set_autoscale_on(False)  # Req. by Mark Bakker\n                elif s == 'tight':\n                    self.autoscale_view(tight=True)\n                    self.set_autoscale_on(False)\n                elif s == 'image':\n                    self.autoscale_view(tight=True)\n                    self.set_autoscale_on(False)\n                    self.set_aspect('equal', adjustable='box', anchor='C')\n                elif s == 'square':\n                    self.set_aspect('equal', adjustable='box', anchor='C')\n                    self.set_autoscale_on(False)\n                    xlim = self.get_xlim()\n                    ylim = self.get_ylim()\n                    edge_size = max(np.diff(xlim), np.diff(ylim))\n                    self.set_xlim([xlim[0], xlim[0] + edge_size],\n                                  emit=emit, auto=False)\n                    self.set_ylim([ylim[0], ylim[0] + edge_size],\n                                  emit=emit, auto=False)\n            else:\n                raise ValueError('Unrecognized string %s to axis; '\n                                 'try on or off' % s)\n            xmin, xmax = self.get_xlim()\n            ymin, ymax = self.get_ylim()\n            return xmin, xmax, ymin, ymax\n\n        try:\n            args[0]\n        except IndexError:\n            xmin = kwargs.get('xmin', None)\n            xmax = kwargs.get('xmax', None)\n            auto = False  # turn off autoscaling, unless...\n            if xmin is None and xmax is None:\n                auto = None  # leave autoscaling state alone\n            xmin, xmax = self.set_xlim(xmin, xmax, emit=emit, auto=auto)\n\n            ymin = kwargs.get('ymin', None)\n            ymax = kwargs.get('ymax', None)\n            auto = False  # turn off autoscaling, unless...\n            if ymin is None and ymax is None:\n                auto = None  # leave autoscaling state alone\n            ymin, ymax = self.set_ylim(ymin, ymax, emit=emit, auto=auto)\n            return xmin, xmax, ymin, ymax\n\n        v = args[0]\n        if isinstance(v, bool):\n            if v:\n                self.set_axis_on()\n            else:\n                self.set_axis_off()\n            xmin, xmax = self.get_xlim()\n            ymin, ymax = self.get_ylim()\n            return xmin, xmax, ymin, ymax\n\n        if len(v) != 4:\n            raise ValueError('args must contain [xmin xmax ymin ymax]')\n\n        self.set_xlim([v[0], v[1]], emit=emit, auto=False)\n        self.set_ylim([v[2], v[3]], emit=emit, auto=False)\n\n        return v```\n\nUpdated code:\n```python\ndef axis(self, *args, emit=True, **kwargs):\n        \n        \n        \n        if len(args) == 1 and isinstance(args[0], (str, bool)):\n            s = args[0]\n            if s is True:\n                s = 'on'\n            if s is False:\n                s = 'off'\n            s = s.lower()\n            if s == 'on':\n                self.set_axis_on()\n            elif s == 'off':\n                self.set_axis_off()\n            elif s in ('equal', 'tight', 'scaled', 'normal',\n                       'auto', 'image', 'square'):\n                if s == 'normal':\n                    cbook.warn_deprecated(\n                        \"3.1\", message=\"Passing 'normal' to axis() is \"\n                        \"deprecated since %(since)s; use 'auto' instead.\")\n                self.set_autoscale_on(True)\n                self.set_aspect('auto')\n                self.autoscale_view(tight=False)\n                # self.apply_aspect()\n                if s == 'equal':\n                    self.set_aspect('equal', adjustable='datalim')\n                elif s == 'scaled':\n                    self.set_aspect('equal', adjustable='box', anchor='C')\n                    self.set_autoscale_on(False)  # Req. by Mark Bakker\n                elif s == 'tight':\n                    self.autoscale_view(tight=True)\n                    self.set_autoscale_on(False)\n                elif s == 'image':\n                    self.autoscale_view(tight=True)\n                    self.set_autoscale_on(False)\n                    self.set_aspect('equal', adjustable='box', anchor='C')\n                elif s == 'square':\n                    self.set_aspect('equal', adjustable='box', anchor='C')\n                    self.set_autoscale_on(False)\n                    xlim = self.get_xlim()\n                    ylim = self.get_ylim()\n                    edge_size = max(np.diff(xlim), np.diff(ylim))\n                    self.set_xlim([xlim[0], xlim[0] + edge_size],\n                                  emit=emit, auto=False)\n                    self.set_ylim([ylim[0], ylim[0] + edge_size],\n                                  emit=emit, auto=False)\n            else:\n                raise ValueError('Unrecognized string %s to axis; '\n                                 'try on or off' % s)\n        else:\n            if len(args) >= 1:\n                if len(args) != 1:\n                    cbook.warn_deprecated(\n                        \"3.2\", message=\"Passing more than one positional \"\n                        \"argument to axis() is deprecated and will raise a \"\n                        \"TypeError %(removal)s.\")\n                v = args[0]\n                try:\n                    xmin, xmax, ymin, ymax = v\n                except ValueError:\n                    raise ValueError('args must contain [xmin xmax ymin ymax]')\n            else:\n                xmin = kwargs.pop('xmin', None)\n                xmax = kwargs.pop('xmax', None)\n                ymin = kwargs.pop('ymin', None)\n                ymax = kwargs.pop('ymax', None)\n            xauto = (None  # Keep autoscale state as is.\n                     if xmin is None and xmax is None\n                     else False)  # Turn off autoscale.\n            yauto = (None\n                     if ymin is None and ymax is None\n                     else False)\n            self.set_xlim(xmin, xmax, emit=emit, auto=xauto)\n            self.set_ylim(ymin, ymax, emit=emit, auto=yauto)\n        if kwargs:\n            cbook.warn_deprecated(\n                \"3.1\", message=\"Passing unsupported keyword arguments to \"\n                \"axis() will raise a TypeError %(removal)s.\")\n        return (*self.get_xlim(), *self.get_ylim())\n```\n\nOriginal documentation:\n```python\nConvenience method to get or set some axis properties.\n\n        Call signatures::\n\n          xmin, xmax, ymin, ymax = axis()\n          xmin, xmax, ymin, ymax = axis([xmin, xmax, ymin, ymax])\n          xmin, xmax, ymin, ymax = axis(option)\n          xmin, xmax, ymin, ymax = axis(**kwargs)\n\n        Parameters\n        ----------\n        xmin, xmax, ymin, ymax : float, optional\n            The axis limits to be set. Either none or all of the limits must\n            be given. This can also be achieved using ::\n\n                ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n\n        option : bool or str\n            If a bool, turns axis lines and labels on or off. If a string,\n            possible values are:\n\n            ======== ==========================================================\n            Value    Description\n            ======== ==========================================================\n            'on'     Turn on axis lines and labels. Same as ``True``.\n            'off'    Turn off axis lines and labels. Same as ``False``.\n            'equal'  Set equal scaling (i.e., make circles circular) by\n                     changing axis limits.\n            'scaled' Set equal scaling (i.e., make circles circular) by\n                     changing dimensions of the plot box.\n            'tight'  Set limits just large enough to show all data.\n            'auto'   Automatic scaling (fill plot box with data).\n            'normal' Same as 'auto'; deprecated.\n            'image'  'scaled' with axis limits equal to data limits.\n            'square' Square plot; similar to 'scaled', but initially forcing\n                     ``xmax-xmin = ymax-ymin``.\n            ======== ==========================================================\n\n        emit : bool, optional, default *True*\n            Whether observers are notified of the axis limit change.\n            This option is passed on to `~.Axes.set_xlim` and\n            `~.Axes.set_ylim`.\n\n        Returns\n        -------\n        xmin, xmax, ymin, ymax : float\n            The axis limits.\n\n        See also\n        --------\n        matplotlib.axes.Axes.set_xlim\n        matplotlib.axes.Axes.set_ylim\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convenience method to get or set some axis properties.\n\n        Call signatures::\n\n          xmin, xmax, ymin, ymax = axis()\n          xmin, xmax, ymin, ymax = axis([xmin, xmax, ymin, ymax])\n          xmin, xmax, ymin, ymax = axis(option)\n          xmin, xmax, ymin, ymax = axis(**kwargs)\n\n        Parameters\n        ----------\n        xmin, xmax, ymin, ymax : float, optional\n            The axis limits to be set.  This can also be achieved using ::\n\n                ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n\n        option : bool or str\n            If a bool, turns axis lines and labels on or off. If a string,\n            possible values are:\n\n            ======== ==========================================================\n            Value    Description\n            ======== ==========================================================\n            'on'     Turn on axis lines and labels. Same as ``True``.\n            'off'    Turn off axis lines and labels. Same as ``False``.\n            'equal'  Set equal scaling (i.e., make circles circular) by\n                     changing axis limits.\n            'scaled' Set equal scaling (i.e., make circles circular) by\n                     changing dimensions of the plot box.\n            'tight'  Set limits just large enough to show all data.\n            'auto'   Automatic scaling (fill plot box with data).\n            'normal' Same as 'auto'; deprecated.\n            'image'  'scaled' with axis limits equal to data limits.\n            'square' Square plot; similar to 'scaled', but initially forcing\n                     ``xmax-xmin = ymax-ymin``.\n            ======== ==========================================================\n\n        emit : bool, optional, default *True*\n            Whether observers are notified of the axis limit change.\n            This option is passed on to `~.Axes.set_xlim` and\n            `~.Axes.set_ylim`.\n\n        Returns\n        -------\n        xmin, xmax, ymin, ymax : float\n            The axis limits.\n\n        See also\n        --------\n        matplotlib.axes.Axes.set_xlim\n        matplotlib.axes.Axes.set_ylim", "file": "lib_matplotlib_axes__base.py", "function": "_AxesBase.axis", "project": "matplotlib", "change_stats": {"code_changes": 88, "doc_changes": 5, "total_changes": 93}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef concatenate_paths(paths):\n    \n    \n    \n\n    vertices = []\n    codes = []\n    for p in paths:\n        p = make_path_regular(p)\n        vertices.append(p.vertices)\n        codes.append(p.codes)\n\n    _path = Path(np.concatenate(vertices),\n                 np.concatenate(codes))\n    return _path```\n\nUpdated code:\n```python\ndef concatenate_paths(paths):\n    \n    vertices = np.concatenate([p.vertices for p in paths])\n    codes = np.concatenate([make_path_regular(p).codes for p in paths])\n    return Path(vertices, codes)\n```\n\nOriginal documentation:\n```python\nconcatenate list of paths into a single path.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Concatenate a list of paths into a single path.", "file": "lib_matplotlib_bezier.py", "function": "concatenate_paths", "project": "matplotlib", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _reduce_scatter_base_validate_mp(\n        self,\n        orig_reduce_scatter,\n        mp_config,\n        *args,\n        **kwargs\n    ):\n        \n        \n        \n        tensors = []\n        for x in args:\n            if isinstance(x, torch.Tensor):\n                tensors.append(x)\n        for _, x in kwargs.items():\n            if isinstance(x, torch.Tensor):\n                tensors.append(x)\n\n        # reduce_dtype has higher priority than param_dtype, because mixed_precision\n        # supports overriding param_dtype with reduce_dtype to control the\n        # reduction precision. In the case where reduce_dtype == param_dtype\n        # this tests that gradients are in the expected precision as well.\n        expected_dtype = mp_config.reduce_dtype\n        for t in tensors:\n            self.assertEqual(expected_dtype, t.dtype)\n\n        return orig_reduce_scatter(*args, **kwargs)```\n\nUpdated code:\n```python\ndef _reduce_scatter_base_validate_mp(\n        self,\n        orig_reduce_scatter,\n        mp_config,\n        *args,\n        **kwargs\n    ):\n        \n        \n        \n        tensors = []\n        for x in args:\n            if isinstance(x, torch.Tensor):\n                tensors.append(x)\n        for _, x in kwargs.items():\n            if isinstance(x, torch.Tensor):\n                tensors.append(x)\n\n        # reduce_dtype has higher priority than param_dtype, because mixed_precision\n        # supports overriding param_dtype with reduce_dtype to control the\n        # reduction precision. In the case where reduce_dtype == param_dtype\n        # this tests that gradients are in the expected precision as well.\n        # If reduce_dtype is not specified (is None) we comm. in the param_dtype\n        # if that is specified, otherwise full precision dtype.\n        expected_dtype = (\n            mp_config.reduce_dtype if mp_config.reduce_dtype is not None\n            else (\n                mp_config.param_dtype if mp_config.param_dtype is not None\n                else _CURRENT_FULL_PRECISION_PARAM_DTYPE\n            )\n        )\n\n        # for t in tensors:\n        #     print(f\"tensor type {t.dtype} expected {expected_dtype}\")\n        for t in tensors:\n            self.assertEqual(expected_dtype, t.dtype)\n\n        return orig_reduce_scatter(*args, **kwargs)\n```\n\nOriginal documentation:\n```python\nPerforms dist._reduce_scatter_base but verifies mixed precision settings\n        before. This is to test mixed precision is working as expected during\n        backward pass.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Performs dist._reduce_scatter_base but verifies mixed precision settings\n        before. This is to test mixed precision is working as expected during\n        backward pass. In particular it ensures that the gradients were cast to the right type\n        and comm. is going to happen in the right type.", "file": "test_distributed_fsdp_test_fsdp_mixed_precision.py", "function": "TestFSDPMixedPrecision._reduce_scatter_base_validate_mp", "project": "pytorch", "change_stats": {"code_changes": 15, "doc_changes": 5, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_contract_order_request(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):\n        \n         \n        \n        market = self.market(symbol)\n        request = {\n            'contract_code': market['id'],\n            'volume': self.amount_to_precision(symbol, amount),\n            'direction': side,\n        }\n        postOnly = None\n        postOnly, params = self.handle_post_only(type == 'market', type == 'post_only', params)\n        if postOnly:\n            type = 'post_only'\n        timeInForce = self.safe_string(params, 'timeInForce', 'GTC')\n        if timeInForce == 'FOK':\n            type = 'fok'\n        elif timeInForce == 'IOC':\n            type = 'ioc'\n        triggerPrice = self.safe_number_2(params, 'stopPrice', 'trigger_price')\n        stopLossTriggerPrice = self.safe_number_2(params, 'stopLossPrice', 'sl_trigger_price')\n        takeProfitTriggerPrice = self.safe_number_2(params, 'takeProfitPrice', 'tp_trigger_price')\n        isStop = triggerPrice is not None\n        isStopLossTriggerOrder = stopLossTriggerPrice is not None\n        isTakeProfitTriggerOrder = takeProfitTriggerPrice is not None\n        if isStop:\n            triggerType = self.safe_string_2(params, 'triggerType', 'trigger_type', 'le')\n            request['trigger_type'] = triggerType\n            request['trigger_price'] = self.price_to_precision(symbol, triggerPrice)\n            if price is not None:\n                request['order_price'] = self.price_to_precision(symbol, price)\n        elif isStopLossTriggerOrder or isTakeProfitTriggerOrder:\n            if isStopLossTriggerOrder:\n                request['sl_order_price_type'] = type\n                request['sl_trigger_price'] = self.price_to_precision(symbol, stopLossTriggerPrice)\n                if price is not None:\n                    request['sl_order_price'] = self.price_to_precision(symbol, price)\n            else:\n                request['tp_order_price_type'] = type\n                request['tp_trigger_price'] = self.price_to_precision(symbol, takeProfitTriggerPrice)\n                if price is not None:\n                    request['tp_order_price'] = self.price_to_precision(symbol, price)\n        else:\n            clientOrderId = self.safe_integer_2(params, 'client_order_id', 'clientOrderId')\n            if clientOrderId is not None:\n                request['client_order_id'] = clientOrderId\n                params = self.omit(params, ['clientOrderId'])\n            if type == 'limit' or type == 'ioc' or type == 'fok' or type == 'post_only':\n                request['price'] = self.price_to_precision(symbol, price)\n        if not isStopLossTriggerOrder and not isTakeProfitTriggerOrder:\n            leverRate = self.safe_integer_2(params, 'leverRate', 'lever_rate', 1)\n            reduceOnly = self.safe_value_2(params, 'reduceOnly', 'reduce_only', False)\n            openOrClose = 'close' if (reduceOnly) else 'open'\n            offset = self.safe_string(params, 'offset', openOrClose)\n            request['offset'] = offset\n            if reduceOnly:\n                request['reduce_only'] = 1\n            request['lever_rate'] = leverRate\n            request['order_price_type'] = type\n        broker = self.safe_value(self.options, 'broker', {})\n        brokerId = self.safe_string(broker, 'id')\n        request['channel_code'] = brokerId\n        params = self.omit(params, ['reduceOnly', 'stopPrice', 'stopLossPrice', 'takeProfitPrice', 'triggerType', 'leverRate', 'timeInForce'])\n        return self.extend(request, params)```\n\nUpdated code:\n```python\ndef create_contract_order_request(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):\n        \n         \n        \n        market = self.market(symbol)\n        request = {\n            'contract_code': market['id'],\n            'volume': self.amount_to_precision(symbol, amount),\n            'direction': side,\n        }\n        postOnly = None\n        postOnly, params = self.handle_post_only(type == 'market', type == 'post_only', params)\n        if postOnly:\n            type = 'post_only'\n        timeInForce = self.safe_string(params, 'timeInForce', 'GTC')\n        if timeInForce == 'FOK':\n            type = 'fok'\n        elif timeInForce == 'IOC':\n            type = 'ioc'\n        triggerPrice = self.safe_number_2(params, 'stopPrice', 'trigger_price')\n        stopLossTriggerPrice = self.safe_number_2(params, 'stopLossPrice', 'sl_trigger_price')\n        takeProfitTriggerPrice = self.safe_number_2(params, 'takeProfitPrice', 'tp_trigger_price')\n        trailingPercent = self.safe_string_2(params, 'trailingPercent', 'callback_rate')\n        trailingTriggerPrice = self.safe_number(params, 'trailingTriggerPrice', price)\n        isTrailingPercentOrder = trailingPercent is not None\n        isStop = triggerPrice is not None\n        isStopLossTriggerOrder = stopLossTriggerPrice is not None\n        isTakeProfitTriggerOrder = takeProfitTriggerPrice is not None\n        if isStop:\n            triggerType = self.safe_string_2(params, 'triggerType', 'trigger_type', 'le')\n            request['trigger_type'] = triggerType\n            request['trigger_price'] = self.price_to_precision(symbol, triggerPrice)\n            if price is not None:\n                request['order_price'] = self.price_to_precision(symbol, price)\n        elif isStopLossTriggerOrder or isTakeProfitTriggerOrder:\n            if isStopLossTriggerOrder:\n                request['sl_order_price_type'] = type\n                request['sl_trigger_price'] = self.price_to_precision(symbol, stopLossTriggerPrice)\n                if price is not None:\n                    request['sl_order_price'] = self.price_to_precision(symbol, price)\n            else:\n                request['tp_order_price_type'] = type\n                request['tp_trigger_price'] = self.price_to_precision(symbol, takeProfitTriggerPrice)\n                if price is not None:\n                    request['tp_order_price'] = self.price_to_precision(symbol, price)\n        elif isTrailingPercentOrder:\n            trailingPercentString = Precise.string_div(trailingPercent, '100')\n            request['callback_rate'] = self.parse_to_numeric(trailingPercentString)\n            request['active_price'] = trailingTriggerPrice\n            request['order_price_type'] = self.safe_string(params, 'order_price_type', 'formula_price')\n        else:\n            clientOrderId = self.safe_integer_2(params, 'client_order_id', 'clientOrderId')\n            if clientOrderId is not None:\n                request['client_order_id'] = clientOrderId\n                params = self.omit(params, ['clientOrderId'])\n            if type == 'limit' or type == 'ioc' or type == 'fok' or type == 'post_only':\n                request['price'] = self.price_to_precision(symbol, price)\n        if not isStopLossTriggerOrder and not isTakeProfitTriggerOrder:\n            leverRate = self.safe_integer_n(params, ['leverRate', 'lever_rate', 'leverage'], 1)\n            reduceOnly = self.safe_value_2(params, 'reduceOnly', 'reduce_only', False)\n            openOrClose = 'close' if (reduceOnly) else 'open'\n            offset = self.safe_string(params, 'offset', openOrClose)\n            request['offset'] = offset\n            if reduceOnly:\n                request['reduce_only'] = 1\n            request['lever_rate'] = leverRate\n            if not isTrailingPercentOrder:\n                request['order_price_type'] = type\n        broker = self.safe_value(self.options, 'broker', {})\n        brokerId = self.safe_string(broker, 'id')\n        request['channel_code'] = brokerId\n        params = self.omit(params, ['reduceOnly', 'stopPrice', 'stopLossPrice', 'takeProfitPrice', 'triggerType', 'leverRate', 'timeInForce', 'leverage', 'trailingPercent', 'trailingTriggerPrice'])\n        return self.extend(request, params)\n```\n\nOriginal documentation:\n```python\n* @ignore\n        helper function to build request\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'market' or 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much you want to trade in units of the base currency\n        :param float [price]: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.timeInForce]: supports 'IOC' and 'FOK'\n        :returns dict: request to be sent to the exchange\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "* @ignore\n        helper function to build request\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'market' or 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much you want to trade in units of the base currency\n        :param float [price]: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.timeInForce]: supports 'IOC' and 'FOK'\n        :param float [params.trailingPercent]: *contract only* the percent to trail away from the current market price\n        :param float [params.trailingTriggerPrice]: *contract only* the price to trigger a trailing order, default uses the price argument\n        :returns dict: request to be sent to the exchange", "file": "python_ccxt_htx.py", "function": "htx.create_contract_order_request", "project": "ccxt", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef batch_score(\n        self,\n        hyps: List[Hypothesis],\n    ) -> Tuple[torch.Tensor, None]:\n        \n\n        \n        labels = torch.LongTensor([[h.yseq[-1]] for h in hyps], device=self.device)\n        dec_embed = self.embed(labels)\n\n        return dec_embed.squeeze(1), None```\n\nUpdated code:\n```python\ndef batch_score(self, hyps: List[Hypothesis]) -> Tuple[torch.Tensor, None]:\n        \n\n        \n        labels = torch.LongTensor([[h.yseq[-1]] for h in hyps], device=self.device)\n        embed = self.embed(labels)\n\n        return embed.squeeze(1), None\n```\n\nOriginal documentation:\n```python\nOne-step forward hypotheses.\n\n        Args:\n            hyps: Hypotheses.\n\n        Returns:\n            dec_out: Decoder output sequences. (B, D_dec)\n            states: Decoder hidden states. None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "One-step forward hypotheses.\n\n        Args:\n            hyps: Hypotheses.\n\n        Returns:\n            out: Decoder output sequences. (B, D_dec)\n            states: Decoder hidden states. None", "file": "espnet2_asr_transducer_decoder_stateless_decoder.py", "function": "StatelessDecoder.batch_score", "project": "espnet", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _read_register(self, register) -> Optional[float]:\n        \n        try:\n            result = self._hub.read_holding_registers(\n                self._slave, register, self._count\n            )\n        except ConnectionException:\n            self._set_unavailable(register)\n            return\n\n        if isinstance(result, (ModbusException, ExceptionResponse)):\n            self._set_unavailable(register)\n            return\n\n        byte_string = b\"\".join(\n            [x.to_bytes(2, byteorder=\"big\") for x in result.registers]\n        )\n        val = struct.unpack(self._structure, byte_string)[0]\n        register_value = format(\n            (self._scale * val) + self._offset, f\".{self._precision}f\"\n        )\n        register_value = float(register_value)\n        self._available = True\n\n        return register_value```\n\nUpdated code:\n```python\ndef _read_register(self, register_type, register) -> Optional[float]:\n        \n        try:\n            if register_type == DEFAULT_REGISTER_TYPE_INPUT:\n                result = self._hub.read_input_registers(\n                    self._slave, register, self._count\n                )\n            else:\n                result = self._hub.read_holding_registers(\n                    self._slave, register, self._count\n                )\n        except ConnectionException:\n            self._set_unavailable(register)\n            return\n\n        if isinstance(result, (ModbusException, ExceptionResponse)):\n            self._set_unavailable(register)\n            return\n\n        byte_string = b\"\".join(\n            [x.to_bytes(2, byteorder=\"big\") for x in result.registers]\n        )\n        val = struct.unpack(self._structure, byte_string)[0]\n        register_value = format(\n            (self._scale * val) + self._offset, f\".{self._precision}f\"\n        )\n        register_value = float(register_value)\n        self._available = True\n\n        return register_value\n```\n\nOriginal documentation:\n```python\nRead holding register using the Modbus hub slave.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Read register using the Modbus hub slave.", "file": "homeassistant_components_modbus_climate.py", "function": "ModbusThermostat._read_register", "project": "core", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_deposit_address(self, code: str, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        chainName = self.safe_string(params, 'chainName')\n        params = self.omit(params, 'chainName')\n        request = {\n            'asset': currency['id'],\n        }\n        response = self.v1PrivateGetWalletDepositAddress(self.extend(request, params))\n        #\n        #     {\n        #         \"code\":0,\n        #         \"data\":{\n        #             \"asset\":\"USDT\",\n        #             \"assetName\":\"Tether\",\n        #             \"address\":[\n        #                 {\n        #                     \"address\":\"1N22odLHXnLPCjC8kwBJPTayarr9RtPod6\",\n        #                     \"destTag\":\"\",\n        #                     \"tagType\":\"\",\n        #                     \"tagId\":\"\",\n        #                     \"chainName\":\"Omni\",\n        #                     \"numConfirmations\":3,\n        #                     \"withdrawalFee\":4.7,\n        #                     \"nativeScale\":4,\n        #                     \"tips\":[]\n        #                 },\n        #                 {\n        #                     \"address\":\"0xe7c70b4e73b6b450ee46c3b5c0f5fb127ca55722\",\n        #                     \"destTag\":\"\",\n        #                     \"tagType\":\"\",\n        #                     \"tagId\":\"\",\n        #                     \"chainName\":\"ERC20\",\n        #                     \"numConfirmations\":20,\n        #                     \"withdrawalFee\":1.0,\n        #                     \"nativeScale\":4,\n        #                     \"tips\":[]\n        #                 }\n        #             ]\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data', {})\n        addresses = self.safe_value(data, 'address', [])\n        numAddresses = len(addresses)\n        address = None\n        if numAddresses > 1:\n            addressesByChainName = self.index_by(addresses, 'chainName')\n            if chainName is None:\n                chainNames = list(addressesByChainName.keys())\n                chains = ', '.join(chainNames)\n                raise ArgumentsRequired(self.id + ' fetchDepositAddress() returned more than one address, a chainName parameter is required, one of ' + chains)\n            address = self.safe_value(addressesByChainName, chainName, {})\n        else:\n            # first address\n            address = self.safe_value(addresses, 0, {})\n        result = self.parse_deposit_address(address, currency)\n        return self.extend(result, {\n            'info': response,\n        })```\n\nUpdated code:\n```python\ndef fetch_deposit_address(self, code: str, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        networkCode = self.safe_string_2(params, 'network', 'chainName')\n        networkId = self.network_code_to_id(networkCode)\n        params = self.omit(params, ['chainName'])\n        request = {\n            'asset': currency['id'],\n            'blockchain': networkId,\n        }\n        response = self.v1PrivateGetWalletDepositAddress(self.extend(request, params))\n        #\n        #     {\n        #         \"code\":0,\n        #         \"data\":{\n        #             \"asset\":\"USDT\",\n        #             \"assetName\":\"Tether\",\n        #             \"address\":[\n        #                 {\n        #                     \"address\":\"1N22odLHXnLPCjC8kwBJPTayarr9RtPod6\",\n        #                     \"destTag\":\"\",\n        #                     \"tagType\":\"\",\n        #                     \"tagId\":\"\",\n        #                     \"chainName\":\"Omni\",\n        #                     \"numConfirmations\":3,\n        #                     \"withdrawalFee\":4.7,\n        #                     \"nativeScale\":4,\n        #                     \"tips\":[]\n        #                 },\n        #                 {\n        #                     \"address\":\"0xe7c70b4e73b6b450ee46c3b5c0f5fb127ca55722\",\n        #                     \"destTag\":\"\",\n        #                     \"tagType\":\"\",\n        #                     \"tagId\":\"\",\n        #                     \"chainName\":\"ERC20\",\n        #                     \"numConfirmations\":20,\n        #                     \"withdrawalFee\":1.0,\n        #                     \"nativeScale\":4,\n        #                     \"tips\":[]\n        #                 }\n        #             ]\n        #         }\n        #     }\n        #\n        data = self.safe_dict(response, 'data', {})\n        addresses = self.safe_list(data, 'address', [])\n        numAddresses = len(addresses)\n        address = None\n        if numAddresses > 1:\n            addressesByChainName = self.index_by(addresses, 'chainName')\n            if networkId is None:\n                chainNames = list(addressesByChainName.keys())\n                chains = ', '.join(chainNames)\n                raise ArgumentsRequired(self.id + ' fetchDepositAddress() returned more than one address, a chainName parameter is required, one of ' + chains)\n            address = self.safe_dict(addressesByChainName, networkId, {})\n        else:\n            # first address\n            address = self.safe_dict(addresses, 0, {})\n        result = self.parse_deposit_address(address, currency)\n        return self.extend(result, {\n            'info': response,\n        })\n```\n\nOriginal documentation:\n```python\nfetch the deposit address for a currency associated with self account\n        :param str code: unified currency code\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :returns dict: an `address structure <https://docs.ccxt.com/#/?id=address-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch the deposit address for a currency associated with self account\n        :see: https://ascendex.github.io/ascendex-pro-api/#query-deposit-addresses\n        :param str code: unified currency code\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.network]: unified network code for deposit chain\n        :returns dict: an `address structure <https://docs.ccxt.com/#/?id=address-structure>`", "file": "python_ccxt_ascendex.py", "function": "ascendex.fetch_deposit_address", "project": "ccxt", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _rel_message_callback(self, message):\n        \n        if self._relay_addr == message.address and self._relay_chan == message.channel:\n            _LOGGER.debug(\n                \"Relay %d:%d value:%d\", message.address, message.channel, message.value\n            )\n            self._state = message.value\n            self.schedule_update_ha_state()```\n\nUpdated code:\n```python\ndef _rel_message_callback(self, message):\n        \n\n        if self._relay_addr == message.address and self._relay_chan == message.channel:\n            _LOGGER.debug(\n                \"%s %d:%d value:%d\",\n                \"Relay\" if message.type == message.RELAY else \"ZoneExpander\",\n                message.address,\n                message.channel,\n                message.value,\n            )\n            self._state = message.value\n            self.schedule_update_ha_state()\n```\n\nOriginal documentation:\n```python\nUpdate relay state.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update relay / expander state.", "file": "homeassistant_components_alarmdecoder_binary_sensor.py", "function": "AlarmDecoderBinarySensor._rel_message_callback", "project": "core", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef EER(positive_scores, negative_scores):\n    \n    \n\n    # Computing candidate thresholds\n    thresholds, _ = torch.sort(torch.cat([positive_scores, negative_scores]))\n    thresholds = torch.unique(thresholds)\n\n    # Adding intermediate thresholds\n    interm_thresholds = (thresholds[0:-1] + thresholds[1:]) / 2\n    thresholds, _ = torch.sort(torch.cat([thresholds, interm_thresholds]))\n\n    # Computing False Rejection Rate\n    positive_scores = torch.cat(\n        len(thresholds) * [positive_scores.unsqueeze(0)]\n    )\n    pos_scores_threshold = positive_scores.transpose(0, 1) <= thresholds\n    FRR = (pos_scores_threshold.sum(0)).float() / positive_scores.shape[1]\n    del positive_scores\n    del pos_scores_threshold\n\n    # Computing False Aceptance Rate\n    negative_scores = torch.cat(\n        len(thresholds) * [negative_scores.unsqueeze(0)]\n    )\n    neg_scores_threshold = negative_scores.transpose(0, 1) > thresholds\n    FAR = (neg_scores_threshold.sum(0)).float() / negative_scores.shape[1]\n    del negative_scores\n    del neg_scores_threshold\n\n    # Finding the threshold for EER\n    min_index = (FAR - FRR).abs().argmin()\n\n    # It is possible that eer != fpr != fnr. We return (FAR  + FRR) / 2 as EER.\n    EER = (FAR[min_index] + FRR[min_index]) / 2\n\n    return float(EER), float(thresholds[min_index])```\n\nUpdated code:\n```python\ndef EER(positive_scores, negative_scores):\n    \n    \n\n    # Computing candidate thresholds\n    thresholds, _ = torch.sort(torch.cat([positive_scores, negative_scores]))\n    thresholds = torch.unique(thresholds)\n\n    # Adding intermediate thresholds\n    interm_thresholds = (thresholds[0:-1] + thresholds[1:]) / 2\n    thresholds, _ = torch.sort(torch.cat([thresholds, interm_thresholds]))\n\n    # Computing False Rejection Rate (miss detection)\n    positive_scores = torch.cat(\n        len(thresholds) * [positive_scores.unsqueeze(0)]\n    )\n    pos_scores_threshold = positive_scores.transpose(0, 1) <= thresholds\n    FRR = (pos_scores_threshold.sum(0)).float() / positive_scores.shape[1]\n    del positive_scores\n    del pos_scores_threshold\n\n    # Computing False Aceptance Rate (false alarm)\n    negative_scores = torch.cat(\n        len(thresholds) * [negative_scores.unsqueeze(0)]\n    )\n    neg_scores_threshold = negative_scores.transpose(0, 1) > thresholds\n    FAR = (neg_scores_threshold.sum(0)).float() / negative_scores.shape[1]\n    del negative_scores\n    del neg_scores_threshold\n\n    # Finding the threshold for EER\n    min_index = (FAR - FRR).abs().argmin()\n\n    # It is possible that eer != fpr != fnr. We return (FAR  + FRR) / 2 as EER.\n    EER = (FAR[min_index] + FRR[min_index]) / 2\n\n    return float(EER), float(thresholds[min_index])\n```\n\nOriginal documentation:\n```python\nComputes the EER (and its threshold)\n\n    Arguments\n    ---------\n    positive_scores : torch.tensor\n        The scores from entries of the same class.\n    negative_scores : torch.tensor\n        The scores from entries of different classes.\n\n    Example\n    -------\n    >>> postive_scores = torch.tensor([0.6, 0.7, 0.8, 0.5])\n    >>> negative_scores = torch.tensor([0.4, 0.3, 0.2, 0.1])\n    >>> val_eer, threshold = EER(postive_scores, negative_scores)\n    >>> val_eer\n    0.0\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes the EER (and its threshold)\n\n    Arguments\n    ---------\n    positive_scores : torch.tensor\n        The scores from entries of the same class.\n    negative_scores : torch.tensor\n        The scores from entries of different classes.\n\n    Example\n    -------\n    >>> positive_scores = torch.tensor([0.6, 0.7, 0.8, 0.5])\n    >>> negative_scores = torch.tensor([0.4, 0.3, 0.2, 0.1])\n    >>> val_eer, threshold = EER(positive_scores, negative_scores)\n    >>> val_eer\n    0.0", "file": "speechbrain_utils_metric_stats.py", "function": "EER", "project": "speechbrain", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef getatom(self):\n        \n        atomlist = ['']\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.atomends:\n                break\n            else: atomlist.append(self.field[self.pos])\n            self.pos = self.pos + 1\n\n        return ''.join(atomlist)```\n\nUpdated code:\n```python\ndef getatom(self, atomends=None):\n        \n        atomlist = ['']\n        if atomends is None:\n            atomends = self.atomends\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in atomends:\n                break\n            else: atomlist.append(self.field[self.pos])\n            self.pos = self.pos + 1\n\n        return ''.join(atomlist)\n```\n\nOriginal documentation:\n```python\nParse an RFC-822 atom.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parse an RFC 2822 atom.\n\n        Optional atomends specifies a different set of end token delimiters\n        (the default is to use self.atomends).  This is used e.g. in\n        getphraselist() since phrase endings must not include the `.' (which\n        is legal in phrases).", "file": "Lib_email__parseaddr.py", "function": "AddrlistClass.getatom", "project": "cpython", "change_stats": {"code_changes": 8, "doc_changes": 9, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef area_closing(image, area_threshold, connectivity=2):\n    \n    \n    # inversion of the input image\n    if image.dtype in unsigned_int_types:\n        maxval = image.max()\n        image_inv = maxval - image\n    elif image.dtype in signed_int_types:\n        image_inv = -1 - image\n    else:\n        image_inv = (-1) * image\n\n    output = image_inv.copy()\n\n    P, S = build_max_tree(image_inv, connectivity)\n\n    area = _max_tree._compute_area(image_inv.ravel(), P.ravel(), S)\n\n    _max_tree._direct_filter(image_inv.ravel(), output.ravel(), P.ravel(), S,\n                             area, area_threshold)\n\n    # inversion of the output image\n    if image.dtype in unsigned_int_types:\n        output = maxval - output\n    elif image.dtype in signed_int_types:\n        output = -1 - output\n    else:\n        output = (-1) * output\n\n    return output```\n\nUpdated code:\n```python\ndef area_closing(image, area_threshold, connectivity=2,\n                 parent=None, tree_traverser=None):\n    \n    \n    # inversion of the input image\n    image_inv = invert(image)\n    output = image_inv.copy()\n\n    if parent is None or tree_traverser is None:\n        parent, tree_traverser = build_max_tree(image_inv, connectivity)\n\n    area = _max_tree._compute_area(image_inv.ravel(), parent.ravel(), tree_traverser)\n\n    _max_tree._direct_filter(image_inv.ravel(), output.ravel(), parent.ravel(),\n                             tree_traverser, area, area_threshold)\n\n    # inversion of the output image\n    output = invert(output)\n\n    return output\n```\n\nOriginal documentation:\n```python\nPerforms an area closing of the image.\n\n    Area closing removes all dark structures of an image with\n    a surface smaller than area_threshold.\n    The output image is larger than or equal to the input image\n    for every pixel and all local minima have at least a surface of\n    area_threshold pixels.\n\n    Area closings are similar to morphological closings, but\n    they do not use a fixed structuring element, but rather a deformable\n    one, with surface = area_threshold.\n\n    Technically, this operator is based on the max-tree representation of\n    the image.\n\n    Parameters\n    ----------\n    img: ndarray\n        The input image for which the area_closing is to be calculated.\n        This image can be of any type.\n    area_threshold: unsigned int\n        The size parameter (number of pixels).\n    connectivity: unsigned int, optional\n        The neighborhood connectivity. The integer represents the maximum\n        number of orthogonal steps to reach a neighbor. It is 1 for\n        4-connectivity and 2 for 8-connectivity. Default value is 1.\n\n    Returns\n    -------\n    output: ndarray\n        Output image of the same shape and type as img.\n\n    See also\n    --------\n    skimage.morphology.max_tree.area_opening\n    skimage.morphology.max_tree.diameter_opening\n    skimage.morphology.max_tree.diameter_closing\n\n\n    References\n    ----------\n    .. [1] Vincent L., Proc. \"Grayscale area openings and closings,\n           their efficient implementation and applications\",\n           EURASIP Workshop on Mathematical Morphology and its\n           Applications to Signal Processing, Barcelona, Spain, pp.22-27,\n           May 1993.\n    .. [2] Soille, P., \"Morphological Image Analysis: Principles and\n           Applications\" (Chapter 6), 2nd edition (2003), ISBN 3540429883.\n    .. [3] Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive\n           Connected Operators for Image and Sequence Processing.\n           IEEE Transactions on Image Processing, 7(4), 555-570.\n    .. [4] Najman, L., & Couprie, M. (2006). Building the component tree in\n           quasi-linear time. IEEE Transactions on Image Processing, 15(11),\n           3531-3539.\n    .. [5] Carlinet, E., & Geraud, T. (2014). A Comparative Review of\n           Component Tree Computation Algorithms. IEEE Transactions on Image\n           Processing, 23(9), 3885-3895.\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.morphology import max_tree\n\n    >>> import numpy as np\n    >>> from skimage.morphology import attribute\n\n    We create an image (quadratic function with a minimum in the center and\n    4 additional local minima.\n\n    >>> w = 12\n    >>> x, y = np.mgrid[0:w,0:w]\n    >>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n    >>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n    >>> f[9:10,9:11] = 100; f[10,10] = 100\n    >>> f = f.astype(np.int)\n\n    We can calculate the area closing:\n\n    >>> closed = attribute.area_closing(f, 8, connectivity=1)\n\n    All small minima are removed, and the remaining minima have at least\n    a size of 8.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Performs an area closing of the image.\n\n    Area closing removes all dark structures of an image with\n    a surface smaller than area_threshold.\n    The output image is larger than or equal to the input image\n    for every pixel and all local minima have at least a surface of\n    area_threshold pixels.\n\n    Area closings are similar to morphological closings, but\n    they do not use a fixed structuring element, but rather a deformable\n    one, with surface = area_threshold.\n\n    Technically, this operator is based on the max-tree representation of\n    the image.\n\n    Parameters\n    ----------\n    img: ndarray\n        The input image for which the area_closing is to be calculated.\n        This image can be of any type.\n    area_threshold: unsigned int\n        The size parameter (number of pixels).\n    connectivity: unsigned int, optional\n        The neighborhood connectivity. The integer represents the maximum\n        number of orthogonal steps to reach a neighbor. It is 1 for\n        4-connectivity and 2 for 8-connectivity. Default value is 1.\n    parent: ndarray, int64, optional\n        The value of each pixel is the index of its parent in the ravelled\n        array.\n    tree_traverser: 1D array, int64, optional\n        The ordered pixel indices (referring to the ravelled array). The pixels\n        are ordered such that every pixel is preceded by its parent (except for\n        the root which has no parent).\n\n    Returns\n    -------\n    output: ndarray\n        Output image of the same shape and type as img.\n\n    See also\n    --------\n    skimage.morphology.max_tree.area_opening\n    skimage.morphology.max_tree.diameter_opening\n    skimage.morphology.max_tree.diameter_closing\n\n\n    References\n    ----------\n    .. [1] Vincent L., Proc. \"Grayscale area openings and closings,\n           their efficient implementation and applications\",\n           EURASIP Workshop on Mathematical Morphology and its\n           Applications to Signal Processing, Barcelona, Spain, pp.22-27,\n           May 1993.\n    .. [2] Soille, P., \"Morphological Image Analysis: Principles and\n           Applications\" (Chapter 6), 2nd edition (2003), ISBN 3540429883.\n    .. [3] Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive\n           Connected Operators for Image and Sequence Processing.\n           IEEE Transactions on Image Processing, 7(4), 555-570.\n    .. [4] Najman, L., & Couprie, M. (2006). Building the component tree in\n           quasi-linear time. IEEE Transactions on Image Processing, 15(11),\n           3531-3539.\n    .. [5] Carlinet, E., & Geraud, T. (2014). A Comparative Review of\n           Component Tree Computation Algorithms. IEEE Transactions on Image\n           Processing, 23(9), 3885-3895.\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.morphology import max_tree\n\n    >>> import numpy as np\n    >>> from skimage.morphology import attribute\n\n    We create an image (quadratic function with a minimum in the center and\n    4 additional local minima.\n\n    >>> w = 12\n    >>> x, y = np.mgrid[0:w,0:w]\n    >>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n    >>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n    >>> f[9:10,9:11] = 100; f[10,10] = 100\n    >>> f = f.astype(np.int)\n\n    We can calculate the area closing:\n\n    >>> closed = attribute.area_closing(f, 8, connectivity=1)\n\n    All small minima are removed, and the remaining minima have at least\n    a size of 8.", "file": "skimage_morphology_max_tree.py", "function": "area_closing", "project": "scikit-image", "change_stats": {"code_changes": 30, "doc_changes": 9, "total_changes": 39}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef technical_404_response(request, exception):\n    \n    \n    \n    urlconf_is_empty = False\n    try:\n        tried = exception.args[0]['tried']\n    except (IndexError, TypeError):\n        tried = []\n    else:\n        if not tried:\n            # tried exists but is an empty list. The URLconf must've been empty.\n            urlconf_is_empty = True\n\n    t = Template(TECHNICAL_404_TEMPLATE)\n    c = Context({\n        'root_urlconf': settings.ROOT_URLCONF,\n        'urlpatterns': tried,\n        'urlconf_is_empty': urlconf_is_empty,\n        'reason': str(exception),\n        'request': request,\n        'request_protocol': os.environ.get(\"HTTPS\") == \"on\" and \"https\" or \"http\",\n        'settings': dict([(k, getattr(settings, k)) for k in dir(settings) if k.isupper()]),\n    })\n    return HttpResponseNotFound(t.render(c), mimetype='text/html')```\n\nUpdated code:\n```python\ndef technical_404_response(request, exception):\n    \"\"\n    try:\n        tried = exception.args[0]['tried']\n    except (IndexError, TypeError):\n        tried = []\n    else:\n        if not tried:\n            # tried exists but is an empty list. The URLconf must've been empty.\n            return empty_urlconf(request)\n\n    t = Template(TECHNICAL_404_TEMPLATE)\n    c = Context({\n        'root_urlconf': settings.ROOT_URLCONF,\n        'urlpatterns': tried,\n        'reason': str(exception),\n        'request': request,\n        'request_protocol': os.environ.get(\"HTTPS\") == \"on\" and \"https\" or \"http\",\n        'settings': get_safe_settings(),\n    })\n    return HttpResponseNotFound(t.render(c), mimetype='text/html')\n```\n\nOriginal documentation:\n```python\nCreate a technical 404 error response.  The exception should be the Http404\n    exception.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a technical 404 error response. The exception should be the Http404.", "file": "django_views_debug.py", "function": "technical_404_response", "project": "django", "change_stats": {"code_changes": 12, "doc_changes": 5, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef should_kill_all_modules(self, function_start_time) -> bool:\n        \n        \n        \n        now = datetime.now()\n        diff = utils.get_time_diff(function_start_time, now, return_type='minutes')\n        return True if diff >= 15 else False```\n\nUpdated code:\n```python\ndef should_kill_all_modules(self, function_start_time, wait_for_modules_to_finish) -> bool:\n        \n        \n        \n        now = datetime.now()\n        diff = utils.get_time_diff(function_start_time, now, return_type='minutes')\n        return True if diff >= wait_for_modules_to_finish else False\n```\n\nOriginal documentation:\n```python\nchecks if 15 minutes has passed since the start of the function\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "checks if x minutes has passed since the start of the function\n        :param wait_for_modules_to_finish: time in mins to wait before force killing all modules\n                                            defined by wait_for_modules_to_finish in slips.conf", "file": "slips.py", "function": "Main.should_kill_all_modules", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _normalize(self, name, columns, points):\n        \n        ret = []\n\n        # Build initial dict by crossing columns and point\n        data_dict = dict(zip(columns, points))\n\n        # issue1871 - Check if a key exist. If a key exist, the value of\n        # the key should be used as a tag to identify the measurement.\n        keys_list = [k.split('.')[0] for k in columns if k.endswith('.key')]\n        if len(keys_list) == 0:\n            keys_list = [None]\n\n        for measurement in keys_list:\n            # Manage field\n            if measurement is not None:\n                fields = {k.replace('{}.'.format(measurement), ''): data_dict[k]\n                          for k in data_dict\n                          if k.startswith('{}.'.format(measurement))}\n            else:\n                fields = data_dict\n            # Transform to InfluxDB datamodel\n            # https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_reference/\n            for k in fields:\n                #  Do not export empty (None) value\n                if fields[k] is None:\n                    continue\n                # Convert numerical to float\n                try:\n                    fields[k] = float(fields[k])\n                except (TypeError, ValueError):\n                    # Convert others to string\n                    try:\n                        fields[k] = str(fields[k])\n                    except (TypeError, ValueError):\n                        pass\n            # Manage tags\n            tags = self.parse_tags(self.tags)\n            if 'key' in fields and fields['key'] in fields:\n                # Create a tag from the key\n                # Tag should be an string (see InfluxDB data model)\n                tags[fields['key']] = str(fields[fields['key']])\n                # Remove it from the field list (can not be a field and a tag)\n                fields.pop(fields['key'])\n            # Add the hostname as a tag\n            tags['hostname'] = self.hostname\n            # Add the measurement to the list\n            ret.append({'measurement': name,\n                        'tags': tags,\n                        'fields': fields})\n        return ret```\n\nUpdated code:\n```python\ndef _normalize(self, name, columns, points):\n        \n        \n        ret = []\n\n        # Build initial dict by crossing columns and point\n        data_dict = dict(zip(columns, points))\n\n        # issue1871 - Check if a key exist. If a key exist, the value of\n        # the key should be used as a tag to identify the measurement.\n        keys_list = [k.split('.')[0] for k in columns if k.endswith('.key')]\n        if len(keys_list) == 0:\n            keys_list = [None]\n\n        for measurement in keys_list:\n            # Manage field\n            if measurement is not None:\n                fields = {\n                    k.replace('{}.'.format(measurement), ''): data_dict[k]\n                    for k in data_dict\n                    if k.startswith('{}.'.format(measurement))\n                }\n            else:\n                fields = data_dict\n            # Transform to InfluxDB data model\n            # https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_reference/\n            for k in fields:\n                #  Do not export empty (None) value\n                if fields[k] is None:\n                    continue\n                # Convert numerical to float\n                try:\n                    fields[k] = float(fields[k])\n                except (TypeError, ValueError):\n                    # Convert others to string\n                    try:\n                        fields[k] = str(fields[k])\n                    except (TypeError, ValueError):\n                        pass\n            # Manage tags\n            tags = self.parse_tags(self.tags)\n            if 'key' in fields and fields['key'] in fields:\n                # Create a tag from the key\n                # Tag should be an string (see InfluxDB data model)\n                tags[fields['key']] = str(fields[fields['key']])\n                # Remove it from the field list (can not be a field and a tag)\n                fields.pop(fields['key'])\n            # Add the hostname as a tag\n            tags['hostname'] = self.hostname\n            # Add name as a tag (example for the process list)\n            for k in FIELD_TO_TAG:\n                if k in fields:\n                    tags[k] = str(fields[k])\n                    # Remove it from the field list (can not be a field and a tag)\n                    if k in fields:\n                        fields.pop(fields[k])\n            # Add the measurement to the list\n            ret.append({'measurement': name, 'tags': tags, 'fields': fields})\n        return ret\n```\n\nOriginal documentation:\n```python\nNormalize data for the InfluxDB's data model.\n        Output is a list of measurements.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Normalize data for the InfluxDB's data model.\n\n        :return: a list of measurements.", "file": "glances_exports_glances_influxdb___init__.py", "function": "Export._normalize", "project": "glances", "change_stats": {"code_changes": 24, "doc_changes": 5, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_successful_subscriptions_add(self):\n        \n        \n        \n        self.assertNotEqual(len(self.streams), 0)  # necessary for full test coverage\n        add_streams = self.make_random_stream_names(self.streams)\n        self.assertNotEqual(len(add_streams), 0)  # necessary for full test coverage\n        events = []\n        with tornado_redirected_to_list(events):\n            self.helper_check_subs_before_and_after_add(self.streams + add_streams, {},\n                add_streams, self.streams, self.test_email, self.streams + add_streams)\n        self.assert_length(events, 4, True)```\n\nUpdated code:\n```python\ndef test_successful_subscriptions_add(self):\n        \n        \n        \n        self.assertNotEqual(len(self.streams), 0)  # necessary for full test coverage\n        add_streams = [\"Verona2\", \"Denmark5\"]\n        self.assertNotEqual(len(add_streams), 0)  # necessary for full test coverage\n        events = []\n        with tornado_redirected_to_list(events):\n            self.helper_check_subs_before_and_after_add(self.streams + add_streams, {},\n                add_streams, self.streams, self.test_email, self.streams + add_streams)\n        self.assert_length(events, 4, True)\n```\n\nOriginal documentation:\n```python\nCalling POST /json/users/me/subscriptions should successfully add streams, and\n        should determine which are new subscriptions vs which were already\n        subscribed. We randomly generate stream names to add, because it\n        doesn't matter whether the stream already exists.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calling POST /json/users/me/subscriptions should successfully add\n        streams, and should determine which are new subscriptions vs\n        which were already subscribed. We add 2 new streams to the\n        list of subscriptions and confirm the right number of events\n        are generated.", "file": "zerver_tests_test_subs.py", "function": "SubscriptionAPITest.test_successful_subscriptions_add", "project": "zulip", "change_stats": {"code_changes": 4, "doc_changes": 11, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef loaded_api():\n    \n    \n    if 'PyQt4.QtCore' in sys.modules:\n        if qtapi_version() == 2:\n            return QT_API_PYQT\n        else:\n            return QT_API_PYQTv1\n    elif 'PySide.QtCore' in sys.modules:\n        return QT_API_PYSIDE\n    elif 'PySide2.QtCore' in sys.modules:\n        return QT_API_PYSIDE2\n    elif 'PyQt5.QtCore' in sys.modules:\n        return QT_API_PYQT5\n    return None```\n\nUpdated code:\n```python\ndef loaded_api():\n    \n    \n    if sys.modules.get(\"PyQt6.QtCore\"):\n        return QT_API_PYQT6\n    elif sys.modules.get(\"PySide6.QtCore\"):\n        return QT_API_PYSIDE6\n    elif sys.modules.get(\"PyQt5.QtCore\"):\n        return QT_API_PYQT5\n    elif sys.modules.get(\"PySide2.QtCore\"):\n        return QT_API_PYSIDE2\n    elif sys.modules.get('PyQt4.QtCore'):\n        if qtapi_version() == 2:\n            return QT_API_PYQT\n        else:\n            return QT_API_PYQTv1\n    elif sys.modules.get('PySide.QtCore'):\n        return QT_API_PYSIDE\n\n    return None\n```\n\nOriginal documentation:\n```python\nReturn which API is loaded, if any\n\n    If this returns anything besides None,\n    importing any other Qt binding is unsafe.\n\n    Returns\n    -------\n    None, 'pyside2', 'pyside', 'pyqt', 'pyqt5', or 'pyqtv1'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return which API is loaded, if any\n\n    If this returns anything besides None,\n    importing any other Qt binding is unsafe.\n\n    Returns\n    -------\n    None, 'pyside6', 'pyqt6', 'pyside2', 'pyside', 'pyqt', 'pyqt5', 'pyqtv1'", "file": "IPython_external_qt_loaders.py", "function": "loaded_api", "project": "ipython", "change_stats": {"code_changes": 19, "doc_changes": 4, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _make_zipfile(base_name, base_dir, verbose=0, dry_run=0, logger=None):\n    \n    \n    import zipfile\n\n    zip_filename = base_name + \".zip\"\n    archive_dir = os.path.dirname(base_name)\n\n    if archive_dir and not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    if logger is not None:\n        logger.info(\"creating '%s' and adding '%s' to it\",\n                    zip_filename, base_dir)\n\n    if not dry_run:\n        with zipfile.ZipFile(zip_filename, \"w\",\n                             compression=zipfile.ZIP_DEFLATED) as zf:\n            path = os.path.normpath(base_dir)\n            if path != os.curdir:\n                zf.write(path, path)\n                if logger is not None:\n                    logger.info(\"adding '%s'\", path)\n            for dirpath, dirnames, filenames in os.walk(base_dir):\n                for name in sorted(dirnames):\n                    path = os.path.normpath(os.path.join(dirpath, name))\n                    zf.write(path, path)\n                    if logger is not None:\n                        logger.info(\"adding '%s'\", path)\n                for name in filenames:\n                    path = os.path.normpath(os.path.join(dirpath, name))\n                    if os.path.isfile(path):\n                        zf.write(path, path)\n                        if logger is not None:\n                            logger.info(\"adding '%s'\", path)\n\n    return zip_filename```\n\nUpdated code:\n```python\ndef _make_zipfile(base_name, base_dir, verbose=0, dry_run=0, logger=None):\n    \n    \n    import zipfile  # late import for breaking circular dependency\n\n    zip_filename = base_name + \".zip\"\n    archive_dir = os.path.dirname(base_name)\n\n    if archive_dir and not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    if logger is not None:\n        logger.info(\"creating '%s' and adding '%s' to it\",\n                    zip_filename, base_dir)\n\n    if not dry_run:\n        with zipfile.ZipFile(zip_filename, \"w\",\n                             compression=zipfile.ZIP_DEFLATED) as zf:\n            path = os.path.normpath(base_dir)\n            if path != os.curdir:\n                zf.write(path, path)\n                if logger is not None:\n                    logger.info(\"adding '%s'\", path)\n            for dirpath, dirnames, filenames in os.walk(base_dir):\n                for name in sorted(dirnames):\n                    path = os.path.normpath(os.path.join(dirpath, name))\n                    zf.write(path, path)\n                    if logger is not None:\n                        logger.info(\"adding '%s'\", path)\n                for name in filenames:\n                    path = os.path.normpath(os.path.join(dirpath, name))\n                    if os.path.isfile(path):\n                        zf.write(path, path)\n                        if logger is not None:\n                            logger.info(\"adding '%s'\", path)\n\n    return zip_filename\n```\n\nOriginal documentation:\n```python\nCreate a zip file from all the files under 'base_dir'.\n\n    The output zip file will be named 'base_name' + \".zip\".  Uses either the\n    \"zipfile\" Python module (if available) or the InfoZIP \"zip\" utility\n    (if installed and found on the default search path).  If neither tool is\n    available, raises ExecError.  Returns the name of the output zip\n    file.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a zip file from all the files under 'base_dir'.\n\n    The output zip file will be named 'base_name' + \".zip\".  Returns the\n    name of the output zip file.", "file": "Lib_shutil.py", "function": "_make_zipfile", "project": "cpython", "change_stats": {"code_changes": 4, "doc_changes": 9, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, func, args, kwargs, hooks, unique_arrays, func_name=None):\n        \n        \n        self.func = func\n        # If the type is a tuple, change to a list.\n        self.args = list(args)\n\n        self.kwargs = kwargs\n        self.hooks = hooks\n        self.unique_arrays = unique_arrays\n        self.func_name = func_name```\n\nUpdated code:\n```python\ndef __init__(self, func, args, kwargs, hooks, unique_arrays,\n                 func_name=None):\n        \n        \n        self.func = func\n        self.args = args\n        self.kwargs = kwargs\n        self.hooks = hooks\n        self.unique_arrays = unique_arrays\n        self.func_name = func_name\n```\n\nOriginal documentation:\n```python\nA callable wrapper for a function in the static schedule.\n\n        Args:\n            func (FunctionNode): A function in the static schedule.\n            args: Arguments to 'func'.\n            kwargs: Keyword arguments to 'func'.\n            hooks (list of tuples): A list of hooks that instruct how to\n                update the ndarray references in 'args' so that they\n                refer to the correct master array in 'unique_arrays'.\n            unique_arrays (list of ndarray): The master list of all unique ndarrays\n                that appear in the static schedule.\n            func_name (str): An optional name of the static function.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A callable wrapper for a function in the static schedule.\n\n        Args:\n            func (FunctionNode): A function in the static schedule.\n            args: Arguments to 'func'.\n            kwargs: Keyword arguments to 'func'.\n            hooks (list of tuples): A list of hooks that instruct how to\n                update the ndarray references in 'args' so that they\n                refer to the correct master array in 'unique_arrays'.\n            unique_arrays (list of ndarray): The master list of all unique\n                ndarrays that appear in the static schedule.\n            func_name (str): An optional name of the static function.", "file": "chainer_graph_optimizations_static_graph.py", "function": "ScheduleInfo.__init__", "project": "chainer", "change_stats": {"code_changes": 9, "doc_changes": 6, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mean(self, *args, **kwargs):\n        \n        \n        \n        nv.validate_groupby_func(\"mean\", args, kwargs, [\"numeric_only\"])\n        return self._cython_agg_general(\n            \"mean\", alt=lambda x, axis: Series(x).mean(**kwargs), **kwargs\n        )```\n\nUpdated code:\n```python\ndef mean(self, numeric_only: bool = True):\n        \n        \n        \n        return self._cython_agg_general(\n            \"mean\",\n            alt=lambda x, axis: Series(x).mean(numeric_only=numeric_only),\n            numeric_only=numeric_only,\n        )\n```\n\nOriginal documentation:\n```python\nCompute mean of groups, excluding missing values.\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n               C\n        A B\n        1 2.0  2\n          4.0  1\n        2 3.0  1\n          5.0  2\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute mean of groups, excluding missing values.\n\n        Parameters\n        ----------\n        numeric_only : bool, default True\n            Include only float, int, boolean columns. If None, will attempt to use\n            everything, then use only numeric data.\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n               C\n        A B\n        1 2.0  2\n          4.0  1\n        2 3.0  1\n          5.0  2\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64", "file": "pandas_core_groupby_groupby.py", "function": "GroupBy.mean", "project": "pandas", "change_stats": {"code_changes": 9, "doc_changes": 8, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef incr_mean_variance_axis(X, *, axis, last_mean, last_var, last_n):\n    \n\n    \n    _raise_error_wrong_axis(axis)\n\n    if isinstance(X, sp.csr_matrix):\n        if axis == 0:\n            return _incr_mean_var_axis0(X, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n        else:\n            return _incr_mean_var_axis0(X.T, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n    elif isinstance(X, sp.csc_matrix):\n        if axis == 0:\n            return _incr_mean_var_axis0(X, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n        else:\n            return _incr_mean_var_axis0(X.T, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n    else:\n        _raise_typeerror(X)```\n\nUpdated code:\n```python\ndef incr_mean_variance_axis(X, *, axis, last_mean, last_var, last_n):\n    \n\n    \n    _raise_error_wrong_axis(axis)\n\n    if not isinstance(X, (sp.csr_matrix, sp.csc_matrix)):\n        _raise_typeerror(X)\n\n    if not (np.size(last_mean) == np.size(last_var) == np.size(last_n)):\n        raise ValueError(\n            \"last_mean, last_var, last_n do not have the same shapes.\"\n        )\n\n    if axis == 1:\n        if np.size(last_mean) != X.shape[0]:\n            raise ValueError(\n                f\"If axis=1, then last_mean, last_n, last_var should be of \"\n                f\"size n_samples {X.shape[0]} (Got {np.size(last_mean)}).\"\n            )\n    else:  # axis == 0\n        if np.size(last_mean) != X.shape[1]:\n            raise ValueError(\n                f\"If axis=0, then last_mean, last_n, last_var should be of \"\n                f\"size n_features {X.shape[1]} (Got {np.size(last_mean)}).\"\n            )\n\n    if isinstance(X, sp.csr_matrix):\n        if axis == 0:\n            return _incr_mean_var_axis0(X, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n        else:\n            return _incr_mean_var_axis0(X.T, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n    elif isinstance(X, sp.csc_matrix):\n        if axis == 0:\n            return _incr_mean_var_axis0(X, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n        else:\n            return _incr_mean_var_axis0(X.T, last_mean=last_mean,\n                                        last_var=last_var, last_n=last_n)\n```\n\nOriginal documentation:\n```python\nCompute incremental mean and variance along an axix on a CSR or\n    CSC matrix.\n\n    last_mean, last_var are the statistics computed at the last step by this\n    function. Both must be initialized to 0-arrays of the proper size, i.e.\n    the number of features in X. last_n is the number of samples encountered\n    until now.\n\n    Parameters\n    ----------\n    X : CSR or CSC sparse matrix, shape (n_samples, n_features)\n        Input data.\n\n    axis : int (either 0 or 1)\n        Axis along which the axis should be computed.\n\n    last_mean : float array with shape (n_features,)\n        Array of feature-wise means to update with the new data X.\n\n    last_var : float array with shape (n_features,)\n        Array of feature-wise var to update with the new data X.\n\n    last_n : int with shape (n_features,)\n        Number of samples seen so far, excluded X.\n\n    Returns\n    -------\n\n    means : float array with shape (n_features,)\n        Updated feature-wise means.\n\n    variances : float array with shape (n_features,)\n        Updated feature-wise variances.\n\n    n : int with shape (n_features,)\n        Updated number of seen samples.\n\n    Notes\n    -----\n    NaNs are ignored in the algorithm.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute incremental mean and variance along an axix on a CSR or\n    CSC matrix.\n\n    last_mean, last_var are the statistics computed at the last step by this\n    function. Both must be initialized to 0-arrays of the proper size, i.e.\n    the number of features in X. last_n is the number of samples encountered\n    until now.\n\n    Parameters\n    ----------\n    X : CSR or CSC sparse matrix, shape (n_samples, n_features)\n        Input data.\n\n    axis : int (either 0 or 1)\n        Axis along which the axis should be computed.\n\n    last_mean : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n        Array of means to update with the new data X.\n        Should be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\n\n    last_var : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n        Array of variances to update with the new data X.\n        Should be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\n\n    last_n : ndarray of shape (n_features,) or (n_samples,), dtype=integral\n        Sum of the weights seen so far, excluding the current weights\n        Should be of shape (n_samples,) if axis=0 or (n_features,) if axis=1.\n\n    Returns\n    -------\n    means : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n        Updated feature-wise means if axis = 0 or\n        sample-wise means if axis = 1.\n\n    variances : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n        Updated feature-wise variances if axis = 0 or\n        sample-wise variances if axis = 1.\n\n    n : ndarray of shape (n_features,) or (n_samples,), dtype=integral\n        Updated number of seen samples per feature if axis=0\n        or number of seen features per sample if axis=1.\n\n    Notes\n    -----\n    NaNs are ignored in the algorithm.", "file": "sklearn_utils_sparsefuncs.py", "function": "incr_mean_variance_axis", "project": "scikit-learn", "change_stats": {"code_changes": 25, "doc_changes": 33, "total_changes": 58}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, xs_pad, ilens, ys_pad):\n        \n        \n        # 1. Encoder\n        xs_pad, ys_pad = self.target_language_biasing(xs_pad, ilens, ys_pad)\n        hs_pad, hlens, _ = self.enc(self.dropout(self.embed(xs_pad)), ilens)\n\n        # 3. attention loss\n        loss, acc, ppl = self.dec(hs_pad, hlens, ys_pad)\n        self.acc = acc\n        self.ppl = ppl\n\n        # 5. compute bleu\n        if self.training or not self.report_bleu:\n            self.bleu = 0.0\n        else:\n            lpz = None\n\n            nbest_hyps = self.dec.recognize_beam_batch(\n                hs_pad,\n                torch.tensor(hlens),\n                lpz,\n                self.trans_args,\n                self.char_list,\n                self.rnnlm,\n            )\n            # remove <sos> and <eos>\n            list_of_refs = []\n            hyps = []\n            y_hats = [nbest_hyp[0][\"yseq\"][1:-1] for nbest_hyp in nbest_hyps]\n            for i, y_hat in enumerate(y_hats):\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = \"\".join(seq_hat).replace(self.trans_args.space, \" \")\n                seq_hat_text = seq_hat_text.replace(self.trans_args.blank, \"\")\n                seq_true_text = \"\".join(seq_true).replace(self.trans_args.space, \" \")\n\n                hyps += [seq_hat_text.split(\" \")]\n                list_of_refs += [[seq_true_text.split(\" \")]]\n\n            self.bleu = nltk.corpus_bleu(list_of_refs, hyps) * 100\n\n        self.loss = loss\n\n        loss_data = float(self.loss)\n        if not math.isnan(loss_data):\n            self.reporter.report(loss_data, acc, ppl, self.bleu)\n        else:\n            logging.warning(\"loss (=%f) is not correct\", loss_data)\n        return self.loss```\n\nUpdated code:\n```python\ndef forward(self, xs_pad, ilens, ys_pad):\n        \n        \n        # 1. Encoder\n        xs_pad, ys_pad = self.target_language_biasing(xs_pad, ilens, ys_pad)\n        hs_pad, hlens, _ = self.enc(self.dropout(self.embed(xs_pad)), ilens)\n\n        # 3. attention loss\n        self.loss, self.acc, self.ppl = self.dec(hs_pad, hlens, ys_pad)\n\n        # 4. compute bleu\n        if self.training or not self.report_bleu:\n            self.bleu = 0.0\n        else:\n            lpz = None\n\n            nbest_hyps = self.dec.recognize_beam_batch(\n                hs_pad,\n                torch.tensor(hlens),\n                lpz,\n                self.trans_args,\n                self.char_list,\n                self.rnnlm,\n            )\n            # remove <sos> and <eos>\n            list_of_refs = []\n            hyps = []\n            y_hats = [nbest_hyp[0][\"yseq\"][1:-1] for nbest_hyp in nbest_hyps]\n            for i, y_hat in enumerate(y_hats):\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = \"\".join(seq_hat).replace(self.trans_args.space, \" \")\n                seq_hat_text = seq_hat_text.replace(self.trans_args.blank, \"\")\n                seq_true_text = \"\".join(seq_true).replace(self.trans_args.space, \" \")\n\n                hyps += [seq_hat_text.split(\" \")]\n                list_of_refs += [[seq_true_text.split(\" \")]]\n\n            self.bleu = nltk.corpus_bleu(list_of_refs, hyps) * 100\n\n        loss_data = float(self.loss)\n        if not math.isnan(loss_data):\n            self.reporter.report(loss_data, self.acc, self.ppl, self.bleu)\n        else:\n            logging.warning(\"loss (=%f) is not correct\", loss_data)\n        return self.loss\n```\n\nOriginal documentation:\n```python\nE2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, Lmax)\n        :return: loss value\n        :rtype: torch.Tensor\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: loss value\n        :rtype: torch.Tensor", "file": "espnet_nets_pytorch_backend_e2e_mt.py", "function": "E2E.forward", "project": "espnet", "change_stats": {"code_changes": 12, "doc_changes": 5, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef expand(self, size: tuple[int, int]) -> Region:\n        \n        \n        add_width, add_height = size\n        x, y, width, height = self\n        return Region(x, y, width + add_width, height + add_height)```\n\nUpdated code:\n```python\ndef expand(self, size: tuple[int, int]) -> Region:\n        \n        \n        expand_width, expand_height = size\n        x, y, width, height = self\n        return Region(\n            x - expand_width,\n            y - expand_height,\n            width + expand_width * 2,\n            height + expand_height * 2,\n        )\n```\n\nOriginal documentation:\n```python\nAdd additional height.\n\n        Args:\n            size (tuple[int, int]): Additional width and height.\n\n        Returns:\n            Region: A new region.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Increase the size of the region by adding a border.\n\n        Args:\n            size (tuple[int, int]): Additional width and height.\n\n        Returns:\n            Region: A new region.", "file": "src_textual_geometry.py", "function": "Region.expand", "project": "textual", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef captured_sequence(self, end_time=None):\n    \n    \n    # Make a copy of the sequence currently being captured.\n    current_captured_sequence = music_pb2.NoteSequence()\n    with self._lock:\n      current_captured_sequence.CopyFrom(self._captured_sequence)\n\n    if self.is_alive():\n      if end_time is None:\n        raise MidiHubException(\n            '`end_time` must be provided when capture thread is still running.')\n      for i, note in enumerate(current_captured_sequence.notes):\n        if note.start_time >= end_time:\n          del current_captured_sequence.notes[i:]\n          break\n        if not note.end_time or note.end_time > end_time:\n          note.end_time = end_time\n      current_captured_sequence.total_time = end_time\n    elif end_time is not None:\n      raise MidiHubException(\n          '`end_time` must not be provided when capture is complete.')\n\n    return current_captured_sequence```\n\nUpdated code:\n```python\ndef captured_sequence(self, end_time=None):\n    \n    \n    # Make a copy of the sequence currently being captured.\n    current_captured_sequence = music_pb2.NoteSequence()\n    with self._lock:\n      current_captured_sequence.CopyFrom(self._captured_sequence)\n\n    if self.is_alive():\n      if end_time is None:\n        raise MidiHubError(\n            '`end_time` must be provided when capture thread is still running.')\n      for i, note in enumerate(current_captured_sequence.notes):\n        if note.start_time >= end_time:\n          del current_captured_sequence.notes[i:]\n          break\n        if not note.end_time or note.end_time > end_time:\n          note.end_time = end_time\n      current_captured_sequence.total_time = end_time\n    elif end_time is not None:\n      raise MidiHubError(\n          '`end_time` must not be provided when capture is complete.')\n\n    return current_captured_sequence\n```\n\nOriginal documentation:\n```python\nReturns a copy of the current captured sequence.\n\n    If called before the thread terminates, `end_time` is required and any open\n    notes will have their end time set to it, any notes starting after it will\n    be removed, and any notes ending after it will be truncated. `total_time`\n    will also be set to `end_time`.\n\n    Args:\n      end_time: The float time in seconds to close any open notes and after\n          which to close or truncate notes, if the thread is still alive.\n          Otherwise, must be None.\n\n    Returns:\n      A copy of the current captured NoteSequence proto with open notes closed\n      at and later notes removed or truncated to `end_time`.\n\n    Raises:\n      MidiHubException: When the thread is alive and `end_time` is None or the\n         thread is terminated and `end_time` is not None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a copy of the current captured sequence.\n\n    If called before the thread terminates, `end_time` is required and any open\n    notes will have their end time set to it, any notes starting after it will\n    be removed, and any notes ending after it will be truncated. `total_time`\n    will also be set to `end_time`.\n\n    Args:\n      end_time: The float time in seconds to close any open notes and after\n          which to close or truncate notes, if the thread is still alive.\n          Otherwise, must be None.\n\n    Returns:\n      A copy of the current captured NoteSequence proto with open notes closed\n      at and later notes removed or truncated to `end_time`.\n\n    Raises:\n      MidiHubError: When the thread is alive and `end_time` is None or the\n         thread is terminated and `end_time` is not None.", "file": "magenta_interfaces_midi_midi_hub.py", "function": "MidiCaptor.captured_sequence", "project": "magenta", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_cupy_array(self):\n        \n        \n        \n        if self.base is None:\n            if self._cupy_array is None:\n                self._cupy_array = cp.array(self._numpy_array)\n        else:\n            base_arg = self.base\n            if base_arg._cupy_array is None:\n                base_arg._cupy_array = cp.array(base_arg._numpy_array)\n                base_arg._numpy_array = None # forget\n                self._cupy_array = base_arg._cupy_array.view()\n            else:\n                if self._cupy_array is None:\n                    self._cupy_array = base_arg._cupy_array.view()\n        self._numpy_array = None # forget\n        return self._cupy_array```\n\nUpdated code:\n```python\ndef _get_cupy_array(self):\n        \n        \n        \n        base = self.base\n        if base is not None:\n            base._remember_numpy = False\n        self._remember_numpy = False\n        return self._cupy_array\n```\n\nOriginal documentation:\n```python\nReturns _cupy_array (cupy.ndarray) of ndarray object.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns _cupy_array (cupy.ndarray) of ndarray object. And marks \n        self(ndarray) and it's base (if exist) as numpy not up-to-date.", "file": "cupyx_fallback_mode_fallback.py", "function": "ndarray._get_cupy_array", "project": "cupy", "change_stats": {"code_changes": 19, "doc_changes": 5, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rayleigh_quotient(A: np.matrix, v: np.matrix) -> float:\n    \n    \n    \n    v_star = v.H\n    return (v_star * A * v) / (v_star * v)```\n\nUpdated code:\n```python\ndef rayleigh_quotient(A: np.array, v: np.array) -> float:\n    \n    \n    \n    v_star = v.conjugate().T\n    return (v_star.dot(A).dot(v)) / (v_star.dot(v))\n```\n\nOriginal documentation:\n```python\nReturns the Rayleigh quotient of a Hermitian matrix A and\n    vector v.\n    >>> import numpy as np\n    >>> A = np.matrix([\n    ... [1,  2, 4],\n    ... [2,  3,  -1],\n    ... [4, -1,  1]\n    ... ])\n    >>> v = np.matrix([\n    ... [1],\n    ... [2],\n    ... [3]\n    ... ])\n    >>> rayleigh_quotient(A, v)\n    matrix([[3.]])\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the Rayleigh quotient of a Hermitian matrix A and\n    vector v.\n    >>> import numpy as np\n    >>> A = np.array([\n    ... [1,  2, 4],\n    ... [2,  3,  -1],\n    ... [4, -1,  1]\n    ... ])\n    >>> v = np.array([\n    ... [1],\n    ... [2],\n    ... [3]\n    ... ])\n    >>> rayleigh_quotient(A, v)\n    array([[3.]])", "file": "linear_algebra_src_rayleigh_quotient.py", "function": "rayleigh_quotient", "project": "Python", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_docker_compose_yaml(\n        self,\n        output_path: Optional[str] = None,\n        network_name: Optional[str] = None,\n    ):\n        \n        \n        \n        import yaml\n\n        output_path = output_path or 'docker-compose.yml'\n        network_name = network_name or 'jina-network'\n\n        docker_compose_dict = {\n            'version': '3.3',\n            'networks': {network_name: {'driver': 'bridge'}},\n        }\n        services = {}\n\n        service_configs = self._to_docker_compose_config()\n\n        for service_name, service in service_configs:\n            service['networks'] = [network_name]\n            services[service_name] = service\n\n        if self._include_gateway:\n            service_configs = self._inner_gateway_to_docker_compose_config()\n\n            for service_name, service in service_configs:\n                service['networks'] = [network_name]\n                services[service_name] = service\n\n        docker_compose_dict['services'] = services\n        with open(output_path, 'w+') as fp:\n            yaml.dump(docker_compose_dict, fp, sort_keys=False)\n\n        command = (\n            'docker-compose up'\n            if output_path is None\n            else f'docker-compose -f {output_path} up'\n        )\n\n        self.logger.info(\n            f'Docker compose file has been created under [b]{output_path}[/b]. You can use it by running [b]{command}[/b]'\n        )```\n\nUpdated code:\n```python\ndef to_docker_compose_yaml(\n        self,\n        output_path: Optional[str] = None,\n        network_name: Optional[str] = None,\n    ):\n        \n        \n        \n        import yaml\n\n        output_path = output_path or 'docker-compose.yml'\n        network_name = network_name or 'jina-network'\n\n        docker_compose_dict = {\n            'version': '3.3',\n            'networks': {network_name: {'driver': 'bridge'}},\n        }\n        services = {}\n\n        service_configs = self._to_docker_compose_config()\n\n        for service_name, service in service_configs:\n            service['networks'] = [network_name]\n            services[service_name] = service\n\n        if self._include_gateway:\n            service_configs = self._inner_gateway_to_docker_compose_config()\n\n            for service_name, service in service_configs:\n                service['networks'] = [network_name]\n                services[service_name] = service\n\n        docker_compose_dict['services'] = services\n        with open(output_path, 'w+') as fp:\n            yaml.dump(docker_compose_dict, fp, sort_keys=False)\n\n        command = (\n            'docker-compose up'\n            if output_path is None\n            else f'docker-compose -f {output_path} up'\n        )\n\n        self.logger.info(\n            f'Docker Compose file has been created under [b]{output_path}[/b]. You can use it by running [b]{command}[/b]'\n        )\n```\n\nOriginal documentation:\n```python\nConverts a Jina Deployment into a Docker compose YAML file\n\n        If you don't want to rebuild image on Jina Hub,\n        you can set `JINA_HUB_NO_IMAGE_REBUILD` environment variable.\n\n        :param output_path: The path where to dump the yaml file\n        :param network_name: The name of the network that will be used by the deployment\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Converts a Jina Deployment into a Docker compose YAML file\n\n        If you don't want to rebuild image on Executor Hub,\n        you can set `JINA_HUB_NO_IMAGE_REBUILD` environment variable.\n\n        :param output_path: The path to dump the YAML file to\n        :param network_name: The name of the network that will be used by the deployment", "file": "jina_orchestrate_deployments___init__.py", "function": "Deployment.to_docker_compose_yaml", "project": "jina", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_auth_string(method, path, headers, data=None):\n    \n    \n    \n\n    auth_header = headers.get(\"authorization\", \"\")\n\n    if auth_header:\n        return auth_header\n\n    data_components = parse_request_data(method, path, data)\n    algorithm = data_components.get(\"X-Amz-Algorithm\")\n    credential = data_components.get(\"X-Amz-Credential\")\n    signature = data_components.get(\"X-Amz-Signature\")\n    signed_headers = data_components.get(\"X-Amz-SignedHeaders\")\n\n    if algorithm and credential and signature and signed_headers:\n        return (\n            f\"{algorithm} Credential={credential}, \"\n            + f\"SignedHeaders={signed_headers}, \"\n            + f\"Signature={signature}\"\n        )\n\n    return \"\"```\n\nUpdated code:\n```python\ndef get_auth_string(method, path, headers, data=None):\n    \n    \n    \n\n    if auth_header := headers.get(\"authorization\", \"\"):\n        return auth_header\n\n    data_components = parse_request_data(method, path, data)\n    algorithm = data_components.get(\"X-Amz-Algorithm\")\n    credential = data_components.get(\"X-Amz-Credential\")\n    signature = data_components.get(\"X-Amz-Signature\")\n    signed_headers = data_components.get(\"X-Amz-SignedHeaders\")\n\n    if algorithm and credential and signature and signed_headers:\n        return (\n            f\"{algorithm} Credential={credential}, \"\n            + f\"SignedHeaders={signed_headers}, \"\n            + f\"Signature={signature}\"\n        )\n\n    return \"\"\n```\n\nOriginal documentation:\n```python\nGet Auth header from Header (this is how aws client's like boto typically\n    provide it) or from query string or url encoded parameters (sometimes\n    happens with presigned requests. Always return in the Authorization Header\n    form.\n\n    Typically an auth string comes in as a header:\n\n        Authorization: AWS4-HMAC-SHA256 \\\n        Credential=_not_needed_locally_/20210312/us-east-1/sqs/aws4_request, \\\n        SignedHeaders=content-type;host;x-amz-date, \\\n        Signature=9277c941f4ecafcc0f290728e50cd7a3fa0e41763fbd2373fcdd3faf2dbddc2e\n\n    Here's what Authorization looks like as part of an presigned GET request:\n\n       &X-Amz-Algorithm=AWS4-HMAC-SHA256\\\n       &X-Amz-Credential=test%2F20210313%2Fus-east-1%2Fsqs%2Faws4_request\\\n       &X-Amz-Date=20210313T011059Z&X-Amz-Expires=86400000&X-Amz-SignedHeaders=host\\\n       &X-Amz-Signature=2c652c7bc9a3b75579db3d987d1e6dd056f0ac776c1e1d4ec91e2ce84e5ad3ae\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get Auth header from Header (this is how aws client's like boto typically\n    provide it) or from query string or url encoded parameters sometimes\n    happens with presigned requests. Always return to the Authorization Header\n    form.\n\n    Typically, an auth string comes in as a header:\n\n        Authorization: AWS4-HMAC-SHA256 \\\n        Credential=_not_needed_locally_/20210312/us-east-1/sqs/aws4_request, \\\n        SignedHeaders=content-type;host;x-amz-date, \\\n        Signature=9277c941f4ecafcc0f290728e50cd7a3fa0e41763fbd2373fcdd3faf2dbddc2e\n\n    Here's what Authorization looks like as part of an presigned GET request:\n\n       &X-Amz-Algorithm=AWS4-HMAC-SHA256\\\n       &X-Amz-Credential=test%2F20210313%2Fus-east-1%2Fsqs%2Faws4_request\\\n       &X-Amz-Date=20210313T011059Z&X-Amz-Expires=86400000&X-Amz-SignedHeaders=host\\\n       &X-Amz-Signature=2c652c7bc9a3b75579db3d987d1e6dd056f0ac776c1e1d4ec91e2ce84e5ad3ae", "file": "localstack_services_edge.py", "function": "get_auth_string", "project": "localstack", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef multizone_new_cast_status(self, group_uuid, cast_status):\n        \n        if self._valid:\n            self._cast_device.multizone_new_cast_status(\n                group_uuid, cast_status)```\n\nUpdated code:\n```python\ndef multizone_new_cast_status(self, group_uuid, cast_status):\n        \n        pass\n```\n\nOriginal documentation:\n```python\nHandle reception of a new MediaStatus for a group.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Handle reception of a new CastStatus for a group.", "file": "homeassistant_components_cast_media_player.py", "function": "CastStatusListener.multizone_new_cast_status", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef getlines(filename, module_globals=None):\n    \n\n    if filename in cache:\n        entry = cache[filename]\n        if len(entry) == 1:\n            return updatecache(filename, module_globals)\n        return cache[filename][2]\n    else:\n        return updatecache(filename, module_globals)```\n\nUpdated code:\n```python\ndef getlines(filename, module_globals=None):\n    \n\n    if filename in cache:\n        return cache[filename][2]\n    else:\n        return updatecache(filename, module_globals)\n```\n\nOriginal documentation:\n```python\nGet the lines for a file from the cache.\n    Update the cache if it doesn't contain an entry for this file already.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the lines for a Python source file from the cache.\n    Update the cache if it doesn't contain an entry for this file already.", "file": "Lib_linecache.py", "function": "getlines", "project": "cpython", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef bulk_rename(doctype, rows=None, via_console = False):\n\t\n\tif not rows:\n\t\tfrappe.throw(_(\"Please select a valid csv file with data\"))\n\n\tif not via_console:\n\t\tmax_rows = 500\n\t\tif len(rows) > max_rows:\n\t\t\tfrappe.throw(_(\"Maximum {0} rows allowed\").format(max_rows))\n\n\trename_log = []\n\tfor row in rows:\n\t\t# if row has some content\n\t\tif len(row) > 1 and row[0] and row[1]:\n\t\t\ttry:\n\t\t\t\tif rename_doc(doctype, row[0], row[1]):\n\t\t\t\t\tmsg = _(\"Successful: {0} to {1}\").format(row[0], row[1])\n\t\t\t\t\tfrappe.db.commit()\n\t\t\t\telse:\n\t\t\t\t\tmsg = _(\"Ignored: {0} to {1}\").format(row[0], row[1])\n\t\t\texcept Exception as e:\n\t\t\t\tmsg = _(\"** Failed: {0} to {1}: {2}\").format(row[0], row[1], repr(e))\n\t\t\t\tfrappe.db.rollback()\n\n\t\t\tif via_console:\n\t\t\t\tprint(msg)\n\t\t\telse:\n\t\t\t\trename_log.append(msg)\n\n\tfrappe.enqueue('frappe.utils.global_search.rebuild_for_doctype', doctype=doctype)\n\n\tif not via_console:\n\t\treturn rename_log```\n\nUpdated code:\n```python\ndef bulk_rename(doctype, rows=None, via_console = False):\n\t\n\tif not rows:\n\t\tfrappe.throw(_(\"Please select a valid csv file with data\"))\n\n\tif not via_console:\n\t\tmax_rows = 500\n\t\tif len(rows) > max_rows:\n\t\t\tfrappe.throw(_(\"Maximum {0} rows allowed\").format(max_rows))\n\n\trename_log = []\n\tfor row in rows:\n\t\t# if row has some content\n\t\tif len(row) > 1 and row[0] and row[1]:\n\t\t\tmerge = True if len(row) > 2 and (row[2] == \"1\" or row[2].lower() == \"true\") else False\n\t\t\ttry:\n\t\t\t\tif rename_doc(doctype, row[0], row[1], merge=merge):\n\t\t\t\t\tmsg = _(\"Successful: {0} to {1}\").format(row[0], row[1])\n\t\t\t\t\tfrappe.db.commit()\n\t\t\t\telse:\n\t\t\t\t\tmsg = _(\"Ignored: {0} to {1}\").format(row[0], row[1])\n\t\t\texcept Exception as e:\n\t\t\t\tmsg = _(\"** Failed: {0} to {1}: {2}\").format(row[0], row[1], repr(e))\n\t\t\t\tfrappe.db.rollback()\n\n\t\t\tif via_console:\n\t\t\t\tprint(msg)\n\t\t\telse:\n\t\t\t\trename_log.append(msg)\n\n\tfrappe.enqueue('frappe.utils.global_search.rebuild_for_doctype', doctype=doctype)\n\n\tif not via_console:\n\t\treturn rename_log\n```\n\nOriginal documentation:\n```python\nBulk rename documents\n\n\t:param doctype: DocType to be renamed\n\t:param rows: list of documents as `((oldname, newname), ..)`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Bulk rename documents\n\n\t:param doctype: DocType to be renamed\n\t:param rows: list of documents as `((oldname, newname, merge(optional)), ..)`", "file": "frappe_model_rename_doc.py", "function": "bulk_rename", "project": "frappe", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        input_size = input.size()\n        B = input_size[0]\n        T = input_size[1]\n        C = self.channels\n        D = self.win_length\n        # (B, T, C) --> (T, B, C)\n        continuous = input.view(B, T, C).permute(1, 0, 2)\n        windowed = continuous.unfold(0, D, self.hop_length)\n        # (T, B, C, D) --> (B, T, C, D)\n        output = windowed.permute(1, 0, 2, 3).contiguous()\n        # After unfold(), windowed lengths change:\n        output_lengths = (input_lengths - self.win_length) // self.hop_length + 1\n        return output, output_lengths```\n\nUpdated code:\n```python\ndef forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        \n        input_size = input.size()\n        B = input_size[0]\n        T = input_size[1]\n        C = self.channels\n        D = self.win_length\n        # (B, T, C) --> (T, B, C)\n        continuous = input.view(B, T, C).permute(1, 0, 2)\n        windowed = continuous.unfold(0, D, self.hop_length)\n        # (T, B, C, D) --> (B, T, C, D)\n        output = windowed.permute(1, 0, 2, 3).contiguous()\n        # After unfold(), windowed lengths change:\n        output_lengths = (input_lengths - self.win_length) // self.hop_length + 1\n        return output, output_lengths\n```\n\nOriginal documentation:\n```python\nForward.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward.\n\n        Args:\n            input: Input (B, T, C*D) or (B, T*C*D), with D=1.\n            input_lengths: Input lengths within batch.\n\n        Returns:\n            Tensor: Output with dimensions (B, T, C, D), with D=win_length.\n            Tensor: Output lengths within batch.", "file": "espnet2_asr_frontend_windowing.py", "function": "SlidingWindow.forward", "project": "espnet", "change_stats": {"code_changes": 3, "doc_changes": 10, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef make_random_stream_names(self, existing_stream_names: List[str]) -> List[str]:\n        \n        \n        \n        random_streams = []\n        all_stream_names = [stream.name for stream in Stream.objects.filter(realm=self.test_realm)]\n        for stream in existing_stream_names:\n            random_stream = stream + str(random.randint(0, 9))\n            if random_stream not in all_stream_names:\n                random_streams.append(random_stream)\n        return random_streams```\n\nUpdated code:\n```python\ndef make_random_stream_names(self, existing_stream_names: List[str]) -> List[str]:\n        \n        \n        \n        all_stream_names = [stream.name for stream in Stream.objects.filter(realm=self.test_realm)]\n        return [\n            random_stream\n            for stream in existing_stream_names\n            if (random_stream := stream + str(random.randint(0, 9))) not in all_stream_names\n        ]\n```\n\nOriginal documentation:\n```python\nHelper function to make up random stream names. It takes\n        existing_stream_names and randomly appends a digit to the end of each,\n        but avoids names that appear in the list names_to_avoid.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Helper function to make up random stream names. It takes\n        existing_stream_names and randomly appends a digit to the end of each,\n        but avoids names of streams already in the realm.", "file": "zerver_tests_test_subs.py", "function": "SubscriptionAPITest.make_random_stream_names", "project": "zulip", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef zero_grad(self):\n        \n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is not None:\n                    if p.grad.grad_fn is not None:\n                        p.grad.detach_()\n                    else:\n                        p.grad.requires_grad_(False)\n                    p.grad.zero_()```\n\nUpdated code:\n```python\ndef zero_grad(self, set_to_none: bool = False):\n        \n        \n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is not None:\n                    if set_to_none:\n                        p.grad = None\n                    else:\n                        if p.grad.grad_fn is not None:\n                            p.grad.detach_()\n                        else:\n                            p.grad.requires_grad_(False)\n                        p.grad.zero_()\n```\n\nOriginal documentation:\n```python\nr\"\"\"Clears the gradients of all optimized :class:`torch.Tensor` s.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Set the gradients of all optimized :class:`torch.Tensor` s to zero.\n\n        Arguments:\n            set_to_none (bool): instead of setting to zero, set the grad to None.\n                This is will in general have lower memory footprint, and can modestly improve performance.\n                However, it changes certain behaviors. For example:\n                1. When user tries to access the gradient value and perform manual ops on it.\n                A None attribute or a Tensor full of 0s will be different.\n                2. If the user requests `zero_grad(set_to_none=True)` followed by a backward pass, `.grad` s\n                are guaranteed to be None for params that did not receive a gradient.\n                3. `torch.optim` optimizers have a different behavior if the gradient is 0 or None\n                (in one case it does the step with a gradient of 0 and in the other it skip\n                the step altogether).", "file": "torch_optim_optimizer.py", "function": "Optimizer.zero_grad", "project": "pytorch", "change_stats": {"code_changes": 16, "doc_changes": 16, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef insert(\n        self, loc: int, item: Hashable, value: ArrayLike, allow_duplicates: bool = False\n    ):\n        \n        \n\n        \n        if not allow_duplicates and item in self.items:\n            # Should this be a different kind of error??\n            raise ValueError(f\"cannot insert {item}, already exists\")\n\n        if not isinstance(loc, int):\n            raise TypeError(\"loc must be int\")\n\n        # insert to the axis; this could possibly raise a TypeError\n        new_axis = self.items.insert(loc, item)\n\n        if value.ndim == 2:\n            value = value.T\n        else:\n            value = ensure_block_shape(value, ndim=self.ndim)\n\n        block = new_block(values=value, ndim=self.ndim, placement=slice(loc, loc + 1))\n\n        for blkno, count in _fast_count_smallints(self.blknos[loc:]):\n            blk = self.blocks[blkno]\n            if count == len(blk.mgr_locs):\n                blk.mgr_locs = blk.mgr_locs.add(1)\n            else:\n                new_mgr_locs = blk.mgr_locs.as_array.copy()\n                new_mgr_locs[new_mgr_locs >= loc] += 1\n                blk.mgr_locs = BlockPlacement(new_mgr_locs)\n\n        # Accessing public blklocs ensures the public versions are initialized\n        if loc == self.blklocs.shape[0]:\n            # np.append is a lot faster, let's use it if we can.\n            self._blklocs = np.append(self._blklocs, 0)\n            self._blknos = np.append(self._blknos, len(self.blocks))\n        else:\n            self._blklocs = np.insert(self._blklocs, loc, 0)\n            self._blknos = np.insert(self._blknos, loc, len(self.blocks))\n\n        self.axes[0] = new_axis\n        self.blocks += (block,)\n\n        self._known_consolidated = False\n\n        if len(self.blocks) > 100:\n            warnings.warn(\n                \"DataFrame is highly fragmented.  This is usually the result \"\n                \"of calling `frame.insert` many times, which has poor performance.  \"\n                \"Consider using pd.concat instead.  To get a de-fragmented frame, \"\n                \"use `newframe = frame.copy()`\",\n                PerformanceWarning,\n                stacklevel=5,\n            )```\n\nUpdated code:\n```python\ndef insert(self, loc: int, item: Hashable, value: ArrayLike) -> None:\n        \n        \n        \n        # insert to the axis; this could possibly raise a TypeError\n        new_axis = self.items.insert(loc, item)\n\n        if value.ndim == 2:\n            value = value.T\n        else:\n            value = ensure_block_shape(value, ndim=self.ndim)\n\n        block = new_block(values=value, ndim=self.ndim, placement=slice(loc, loc + 1))\n\n        for blkno, count in _fast_count_smallints(self.blknos[loc:]):\n            blk = self.blocks[blkno]\n            if count == len(blk.mgr_locs):\n                blk.mgr_locs = blk.mgr_locs.add(1)\n            else:\n                new_mgr_locs = blk.mgr_locs.as_array.copy()\n                new_mgr_locs[new_mgr_locs >= loc] += 1\n                blk.mgr_locs = BlockPlacement(new_mgr_locs)\n\n        # Accessing public blklocs ensures the public versions are initialized\n        if loc == self.blklocs.shape[0]:\n            # np.append is a lot faster, let's use it if we can.\n            self._blklocs = np.append(self._blklocs, 0)\n            self._blknos = np.append(self._blknos, len(self.blocks))\n        else:\n            self._blklocs = np.insert(self._blklocs, loc, 0)\n            self._blknos = np.insert(self._blknos, loc, len(self.blocks))\n\n        self.axes[0] = new_axis\n        self.blocks += (block,)\n\n        self._known_consolidated = False\n\n        if len(self.blocks) > 100:\n            warnings.warn(\n                \"DataFrame is highly fragmented.  This is usually the result \"\n                \"of calling `frame.insert` many times, which has poor performance.  \"\n                \"Consider using pd.concat instead.  To get a de-fragmented frame, \"\n                \"use `newframe = frame.copy()`\",\n                PerformanceWarning,\n                stacklevel=5,\n            )\n```\n\nOriginal documentation:\n```python\nInsert item at selected position.\n\n        Parameters\n        ----------\n        loc : int\n        item : hashable\n        value : np.ndarray or ExtensionArray\n        allow_duplicates: bool\n            If False, trying to insert non-unique item will raise\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Insert item at selected position.\n\n        Parameters\n        ----------\n        loc : int\n        item : hashable\n        value : np.ndarray or ExtensionArray", "file": "pandas_core_internals_managers.py", "function": "BlockManager.insert", "project": "pandas", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _fallback_range(self, utterances, lang, message, fb_range):\n        \n        \n        msg = message.reply(\n            'mycroft.skills.fallback',\n            data={'utterance': utterances[0],\n                  'lang': lang,\n                  'fallback_range': (fb_range.start, fb_range.stop)}\n        )\n        response = self.bus.wait_for_response(msg, timeout=10)\n        if response and response.data['handled']:\n            ret = IntentMatch('Fallback', None, {}, None)\n        else:\n            ret = None\n        return ret```\n\nUpdated code:\n```python\ndef _fallback_range(self, utterances, lang, message, fb_range):\n        \n        \n        msg = message.reply(\n            'mycroft.skills.fallback',\n            data={'utterance': utterances[0][0],\n                  'lang': lang,\n                  'fallback_range': (fb_range.start, fb_range.stop)}\n        )\n        response = self.bus.wait_for_response(msg, timeout=10)\n        if response and response.data['handled']:\n            ret = IntentMatch('Fallback', None, {}, None)\n        else:\n            ret = None\n        return ret\n```\n\nOriginal documentation:\n```python\nSend fallback request for a specified priority range.\n\n        Arguments:\n            combined (list): List of unique versions of utterances\n            lang (str): Langauge code\n            message: Message for session context\n            fb_range (FallbackRange): fallback order start and stop.\n\n        Returns:\n            IntentMatch or None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Send fallback request for a specified priority range.\n\n        Arguments:\n            utterances (list): List of tuples,\n                               utterances and normalized version\n            lang (str): Langauge code\n            message: Message for session context\n            fb_range (FallbackRange): fallback order start and stop.\n\n        Returns:\n            IntentMatch or None", "file": "mycroft_skills_intent_services_fallback_service.py", "function": "FallbackService._fallback_range", "project": "mycroft-core", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, image):\n        \n        \n        \n        image = np.array(image)\n        if image.shape == self.IMAGE_SHAPE:\n            self.image = image\n        else:\n            self.image = cv2.resize(image, self.IMAGE_SHAPE, interpolation=cv2.INTER_CUBIC)\n        self.is_valid = np.mean(rgb2gray(self.image) > 127) > 0.1\n        self._name = 'DefaultItem'\n        self.amount = 1```\n\nUpdated code:\n```python\ndef __init__(self, image, button):\n        \n        \n        \n        self.image_raw = image\n        self._button = button\n        image = np.array(image.crop(button.area))\n        if image.shape == self.IMAGE_SHAPE:\n            self.image = image\n        else:\n            self.image = cv2.resize(image, self.IMAGE_SHAPE, interpolation=cv2.INTER_CUBIC)\n        self.is_valid = np.mean(rgb2gray(self.image) > 127) > 0.1\n        self._name = 'DefaultItem'\n        self.amount = 1\n        self.cost = 'DefaultCost'\n        self.price = 0\n```\n\nOriginal documentation:\n```python\nArgs:\n            image:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            image:\n            button:", "file": "module_statistics_item.py", "function": "Item.__init__", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef post_process(self, outputs, target_sizes):\n        \n        \n        \n        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n\n        if len(out_logits) != len(target_sizes):\n            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n        if target_sizes.shape[1] != 2:\n            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n\n        prob = out_logits.sigmoid()\n        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 300, dim=1)\n        scores = topk_values\n        topk_boxes = topk_indexes // out_logits.shape[2]\n        labels = topk_indexes % out_logits.shape[2]\n        boxes = center_to_corners_format(out_bbox)\n        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n\n        # and from relative [0, 1] to absolute [0, height] coordinates\n        img_h, img_w = target_sizes.unbind(1)\n        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n        boxes = boxes * scale_fct[:, None, :]\n\n        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n\n        return results```\n\nUpdated code:\n```python\ndef post_process(self, outputs, target_sizes):\n        \n        \n        \n        warnings.warn(\n            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n            \" `post_process_object_detection`\",\n            FutureWarning,\n        )\n\n        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n\n        if len(out_logits) != len(target_sizes):\n            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n        if target_sizes.shape[1] != 2:\n            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n\n        prob = out_logits.sigmoid()\n        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 300, dim=1)\n        scores = topk_values\n        topk_boxes = topk_indexes // out_logits.shape[2]\n        labels = topk_indexes % out_logits.shape[2]\n        boxes = center_to_corners_format(out_bbox)\n        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n\n        # and from relative [0, 1] to absolute [0, height] coordinates\n        img_h, img_w = target_sizes.unbind(1)\n        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n        boxes = boxes * scale_fct[:, None, :]\n\n        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n\n        return results\n```\n\nOriginal documentation:\n```python\nConverts the output of [`ConditionalDetrForObjectDetection`] into the format expected by the COCO api. Only\n        supports PyTorch.\n\n        Args:\n            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n                Raw outputs of the model.\n            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n                image size (before any data augmentation). For visualization, this should be the image size after data\n                augment, but before padding.\n\n        Returns:\n            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n            in the batch as predicted by the model.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n        Converts the output of [`ConditionalDetrForObjectDetection`] into the format expected by the COCO api. Only\n        supports PyTorch.\n            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n                Raw outputs of the model.\n            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n                image size (before any data augmentation). For visualization, this should be the image size after data\n                augment, but before padding.\n        Returns:\n            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n            in the batch as predicted by the model.", "file": "src_transformers_models_conditional_detr_feature_extraction_conditional_detr.py", "function": "ConditionalDetrFeatureExtractor.post_process", "project": "transformers", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_tickers(self, symbols=None, params={}):\n        \n        \n        \n        self.load_markets()\n        request = {}\n        if symbols is not None:\n            marketIds = self.market_ids(symbols)\n            request['symbol'] = ','.join(marketIds)\n        response = self.v1PublicGetTicker(self.extend(request, params))\n        #\n        #     {\n        #         \"code\":0,\n        #         \"data\":[\n        #             {\n        #                 \"symbol\":\"QTUM/BTC\",\n        #                 \"open\":\"0.00016537\",\n        #                 \"close\":\"0.00019077\",\n        #                 \"high\":\"0.000192\",\n        #                 \"low\":\"0.00016537\",\n        #                 \"volume\":\"846.6\",\n        #                 \"ask\":[\"0.00018698\",\"26.2\"],\n        #                 \"bid\":[\"0.00018408\",\"503.7\"],\n        #                 \"type\":\"spot\"\n        #             }\n        #         ]\n        #     }\n        #\n        data = self.safe_value(response, 'data', [])\n        return self.parse_tickers(data, symbols)```\n\nUpdated code:\n```python\ndef fetch_tickers(self, symbols=None, params={}):\n        \n        \n        \n        self.load_markets()\n        request = {}\n        market = None\n        if symbols is not None:\n            symbol = self.safe_value(symbols, 0)\n            market = self.market(symbol)\n            marketIds = self.market_ids(symbols)\n            request['symbol'] = ','.join(marketIds)\n        type = None\n        type, params = self.handle_market_type_and_params('fetchTickers', market, params)\n        response = None\n        if type == 'spot':\n            response = self.v1PublicGetTicker(self.extend(request, params))\n        else:\n            response = self.v2PublicGetFuturesTicker(self.extend(request, params))\n        #\n        #     {\n        #         \"code\":0,\n        #         \"data\":[\n        #             {\n        #                 \"symbol\":\"QTUM/BTC\",\n        #                 \"open\":\"0.00016537\",\n        #                 \"close\":\"0.00019077\",\n        #                 \"high\":\"0.000192\",\n        #                 \"low\":\"0.00016537\",\n        #                 \"volume\":\"846.6\",\n        #                 \"ask\":[\"0.00018698\",\"26.2\"],\n        #                 \"bid\":[\"0.00018408\",\"503.7\"],\n        #                 \"type\":\"spot\"\n        #             }\n        #         ]\n        #     }\n        #\n        data = self.safe_value(response, 'data', [])\n        if not isinstance(data, list):\n            return self.parse_tickers([data], symbols)\n        return self.parse_tickers(data, symbols)\n```\n\nOriginal documentation:\n```python\nfetches price tickers for multiple markets, statistical calculations with the information calculated over the past 24 hours each market\n        :param [str]|None symbols: unified symbols of the markets to fetch the ticker for, all market tickers are returned if not assigned\n        :param dict params: extra parameters specific to the ascendex api endpoint\n        :returns dict: an array of `ticker structures <https://docs.ccxt.com/en/latest/manual.html#ticker-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetches price tickers for multiple markets, statistical calculations with the information calculated over the past 24 hours each market\n        see https://ascendex.github.io/ascendex-pro-api/#ticker\n        see https://ascendex.github.io/ascendex-futures-pro-api-v2/#ticker\n        :param [str]|None symbols: unified symbols of the markets to fetch the ticker for, all market tickers are returned if not assigned\n        :param dict params: extra parameters specific to the ascendex api endpoint\n        :returns dict: an array of `ticker structures <https://docs.ccxt.com/en/latest/manual.html#ticker-structure>`", "file": "python_ccxt_ascendex.py", "function": "ascendex.fetch_tickers", "project": "ccxt", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.check_required_symbol('fetchMyTrades', symbol)\n        self.load_markets()\n        market = self.market(symbol)\n        if market['swap']:\n            raise BadSymbol(self.id + ' fetchMyTrades() only supports spot markets')\n        request = {\n            'symbol': market['id'],\n        }\n        if limit is not None:\n            request['limit'] = limit\n        response = self.privateSpotPostTradeFills(self.extend(request, params))\n        #\n        #     {\n        #       code: '00000',\n        #       msg: 'success',\n        #       requestTime: '1645918954082',\n        #       data: [\n        #         {\n        #           accountId: '6394957606',\n        #           symbol: 'LTCUSDT_SPBL',\n        #           orderId: '864752115272552448',\n        #           fillId: '864752115685969921',\n        #           orderType: 'limit',\n        #           side: 'buy',\n        #           fillPrice: '127.92000000',\n        #           fillQuantity: '0.10000000',\n        #           fillTotalAmount: '12.79200000',\n        #           feeCcy: 'LTC',\n        #           fees: '0.00000000',\n        #           cTime: '1641898891373'\n        #         }\n        #       ]\n        #     }\n        #\n        data = self.safe_value(response, 'data')\n        return self.parse_trades(data, market, since, limit)```\n\nUpdated code:\n```python\ndef fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.check_required_symbol('fetchMyTrades', symbol)\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'symbol': market['id'],\n        }\n        if limit is not None:\n            request['limit'] = limit\n        response = None\n        if market['spot']:\n            response = self.privateSpotPostTradeFills(self.extend(request, params))\n        else:\n            response = self.privateMixGetOrderFills(self.extend(request, params))\n        #\n        #     {\n        #       code: '00000',\n        #       msg: 'success',\n        #       requestTime: '1645918954082',\n        #       data: [\n        #         {\n        #           accountId: '6394957606',\n        #           symbol: 'LTCUSDT_SPBL',\n        #           orderId: '864752115272552448',\n        #           fillId: '864752115685969921',\n        #           orderType: 'limit',\n        #           side: 'buy',\n        #           fillPrice: '127.92000000',\n        #           fillQuantity: '0.10000000',\n        #           fillTotalAmount: '12.79200000',\n        #           feeCcy: 'LTC',\n        #           fees: '0.00000000',\n        #           cTime: '1641898891373'\n        #         }\n        #       ]\n        #     }\n        #\n        data = self.safe_value(response, 'data')\n        return self.parse_trades(data, market, since, limit)\n```\n\nOriginal documentation:\n```python\nfetch all trades made by the user\n        :param str symbol: unified market symbol\n        :param int [since]: the earliest time in ms to fetch trades for\n        :param int [limit]: the maximum number of trades structures to retrieve\n        :param dict [params]: extra parameters specific to the bitget api endpoint\n        :returns Trade[]: a list of `trade structures <https://github.com/ccxt/ccxt/wiki/Manual#trade-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch all trades made by the user\n        see https://bitgetlimited.github.io/apidoc/en/spot/#get-transaction-details\n        see https://bitgetlimited.github.io/apidoc/en/mix/#get-order-fill-detail\n        :param str symbol: unified market symbol\n        :param int [since]: the earliest time in ms to fetch trades for\n        :param int [limit]: the maximum number of trades structures to retrieve\n        :param dict [params]: extra parameters specific to the bitget api endpoint\n        :returns Trade[]: a list of `trade structures <https://github.com/ccxt/ccxt/wiki/Manual#trade-structure>`", "file": "python_ccxt_bitget.py", "function": "bitget.fetch_my_trades", "project": "ccxt", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _try_cast(\n    arr: list | np.ndarray,\n    dtype: DtypeObj | None,\n    copy: bool,\n) -> ArrayLike:\n    \n    \n    \n    is_ndarray = isinstance(arr, np.ndarray)\n\n    if dtype is None:\n        # perf shortcut as this is the most common case\n        if is_ndarray:\n            arr = cast(np.ndarray, arr)\n            if arr.dtype != object:\n                if copy:\n                    return arr.copy()\n                return arr\n\n            out = maybe_infer_to_datetimelike(arr)\n            if out is arr and copy:\n                out = out.copy()\n            return out\n\n        else:\n            # i.e. list\n            varr = np.array(arr, copy=False)\n            # filter out cases that we _dont_ want to go through\n            #  maybe_infer_to_datetimelike\n            if varr.dtype != object or varr.size == 0:\n                return varr\n            return maybe_infer_to_datetimelike(varr)\n\n    elif isinstance(dtype, ExtensionDtype):\n        # create an extension array from its dtype\n        array_type = dtype.construct_array_type()._from_sequence\n        subarr = array_type(arr, dtype=dtype, copy=copy)\n        return subarr\n\n    elif is_object_dtype(dtype):\n        if not is_ndarray:\n            subarr = construct_1d_object_array_from_listlike(arr)\n            return subarr\n        return ensure_wrapped_if_datetimelike(arr).astype(dtype, copy=copy)\n\n    elif dtype.kind == \"U\":\n        # TODO: test cases with arr.dtype.kind in [\"m\", \"M\"]\n        if is_ndarray:\n            arr = cast(np.ndarray, arr)\n            shape = arr.shape\n            if arr.ndim > 1:\n                arr = arr.ravel()\n        else:\n            shape = (len(arr),)\n        return lib.ensure_string_array(arr, convert_na_value=False, copy=copy).reshape(\n            shape\n        )\n\n    elif dtype.kind in [\"m\", \"M\"]:\n        return maybe_cast_to_datetime(arr, dtype)\n\n    # GH#15832: Check if we are requesting a numeric dtype and\n    # that we can convert the data to the requested dtype.\n    elif is_integer_dtype(dtype):\n        # this will raise if we have e.g. floats\n\n        subarr = maybe_cast_to_integer_array(arr, dtype)\n    else:\n        subarr = np.array(arr, dtype=dtype, copy=copy)\n\n    return subarr```\n\nUpdated code:\n```python\ndef _try_cast(\n    arr: list | np.ndarray,\n    dtype: np.dtype | None,\n    copy: bool,\n) -> ArrayLike:\n    \n    \n    \n    is_ndarray = isinstance(arr, np.ndarray)\n\n    if dtype is None:\n        # perf shortcut as this is the most common case\n        if is_ndarray:\n            arr = cast(np.ndarray, arr)\n            if arr.dtype != object:\n                if copy:\n                    return arr.copy()\n                return arr\n\n            out = maybe_infer_to_datetimelike(arr)\n            if out is arr and copy:\n                out = out.copy()\n            return out\n\n        else:\n            # i.e. list\n            varr = np.array(arr, copy=False)\n            # filter out cases that we _dont_ want to go through\n            #  maybe_infer_to_datetimelike\n            if varr.dtype != object or varr.size == 0:\n                return varr\n            return maybe_infer_to_datetimelike(varr)\n\n    elif is_object_dtype(dtype):\n        if not is_ndarray:\n            subarr = construct_1d_object_array_from_listlike(arr)\n            return subarr\n        return ensure_wrapped_if_datetimelike(arr).astype(dtype, copy=copy)\n\n    elif dtype.kind == \"U\":\n        # TODO: test cases with arr.dtype.kind in [\"m\", \"M\"]\n        if is_ndarray:\n            arr = cast(np.ndarray, arr)\n            shape = arr.shape\n            if arr.ndim > 1:\n                arr = arr.ravel()\n        else:\n            shape = (len(arr),)\n        return lib.ensure_string_array(arr, convert_na_value=False, copy=copy).reshape(\n            shape\n        )\n\n    elif dtype.kind in [\"m\", \"M\"]:\n        return maybe_cast_to_datetime(arr, dtype)\n\n    # GH#15832: Check if we are requesting a numeric dtype and\n    # that we can convert the data to the requested dtype.\n    elif is_integer_dtype(dtype):\n        # this will raise if we have e.g. floats\n\n        subarr = maybe_cast_to_integer_array(arr, dtype)\n    else:\n        subarr = np.array(arr, dtype=dtype, copy=copy)\n\n    return subarr\n```\n\nOriginal documentation:\n```python\nConvert input to numpy ndarray and optionally cast to a given dtype.\n\n    Parameters\n    ----------\n    arr : ndarray or list\n        Excludes: ExtensionArray, Series, Index.\n    dtype : np.dtype, ExtensionDtype or None\n    copy : bool\n        If False, don't copy the data if not needed.\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert input to numpy ndarray and optionally cast to a given dtype.\n\n    Parameters\n    ----------\n    arr : ndarray or list\n        Excludes: ExtensionArray, Series, Index.\n    dtype : np.dtype or None\n    copy : bool\n        If False, don't copy the data if not needed.\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray", "file": "pandas_core_construction.py", "function": "_try_cast", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef tamper(payload, **kwargs):\n    \n    \n    \n\n    retVal = payload\n\n    if payload:\n        prefix, suffix = '+' * len(re.search(r\"\\A(\\+*)\", payload).group(0)), '+' * len(re.search(r\"(\\+*)\\Z\", payload).group(0))\n        retVal = retVal.strip('+')\n\n        while True:\n            indexes = zeroDepthSearch(retVal, '+')\n\n            if indexes:\n                first, last = 0, 0\n                for i in xrange(1, len(indexes)):\n                    if ' ' in retVal[indexes[0]:indexes[i]]:\n                        break\n                    else:\n                        last = i\n\n                start = retVal[:indexes[first]].rfind(' ') + 1\n                end = (retVal[indexes[last] + 1:].find(' ') + indexes[last] + 1) if ' ' in retVal[indexes[last] + 1:] else len(retVal) - 1\n\n                chars = [char for char in retVal]\n                for index in indexes[first:last + 1]:\n                    chars[index] = ','\n\n                retVal = \"%sCONCAT(%s)%s\" % (retVal[:start], ''.join(chars)[start:end], retVal[end:])\n            else:\n                match = re.search(r\"\\((CHAR\\(\\d+.+\\bCHAR\\(\\d+\\))\\)\", retVal)\n                if match:\n                    part = match.group(0)\n                    indexes = set(zeroDepthSearch(match.group(1), '+'))\n                    if not indexes:\n                        break\n                    chars = [char for char in part]\n                    for i in xrange(1, len(chars)):\n                        if i - 1 in indexes:\n                            chars[i] = ','\n                    replacement = \"CONCAT%s\" % \"\".join(chars)\n                    retVal = retVal.replace(part, replacement)\n                else:\n                    break\n\n        retVal = \"%s%s%s\" % (prefix, retVal, suffix)\n\n    return retVal```\n\nUpdated code:\n```python\ndef tamper(payload, **kwargs):\n    \n    \n    \n\n    retVal = payload\n\n    if payload:\n        match = re.search(r\"('[^']+'|CHAR\\(\\d+\\))\\+.*(?<=\\+)('[^']+'|CHAR\\(\\d+\\))\", retVal)\n        if match:\n            part = match.group(0)\n\n            chars = [char for char in part]\n            for index in zeroDepthSearch(part, '+'):\n                chars[index] = ','\n\n            replacement = \"CONCAT(%s)\" % \"\".join(chars)\n            retVal = retVal.replace(part, replacement)\n\n    return retVal\n```\n\nOriginal documentation:\n```python\nReplaces plus operator ('+') with (MsSQL) function CONCAT() counterpart\n\n    Tested against:\n        * Microsoft SQL Server 2012\n\n    Requirements:\n        * Microsoft SQL Server 2012+\n\n    Notes:\n        * Useful in case ('+') character is filtered\n\n    >>> tamper('SELECT CHAR(113)+CHAR(114)+CHAR(115) FROM DUAL')\n    'SELECT CONCAT(CHAR(113),CHAR(114),CHAR(115)) FROM DUAL'\n\n    >>> tamper('SELECT (CHAR(113)+CHAR(114)+CHAR(115)) FROM DUAL')\n    'SELECT CONCAT(CHAR(113),CHAR(114),CHAR(115)) FROM DUAL'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replaces plus operator ('+') with (MsSQL) function CONCAT() counterpart\n\n    Tested against:\n        * Microsoft SQL Server 2012\n\n    Requirements:\n        * Microsoft SQL Server 2012+\n\n    Notes:\n        * Useful in case ('+') character is filtered\n\n    >>> tamper('SELECT CHAR(113)+CHAR(114)+CHAR(115) FROM DUAL')\n    'SELECT CONCAT(CHAR(113),CHAR(114),CHAR(115)) FROM DUAL'\n\n    >>> tamper('1 UNION ALL SELECT NULL,NULL,CHAR(113)+CHAR(118)+CHAR(112)+CHAR(112)+CHAR(113)+ISNULL(CAST(@@VERSION AS NVARCHAR(4000)),CHAR(32))+CHAR(113)+CHAR(112)+CHAR(107)+CHAR(112)+CHAR(113)-- qtfe')\n    '1 UNION ALL SELECT NULL,NULL,CONCAT(CHAR(113),CHAR(118),CHAR(112),CHAR(112),CHAR(113),ISNULL(CAST(@@VERSION AS NVARCHAR(4000)),CHAR(32)),CHAR(113),CHAR(112),CHAR(107),CHAR(112),CHAR(113))-- qtfe'", "file": "tamper_plus2concat.py", "function": "tamper", "project": "sqlmap", "change_stats": {"code_changes": 47, "doc_changes": 6, "total_changes": 53}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef autoscale(self, max, min, destination=None, **kwargs):\n        \n\n        \n        return self.broadcast(\n            'autoscale', {'max': max, 'min': min}, destination, **kwargs)```\n\nUpdated code:\n```python\ndef autoscale(self, max, min, destination=None, **kwargs):\n        \n        \n        return self.broadcast(\n            'autoscale', {'max': max, 'min': min}, destination, **kwargs)\n```\n\nOriginal documentation:\n```python\nChange worker(s) autoscale setting.\n\n        Supports the same arguments as :meth:`broadcast`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Change worker(s) autoscale setting.\n\n        See Also:\n            Supports the same arguments as :meth:`broadcast`.", "file": "celery_app_control.py", "function": "Control.autoscale", "project": "celery", "change_stats": {"code_changes": 3, "doc_changes": 5, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef map_to_legacy_structure(parameter_type: str, new_parameter: Parameter) -> LegacyParameter:\n    \n    \n    \n    return LegacyParameter(\n        LogicalResourceId=new_parameter[\"ParameterKey\"],\n        Type=\"Parameter\",\n        Properties=LegacyParameterProperties(\n            ParameterType=parameter_type,\n            ParameterValue=new_parameter.get(\"ParameterValue\"),\n            ResolvedValue=new_parameter.get(\"ResolvedValue\"),\n            Value=new_parameter.get(\"ResolvedValue\", new_parameter.get(\"ParameterValue\")),\n        ),\n    )```\n\nUpdated code:\n```python\ndef map_to_legacy_structure(parameter_name: str, new_parameter: StackParameter) -> LegacyParameter:\n    \n    \n    \n    return LegacyParameter(\n        LogicalResourceId=new_parameter[\"ParameterKey\"],\n        Type=\"Parameter\",\n        Properties=LegacyParameterProperties(\n            ParameterType=new_parameter.get(\"ParameterType\"),\n            ParameterValue=new_parameter.get(\"ParameterValue\"),\n            ResolvedValue=new_parameter.get(\"ResolvedValue\"),\n            Value=new_parameter.get(\"ResolvedValue\", new_parameter.get(\"ParameterValue\")),\n        ),\n    )\n```\n\nOriginal documentation:\n```python\nHelper util to convert a normal (resolved) stack parameter to a legacy parameter structure that can then be merged with stack resources.\n\n    :param parameter_type: the stack parameter type (e.g. \"String\", \"AWS::SSM::Parameter::Value<String>\", ...)\n    :param new_parameter: a resolved stack parameter\n    :return: legacy parameter that can be merged with stack resources for uniform lookup based on logical ID\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Helper util to convert a normal (resolved) stack parameter to a legacy parameter structure that can then be merged with stack resources.\n\n    :param new_parameter: a resolved stack parameter\n    :return: legacy parameter that can be merged with stack resources for uniform lookup based on logical ID", "file": "localstack_services_cloudformation_engine_parameters.py", "function": "map_to_legacy_structure", "project": "localstack", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef read_indented_block(self, text, indent):\n        \n        \n        if indent == '':\n            return '', text\n            \n        block = \"\"\n        while True:\n            if text.startswith(indent):\n                line, text = splitline(text)\n                block += line[len(indent):]\n            else:\n                break\n        return block, text```\n\nUpdated code:\n```python\ndef read_indented_block(self, text, indent):\n        \n        \n        if indent == '':\n            return '', text\n            \n        block = \"\"\n        while text:\n            line, text2 = splitline(text)\n            if line.strip() == \"\":\n                block += '\\n'\n            elif line.startswith(indent):\n                block += line[len(indent):]\n            else:\n                break\n            text = text2\n        return block, text\n```\n\nOriginal documentation:\n```python\nr\"\"\"Read a block of text. A block is what typically follows a for or it statement.\n        It can be in the same line as that of the statement or an indented block.\n\n            >>> read_indented_block = Parser('').read_indented_block\n            >>> read_indented_block('  a\\n  b\\nc', '  ')\n            ('a\\nb\\n', 'c')\n            >>> read_indented_block('  a\\n    b\\n  c\\nd', '  ')\n            ('a\\n  b\\nc\\n', 'd')\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Read a block of text. A block is what typically follows a for or it statement.\n        It can be in the same line as that of the statement or an indented block.\n\n            >>> read_indented_block = Parser('').read_indented_block\n            >>> read_indented_block('  a\\n  b\\nc', '  ')\n            ('a\\nb\\n', 'c')\n            >>> read_indented_block('  a\\n    b\\n  c\\nd', '  ')\n            ('a\\n  b\\nc\\n', 'd')\n            >>> read_indented_block('  a\\n\\n    b\\nc', '  ')\n            ('a\\n\\n  b\\n', 'c')", "file": "web_template.py", "function": "Parser.read_indented_block", "project": "webpy", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,\n               params: cfg.DataConfig,\n               dataset_fn=tf.data.TFRecordDataset,\n               decoder_fn: Optional[Callable[..., Any]] = None,\n               combine_fn: Optional[Callable[..., Any]] = None,\n               sample_fn: Optional[Callable[..., Any]] = None,\n               parser_fn: Optional[Callable[..., Any]] = None,\n               transform_and_batch_fn: Optional[Callable[\n                   [tf.data.Dataset, Optional[tf.distribute.InputContext]],\n                   tf.data.Dataset]] = None,\n               postprocess_fn: Optional[Callable[..., Any]] = None):\n    \n    \n    if params.input_path and params.tfds_name:\n      raise ValueError('At most one of `input_path` and `tfds_name` can be '\n                       'specified, but got %s and %s.' %\n                       (params.input_path, params.tfds_name))\n\n    if (isinstance(params.input_path, cfg.base_config.Config) or\n        isinstance(params.tfds_name, cfg.base_config.Config)\n        ) and combine_fn is None:\n      raise ValueError(\n          'A combine_fn is required if `input_path` or `tfds_name` is a dict.')\n\n    self._tfds_name = params.tfds_name\n    self._tfds_data_dir = params.tfds_data_dir\n    self._matched_files = None\n    if not params.input_path:\n      # Read dataset from TFDS.\n      if not params.tfds_split:\n        raise ValueError(\n            '`tfds_name` is %s, but `tfds_split` is not specified.' %\n            params.tfds_name)\n    else:\n      self._matched_files = self.get_files(params.input_path)\n\n    self._global_batch_size = params.global_batch_size\n    self._is_training = params.is_training\n    self._drop_remainder = params.drop_remainder\n    self._shuffle_buffer_size = params.shuffle_buffer_size\n    self._cache = params.cache\n    self._cycle_length = params.cycle_length\n    self._block_length = params.block_length\n    self._deterministic = params.deterministic\n    self._sharding = params.sharding\n    self._tfds_split = params.tfds_split\n    self._tfds_as_supervised = params.tfds_as_supervised\n    self._tfds_skip_decoding_feature = params.tfds_skip_decoding_feature\n\n    self._dataset_fn = dataset_fn\n    self._decoder_fn = decoder_fn\n    self._combine_fn = combine_fn\n    self._sample_fn = sample_fn\n    self._parser_fn = parser_fn\n    self._transform_and_batch_fn = transform_and_batch_fn\n    self._postprocess_fn = postprocess_fn\n    self._seed = params.seed\n    self._prefetch_buffer_size = (\n        params.prefetch_buffer_size or tf.data.experimental.AUTOTUNE)\n    self._autotune_algorithm = params.autotune_algorithm\n\n    # When tf.data service is enabled, each data service worker should get\n    # different random seeds. Thus, we set `seed` to None.\n    # Sharding should also be disabled because tf data service handles how\n    # each worker shard data with `processing_mode` in distribute method.\n    if params.enable_tf_data_service:\n      self._seed = None\n      self._sharding = False\n\n    self._enable_tf_data_service = (\n        params.enable_tf_data_service and params.tf_data_service_address)\n    self._tf_data_service_address = params.tf_data_service_address\n    self._enable_shared_tf_data_service_between_parallel_trainers = (\n        params.enable_shared_tf_data_service_between_parallel_trainers)\n    self._apply_tf_data_service_before_batching = (\n        params.apply_tf_data_service_before_batching)\n    self._trainer_id = params.trainer_id\n    if self._enable_tf_data_service:\n      # Add a random seed as the tf.data service job name suffix, so tf.data\n      # service doesn't reuse the previous state if TPU worker gets preempted.\n      # It's necessary to add global batch size into the tf data service job\n      # name because when tuning batch size with vizier and tf data service is\n      # also enable, the tf data servce job name should be different for\n      # different vizier trials since once batch size is changed, from the\n      # tf.data perspective, the dataset is a different instance, and a\n      # different job name should be used for tf data service. Otherwise, the\n      # model would read tensors from the incorrect tf data service job, which\n      # would causes dimension mismatch on the batch size dimension.\n      self._tf_data_service_job_name = (\n          f'{params.tf_data_service_job_name}_bs{params.global_batch_size}_'\n          f'{self.static_randnum}')\n      self._enable_round_robin_tf_data_service = params.get(\n          'enable_round_robin_tf_data_service', False)\n      if self._enable_shared_tf_data_service_between_parallel_trainers:\n        # When shared tf.data service is enabled, only a single tf.data service\n        # instance should be created and shared between parallel trainers. If\n        # the global batch size is different across trainers,\n        # params.apply_tf_data_service_before_batching should be set to true\n        # because tf.data service with different batch sizes will be considered\n        # separate tf.data service instances.\n        self._tf_data_service_job_name = (\n            f'{params.tf_data_service_job_name}_{self.static_randnum}')```\n\nUpdated code:\n```python\ndef __init__(\n      self,\n      params: cfg.DataConfig,\n      dataset_fn=tf.data.TFRecordDataset,\n      decoder_fn: Optional[Callable[..., Any]] = None,\n      combine_fn: Optional[Callable[..., Any]] = None,\n      sample_fn: Optional[Callable[..., Any]] = None,\n      parser_fn: Optional[Callable[..., Any]] = None,\n      filter_fn: Optional[Callable[..., tf.Tensor]] = None,\n      transform_and_batch_fn: Optional[\n          Callable[\n              [tf.data.Dataset, Optional[tf.distribute.InputContext]],\n              tf.data.Dataset,\n          ]\n      ] = None,\n      postprocess_fn: Optional[Callable[..., Any]] = None,\n  ):\n    \n    \n    if params.input_path and params.tfds_name:\n      raise ValueError('At most one of `input_path` and `tfds_name` can be '\n                       'specified, but got %s and %s.' %\n                       (params.input_path, params.tfds_name))\n\n    if (isinstance(params.input_path, cfg.base_config.Config) or\n        isinstance(params.tfds_name, cfg.base_config.Config)\n        ) and combine_fn is None:\n      raise ValueError(\n          'A combine_fn is required if `input_path` or `tfds_name` is a dict.')\n\n    self._tfds_name = params.tfds_name\n    self._tfds_data_dir = params.tfds_data_dir\n    self._matched_files = None\n    if not params.input_path:\n      # Read dataset from TFDS.\n      if not params.tfds_split:\n        raise ValueError(\n            '`tfds_name` is %s, but `tfds_split` is not specified.' %\n            params.tfds_name)\n    else:\n      self._matched_files = self.get_files(params.input_path)\n\n    self._global_batch_size = params.global_batch_size\n    self._is_training = params.is_training\n    self._drop_remainder = params.drop_remainder\n    self._shuffle_buffer_size = params.shuffle_buffer_size\n    self._cache = params.cache\n    self._cycle_length = params.cycle_length\n    self._block_length = params.block_length\n    self._deterministic = params.deterministic\n    self._sharding = params.sharding\n    self._tfds_split = params.tfds_split\n    self._tfds_as_supervised = params.tfds_as_supervised\n    self._tfds_skip_decoding_feature = params.tfds_skip_decoding_feature\n\n    self._dataset_fn = dataset_fn\n    self._decoder_fn = decoder_fn\n    self._combine_fn = combine_fn\n    self._sample_fn = sample_fn\n    self._parser_fn = parser_fn\n    self._transform_and_batch_fn = transform_and_batch_fn\n    self._postprocess_fn = postprocess_fn\n    self._filter_fn = filter_fn\n    self._seed = params.seed\n    self._prefetch_buffer_size = (\n        params.prefetch_buffer_size or tf.data.experimental.AUTOTUNE)\n    self._autotune_algorithm = params.autotune_algorithm\n\n    # When tf.data service is enabled, each data service worker should get\n    # different random seeds. Thus, we set `seed` to None.\n    # Sharding should also be disabled because tf data service handles how\n    # each worker shard data with `processing_mode` in distribute method.\n    if params.enable_tf_data_service:\n      self._seed = None\n      self._sharding = False\n\n    self._enable_tf_data_service = (\n        params.enable_tf_data_service and params.tf_data_service_address)\n    self._tf_data_service_address = params.tf_data_service_address\n    self._enable_shared_tf_data_service_between_parallel_trainers = (\n        params.enable_shared_tf_data_service_between_parallel_trainers)\n    self._apply_tf_data_service_before_batching = (\n        params.apply_tf_data_service_before_batching)\n    self._trainer_id = params.trainer_id\n    if self._enable_tf_data_service:\n      # Add a random seed as the tf.data service job name suffix, so tf.data\n      # service doesn't reuse the previous state if TPU worker gets preempted.\n      # It's necessary to add global batch size into the tf data service job\n      # name because when tuning batch size with vizier and tf data service is\n      # also enable, the tf data servce job name should be different for\n      # different vizier trials since once batch size is changed, from the\n      # tf.data perspective, the dataset is a different instance, and a\n      # different job name should be used for tf data service. Otherwise, the\n      # model would read tensors from the incorrect tf data service job, which\n      # would causes dimension mismatch on the batch size dimension.\n      self._tf_data_service_job_name = (\n          f'{params.tf_data_service_job_name}_bs{params.global_batch_size}_'\n          f'{self.static_randnum}')\n      self._enable_round_robin_tf_data_service = params.get(\n          'enable_round_robin_tf_data_service', False)\n      if self._enable_shared_tf_data_service_between_parallel_trainers:\n        # When shared tf.data service is enabled, only a single tf.data service\n        # instance should be created and shared between parallel trainers. If\n        # the global batch size is different across trainers,\n        # params.apply_tf_data_service_before_batching should be set to true\n        # because tf.data service with different batch sizes will be considered\n        # separate tf.data service instances.\n        self._tf_data_service_job_name = (\n            f'{params.tf_data_service_job_name}_{self.static_randnum}')\n```\n\nOriginal documentation:\n```python\nInitializes an InputReader instance.\n\n    Args:\n      params: A config_definitions.DataConfig object.\n      dataset_fn: A `tf.data.Dataset` that consumes the input files. For\n        example, it can be `tf.data.TFRecordDataset`.\n      decoder_fn: An optional `callable` that takes the serialized data string\n        and decodes them into the raw tensor dictionary.\n      combine_fn: An optional `callable` that takes a dictionarty of\n        `tf.data.Dataset` objects as input and outputs a combined dataset. It\n        will be executed after the decoder_fn and before the sample_fn.\n      sample_fn: An optional `callable` that takes a `tf.data.Dataset` object as\n        input and outputs the transformed dataset. It performs sampling on the\n        decoded raw tensors dict before the parser_fn.\n      parser_fn: An optional `callable` that takes the decoded raw tensors dict\n        and parse them into a dictionary of tensors that can be consumed by the\n        model. It will be executed after decoder_fn.\n      transform_and_batch_fn: An optional `callable` that takes a\n        `tf.data.Dataset` object and an optional `tf.distribute.InputContext` as\n        input, and returns a `tf.data.Dataset` object. It will be executed after\n        `parser_fn` to transform and batch the dataset; if None, after\n        `parser_fn` is executed, the dataset will be batched into per-replica\n        batch size.\n      postprocess_fn: A optional `callable` that processes batched tensors. It\n        will be executed after batching.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initializes an InputReader instance.\n\n    Args:\n      params: A config_definitions.DataConfig object.\n      dataset_fn: A `tf.data.Dataset` that consumes the input files. For\n        example, it can be `tf.data.TFRecordDataset`.\n      decoder_fn: An optional `callable` that takes the serialized data string\n        and decodes them into the raw tensor dictionary.\n      combine_fn: An optional `callable` that takes a dictionarty of\n        `tf.data.Dataset` objects as input and outputs a combined dataset. It\n        will be executed after the decoder_fn and before the sample_fn.\n      sample_fn: An optional `callable` that takes a `tf.data.Dataset` object as\n        input and outputs the transformed dataset. It performs sampling on the\n        decoded raw tensors dict before the parser_fn.\n      parser_fn: An optional `callable` that takes the decoded raw tensors dict\n        and parse them into a dictionary of tensors that can be consumed by the\n        model. It will be executed after decoder_fn.\n      filter_fn: An optional `callable` mapping a dataset element to a boolean.\n        It will be executed after parser_fn.\n      transform_and_batch_fn: An optional `callable` that takes a\n        `tf.data.Dataset` object and an optional `tf.distribute.InputContext` as\n        input, and returns a `tf.data.Dataset` object. It will be executed after\n        `parser_fn` to transform and batch the dataset; if None, after\n        `parser_fn` is executed, the dataset will be batched into per-replica\n        batch size.\n      postprocess_fn: A optional `callable` that processes batched tensors. It\n        will be executed after batching.", "file": "official_core_input_reader.py", "function": "InputReader.__init__", "project": "models", "change_stats": {"code_changes": 31, "doc_changes": 4, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef log_softmax(x, axis=1):\n    \n\n    \n    if backend.get_array_module(x) is chainerx:\n        return function._chainerx_op(\n            lambda a: chainerx.log_softmax(a, axis), x)\n\n    return LogSoftmax(axis=axis).apply((x,))[0]```\n\nUpdated code:\n```python\ndef log_softmax(x, axis=1):\n    \n\n    \n    return LogSoftmax(axis=axis).apply((x,))[0]\n```\n\nOriginal documentation:\n```python\nChannel-wise log-softmax function.\n\n    This function computes its logarithm of softmax along the second axis.\n    Let :math:`c = (c_1, c_2, \\\\dots, c_D)` be the slice of ``x`` along with\n    the second axis. For each slice :math:`c`, it computes the logarithm of\n    the function :math:`f(c)` defined as\n\n    .. math::\n        f(c) = {\\\\exp(c) \\\\over \\\\sum_{d} \\\\exp(c_d)}.\n\n    This method is theoretically equivalent to ``log(softmax(x))`` but is more\n    stable.\n\n    .. note::\n        ``log(softmax(x))`` may cause underflow when ``x`` is too small,\n        because ``softmax(x)`` may returns ``0``.\n        ``log_softmax`` method is more stable.\n\n    Args:\n        x (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`):\n            Input variable.\n            A :math:`n`-dimensional (:math:`n \\\\geq 2`) float array.\n        axis (int): The axis along which the softmax is to be computed.\n\n    Returns:\n        ~chainer.Variable: Output variable.\n        A :math:`n`-dimensional (:math:`n \\\\geq 2`) float array, which is the\n        same shape with x.\n\n    .. seealso:: :func:`~chainer.functions.softmax`\n\n    .. admonition:: Example\n\n        >>> x = np.array([[0, 1, 2], [0, 2, 4]], np.float32)\n        >>> x\n        array([[0., 1., 2.],\n               [0., 2., 4.]], dtype=float32)\n        >>> F.log_softmax(x).data\n        array([[-2.407606  , -1.4076059 , -0.4076059 ],\n               [-4.1429315 , -2.1429315 , -0.14293146]], dtype=float32)\n        >>> np.allclose(F.log_softmax(x).data, F.log(F.softmax(x)).data)\n        True\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Channel-wise log-softmax function.\n\n    This function computes its logarithm of softmax along the second axis.\n    Let :math:`c = (c_1, c_2, \\\\dots, c_D)` be the slice of ``x`` along with\n    the second axis. For each slice :math:`c`, it computes the logarithm of\n    the function :math:`f(c)` defined as\n\n    .. math::\n        f(c) = {\\\\exp(c) \\\\over \\\\sum_{d} \\\\exp(c_d)}.\n\n    This method is theoretically equivalent to ``log(softmax(x))`` but is more\n    stable.\n\n    .. note::\n        ``log(softmax(x))`` may cause underflow when ``x`` is too small,\n        because ``softmax(x)`` may returns ``0``.\n        ``log_softmax`` method is more stable.\n\n    Args:\n        x (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`):\n            Input variable.\n            A :math:`n`-dimensional (:math:`n \\\\geq 2`) float array.\n        axis (int): The axis along which the softmax is to be computed.\n\n    Returns:\n        ~chainer.Variable: Output variable.\n        A :math:`n`-dimensional (:math:`n \\\\geq 2`) float array, which is the\n        same shape with x.\n\n    .. seealso:: :func:`~chainer.functions.softmax`\n\n    .. admonition:: Example\n\n        >>> x = np.array([[0, 1, 2], [0, 2, 4]], np.float32)\n        >>> x\n        array([[0., 1., 2.],\n               [0., 2., 4.]], dtype=float32)\n        >>> F.log_softmax(x).array\n        array([[-2.407606  , -1.4076059 , -0.4076059 ],\n               [-4.1429315 , -2.1429315 , -0.14293146]], dtype=float32)\n        >>> np.allclose(F.log_softmax(x).data, F.log(F.softmax(x)).data)\n        True", "file": "chainer_", "function": "log_softmax", "project": "chainer", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pil_to_array(pilImage):\n    \n    \n    if pilImage.mode in ['RGBA', 'RGBX', 'RGB', 'L']:\n        # return MxNx4 RGBA, MxNx3 RBA, or MxN luminance array\n        return np.asarray(pilImage)\n    elif pilImage.mode.startswith('I;16'):\n        # return MxN luminance array of uint16\n        raw = pilImage.tobytes('raw', pilImage.mode)\n        if pilImage.mode.endswith('B'):\n            x = np.fromstring(raw, '>u2')\n        else:\n            x = np.fromstring(raw, '<u2')\n        return x.reshape(pilImage.size[::-1]).astype('=u2')\n    else:  # try to convert to an rgba image\n        try:\n            pilImage = pilImage.convert('RGBA')\n        except ValueError:\n            raise RuntimeError('Unknown image mode')\n        return np.asarray(pilImage)```\n\nUpdated code:\n```python\ndef pil_to_array(pilImage):\n    \n\n    \n    if pilImage.mode in ['RGBA', 'RGBX', 'RGB', 'L']:\n        # return MxNx4 RGBA, MxNx3 RBA, or MxN luminance array\n        return np.asarray(pilImage)\n    elif pilImage.mode.startswith('I;16'):\n        # return MxN luminance array of uint16\n        raw = pilImage.tobytes('raw', pilImage.mode)\n        if pilImage.mode.endswith('B'):\n            x = np.fromstring(raw, '>u2')\n        else:\n            x = np.fromstring(raw, '<u2')\n        return x.reshape(pilImage.size[::-1]).astype('=u2')\n    else:  # try to convert to an rgba image\n        try:\n            pilImage = pilImage.convert('RGBA')\n        except ValueError:\n            raise RuntimeError('Unknown image mode')\n        return np.asarray(pilImage)\n```\n\nOriginal documentation:\n```python\nLoad a PIL image and return it as a numpy array.\n\n    Grayscale images are returned as ``(M, N)`` arrays.  RGB images are\n    returned as ``(M, N, 3)`` arrays.  RGBA images are returned as ``(M, N,\n    4)`` arrays.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a `PIL image`_ and return it as a numpy array.\n\n    .. _PIL image: https://pillow.readthedocs.io/en/latest/reference/Image.html\n\n    Returns\n    -------\n    numpy.array\n\n        The array shape depends on the image type:\n\n        - (M, N) for grayscale images.\n        - (M, N, 3) for RGB images.\n        - (M, N, 4) for RGBA images.", "file": "lib_matplotlib_image.py", "function": "pil_to_array", "project": "matplotlib", "change_stats": {"code_changes": 3, "doc_changes": 18, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _fast_hmac(key, msg, digest):\n    \n    \n    \n    dig1, dig2 = digest(), digest()\n    key = smart_bytes(key)\n    if len(key) > dig1.block_size:\n        key = digest(key).digest()\n    key += chr(0) * (dig1.block_size - len(key))\n    dig1.update(key.translate(_trans_36))\n    dig1.update(msg)\n    dig2.update(key.translate(_trans_5c))\n    dig2.update(dig1.digest())\n    return dig2```\n\nUpdated code:\n```python\ndef _fast_hmac(key, msg, digest):\n    \n    \n    \n    dig1, dig2 = digest(), digest()\n    if len(key) > dig1.block_size:\n        key = digest(key).digest()\n    key += b'\\x00' * (dig1.block_size - len(key))\n    dig1.update(key.translate(_trans_36))\n    dig1.update(msg)\n    dig2.update(key.translate(_trans_5c))\n    dig2.update(dig1.digest())\n    return dig2\n```\n\nOriginal documentation:\n```python\nA trimmed down version of Python's HMAC implementation\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A trimmed down version of Python's HMAC implementation.\n\n    This function operates on bytes.", "file": "django_utils_crypto.py", "function": "_fast_hmac", "project": "django", "change_stats": {"code_changes": 5, "doc_changes": 6, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _prepare_single_hop_path_and_storage_options(\n    urlpath: str, download_config: Optional[DownloadConfig] = None\n) -> Tuple[str, Dict[str, Dict[str, Any]]]:\n    \n    \n    \n    token = None if download_config is None else download_config.token\n    protocol = urlpath.split(\"://\")[0] if \"://\" in urlpath else \"file\"\n    if download_config is not None and protocol in download_config.storage_options:\n        storage_options = {protocol: download_config.storage_options[protocol]}\n    else:\n        storage_options = {}\n    if protocol in [\"http\", \"https\"]:\n        storage_options[protocol] = {\n            \"headers\": {\n                **get_authentication_headers_for_url(urlpath, token=token),\n                \"user-agent\": get_datasets_user_agent(),\n            },\n            \"client_kwargs\": {\"trust_env\": True},  # Enable reading proxy env variables.\n            **(storage_options.get(protocol, {})),\n        }\n        if \"drive.google.com\" in urlpath:\n            response = http_head(urlpath)\n            cookies = None\n            for k, v in response.cookies.items():\n                if k.startswith(\"download_warning\"):\n                    urlpath += \"&confirm=\" + v\n                    cookies = response.cookies\n                    storage_options[protocol] = {\"cookies\": cookies, **storage_options.get(protocol, {})}\n        # Fix Google Drive URL to avoid Virus scan warning\n        if \"drive.google.com\" in urlpath and \"confirm=\" not in urlpath:\n            urlpath += \"&confirm=t\"\n        if urlpath.startswith(\"https://raw.githubusercontent.com/\"):\n            # Workaround for served data with gzip content-encoding: https://github.com/fsspec/filesystem_spec/issues/389\n            storage_options[protocol] = {\"block_size\": 0, **storage_options.get(protocol, {})}\n    elif protocol == \"hf\":\n        storage_options = {\"token\": token, \"endpoint\": config.HF_ENDPOINT, **(storage_options or {})}\n    return urlpath, storage_options```\n\nUpdated code:\n```python\ndef _prepare_single_hop_path_and_storage_options(\n    urlpath: str, download_config: Optional[DownloadConfig] = None\n) -> Tuple[str, Dict[str, Dict[str, Any]]]:\n    \n    \n    \n    token = None if download_config is None else download_config.token\n    protocol = urlpath.split(\"://\")[0] if \"://\" in urlpath else \"file\"\n    if download_config is not None and protocol in download_config.storage_options:\n        storage_options = download_config.storage_options[protocol]\n    elif download_config is not None and protocol not in download_config.storage_options:\n        storage_options = {\n            option_name: option_value\n            for option_name, option_value in download_config.storage_options.items()\n            if option_name not in fsspec.available_protocols()\n        }\n    else:\n        storage_options = {}\n    if storage_options:\n        storage_options = {protocol: storage_options}\n    if protocol in [\"http\", \"https\"]:\n        storage_options[protocol] = {\n            \"headers\": {\n                **get_authentication_headers_for_url(urlpath, token=token),\n                \"user-agent\": get_datasets_user_agent(),\n            },\n            \"client_kwargs\": {\"trust_env\": True},  # Enable reading proxy env variables.\n            **(storage_options.get(protocol, {})),\n        }\n        if \"drive.google.com\" in urlpath:\n            response = http_head(urlpath)\n            cookies = None\n            for k, v in response.cookies.items():\n                if k.startswith(\"download_warning\"):\n                    urlpath += \"&confirm=\" + v\n                    cookies = response.cookies\n                    storage_options[protocol] = {\"cookies\": cookies, **storage_options.get(protocol, {})}\n        # Fix Google Drive URL to avoid Virus scan warning\n        if \"drive.google.com\" in urlpath and \"confirm=\" not in urlpath:\n            urlpath += \"&confirm=t\"\n        if urlpath.startswith(\"https://raw.githubusercontent.com/\"):\n            # Workaround for served data with gzip content-encoding: https://github.com/fsspec/filesystem_spec/issues/389\n            storage_options[protocol] = {\"block_size\": 0, **storage_options.get(protocol, {})}\n    elif protocol == \"hf\":\n        storage_options[protocol] = {\n            \"token\": token,\n            \"endpoint\": config.HF_ENDPOINT,\n            **storage_options.get(protocol, {}),\n        }\n    return urlpath, storage_options\n```\n\nOriginal documentation:\n```python\nPrepare the URL and the kwargs that must be passed to the HttpFileSystem or to requests.get/head\n\n    In particular it resolves google drive URLs\n    It also adds the authentication headers for the Hugging Face Hub, for both https:// and hf:// paths.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Prepare the URL and the kwargs that must be passed to the HttpFileSystem or to requests.get/head\n\n    In particular it resolves google drive URLs\n    It also adds the authentication headers for the Hugging Face Hub, for both https:// and hf:// paths.\n\n    Storage options are formatted in the form {protocol: storage_options_for_protocol}", "file": "src_datasets_download_streaming_download_manager.py", "function": "_prepare_single_hop_path_and_storage_options", "project": "datasets", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convert_model_output(\n        self, model_output: torch.FloatTensor, timestep: int, sample: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        \n        \n        \n\n        # DPM-Solver++ needs to solve an integral of the data prediction model.\n        if self.config.algorithm_type in [\"dpmsolver++\", \"sde-dpmsolver++\"]:\n            if self.config.prediction_type == \"epsilon\":\n                # DPM-Solver and DPM-Solver++ only need the \"mean\" output.\n                if self.config.variance_type in [\"learned\", \"learned_range\"]:\n                    model_output = model_output[:, :3]\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                x0_pred = (sample - sigma_t * model_output) / alpha_t\n            elif self.config.prediction_type == \"sample\":\n                x0_pred = model_output\n            elif self.config.prediction_type == \"v_prediction\":\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                x0_pred = alpha_t * sample - sigma_t * model_output\n            else:\n                raise ValueError(\n                    f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n                    \" `v_prediction` for the DPMSolverMultistepScheduler.\"\n                )\n\n            if self.config.thresholding:\n                x0_pred = self._threshold_sample(x0_pred)\n\n            return x0_pred\n\n        # DPM-Solver needs to solve an integral of the noise prediction model.\n        elif self.config.algorithm_type in [\"dpmsolver\", \"sde-dpmsolver\"]:\n            if self.config.prediction_type == \"epsilon\":\n                # DPM-Solver and DPM-Solver++ only need the \"mean\" output.\n                if self.config.variance_type in [\"learned\", \"learned_range\"]:\n                    epsilon = model_output[:, :3]\n                else:\n                    epsilon = model_output\n            elif self.config.prediction_type == \"sample\":\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                epsilon = (sample - alpha_t * model_output) / sigma_t\n            elif self.config.prediction_type == \"v_prediction\":\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                epsilon = alpha_t * model_output + sigma_t * sample\n            else:\n                raise ValueError(\n                    f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n                    \" `v_prediction` for the DPMSolverMultistepScheduler.\"\n                )\n\n            if self.config.thresholding:\n                alpha_t, sigma_t = self.alpha_t[timestep], self.sigma_t[timestep]\n                x0_pred = (sample - sigma_t * epsilon) / alpha_t\n                x0_pred = self._threshold_sample(x0_pred)\n                epsilon = (sample - alpha_t * x0_pred) / sigma_t\n\n            return epsilon```\n\nUpdated code:\n```python\ndef convert_model_output(\n        self,\n        model_output: torch.FloatTensor,\n        *args,\n        sample: torch.FloatTensor = None,\n        **kwargs,\n    ) -> torch.FloatTensor:\n        \n        \n        \n        timestep = args[0] if len(args) > 0 else kwargs.pop(\"timestep\", None)\n        if sample is None:\n            if len(args) > 1:\n                sample = args[1]\n            else:\n                raise ValueError(\"missing `sample` as a required keyward argument\")\n        if timestep is not None:\n            deprecate(\n                \"timesteps\",\n                \"1.0.0\",\n                \"Passing `timesteps` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\n            )\n\n        # DPM-Solver++ needs to solve an integral of the data prediction model.\n        if self.config.algorithm_type in [\"dpmsolver++\", \"sde-dpmsolver++\"]:\n            if self.config.prediction_type == \"epsilon\":\n                # DPM-Solver and DPM-Solver++ only need the \"mean\" output.\n                if self.config.variance_type in [\"learned\", \"learned_range\"]:\n                    model_output = model_output[:, :3]\n                sigma = self.sigmas[self.step_index]\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\n                x0_pred = (sample - sigma_t * model_output) / alpha_t\n            elif self.config.prediction_type == \"sample\":\n                x0_pred = model_output\n            elif self.config.prediction_type == \"v_prediction\":\n                sigma = self.sigmas[self.step_index]\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\n                x0_pred = alpha_t * sample - sigma_t * model_output\n            else:\n                raise ValueError(\n                    f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n                    \" `v_prediction` for the DPMSolverMultistepScheduler.\"\n                )\n\n            if self.config.thresholding:\n                x0_pred = self._threshold_sample(x0_pred)\n\n            return x0_pred\n\n        # DPM-Solver needs to solve an integral of the noise prediction model.\n        elif self.config.algorithm_type in [\"dpmsolver\", \"sde-dpmsolver\"]:\n            if self.config.prediction_type == \"epsilon\":\n                # DPM-Solver and DPM-Solver++ only need the \"mean\" output.\n                if self.config.variance_type in [\"learned\", \"learned_range\"]:\n                    epsilon = model_output[:, :3]\n                else:\n                    epsilon = model_output\n            elif self.config.prediction_type == \"sample\":\n                sigma = self.sigmas[self.step_index]\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\n                epsilon = (sample - alpha_t * model_output) / sigma_t\n            elif self.config.prediction_type == \"v_prediction\":\n                sigma = self.sigmas[self.step_index]\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\n                epsilon = alpha_t * model_output + sigma_t * sample\n            else:\n                raise ValueError(\n                    f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n                    \" `v_prediction` for the DPMSolverMultistepScheduler.\"\n                )\n\n            if self.config.thresholding:\n                sigma = self.sigmas[self.step_index]\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\n                x0_pred = (sample - sigma_t * epsilon) / alpha_t\n                x0_pred = self._threshold_sample(x0_pred)\n                epsilon = (sample - alpha_t * x0_pred) / sigma_t\n\n            return epsilon\n```\n\nOriginal documentation:\n```python\nConvert the model output to the corresponding type the DPMSolver/DPMSolver++ algorithm needs. DPM-Solver is\n        designed to discretize an integral of the noise prediction model, and DPM-Solver++ is designed to discretize an\n        integral of the data prediction model.\n\n        <Tip>\n\n        The algorithm and model type are decoupled. You can use either DPMSolver or DPMSolver++ for both noise\n        prediction and data prediction models.\n\n        </Tip>\n\n        Args:\n            model_output (`torch.FloatTensor`):\n                The direct output from the learned diffusion model.\n            timestep (`int`):\n                The current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                A current instance of a sample created by the diffusion process.\n\n        Returns:\n            `torch.FloatTensor`:\n                The converted model output.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert the model output to the corresponding type the DPMSolver/DPMSolver++ algorithm needs. DPM-Solver is\n        designed to discretize an integral of the noise prediction model, and DPM-Solver++ is designed to discretize an\n        integral of the data prediction model.\n\n        <Tip>\n\n        The algorithm and model type are decoupled. You can use either DPMSolver or DPMSolver++ for both noise\n        prediction and data prediction models.\n\n        </Tip>\n\n        Args:\n            model_output (`torch.FloatTensor`):\n                The direct output from the learned diffusion model.\n            sample (`torch.FloatTensor`):\n                A current instance of a sample created by the diffusion process.\n\n        Returns:\n            `torch.FloatTensor`:\n                The converted model output.", "file": "src_diffusers_schedulers_scheduling_dpmsolver_multistep_inverse.py", "function": "DPMSolverMultistepInverseScheduler.convert_model_output", "project": "diffusers", "change_stats": {"code_changes": 35, "doc_changes": 4, "total_changes": 39}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef norm(x,y=2):\n    \n    \n    \n\n    x = asarray(x)\n    if numerix.mlab.rank(x)==2:\n        if y==2:\n            return numerix.mlab.max(numerix.mlab.svd(x)[1])\n        elif y==1:\n            return numerix.mlab.max(asum(absolute((x))))\n        elif y=='inf':\n            return numerix.mlab.max(asum(absolute((transpose(x)))))\n        elif y=='fro':\n            return numerix.mlab.sqrt(asum(numerix.mlab.diag(matrixmultiply(transpose(x),x))))\n        else:\n            raise RuntimeError('Second argument not permitted for matrices')\n        \n    else:\n        if y == 'inf':\n            return numerix.mlab.max(absolute(x))\n        elif y == '-inf':\n            return numerix.mlab.min(absolute(x))\n        else:\n            return power(asum(power(absolute(x),y)),1/float(y))```\n\nUpdated code:\n```python\ndef norm(a,p=2):\n    \n    \n    if p=='Infinity':\n        return max(absolute(a).flat)\n    else:\n        return (sum_flat(absolute(a)**p))**(1.0/p)\n```\n\nOriginal documentation:\n```python\nNorm of a matrix or a vector according to Matlab.\n    The description is taken from Matlab:\n    \n        For matrices...\n          NORM(X) is the largest singular value of X, max(svd(X)).\n          NORM(X,2) is the same as NORM(X).\n          NORM(X,1) is the 1-norm of X, the largest column sum,\n                          = max(sum(abs((X)))).\n          NORM(X,inf) is the infinity norm of X, the largest row sum,\n                          = max(sum(abs((X')))).\n          NORM(X,'fro') is the Frobenius norm, sqrt(sum(diag(X'*X))).\n          NORM(X,P) is available for matrix X only if P is 1, 2, inf or 'fro'.\n     \n        For vectors...\n          NORM(V,P) = sum(abs(V).^P)^(1/P).\n          NORM(V) = norm(V,2).\n          NORM(V,inf) = max(abs(V)).\n          NORM(V,-inf) = min(abs(V)).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "norm(a,p=2) -> l-p norm of a.flat\n\n    Return the l-p norm of a, considered as a flat array.  This is NOT a true\n    matrix norm, since arrays of arbitrary rank are always flattened.\n\n    p can be a number or the string 'Infinity' to get the L-infinity norm.", "file": "lib_matplotlib_mlab.py", "function": "norm", "project": "matplotlib", "change_stats": {"code_changes": 28, "doc_changes": 26, "total_changes": 54}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef async_all(self) -> List[State]:\n        \n        \n        return list(self._states.values())```\n\nUpdated code:\n```python\ndef async_all(\n        self, domain_filter: Optional[Union[str, Iterable]] = None\n    ) -> List[State]:\n        \n        \n        if domain_filter is None:\n            return list(self._states.values())\n\n        if isinstance(domain_filter, str):\n            domain_filter = (domain_filter.lower(),)\n\n        return [\n            state for state in self._states.values() if state.domain in domain_filter\n        ]\n```\n\nOriginal documentation:\n```python\nCreate a list of all states.\n\n        This method must be run in the event loop.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a list of all states matching the filter.\n\n        This method must be run in the event loop.", "file": "homeassistant_core.py", "function": "StateMachine.async_all", "project": "core", "change_stats": {"code_changes": 16, "doc_changes": 4, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _verify_ack_challenge(data, state, challenge):\n    \n    if not challenge or not challenge.get(\"ack\"):\n        raise ChallengeNeeded(CHALLENGE_ACK_NEEDED)```\n\nUpdated code:\n```python\ndef _verify_ack_challenge(data, state, challenge):\n    \n    if not data.config.should_2fa(state):\n        return\n    if not challenge or not challenge.get(\"ack\"):\n        raise ChallengeNeeded(CHALLENGE_ACK_NEEDED)\n```\n\nOriginal documentation:\n```python\nVerify a pin challenge.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Verify an ack challenge.", "file": "homeassistant_components_google_assistant_trait.py", "function": "_verify_ack_challenge", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef compile(self, metrics=None, weighted_metrics=None):\n    \n    \n\n    super(InferenceCoreModel, self).compile(\n        metrics=metrics, weighted_metrics=weighted_metrics\n    )```\n\nUpdated code:\n```python\ndef compile(self, metrics=None, weighted_metrics=None, **kwargs):\n    \n    \n\n    super(InferenceCoreModel, self).compile(\n        metrics=metrics, weighted_metrics=weighted_metrics, **kwargs\n    )\n```\n\nOriginal documentation:\n```python\nConfigure the model for training.\n\n    Unlike for most Keras model, calling \"compile\" is optional before calling\n    \"fit\".\n\n    Args:\n      metrics: List of metrics to be evaluated by the model during training and\n        testing.\n      weighted_metrics: List of metrics to be evaluated and weighted by\n        `sample_weight` or `class_weight` during training and testing.\n\n    Raises:\n      ValueError: Invalid arguments.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Configure the model for training.\n\n    Unlike for most Keras model, calling \"compile\" is optional before calling\n    \"fit\".\n\n    Args:\n      metrics: List of metrics to be evaluated by the model during training and\n        testing.\n      weighted_metrics: List of metrics to be evaluated and weighted by\n        `sample_weight` or `class_weight` during training and testing.\n      **kwargs: Other arguments passed to compile.\n\n    Raises:\n      ValueError: Invalid arguments.", "file": "tensorflow_decision_forests_keras_core_inference.py", "function": "InferenceCoreModel.compile", "project": "decision-forests", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _replace_coerce(\n        self,\n        to_replace,\n        value,\n        inplace: bool = True,\n        regex: bool = False,\n        convert: bool = False,\n        mask=None,\n    ):\n        \n        \n        \n        if mask.any():\n            if not regex:\n                self = self.coerce_to_target_dtype(value)\n                return self.putmask(mask, value, inplace=inplace)\n            else:\n                return self._replace_single(\n                    to_replace,\n                    value,\n                    inplace=inplace,\n                    regex=regex,\n                    convert=convert,\n                    mask=mask,\n                )\n        return self```\n\nUpdated code:\n```python\ndef _replace_coerce(\n        self,\n        to_replace,\n        value,\n        inplace: bool = True,\n        regex: bool = False,\n        mask=None,\n    ) -> List[\"Block\"]:\n        \n        \n        \n        if mask.any():\n            if not regex:\n                self = self.coerce_to_target_dtype(value)\n                return self.putmask(mask, value, inplace=inplace)\n            else:\n                return self._replace_single(\n                    to_replace,\n                    value,\n                    inplace=inplace,\n                    regex=regex,\n                    convert=False,\n                    mask=mask,\n                )\n        return [self]\n```\n\nOriginal documentation:\n```python\nReplace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default True\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        A new block if there is anything to replace or the original block.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default True\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        List[Block]", "file": "pandas_core_internals_blocks.py", "function": "Block._replace_coerce", "project": "pandas", "change_stats": {"code_changes": 9, "doc_changes": 6, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _index_to_param(self) -> Dict[int, torch.Tensor]:\n        \n        if len(self._index_to_param_cache) == 0:\n            self._index_to_param_cache = {i: p for i, p in enumerate(chain(*(g[\"params\"] for g in self.param_groups)))}\n\n        return self._index_to_param_cache```\n\nUpdated code:\n```python\ndef _index_to_param(self) -> Dict[int, torch.Tensor]:\n        \n        \n        if len(self._index_to_param_cache) == 0:\n            self._index_to_param_cache = {i: p for i, p in enumerate(chain(*(g[\"params\"] for g in self.param_groups)))}\n\n        return self._index_to_param_cache\n```\n\nOriginal documentation:\n```python\nHash table in between parameter indices in the global optimizer scheme, and the actual params\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Hash table in between parameter indices in the global optimizer scheme,\n        and the actual params.", "file": "torch_distributed_optim_zero_redundancy_optimizer.py", "function": "ZeroRedundancyOptimizer._index_to_param", "project": "pytorch", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_keys(self):\n        \n        \n        \n        msg = (\n            'Unsupported configuration option: {}. '\n            'Make sure the key name is correct.'\n        )\n        self.pop_config('version', None)\n        wrong_key = '.'.join(self._get_extra_key(self.raw_config))\n        if wrong_key:\n            self.error(\n                wrong_key,\n                msg.format(wrong_key),\n                code=INVALID_KEY,\n            )```\n\nUpdated code:\n```python\ndef validate_keys(self):\n        \n        \n        \n        msg = (\n            'Invalid configuration option: {}. '\n            'Make sure the key name is correct.'\n        )\n        # The version key isn't popped, but it's\n        # validated in `load`.\n        self.pop_config('version', None)\n        wrong_key = '.'.join(self._get_extra_key(self.raw_config))\n        if wrong_key:\n            self.error(\n                wrong_key,\n                msg.format(wrong_key),\n                code=INVALID_KEY,\n            )\n```\n\nOriginal documentation:\n```python\nChecks that we don't have extra keys.\n\n        This should be called after all the validations are done.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks that we don't have extra keys (invalid ones).\n\n        This should be called after all the validations are done\n        and all keys are popped from `self.raw_config`.", "file": "readthedocs_config_config.py", "function": "BuildConfigV2.validate_keys", "project": "readthedocs.org", "change_stats": {"code_changes": 6, "doc_changes": 7, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _subscribe_project_to_alert_rule(\n        project: Project, release: Release, trigger: str\n    ) -> list[QuerySubscription]:\n        \n        \n        \n        from sentry.incidents.models.alert_rule import AlertRule\n\n        query_extra = f\"release:{release.version} AND event.timestamp:>{timezone.now().isoformat()}\"\n        return AlertRule.objects.conditionally_subscribe_project_to_alert_rules(\n            project=project,\n            activation_condition=AlertRuleActivationConditionType.RELEASE_CREATION,\n            query_extra=query_extra,\n            trigger=trigger,\n        )```\n\nUpdated code:\n```python\ndef _subscribe_project_to_alert_rule(\n        project: Project, release: Release, trigger: str\n    ) -> list[QuerySubscription]:\n        \n        \n        \n        from sentry.incidents.models.alert_rule import AlertRule\n\n        query_extra = f\"release:{release.version} AND event.timestamp:>{timezone.now().isoformat()}\"\n        return AlertRule.objects.conditionally_subscribe_project_to_alert_rules(\n            project=project,\n            activation_condition=AlertRuleActivationConditionType.RELEASE_CREATION,\n            query_extra=query_extra,\n            origin=trigger,\n            activator=release.version,\n        )\n```\n\nOriginal documentation:\n```python\nTODO: potentially enable custom query_extra to be passed on ReleaseProject creation (on release/deploy)\n        NOTE: import AlertRule model here to avoid circular dependency\n        TODO: move once AlertRule has been split into separate subdirectory files\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "TODO: potentially enable custom query_extra to be passed on ReleaseProject creation (on release/deploy)\n\n        NOTE: import AlertRule model here to avoid circular dependency", "file": "src_sentry_models_releases_release_project.py", "function": "ReleaseProjectModelManager._subscribe_project_to_alert_rule", "project": "sentry", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef analyze_ticker(self, dataframe: DataFrame, metadata: dict) -> DataFrame:\n        \n        \n        \n        logger.debug(\"TA Analysis Launched\")\n        dataframe = self.advise_indicators(dataframe, metadata)\n        dataframe = self.advise_buy(dataframe, metadata)\n        dataframe = self.advise_sell(dataframe, metadata)\n        return dataframe```\n\nUpdated code:\n```python\ndef analyze_ticker(self, dataframe: DataFrame, metadata: dict) -> DataFrame:\n        \n        \n        \n        logger.debug(\"TA Analysis Launched\")\n        dataframe = self.advise_indicators(dataframe, metadata)\n        dataframe = self.advise_entry(dataframe, metadata)\n        dataframe = self.advise_exit(dataframe, metadata)\n        return dataframe\n```\n\nOriginal documentation:\n```python\nParses the given candle (OHLCV) data and returns a populated DataFrame\n        add several TA indicators and buy signal to it\n        :param dataframe: Dataframe containing data from exchange\n        :param metadata: Metadata dictionary with additional data (e.g. 'pair')\n        :return: DataFrame of candle (OHLCV) data with indicator data and signals added\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parses the given candle (OHLCV) data and returns a populated DataFrame\n        add several TA indicators and entry order signal to it\n        :param dataframe: Dataframe containing data from exchange\n        :param metadata: Metadata dictionary with additional data (e.g. 'pair')\n        :return: DataFrame of candle (OHLCV) data with indicator data and signals added", "file": "freqtrade_strategy_interface.py", "function": "IStrategy.analyze_ticker", "project": "freqtrade", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_adjustable(self, adjustable, share=False):\n        \n        \n        \n        if adjustable == 'box-forced':\n            warnings.warn(\"The 'box-forced' keyword argument is deprecated\"\n                          \" since 2.2.\", cbook.mplDeprecation)\n        if adjustable not in ('box', 'datalim', 'box-forced'):\n            raise ValueError(\"argument must be 'box', or 'datalim'\")\n        if share:\n            axes = set(self._shared_x_axes.get_siblings(self)\n                       + self._shared_y_axes.get_siblings(self))\n        else:\n            axes = [self]\n        for ax in axes:\n            ax._adjustable = adjustable\n        self.stale = True```\n\nUpdated code:\n```python\ndef set_adjustable(self, adjustable, share=False):\n        \n        \n        \n        if adjustable == 'box-forced':\n            warnings.warn(\"The 'box-forced' keyword argument is deprecated\"\n                          \" since 2.2.\", cbook.mplDeprecation, stacklevel=2)\n        if adjustable not in ('box', 'datalim', 'box-forced'):\n            raise ValueError(\"argument must be 'box', or 'datalim'\")\n        if share:\n            axes = set(self._shared_x_axes.get_siblings(self)\n                       + self._shared_y_axes.get_siblings(self))\n        else:\n            axes = [self]\n        for ax in axes:\n            ax._adjustable = adjustable\n        self.stale = True\n```\n\nOriginal documentation:\n```python\nDefine which parameter the Axes will change to achieve a given aspect.\n\n        Parameters\n        ----------\n        adjustable : ['box' | 'datalim']\n            If 'box', change the physical dimensions of the Axes.\n            If 'datalim', change the ``x`` or ``y`` data limits.\n\n        share : bool, optional\n            If ``True``, apply the settings to all shared Axes.\n            Default is ``False``.\n\n        .. ACCEPTS: [ 'box' | 'datalim']\n\n        See Also\n        --------\n        matplotlib.axes.Axes.set_aspect\n            for a description of aspect handling.\n\n        Notes\n        -----\n        Shared Axes (of which twinned Axes are a special case)\n        impose restrictions on how aspect ratios can be imposed.\n        For twinned Axes, use 'datalim'.  For Axes that share both\n        x and y, use 'box'.  Otherwise, either 'datalim' or 'box'\n        may be used.  These limitations are partly a requirement\n        to avoid over-specification, and partly a result of the\n        particular implementation we are currently using, in\n        which the adjustments for aspect ratios are done sequentially\n        and independently on each Axes as it is drawn.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Define which parameter the Axes will change to achieve a given aspect.\n\n        Parameters\n        ----------\n        adjustable : ['box' | 'datalim']\n            If 'box', change the physical dimensions of the Axes.\n            If 'datalim', change the ``x`` or ``y`` data limits.\n\n        share : bool, optional\n            If ``True``, apply the settings to all shared Axes.\n            Default is ``False``.\n\n        See Also\n        --------\n        matplotlib.axes.Axes.set_aspect\n            for a description of aspect handling.\n\n        Notes\n        -----\n        Shared Axes (of which twinned Axes are a special case)\n        impose restrictions on how aspect ratios can be imposed.\n        For twinned Axes, use 'datalim'.  For Axes that share both\n        x and y, use 'box'.  Otherwise, either 'datalim' or 'box'\n        may be used.  These limitations are partly a requirement\n        to avoid over-specification, and partly a result of the\n        particular implementation we are currently using, in\n        which the adjustments for aspect ratios are done sequentially\n        and independently on each Axes as it is drawn.", "file": "lib_matplotlib_axes__base.py", "function": "_AxesBase.set_adjustable", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _offsets_to_raveled_neighbors(image_shape, selem, center, order='C'):\n    \n    \n    if not selem.ndim == len(image_shape) == len(center):\n        raise ValueError(\n            \"number of dimensions in image shape, structuring element and its\"\n            \"center index does not match\"\n        )\n\n    selem_indices = np.stack(np.nonzero(selem), axis=-1)\n    offsets = selem_indices - center\n\n    if order == 'F':\n        offsets = offsets[:, ::-1]\n        image_shape = image_shape[::-1]\n    elif order != 'C':\n        raise ValueError(\"order must be 'C' or 'F'\")\n\n    # Scale offsets in each dimension and sum\n    ravel_factors = image_shape[1:] + (1,)\n    ravel_factors = np.cumprod(ravel_factors[::-1])[::-1]\n    raveled_offsets = (offsets * ravel_factors).sum(axis=1)\n\n    # Sort by distance\n    distances = np.abs(offsets).sum(axis=1)\n    raveled_offsets = raveled_offsets[np.argsort(distances)]\n\n    # If any dimension in image_shape is smaller than selem.shape\n    # duplicates might occur, remove them\n    if any(x < y for x, y in zip(image_shape, selem.shape)):\n        # np.unique reorders, which we don't want\n        _, indices = np.unique(raveled_offsets, return_index=True)\n        raveled_offsets = raveled_offsets[np.sort(indices)]\n\n    # Remove \"offset to center\"\n    raveled_offsets = raveled_offsets[1:]\n\n    return raveled_offsets```\n\nUpdated code:\n```python\ndef _offsets_to_raveled_neighbors(image_shape, footprint, center, order='C'):\n    \n    \n    if not footprint.ndim == len(image_shape) == len(center):\n        raise ValueError(\n            \"number of dimensions in image shape, footprint and its\"\n            \"center index does not match\"\n        )\n\n    footprint_indices = np.stack(np.nonzero(footprint), axis=-1)\n    offsets = footprint_indices - center\n\n    if order == 'F':\n        offsets = offsets[:, ::-1]\n        image_shape = image_shape[::-1]\n    elif order != 'C':\n        raise ValueError(\"order must be 'C' or 'F'\")\n\n    # Scale offsets in each dimension and sum\n    ravel_factors = image_shape[1:] + (1,)\n    ravel_factors = np.cumprod(ravel_factors[::-1])[::-1]\n    raveled_offsets = (offsets * ravel_factors).sum(axis=1)\n\n    # Sort by distance\n    distances = np.abs(offsets).sum(axis=1)\n    raveled_offsets = raveled_offsets[np.argsort(distances)]\n\n    # If any dimension in image_shape is smaller than footprint.shape\n    # duplicates might occur, remove them\n    if any(x < y for x, y in zip(image_shape, footprint.shape)):\n        # np.unique reorders, which we don't want\n        _, indices = np.unique(raveled_offsets, return_index=True)\n        raveled_offsets = raveled_offsets[np.sort(indices)]\n\n    # Remove \"offset to center\"\n    raveled_offsets = raveled_offsets[1:]\n\n    return raveled_offsets\n```\n\nOriginal documentation:\n```python\nCompute offsets to a samples neighbors if the image would be raveled.\n\n    Parameters\n    ----------\n    image_shape : tuple\n        The shape of the image for which the offsets are computed.\n    selem : ndarray\n        A structuring element determining the neighborhood expressed as an\n        n-D array of 1's and 0's.\n    center : tuple\n        Tuple of indices to the center of `selem`.\n    order : {\"C\", \"F\"}, optional\n        Whether the image described by `image_shape` is in row-major (C-style)\n        or column-major (Fortran-style) order.\n\n    Returns\n    -------\n    raveled_offsets : ndarray\n        Linear offsets to a samples neighbors in the raveled image, sorted by\n        their distance from the center.\n\n    Notes\n    -----\n    This function will return values even if `image_shape` contains a dimension\n    length that is smaller than `selem`.\n\n    Examples\n    --------\n    >>> _offsets_to_raveled_neighbors((4, 5), np.ones((4, 3)), (1, 1))\n    array([-5, -1,  1,  5, -6, -4,  4,  6, 10,  9, 11])\n    >>> _offsets_to_raveled_neighbors((2, 3, 2), np.ones((3, 3, 3)), (1, 1, 1))\n    array([ 2, -6,  1, -1,  6, -2,  3,  8, -3, -4,  7, -5, -7, -8,  5,  4, -9,\n            9])\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute offsets to a samples neighbors if the image would be raveled.\n\n    Parameters\n    ----------\n    image_shape : tuple\n        The shape of the image for which the offsets are computed.\n    footprint : ndarray\n        The footprint (structuring element) determining the neighborhood\n        expressed as an n-D array of 1's and 0's.\n    center : tuple\n        Tuple of indices to the center of `footprint`.\n    order : {\"C\", \"F\"}, optional\n        Whether the image described by `image_shape` is in row-major (C-style)\n        or column-major (Fortran-style) order.\n\n    Returns\n    -------\n    raveled_offsets : ndarray\n        Linear offsets to a samples neighbors in the raveled image, sorted by\n        their distance from the center.\n\n    Notes\n    -----\n    This function will return values even if `image_shape` contains a dimension\n    length that is smaller than `footprint`.\n\n    Examples\n    --------\n    >>> _offsets_to_raveled_neighbors((4, 5), np.ones((4, 3)), (1, 1))\n    array([-5, -1,  1,  5, -6, -4,  4,  6, 10,  9, 11])\n    >>> _offsets_to_raveled_neighbors((2, 3, 2), np.ones((3, 3, 3)), (1, 1, 1))\n    array([ 2, -6,  1, -1,  6, -2,  3,  8, -3, -4,  7, -5, -7, -8,  5,  4, -9,\n            9])", "file": "skimage_morphology__util.py", "function": "_offsets_to_raveled_neighbors", "project": "scikit-image", "change_stats": {"code_changes": 16, "doc_changes": 12, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n        \n        \n        return self(batch)```\n\nUpdated code:\n```python\ndef predict_step(self, *args: Any, **kwargs: Any) -> Any:\n        \n        \n        # For backwards compatibility\n        batch = kwargs.get(\"batch\", args[0])\n        return self(batch)\n```\n\nOriginal documentation:\n```python\nStep function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it\n        calls :meth:`~lightning.pytorch.core.module.LightningModule.forward`. Override to add any processing logic.\n\n        The :meth:`~lightning.pytorch.core.module.LightningModule.predict_step` is used\n        to scale inference on multi-devices.\n\n        To prevent an OOM error, it is possible to use :class:`~lightning.pytorch.callbacks.BasePredictionWriter`\n        callback to write the predictions to disk or database after each batch or on epoch end.\n\n        The :class:`~lightning.pytorch.callbacks.BasePredictionWriter` should be used while using a spawn\n        based accelerator. This happens for ``Trainer(strategy=\"ddp_spawn\")``\n        or training on 8 TPU cores with ``Trainer(accelerator=\"tpu\", devices=8)`` as predictions won't be returned.\n\n        Example ::\n\n            class MyModel(LightningModule):\n\n                def predict_step(self, batch, batch_idx, dataloader_idx=0):\n                    return self(batch)\n\n            dm = ...\n            model = MyModel()\n            trainer = Trainer(accelerator=\"gpu\", devices=2)\n            predictions = trainer.predict(model, dm)\n\n\n        Args:\n            batch: Current batch.\n            batch_idx: Index of current batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Return:\n            Predicted output\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it\n        calls :meth:`~lightning.pytorch.core.module.LightningModule.forward`. Override to add any processing logic.\n\n        The :meth:`~lightning.pytorch.core.module.LightningModule.predict_step` is used\n        to scale inference on multi-devices.\n\n        To prevent an OOM error, it is possible to use :class:`~lightning.pytorch.callbacks.BasePredictionWriter`\n        callback to write the predictions to disk or database after each batch or on epoch end.\n\n        The :class:`~lightning.pytorch.callbacks.BasePredictionWriter` should be used while using a spawn\n        based accelerator. This happens for ``Trainer(strategy=\"ddp_spawn\")``\n        or training on 8 TPU cores with ``Trainer(accelerator=\"tpu\", devices=8)`` as predictions won't be returned.\n\n        Args:\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n            batch_idx: The index of this batch.\n            dataloader_idx: The index of the dataloader that produced this batch.\n                (only if multiple dataloaders used)\n\n        Return:\n            Predicted output (optional).\n\n        Example ::\n\n            class MyModel(LightningModule):\n\n                def predict_step(self, batch, batch_idx, dataloader_idx=0):\n                    return self(batch)\n\n            dm = ...\n            model = MyModel()\n            trainer = Trainer(accelerator=\"gpu\", devices=2)\n            predictions = trainer.predict(model, dm)", "file": "src_lightning_pytorch_core_module.py", "function": "LightningModule.predict_step", "project": "pytorch-lightning", "change_stats": {"code_changes": 6, "doc_changes": 20, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' fetchOpenOrders() requires a symbol argument')\n        self.load_markets()\n        market = self.market(symbol)\n        method = 'privateGetSpotOrders'\n        if market['inverse']:\n            method = 'privateGetOrdersActiveList'\n        elif market['settle'] == 'USDT':\n            method = 'privateGetGOrdersActiveList'\n        request = {\n            'symbol': market['id'],\n        }\n        response = None\n        try:\n            response = getattr(self, method)(self.extend(request, params))\n        except Exception as e:\n            if isinstance(e, OrderNotFound):\n                return []\n            raise e\n        data = self.safe_value(response, 'data', {})\n        if isinstance(data, list):\n            return self.parse_orders(data, market, since, limit)\n        else:\n            rows = self.safe_value(data, 'rows', [])\n            return self.parse_orders(rows, market, since, limit)```\n\nUpdated code:\n```python\ndef fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' fetchOpenOrders() requires a symbol argument')\n        self.load_markets()\n        market = self.market(symbol)\n        method = 'privateGetSpotOrders'\n        if market['settle'] == 'USDT':\n            method = 'privateGetGOrdersActiveList'\n        elif market['swap']:\n            method = 'privateGetOrdersActiveList'\n        request = {\n            'symbol': market['id'],\n        }\n        response = None\n        try:\n            response = getattr(self, method)(self.extend(request, params))\n        except Exception as e:\n            if isinstance(e, OrderNotFound):\n                return []\n            raise e\n        data = self.safe_value(response, 'data', {})\n        if isinstance(data, list):\n            return self.parse_orders(data, market, since, limit)\n        else:\n            rows = self.safe_value(data, 'rows', [])\n            return self.parse_orders(rows, market, since, limit)\n```\n\nOriginal documentation:\n```python\nfetch all unfilled currently open orders\n        see https://github.com/phemex/phemex-api-docs/blob/master/Public-Hedged-Perpetual-API.md#queryopenorder\n        :param str symbol: unified market symbol\n        :param int|None since: the earliest time in ms to fetch open orders for\n        :param int|None limit: the maximum number of  open orders structures to retrieve\n        :param dict params: extra parameters specific to the phemex api endpoint\n        :returns [dict]: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch all unfilled currently open orders\n        see https://github.com/phemex/phemex-api-docs/blob/master/Public-Hedged-Perpetual-API.md#queryopenorder\n        see https://github.com/phemex/phemex-api-docs/blob/master/Public-Contract-API-en.md\n        :param str symbol: unified market symbol\n        :param int|None since: the earliest time in ms to fetch open orders for\n        :param int|None limit: the maximum number of  open orders structures to retrieve\n        :param dict params: extra parameters specific to the phemex api endpoint\n        :returns [dict]: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`", "file": "python_ccxt_phemex.py", "function": "phemex.fetch_open_orders", "project": "ccxt", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rsqrt(x):\n    \n    \n    xp = backend.get_array_module(x)\n    if xp is numpy:\n        return 1.0 / sqrt(x)\n\n    # CuPy provides `rsqrt` which is faster than `1.0 / sqrt(x)`.\n    return RsqrtGPU().apply((x,))[0]```\n\nUpdated code:\n```python\ndef rsqrt(x):\n    \n    \n    xp = backend.get_array_module(x)\n    if xp is numpy or xp is chainerx:\n        return 1.0 / sqrt(x)\n\n    # CuPy provides `rsqrt` which is faster than `1.0 / sqrt(x)`.\n    return RsqrtGPU().apply((x,))[0]\n```\n\nOriginal documentation:\n```python\nComputes elementwise reciprocal of square root of input :math:`x_i`.\n\n    .. math::\n       y_i = {1 \\\\over \\\\sqrt x_i}.\n\n    Args:\n        x (~chainer.Variable): Input variable.\n\n    Returns:\n        ~chainer.Variable: Output variable.\n\n    .. seealso:: :func:`~chainer.functions.sqrt`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes elementwise reciprocal of square root of input :math:`x_i`.\n\n    .. math::\n       y_i = {1 \\\\over \\\\sqrt x_i}.\n\n    Args:\n        x (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\n\n    Returns:\n        ~chainer.Variable: Output variable.\n\n    .. seealso:: :func:`~chainer.functions.sqrt`", "file": "chainer_", "function": "rsqrt", "project": "chainer", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef infer_dtype_from_array(arr):\n    \n    \n\n    \n\n    if isinstance(arr, np.ndarray):\n        return arr.dtype, arr\n\n    if not is_list_like(arr):\n        arr = [arr]\n\n    # don't force numpy coerce with nan's\n    inferred = lib.infer_dtype(arr)\n    if inferred in ['string', 'bytes', 'unicode',\n                    'mixed', 'mixed-integer']:\n        return (np.object_, arr)\n\n    arr = np.asarray(arr)\n    return arr.dtype, arr```\n\nUpdated code:\n```python\ndef infer_dtype_from_array(arr, pandas_dtype=False):\n    \n    \n\n    \n\n    if isinstance(arr, np.ndarray):\n        return arr.dtype, arr\n\n    if not is_list_like(arr):\n        arr = [arr]\n\n    if pandas_dtype and is_extension_type(arr):\n        return arr.dtype, arr\n\n    elif isinstance(arr, ABCSeries):\n        return arr.dtype, np.asarray(arr)\n\n    # don't force numpy coerce with nan's\n    inferred = lib.infer_dtype(arr)\n    if inferred in ['string', 'bytes', 'unicode',\n                    'mixed', 'mixed-integer']:\n        return (np.object_, arr)\n\n    arr = np.asarray(arr)\n    return arr.dtype, arr\n```\n\nOriginal documentation:\n```python\ninfer the dtype from a scalar or array\n\n    Parameters\n    ----------\n    arr : scalar or array\n\n    Returns\n    -------\n    tuple (numpy-compat dtype, array)\n\n    Notes\n    -----\n    These infer to numpy dtypes exactly\n    with the exception that mixed / object dtypes\n    are not coerced by stringifying or conversion\n\n    Examples\n    --------\n    >>> np.asarray([1, '1'])\n    array(['1', '1'], dtype='<U21')\n\n    >>> infer_dtype_from_array([1, '1'])\n    (numpy.object_, [1, '1'])\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "infer the dtype from a scalar or array\n\n    Parameters\n    ----------\n    arr : scalar or array\n    pandas_dtype : bool, default False\n        whether to infer dtype including pandas extension types.\n        If False, array belongs to pandas extension types\n        is inferred as object\n\n    Returns\n    -------\n    tuple (numpy-compat/pandas-compat dtype, array)\n\n    Notes\n    -----\n    if pandas_dtype=False. these infer to numpy dtypes\n    exactly with the exception that mixed / object dtypes\n    are not coerced by stringifying or conversion\n\n    if pandas_dtype=True. datetime64tz-aware/categorical\n    types will retain there character.\n\n    Examples\n    --------\n    >>> np.asarray([1, '1'])\n    array(['1', '1'], dtype='<U21')\n\n    >>> infer_dtype_from_array([1, '1'])\n    (numpy.object_, [1, '1'])", "file": "pandas_core_dtypes_cast.py", "function": "infer_dtype_from_array", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 15, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef circle(r, c, radius, shape=None):\n    \n    \n    return ellipse(r, c, radius, radius, shape)```\n\nUpdated code:\n```python\ndef circle(r, c, radius, shape=None):\n    \n    \n    warnings.warn(\"circle is deprecated in favor of \"\n                  \"disk.\"\n                  \"circle will be removed in version 0.19\",\n                  FutureWarning, stacklevel=2)\n    return disk((r, c), radius, shape=shape)\n```\n\nOriginal documentation:\n```python\nGenerate coordinates of pixels within circle.\n\n    Parameters\n    ----------\n    r, c : double\n        Centre coordinate of circle.\n    radius : double\n        Radius of circle.\n    shape : tuple, optional\n        Image shape which is used to determine the maximum extent of output\n        pixel coordinates. This is useful for circles that exceed the image\n        size. If None, the full extent of the circle is used.  Must be at least\n        length 2. Only the first two values are used to determine the extent of\n        the input image.\n\n    Returns\n    -------\n    rr, cc : ndarray of int\n        Pixel coordinates of circle.\n        May be used to directly index into an array, e.g.\n        ``img[rr, cc] = 1``.\n\n    Examples\n    --------\n    >>> from skimage.draw import circle\n    >>> img = np.zeros((10, 10), dtype=np.uint8)\n    >>> rr, cc = circle(4, 4, 5)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n           [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n           [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate coordinates of pixels within circle.\n\n    Parameters\n    ----------\n    r, c : double\n        Center coordinate of disk.\n    radius : double\n        Radius of disk.\n    shape : tuple, optional\n        Image shape which is used to determine the maximum extent of output\n        pixel coordinates. This is useful for disks that exceed the image\n        size. If None, the full extent of the disk is used.  Must be at least\n        length 2. Only the first two values are used to determine the extent of\n        the input image.\n\n    Returns\n    -------\n    rr, cc : ndarray of int\n        Pixel coordinates of disk.\n        May be used to directly index into an array, e.g.\n        ``img[rr, cc] = 1``.\n\n    Warns\n    -----\n    Deprecated:\n        .. versionadded:: 0.17\n\n            This function is deprecated and will be removed in scikit-image 0.19.\n            Please use the function named ``disk`` instead.", "file": "skimage_draw_draw.py", "function": "circle", "project": "scikit-image", "change_stats": {"code_changes": 8, "doc_changes": 36, "total_changes": 44}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef translate(text, lang):\n    \n    \n    \n    translated = TRANSLATIONS.get(lang, {}).get(text.lower(), text)\n    if text == translated:\n        print(\"%s: %s\" % (lang, text))\n    return translated```\n\nUpdated code:\n```python\ndef translate(text, lang):\n    \n    \n    \n\n    if \",\" in text:\n        terms = text.split(\",\")\n        translated_terms = [translate(term.strip(), lang) for term in terms]\n        return \", \".join(translated_terms)\n\n    translated = TRANSLATIONS.get(lang, {}).get(text.lower(), text)\n    return translated\n```\n\nOriginal documentation:\n```python\nTranslate `text` into `lang`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Translate `text` into `lang`.\n    If `text` is comma-separated, translate each term independently.\n    If no translation found, leave it untouched.", "file": "bin_proxy.py", "function": "translate", "project": "wttr.in", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef vgg11_bn(**kwargs):\n    \n    return VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)```\n\nUpdated code:\n```python\ndef vgg11_bn(pretrained=False, **kwargs):\n    \n    \n    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n    return model\n```\n\nOriginal documentation:\n```python\nVGG 11-layer model (configuration \"A\") with batch normalization\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "VGG 11-layer model (configuration \"A\") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet", "file": "torchvision_models_vgg.py", "function": "vgg11_bn", "project": "vision", "change_stats": {"code_changes": 10, "doc_changes": 5, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_tensor(x):\n    \n    if is_torch_available():\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n    if is_tf_available():\n        import tensorflow as tf\n\n        if isinstance(x, tf.Tensor):\n            return True\n    return isinstance(x, np.ndarray)```\n\nUpdated code:\n```python\ndef is_tensor(x):\n    \n    \n    \n    if is_torch_available():\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n    if is_tf_available():\n        import tensorflow as tf\n\n        if isinstance(x, tf.Tensor):\n            return True\n\n    if is_flax_available():\n        import jaxlib.xla_extension as jax_xla\n        from jax.interpreters.partial_eval import DynamicJaxprTracer\n\n        if isinstance(x, (jax_xla.DeviceArray, DynamicJaxprTracer)):\n            return True\n\n    return isinstance(x, np.ndarray)\n```\n\nOriginal documentation:\n```python\nTests if ``x`` is a :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tests if ``x`` is a :obj:`torch.Tensor`, :obj:`tf.Tensor`, obj:`jaxlib.xla_extension.DeviceArray` or\n    :obj:`np.ndarray`.", "file": "src_transformers_file_utils.py", "function": "is_tensor", "project": "transformers", "change_stats": {"code_changes": 12, "doc_changes": 5, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef count_records(file_list):\n  \n  num_records = 0\n  for tfrecord_file in file_list:\n    tf.logging.info('Counting records in %s.', tfrecord_file)\n    for _ in tf.python_io.tf_record_iterator(tfrecord_file):\n      num_records += 1\n  tf.logging.info('Total records: %d', num_records)\n  return num_records```\n\nUpdated code:\n```python\ndef count_records(file_list, stop_at=None):\n  \n  \n  num_records = 0\n  for tfrecord_file in file_list:\n    tf.logging.info('Counting records in %s.', tfrecord_file)\n    for _ in tf.python_io.tf_record_iterator(tfrecord_file):\n      num_records += 1\n      if stop_at and num_records >= stop_at:\n        tf.logging.info('Number of records is at least %d.', num_records)\n        return num_records\n  tf.logging.info('Total records: %d', num_records)\n  return num_records\n```\n\nOriginal documentation:\n```python\nReturns number of records in files from file_list.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Counts number of records in files from `file_list` up to `stop_at`.\n\n  Args:\n    file_list: List of TFRecord files to count records in.\n    stop_at: Optional number of records to stop counting at.\n\n  Returns:\n    Integer number of records in files from `file_list` up to `stop_at`.", "file": "magenta_common_sequence_example_lib.py", "function": "count_records", "project": "magenta", "change_stats": {"code_changes": 8, "doc_changes": 11, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_valid(self, request_data, raises=False):\n        \n        \n        \n        self.__error = None\n        try:\n            root = OneLogin_Saml2_XML.to_etree(self.__logout_request)\n\n            idp_data = self.__settings.get_idp_data()\n            idp_entity_id = idp_data['entityId']\n\n            get_data = ('get_data' in request_data and request_data['get_data']) or dict()\n\n            if self.__settings.is_strict():\n                res = OneLogin_Saml2_XML.validate_xml(root, 'saml-schema-protocol-2.0.xsd', self.__settings.is_debug_active())\n                if isinstance(res, str):\n                    raise Exception('Invalid SAML Logout Request. Not match the saml-schema-protocol-2.0.xsd')\n\n                security = self.__settings.get_security_data()\n\n                current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data)\n\n                # Check NotOnOrAfter\n                if root.get('NotOnOrAfter', None):\n                    na = OneLogin_Saml2_Utils.parse_SAML_to_time(root.get('NotOnOrAfter'))\n                    if na <= OneLogin_Saml2_Utils.now():\n                        raise Exception('Timing issues (please check your clock settings)')\n\n                # Check destination\n                if root.get('Destination', None):\n                    destination = root.get('Destination')\n                    if destination != '':\n                        if current_url not in destination:\n                            raise Exception(\n                                'The LogoutRequest was received at '\n                                '%(currentURL)s instead of %(destination)s' %\n                                {\n                                    'currentURL': current_url,\n                                    'destination': destination,\n                                }\n                            )\n\n                # Check issuer\n                issuer = OneLogin_Saml2_Logout_Request.get_issuer(root)\n                if issuer is not None and issuer != idp_entity_id:\n                    raise Exception('Invalid issuer in the Logout Request')\n\n                if security['wantMessagesSigned']:\n                    if 'Signature' not in get_data:\n                        raise Exception('The Message of the Logout Request is not signed and the SP require it')\n            return True\n        except Exception as err:\n            # pylint: disable=R0801\n            self.__error = str(err)\n            debug = self.__settings.is_debug_active()\n            if debug:\n                print(err)\n            if raises:\n                raise\n            return False```\n\nUpdated code:\n```python\ndef is_valid(self, request_data, raise_exceptions=False):\n        \n        \n        \n        self.__error = None\n        try:\n            root = OneLogin_Saml2_XML.to_etree(self.__logout_request)\n\n            idp_data = self.__settings.get_idp_data()\n            idp_entity_id = idp_data['entityId']\n\n            get_data = ('get_data' in request_data and request_data['get_data']) or dict()\n\n            if self.__settings.is_strict():\n                res = OneLogin_Saml2_XML.validate_xml(root, 'saml-schema-protocol-2.0.xsd', self.__settings.is_debug_active())\n                if isinstance(res, str):\n                    raise Exception('Invalid SAML Logout Request. Not match the saml-schema-protocol-2.0.xsd')\n\n                security = self.__settings.get_security_data()\n\n                current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data)\n\n                # Check NotOnOrAfter\n                if root.get('NotOnOrAfter', None):\n                    na = OneLogin_Saml2_Utils.parse_SAML_to_time(root.get('NotOnOrAfter'))\n                    if na <= OneLogin_Saml2_Utils.now():\n                        raise Exception('Timing issues (please check your clock settings)')\n\n                # Check destination\n                if root.get('Destination', None):\n                    destination = root.get('Destination')\n                    if destination != '':\n                        if current_url not in destination:\n                            raise Exception(\n                                'The LogoutRequest was received at '\n                                '%(currentURL)s instead of %(destination)s' %\n                                {\n                                    'currentURL': current_url,\n                                    'destination': destination,\n                                }\n                            )\n\n                # Check issuer\n                issuer = OneLogin_Saml2_Logout_Request.get_issuer(root)\n                if issuer is not None and issuer != idp_entity_id:\n                    raise Exception('Invalid issuer in the Logout Request')\n\n                if security['wantMessagesSigned']:\n                    if 'Signature' not in get_data:\n                        raise Exception('The Message of the Logout Request is not signed and the SP require it')\n            return True\n        except Exception as err:\n            # pylint: disable=R0801\n            self.__error = str(err)\n            debug = self.__settings.is_debug_active()\n            if debug:\n                print(err)\n            if raise_exceptions:\n                raise\n            return False\n```\n\nOriginal documentation:\n```python\nChecks if the Logout Request received is valid\n        :param request_data: Request Data\n        :type request_data: dict\n\n        :param raises: Optional argument. If true, the function will raise an exception as soon as first validation test fails\n        :type raises: bool\n\n        :return: If the Logout Request is or not valid\n        :rtype: boolean\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks if the Logout Request received is valid\n        :param request_data: Request Data\n        :type request_data: dict\n\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n\n        :return: If the Logout Request is or not valid\n        :rtype: boolean", "file": "src_onelogin_saml2_logout_request.py", "function": "OneLogin_Saml2_Logout_Request.is_valid", "project": "python3-saml", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef from_env(cls, values: dict) -> dict:\n        \n        \n        settings = {}\n        settings.update(dotenv_values(ENV_FILE_REPOSITORY))\n        settings.update(dotenv_values(ENV_FILE_PROJECT))\n        settings.update(dotenv_values(ENV_FILE_SETTINGS))\n        settings.update(values)\n        filtered = {k.replace(\"OPENBB_\", \"\"): v for k, v in settings.items()}\n        return filtered```\n\nUpdated code:\n```python\ndef from_env(cls, values: dict) -> dict:\n        \n        settings = {}\n        settings.update(dotenv_values(ENV_FILE_SETTINGS))\n        settings.update(values)\n        filtered = {k.replace(\"OPENBB_\", \"\"): v for k, v in settings.items()}\n        return filtered\n```\n\nOriginal documentation:\n```python\nLoad .env files.\n\n        Loads the dotenv files in the following order:\n        1. Repository .env file\n        2. Package .env file\n        3. User .env file\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load settings from .env.", "file": "cli_openbb_cli_models_settings.py", "function": "Settings.from_env", "project": "OpenBBTerminal", "change_stats": {"code_changes": 5, "doc_changes": 9, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef liquidation_price(\n        self,\n        open_rate: float,   # Entry price of position\n        is_short: bool,\n        leverage: float,\n        mm_ratio: float,\n        position: float,  # Absolute value of position size\n        trading_mode: TradingMode,\n        collateral: Optional[Collateral] = Collateral.ISOLATED,\n        maintenance_amt: Optional[float] = None,  # (Binance)\n        wallet_balance: Optional[float] = None,  # (Binance and Gateio)\n        taker_fee_rate: Optional[float] = None,  # (Gateio & Okex)\n        liability: Optional[float] = None,  # (Okex)\n        interest: Optional[float] = None,  # (Okex)\n        mm_ex_1: Optional[float] = 0.0,  # (Binance) Cross only\n        upnl_ex_1: Optional[float] = 0.0,  # (Binance) Cross only\n    ) -> Optional[float]:\n        \n        \n        \n        if trading_mode == TradingMode.SPOT:\n            return None\n\n        if not collateral:\n            raise OperationalException(\n                \"Parameter collateral is required by liquidation_price when trading_mode is \"\n                f\"{trading_mode}\"\n            )\n\n        return self.liquidation_price_helper(\n            open_rate,\n            is_short,\n            leverage,\n            trading_mode,\n            mm_ratio,\n            collateral,\n            maintenance_amt,\n            position,\n            wallet_balance,\n            taker_fee_rate,\n            liability,\n            interest,\n            mm_ex_1,\n            upnl_ex_1,\n        )```\n\nUpdated code:\n```python\ndef liquidation_price(\n        self,\n        open_rate: float,   # Entry price of position\n        is_short: bool,\n        leverage: float,\n        mm_ratio: float,\n        position: float,  # Absolute value of position size\n        trading_mode: TradingMode,\n        collateral: Optional[Collateral] = Collateral.ISOLATED,\n        maintenance_amt: Optional[float] = None,  # (Binance)\n        wallet_balance: Optional[float] = None,  # (Binance and Gateio)\n        taker_fee_rate: Optional[float] = None,  # (Gateio & Okex)\n        mm_ex_1: Optional[float] = 0.0,  # (Binance) Cross only\n        upnl_ex_1: Optional[float] = 0.0,  # (Binance) Cross only\n    ) -> Optional[float]:\n        \n        \n        \n        if trading_mode == TradingMode.SPOT:\n            return None\n\n        if not collateral:\n            raise OperationalException(\n                \"Parameter collateral is required by liquidation_price when trading_mode is \"\n                f\"{trading_mode}\"\n            )\n\n        return self.liquidation_price_helper(\n            open_rate=open_rate,\n            is_short=is_short,\n            leverage=leverage,\n            mm_ratio=mm_ratio,\n            position=position,\n            trading_mode=trading_mode,\n            collateral=collateral,\n            maintenance_amt=maintenance_amt,\n            wallet_balance=wallet_balance,\n            taker_fee_rate=taker_fee_rate,\n            mm_ex_1=mm_ex_1,\n            upnl_ex_1=upnl_ex_1,\n        )\n```\n\nOriginal documentation:\n```python\n:param exchange_name:\n        :param open_rate: (EP1) Entry price of position\n        :param is_short: True if the trade is a short, false otherwise\n        :param leverage: The amount of leverage on the trade\n        :param trading_mode: SPOT, MARGIN, FUTURES, etc.\n        :param position: Absolute value of position size (in base currency)\n        :param mm_ratio: (MMR)\n            Okex: [assets in the position - (liability +interest) * mark price] /\n                (maintenance margin + liquidation fee)\n            # * Note: Binance's formula specifies maintenance margin rate which is mm_ratio * 100%\n        :param collateral: Either ISOLATED or CROSS\n\n        # * Binance\n        :param maintenance_amt: (CUM) Maintenance Amount of position\n\n        # * Binance and Gateio\n        :param wallet_balance: (WB)\n            Cross-Margin Mode: crossWalletBalance\n            Isolated-Margin Mode: isolatedWalletBalance\n        :param position: Absolute value of position size (in base currency)\n\n        # * Gateio & Okex\n        :param taker_fee_rate:\n\n        # * Okex\n        :param liability:\n            Initial liabilities + deducted interest\n                \u2022 Long positions: Liability is calculated in quote currency.\n                \u2022 Short positions: Liability is calculated in trading currency.\n        :param interest:\n            Interest that has not been deducted yet.\n        :param position_assets:\n            Total position assets \u2013 on-hold by pending order\n\n        # * Cross only (Binance)\n        :param mm_ex_1: (TMM)\n            Cross-Margin Mode: Maintenance Margin of all other contracts, excluding Contract 1\n            Isolated-Margin Mode: 0\n        :param upnl_ex_1: (UPNL)\n            Cross-Margin Mode: Unrealized PNL of all other contracts, excluding Contract 1.\n            Isolated-Margin Mode: 0\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":param exchange_name:\n        :param open_rate: (EP1) Entry price of position\n        :param is_short: True if the trade is a short, false otherwise\n        :param leverage: The amount of leverage on the trade\n        :param mm_ratio: (MMR)\n            Okex: [assets in the position - (liability +interest) * mark price] /\n                (maintenance margin + liquidation fee)\n            # * Note: Binance's formula specifies maintenance margin rate which is mm_ratio * 100%\n        :param position: Absolute value of position size (in base currency)\n        :param trading_mode: SPOT, MARGIN, FUTURES, etc.\n        :param collateral: Either ISOLATED or CROSS\n\n        # * Binance\n        :param maintenance_amt: (CUM) Maintenance Amount of position\n\n        # * Binance and Gateio\n        :param wallet_balance: (WB)\n            Cross-Margin Mode: crossWalletBalance\n            Isolated-Margin Mode: isolatedWalletBalance\n\n        # * Gateio & Okex\n        :param taker_fee_rate:\n\n        # * Cross only (Binance)\n        :param mm_ex_1: (TMM)\n            Cross-Margin Mode: Maintenance Margin of all other contracts, excluding Contract 1\n            Isolated-Margin Mode: 0\n        :param upnl_ex_1: (UPNL)\n            Cross-Margin Mode: Unrealized PNL of all other contracts, excluding Contract 1.\n            Isolated-Margin Mode: 0", "file": "freqtrade_exchange_exchange.py", "function": "Exchange.liquidation_price", "project": "freqtrade", "change_stats": {"code_changes": 30, "doc_changes": 17, "total_changes": 47}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _convert_to_indexer(self, obj, axis=0):\n        \n        \n\n        \n        index = self.frame._get_axis(axis)\n        is_int_index = _is_integer_index(index)\n        if isinstance(obj, slice):\n            if _is_label_slice(index, obj):\n                i, j = index.slice_locs(obj.start, obj.stop)\n                return slice(i, j)\n            else:\n                return obj\n        elif _is_list_like(obj):\n            obj = np.asarray(obj)\n\n            if _is_integer_dtype(obj):\n                if is_int_index:\n                    raise AmbiguousIndexError('integer labels')\n\n                # retrieve the indices corresponding\n                return obj\n            else:\n                indexer, mask = index.get_indexer(obj)\n                if not mask.all():\n                    raise Exception('%s not in index' % obj[-mask])\n\n                return indexer\n        else:\n            if _is_int_like(obj):\n                if is_int_index:\n                    raise AmbiguousIndexError('integer labels')\n                return obj\n            return index.get_loc(obj)```\n\nUpdated code:\n```python\ndef _convert_to_indexer(self, obj, axis=0):\n        \n        \n\n        \n        index = self.frame._get_axis(axis)\n        is_int_index = _is_integer_index(index)\n        if isinstance(obj, slice):\n            if _is_label_slice(index, obj):\n                i, j = index.slice_locs(obj.start, obj.stop)\n                return slice(i, j)\n            else:\n                return obj\n        elif _is_list_like(obj):\n            objarr = np.asarray(obj)\n\n            if _is_integer_dtype(objarr):\n                if is_int_index:\n                    raise AmbiguousIndexError('integer labels')\n\n                # retrieve the indices corresponding\n                return objarr\n            elif objarr.dtype == np.bool_:\n                if not obj.index.equals(index):\n                    raise Exception('Cannot use boolean index with misaligned '\n                                    'or unequal labels')\n                return objarr\n            else:\n                indexer, mask = index.get_indexer(objarr)\n                if not mask.all():\n                    raise KeyError('%s not in index' % objarr[-mask])\n\n                return indexer\n        else:\n            if _is_int_like(obj):\n                if is_int_index:\n                    raise AmbiguousIndexError('integer labels')\n                return obj\n            return index.get_loc(obj)\n```\n\nOriginal documentation:\n```python\nConvert indexing key into something we can use to do actual fancy\n        indexing on an ndarray\n\n        Examples\n        ix[:5] -> slice(0, 5)\n        ix[[1,2,3]] -> [1,2,3]\n        ix[['foo', 'bar', 'baz']] -> [i, j, k] (indices of foo, bar, baz)\n\n        Going by Zen of Python?\n        \"In the face of ambiguity, refuse the temptation to guess.\"\n        Going to raise AmbiguousIndexError with integer labels?\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert indexing key into something we can use to do actual fancy\n        indexing on an ndarray\n\n        Examples\n        ix[:5] -> slice(0, 5)\n        ix[[1,2,3]] -> [1,2,3]\n        ix[['foo', 'bar', 'baz']] -> [i, j, k] (indices of foo, bar, baz)\n\n        Going by Zen of Python?\n        \"In the face of ambiguity, refuse the temptation to guess.\"\n        raise AmbiguousIndexError with integer labels?", "file": "pandas_core_indexing.py", "function": "_DataFrameIndexer._convert_to_indexer", "project": "pandas", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef detect_data_type(self, data):\n        \n        data = data.strip()\n        try:\n            ipaddress.ip_address(data)\n            return 'ip'\n        except (ipaddress.AddressValueError, ValueError):\n            pass\n\n        try:\n            ipaddress.ip_network(data)\n            return 'ip_range'\n        except ValueError:\n            pass\n\n        if validators.md5(data):\n            return 'md5'\n\n        if validators.domain(data):\n            return 'domain'\n\n        # some ti files have / at the end of domains, remove it\n        if data.endswith('/'):\n            data = data[:-1]\n\n        domain = data\n\n        if domain.startswith('http://'):\n            data = data[7:]\n        elif domain.startswith('https://'):\n            data = data[8:]\n\n        if validators.domain(data):\n            return 'domain'\n        elif '/' in data:\n            return 'url'\n\n        if validators.sha256(data):\n            return 'sha256'```\n\nUpdated code:\n```python\ndef detect_data_type(self, data):\n        \n        \n        \n        data = data.strip()\n        try:\n            ipaddress.ip_address(data)\n            return 'ip'\n        except (ipaddress.AddressValueError, ValueError):\n            pass\n\n        try:\n            ipaddress.ip_network(data)\n            return 'ip_range'\n        except ValueError:\n            pass\n\n        if validators.md5(data):\n            return 'md5'\n\n        if validators.domain(data):\n            return 'domain'\n\n        # some ti files have / at the end of domains, remove it\n        if data.endswith('/'):\n            data = data[:-1]\n\n        domain = data\n        if domain.startswith('http://'):\n            data = data[7:]\n        elif domain.startswith('https://'):\n            data = data[8:]\n\n        if validators.domain(data):\n            return 'domain'\n        elif '/' in data:\n            return 'url'\n\n        if validators.sha256(data):\n            return 'sha256'\n\n        if data.startswith(\"AS\"):\n            return 'asn'\n```\n\nOriginal documentation:\n```python\nDetects if incoming data is ipv4, ipv6, domain or ip range\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Detects the type of incoming data: ipv4, ipv6, domain, ip range, asn, md5, etc", "file": "slips_files_common_slips_utils.py", "function": "Utils.detect_data_type", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef heuristic(self):\n        \n        \n        \n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef heuristic(self, num_particles=1024):\n        \n        \n        \n        # Run SMC.\n        model = _SMCModel(self)\n        guide = _SMCGuide(self)\n        smc = SMCFilter(model, guide, num_particles=num_particles,\n                        max_plate_nesting=self.max_plate_nesting)\n        smc.init()\n        for t in range(1, self.duration):\n            smc.step()\n\n        # Select the most probably hypothesis.\n        i = int(smc.state._log_weights.max(0).indices)\n        init = {key: value[i] for key, value in smc.state.items()}\n\n        # Fill in sample site values.\n        init = self.generate(init)\n        init[\"auxiliary\"] = torch.stack(\n            [init[name] for name in self.compartments]).clamp_(min=0.5)\n        return init\n```\n\nOriginal documentation:\n```python\nFinds an initial feasible guess of all latent variables, consistent\n        with observed data. This is needed because not all hypotheses are\n        feasible and HMC needs to start at a feasible solution to progress.\n\n        :returns: A dictionary mapping sample site name to tensor value.\n        :rtype: dict\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Finds an initial feasible guess of all latent variables, consistent\n        with observed data. This is needed because not all hypotheses are\n        feasible and HMC needs to start at a feasible solution to progress.\n\n        The default implementation attempts to find a feasible state using\n        :class:`~pyro.infer.smcfilter.SMCFilter` with proprosals from the\n        prior.  However this method may be overridden in cases where SMC\n        performs poorly e.g. in high-dimensional models.\n\n        :param int num_particles: Number of particles used for SMC.\n        :returns: A dictionary mapping sample site name to tensor value.\n        :rtype: dict", "file": "pyro_contrib_epidemiology_compartmental.py", "function": "CompartmentalModel.heuristic", "project": "pyro", "change_stats": {"code_changes": 23, "doc_changes": 8, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_primary_email(self, user):\n        \n        \n        \n        email_qs = user.emailaddress_set.filter(primary=True)\n        email = email_qs.first()\n        if not email or not email.verified:\n            notification = EmailConfirmNotification(user=user, success=False)\n            notification.send()```\n\nUpdated code:\n```python\ndef validate_primary_email(self, user):\n        \n        \n        \n        email_qs = user.emailaddress_set.filter(primary=True)\n        email = email_qs.first()\n        if not email or not email.verified:\n            Notification.objects.add(\n                attached_to=user,\n                message_id=MESSAGE_EMAIL_VALIDATION_PENDING,\n                dismissable=True,\n                format_values={\n                    \"account_email_url\": reverse(\"account_email\"),\n                },\n            )\n```\n\nOriginal documentation:\n```python\nSends a persistent error notification.\n\n        Checks if the user has a primary email or if the primary email\n        is verified or not. Sends a persistent error notification if\n        either of the condition is False.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sends a dismissable site notification to this user.\n\n        Checks if the user has a primary email or if the primary email\n        is verified or not. Sends a dismissable notification if\n        either of the condition is False.", "file": "readthedocs_projects_views_private.py", "function": "ProjectDashboard.validate_primary_email", "project": "readthedocs.org", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch(self):\n        \n        \n        examples = self.get_examples(None, None)\n        if self.mode is tuple:\n            return examples\n        elif self.mode is dict:\n            return dict(six.moves.zip(self.keys, examples))```\n\nUpdated code:\n```python\ndef fetch(self):\n        \n        \n        examples = self.get_examples(None, None)\n        if self.mode is tuple:\n            return examples\n        elif self.mode is dict:\n            return dict(six.moves.zip(self.keys, examples))\n        elif self.mode is None:\n            return examples[0]\n```\n\nOriginal documentation:\n```python\nFetch data.\n\n        This method fetches all data of the dataset/view.\n        Note that this method returns a column-major data\n        (i.e. :obj:`([a[0], ..., a[3]], ..., [c[0], ... c[3]])` or\n        :obj:`{'a': [a[0], ..., a[3]], ..., 'c': [c[0], ..., c[3]]}`).\n\n        Returns:\n            If :attr:`mode` is :class:`tuple`,\n            this method returns a tuple of lists/arrays.\n            If :attr:`mode` is :class:`dict`,\n            this method returns a dict of lists/arrays.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fetch data.\n\n        This method fetches all data of the dataset/view.\n        Note that this method returns a column-major data\n        (i.e. :obj:`([a[0], ..., a[3]], ..., [c[0], ... c[3]])`,\n        :obj:`{'a': [a[0], ..., a[3]], ..., 'c': [c[0], ..., c[3]]}`, or\n        :obj:`[a[0], ..., a[3]]`).\n\n        Returns:\n            If :attr:`mode` is :class:`tuple`,\n            this method returns a tuple of lists/arrays.\n            If :attr:`mode` is :class:`dict`,\n            this method returns a dict of lists/arrays.", "file": "chainer_dataset_tabular_tabular_dataset.py", "function": "TabularDataset.fetch", "project": "chainer", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef encode_image(image: np.ndarray,\n                 extension: str,\n                 metadata: PNGHeaderDict | None = None) -> bytes:\n     \n    \n    if metadata and extension.lower() != \".png\":\n        raise ValueError(\"Metadata is only supported for .png images\")\n    retval = cv2.imencode(extension, image)[1]\n    if metadata:\n        retval = png_write_meta(retval.tobytes(), metadata)\n    return retval```\n\nUpdated code:\n```python\ndef encode_image(image: np.ndarray,\n                 extension: str,\n                 encoding_args: tuple[int, ...] | None = None,\n                 metadata: PNGHeaderDict | dict[str, T.Any] | bytes | None = None) -> bytes:\n     \n    \n    if metadata and extension.lower() not in (\".png\", \".tif\"):\n        raise ValueError(\"Metadata is only supported for .png and .tif images\")\n    args = tuple() if encoding_args is None else encoding_args\n\n    retval = cv2.imencode(extension, image, args)[1]\n    if metadata:\n        func = {\".png\": png_write_meta, \".tif\": tiff_write_meta}[extension]\n        retval = func(retval.tobytes(), metadata)  # type:ignore[arg-type]\n    return retval\n```\n\nOriginal documentation:\n```python\nEncode an image.\n\n    Parameters\n    ----------\n    image: numpy.ndarray\n        The image to be encoded in `BGR` channel order.\n    extension: str\n        A compatible `cv2` image file extension that the final image is to be saved to.\n    metadata: dict, optional\n        Metadata for the image. If provided, and the extension is png, this information will be\n        written to the PNG itxt header. Default:``None``\n\n    Returns\n    -------\n    encoded_image: bytes\n        The image encoded into the correct file format\n\n    Example\n    -------\n    >>> image_file = \"/path/to/image.png\"\n    >>> image = read_image(image_file)\n    >>> encoded_image = encode_image(image, \".jpg\")\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Encode an image.\n\n    Parameters\n    ----------\n    image: numpy.ndarray\n        The image to be encoded in `BGR` channel order.\n    extension: str\n        A compatible `cv2` image file extension that the final image is to be saved to.\n    encoding_args: tuple[int, ...], optional\n        Any encoding arguments to pass to cv2's imencode function\n    metadata: dict or bytes, optional\n        Metadata for the image. If provided, and the extension is png or tiff, this information\n        will be written to the PNG itxt header. Default:``None`` Can be provided as a python dict\n        or pre-encoded\n\n    Returns\n    -------\n    encoded_image: bytes\n        The image encoded into the correct file format\n\n    Example\n    -------\n    >>> image_file = \"/path/to/image.png\"\n    >>> image = read_image(image_file)\n    >>> encoded_image = encode_image(image, \".jpg\")", "file": "lib_image.py", "function": "encode_image", "project": "faceswap", "change_stats": {"code_changes": 16, "doc_changes": 11, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_create_stream_policy_setting(self) -> None:\n        \n        \n        \n        user_profile = self.example_user('hamlet')\n        user_profile.date_joined = timezone_now()\n        user_profile.save()\n        self.login_user(user_profile)\n        do_change_is_admin(user_profile, False)\n\n        # Allow all members to create streams.\n        do_set_realm_property(user_profile.realm, 'create_stream_policy',\n                              Realm.CREATE_STREAM_POLICY_MEMBERS)\n        # Set waiting period to 10 days.\n        do_set_realm_property(user_profile.realm, 'waiting_period_threshold', 10)\n\n        # Can successfully create stream despite being less than waiting period and not an admin,\n        # due to create stream policy.\n        stream_name = ['all_members']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)\n\n        # Allow only administrators to create streams.\n        do_set_realm_property(user_profile.realm, 'create_stream_policy',\n                              Realm.CREATE_STREAM_POLICY_ADMINS)\n\n        # Cannot create stream because not an admin.\n        stream_name = ['admins_only']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_error(result, 'User cannot create streams.')\n\n        # Make current user an admin.\n        do_change_is_admin(user_profile, True)\n\n        # Can successfully create stream as user is now an admin.\n        stream_name = ['admins_only']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)\n\n        # Allow users older than the waiting period to create streams.\n        do_set_realm_property(user_profile.realm, 'create_stream_policy',\n                              Realm.CREATE_STREAM_POLICY_FULL_MEMBERS)\n\n        # Can successfully create stream despite being under waiting period because user is admin.\n        stream_name = ['waiting_period_as_admin']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)\n\n        # Make current user no longer an admin.\n        do_change_is_admin(user_profile, False)\n\n        # Cannot create stream because user is not an admin and is not older than the waiting\n        # period.\n        stream_name = ['waiting_period']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_error(result, 'User cannot create streams.')\n\n        # Make user account 11 days old..\n        user_profile.date_joined = timezone_now() - timedelta(days=11)\n        user_profile.save()\n\n        # Can successfully create stream now that account is old enough.\n        stream_name = ['waiting_period']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)```\n\nUpdated code:\n```python\ndef test_create_stream_policy_setting(self) -> None:\n        \n        \n        \n        user_profile = self.example_user('hamlet')\n        user_profile.date_joined = timezone_now()\n        user_profile.save()\n        self.login_user(user_profile)\n        do_change_is_admin(user_profile, False)\n\n        # Allow all members to create streams.\n        do_set_realm_property(user_profile.realm, 'create_stream_policy',\n                              Realm.POLICY_MEMBERS_ONLY)\n        # Set waiting period to 10 days.\n        do_set_realm_property(user_profile.realm, 'waiting_period_threshold', 10)\n\n        # Can successfully create stream despite being less than waiting period and not an admin,\n        # due to create stream policy.\n        stream_name = ['all_members']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)\n\n        # Allow only administrators to create streams.\n        do_set_realm_property(user_profile.realm, 'create_stream_policy',\n                              Realm.POLICY_ADMINS_ONLY)\n\n        # Cannot create stream because not an admin.\n        stream_name = ['admins_only']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_error(result, 'User cannot create streams.')\n\n        # Make current user an admin.\n        do_change_is_admin(user_profile, True)\n\n        # Can successfully create stream as user is now an admin.\n        stream_name = ['admins_only']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)\n\n        # Allow users older than the waiting period to create streams.\n        do_set_realm_property(user_profile.realm, 'create_stream_policy',\n                              Realm.POLICY_FULL_MEMBERS_ONLY)\n\n        # Can successfully create stream despite being under waiting period because user is admin.\n        stream_name = ['waiting_period_as_admin']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)\n\n        # Make current user no longer an admin.\n        do_change_is_admin(user_profile, False)\n\n        # Cannot create stream because user is not an admin and is not older than the waiting\n        # period.\n        stream_name = ['waiting_period']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_error(result, 'User cannot create streams.')\n\n        # Make user account 11 days old..\n        user_profile.date_joined = timezone_now() - timedelta(days=11)\n        user_profile.save()\n\n        # Can successfully create stream now that account is old enough.\n        stream_name = ['waiting_period']\n        result = self.common_subscribe_to_streams(user_profile, stream_name)\n        self.assert_json_success(result)\n```\n\nOriginal documentation:\n```python\nWhen realm.create_stream_policy setting is Realm.CREATE_STREAM_POLICY_MEMBERS then\n        test that any user can create a stream.\n\n        When realm.create_stream_policy setting is Realm.CREATE_STREAM_POLICY_ADMINS then\n        test that only admins can create a stream.\n\n        When realm.create_stream_policy setting is Realm.CREATE_STREAM_POLICY_FULL_MEMBERS then\n        test that admins and users with accounts older than the waiting period can create a stream.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "When realm.create_stream_policy setting is Realm.POLICY_MEMBERS_ONLY then\n        test that any user can create a stream.\n\n        When realm.create_stream_policy setting is Realm.POLICY_ADMINS_ONLY then\n        test that only admins can create a stream.\n\n        When realm.create_stream_policy setting is Realm.POLICY_FULL_MEMBERS_ONLY then\n        test that admins and users with accounts older than the waiting period can create a stream.", "file": "zerver_tests_test_subs.py", "function": "StreamAdminTest.test_create_stream_policy_setting", "project": "zulip", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sdpa_flop_count(query_shape, key_shape):\n    \n    \n    \n    b, h, s, d = query_shape\n    b2, h2, s2, d2 = key_shape\n    assert b == b2 and h == h2 and d == d2\n    total_flops = 0\n    # [b, h, s, d] @ [b, h, d, s2] -> [b, h, s, s2]\n    total_flops += bmm_flop([(b * h, s, d), (b * h, d, s2)])\n    # [b, h, s, s2] @ [b, h, s2, d] -> [b, h, s2, d]\n    total_flops += bmm_flop([(b * h, s, s2), (b * h, s2, d)])\n    return total_flops```\n\nUpdated code:\n```python\ndef sdpa_flop_count(query_shape, key_shape):\n    \n    \n    \n    b, h, s, d = query_shape\n    _b2, _h2, s2, _d2 = key_shape\n    assert b == _b2 and h == _h2 and d == _d2\n    total_flops = 0\n    # q: [b, h, s, d] @ k: [b, h, d, s2] -> scores: [b, h, s, s2]\n    total_flops += bmm_flop((b * h, s, d), (b * h, d, s2))\n    # scores: [b, h, s, s2] @ v: [b, h, s2, d] -> out: [b, h, s, d]\n    total_flops += bmm_flop((b * h, s, s2), (b * h, s2, d))\n    return total_flops\n```\n\nOriginal documentation:\n```python\nCount flops for self-attention.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Count flops for self-attention.\n    NB: We can assume that value_shape == key_shape", "file": "torch_utils_flop_counter.py", "function": "sdpa_flop_count", "project": "pytorch", "change_stats": {"code_changes": 14, "doc_changes": 3, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef setDataDirectory(self, data_directory):\n\n        \n\n        \n        \n\n        self.__train_images_folder = os.path.join(data_directory, \"train/sample_images/\")\n        self.__train_annotations_folder = os.path.join(data_directory, \"train/annotations/\")\n        self.__validation_images_folder = os.path.join(data_directory, \"validation/sample_images/\")\n        self.__validation_annotations_folder = os.path.join(data_directory, \"validation/annotations/\")\n\n        if os.path.exists(os.path.join(data_directory, \"cache\")) == False:\n            os.makedirs(os.path.join(data_directory, \"cache\"))\n        self.__train_cache_file = os.path.join(data_directory, \"cache\", \"detection_train_data.pkl\")\n        self.__validation_cache_file = os.path.join(data_directory, \"cache\", \"detection_test_data.pkl\")\n\n        if os.path.exists(os.path.join(data_directory, \"models\")) == False:\n            os.makedirs(os.path.join(data_directory, \"models\"))\n\n        if os.path.exists(os.path.join(data_directory, \"json\")) == False:\n            os.makedirs(os.path.join(data_directory, \"json\"))\n\n        self.__model_directory = os.path.join(data_directory, \"models\")\n        self.__train_weights_name = os.path.join(self.__model_directory, \"detection_model-\")\n        self.__json_directory = os.path.join(data_directory, \"json\")```\n\nUpdated code:\n```python\ndef setDataDirectory(self, data_directory):\n\n        \n\n        \n        \n\n        self.__train_images_folder = os.path.join(data_directory, \"train/data-images/\")\n        self.__train_annotations_folder = os.path.join(data_directory, \"train/annotations/\")\n        self.__validation_images_folder = os.path.join(data_directory, \"validation/data-images/\")\n        self.__validation_annotations_folder = os.path.join(data_directory, \"validation/annotations/\")\n\n        if os.path.exists(os.path.join(data_directory, \"cache\")) == False:\n            os.makedirs(os.path.join(data_directory, \"cache\"))\n        self.__train_cache_file = os.path.join(data_directory, \"cache\", \"detection_train_data.pkl\")\n        self.__validation_cache_file = os.path.join(data_directory, \"cache\", \"detection_test_data.pkl\")\n\n        if os.path.exists(os.path.join(data_directory, \"models\")) == False:\n            os.makedirs(os.path.join(data_directory, \"models\"))\n\n        if os.path.exists(os.path.join(data_directory, \"json\")) == False:\n            os.makedirs(os.path.join(data_directory, \"json\"))\n\n        self.__model_directory = os.path.join(data_directory, \"models\")\n        self.__train_weights_name = os.path.join(self.__model_directory, \"detection_model-\")\n        self.__json_directory = os.path.join(data_directory, \"json\")\n```\n\nOriginal documentation:\n```python\n'setDataDirectory()' is required to set the path to which the data/dataset to be used for\n                 training is kept. The directory can have any name, but it must have 'train' and 'validation'\n                 sub-directory. In the 'train' and 'validation' sub-directories, there must be 'sample_images' and 'annotations'\n                 sub-directories respectively. The 'sample_images' folder will contain the pictures for the dataset and the\n                 'annotations' folder will contain the XML files with details of the annotations for each image in the\n                 'images folder'.\n\n                 N.B: Strictly take note that the filenames (without the extension) of the pictures in the 'images folder'\n                  must be the same as the filenames (without the extension) of their corresponding annotation XML files in\n                  the 'annotations' folder.\n\n                 The structure of the 'train' and 'validation' folder must be as follows:\n\n                >> train    >> images       >> img_1.jpg\n                            >> images       >> img_2.jpg\n                            >> images       >> img_3.jpg\n                            >> annotations  >> img_1.xml\n                            >> annotations  >> img_2.xml\n                            >> annotations  >> img_3.xml\n\n\n                >> validation   >> images       >> img_151.jpg\n                                >> images       >> img_152.jpg\n                                >> images       >> img_153.jpg\n                                >> annotations  >> img_151.xml\n                                >> annotations  >> img_152.xml\n                                >> annotations  >> img_153.xml\n\n        :param data_directory:\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "'setDataDirectory()' is required to set the path to which the data/dataset to be used for\n                 training is kept. The directory can have any name, but it must have 'train' and 'validation'\n                 sub-directory. In the 'train' and 'validation' sub-directories, there must be 'data-images' and 'annotations'\n                 sub-directories respectively. The 'data-images' folder will contain the pictures for the dataset and the\n                 'annotations' folder will contain the XML files with details of the annotations for each image in the\n                 'images folder'.\n\n                 N.B: Strictly take note that the filenames (without the extension) of the pictures in the 'images folder'\n                  must be the same as the filenames (without the extension) of their corresponding annotation XML files in\n                  the 'annotations' folder.\n\n                 The structure of the 'train' and 'validation' folder must be as follows:\n\n                >> train    >> images       >> img_1.jpg\n                            >> images       >> img_2.jpg\n                            >> images       >> img_3.jpg\n                            >> annotations  >> img_1.xml\n                            >> annotations  >> img_2.xml\n                            >> annotations  >> img_3.xml\n\n\n                >> validation   >> images       >> img_151.jpg\n                                >> images       >> img_152.jpg\n                                >> images       >> img_153.jpg\n                                >> annotations  >> img_151.xml\n                                >> annotations  >> img_152.xml\n                                >> annotations  >> img_153.xml\n\n        :param data_directory:\n        :return:", "file": "imageai_Detection_Custom___init__.py", "function": "DetectionModelTrainer.setDataDirectory", "project": "ImageAI", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef remove(self, result):\n        \n\n        \n        if isinstance(result, basestring):\n            result = self.app.AsyncResult(result)\n        try:\n            self.results.remove(result)\n        except ValueError:\n            raise KeyError(result)```\n\nUpdated code:\n```python\ndef remove(self, result):\n        \n\n        \n        if isinstance(result, string_t):\n            result = self.app.AsyncResult(result)\n        try:\n            self.results.remove(result)\n        except ValueError:\n            raise KeyError(result)\n```\n\nOriginal documentation:\n```python\nRemoves result from the set; it must be a member.\n\n        :raises KeyError: if the result is not a member.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Remove result from the set; it must be a member.\n\n        :raises KeyError: if the result is not a member.", "file": "celery_result.py", "function": "ResultSet.remove", "project": "celery", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mock_test_setup(\n    test_api_calendar,\n    mock_calendars_list,\n):\n    \n    mock_calendars_list({\"items\": [test_api_calendar]})```\n\nUpdated code:\n```python\ndef mock_test_setup(\n    hass,\n    test_api_calendar,\n    mock_calendars_list,\n    config_entry,\n):\n    \n    mock_calendars_list({\"items\": [test_api_calendar]})\n    config_entry.add_to_hass(hass)\n    return\n```\n\nOriginal documentation:\n```python\nFixture that sets up the default API responses during integration setup.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fixture that pulls in the default fixtures for tests in this file.", "file": "tests_components_google_test_calendar.py", "function": "mock_test_setup", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _upload(self):\n        \n        \n        \n        amount = len(self.images)\n        logger.info(f'Uploading screenshots to AzurStat, amount: {amount}')\n        if amount == 0:\n            # logger.warning(f'Image upload failed, no images to upload')\n            return False\n        image = pack(self.images)\n        output = io.BytesIO()\n        image.save(output, format='png')\n        output.seek(0)\n\n        now = int(time.time() * 1000)\n        data = {'file': (f'{now}.png', output, 'image/png')}\n        headers = {'user-agent': self._user_agent()}\n        session = requests.Session()\n        session.mount('http://', HTTPAdapter(max_retries=5))\n        session.mount('https://', HTTPAdapter(max_retries=5))\n        try:\n            resp = session.post(self.API, files=data, headers=headers, timeout=self.TIMEOUT)\n        except Exception as e:\n            logger.warning(f'Image upload failed, {e}')\n            return False\n\n        if resp.status_code == 200:\n            # print(resp.text)\n            info = json.loads(resp.text)\n            code = info.get(\"code\", 500)\n            if code == 200:\n                logger.info(f'Image upload success, imgid: {info.get(\"imgid\", \"\")}')\n                return True\n            elif code == 0:\n                logger.warning(f'Image upload failed, msg: {info.get(\"msg\", \"\")}')\n                return False\n\n        logger.warning(f'Image upload failed, unexpected server returns, '\n                       f'status_code: {resp.status_code}, returns: {resp.text}')\n        return False```\n\nUpdated code:\n```python\ndef _upload(self, image, genre, timestamp):\n        \n        \n        \n        output = io.BytesIO()\n        image.save(output, format='png')\n        output.seek(0)\n\n        data = {'file': (f'{timestamp}.png', output, 'image/png')}\n        headers = {'user-agent': self._user_agent()}\n        session = requests.Session()\n        session.mount('http://', HTTPAdapter(max_retries=5))\n        session.mount('https://', HTTPAdapter(max_retries=5))\n        try:\n            resp = session.post(self.API, files=data, headers=headers, timeout=self.TIMEOUT)\n        except Exception as e:\n            logger.warning(f'Image upload failed, {e}')\n            return False\n\n        if resp.status_code == 200:\n            # print(resp.text)\n            info = json.loads(resp.text)\n            code = info.get(\"code\", 500)\n            if code == 200:\n                logger.info(f'Image upload success, imgid: {info.get(\"imgid\", \"\")}')\n                return True\n            elif code == 0:\n                logger.warning(f'Image upload failed, msg: {info.get(\"msg\", \"\")}')\n                return False\n\n        logger.warning(f'Image upload failed, unexpected server returns, '\n                       f'status_code: {resp.status_code}, returns: {resp.text}')\n        return False\n```\n\nOriginal documentation:\n```python\nReturns:\n            bool: If success\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            image: Image to upload.\n            genre (str):\n            timestamp (int): Millisecond timestamp.\n\n        Returns:\n            bool: If success", "file": "module_statistics_azurstats.py", "function": "AzurStats._upload", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 13, "doc_changes": 9, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _concat_datetime(to_concat, axis: AxisInt = 0):\n    \n    \n    \n    from pandas.core.construction import ensure_wrapped_if_datetimelike\n\n    to_concat = [ensure_wrapped_if_datetimelike(x) for x in to_concat]\n\n    single_dtype = len({x.dtype for x in to_concat}) == 1\n\n    # multiple types, need to coerce to object\n    if not single_dtype:\n        # ensure_wrapped_if_datetimelike ensures that astype(object) wraps\n        #  in Timestamp/Timedelta\n        return _concatenate_2d([x.astype(object) for x in to_concat], axis=axis)\n\n    result = type(to_concat[0])._concat_same_type(to_concat, axis=axis)\n    return result```\n\nUpdated code:\n```python\ndef _concat_datetime(to_concat: Sequence[ArrayLike], axis: AxisInt = 0) -> ArrayLike:\n    \n    \n    \n    from pandas.core.construction import ensure_wrapped_if_datetimelike\n\n    to_concat = [ensure_wrapped_if_datetimelike(x) for x in to_concat]\n\n    single_dtype = len({x.dtype for x in to_concat}) == 1\n\n    # multiple types, need to coerce to object\n    if not single_dtype:\n        # ensure_wrapped_if_datetimelike ensures that astype(object) wraps\n        #  in Timestamp/Timedelta\n        return _concatenate_2d([x.astype(object) for x in to_concat], axis=axis)\n\n    # error: Unexpected keyword argument \"axis\" for \"_concat_same_type\" of\n    # \"ExtensionArray\"\n    to_concat_eas = cast(\"list[ExtensionArray]\", to_concat)\n    result = type(to_concat_eas[0])._concat_same_type(  # type: ignore[call-arg]\n        to_concat_eas, axis=axis\n    )\n    return result\n```\n\nOriginal documentation:\n```python\nprovide concatenation of an datetimelike array of arrays each of which is a\n    single M8[ns], datetime64[ns, tz] or m8[ns] dtype\n\n    Parameters\n    ----------\n    to_concat : array of arrays\n    axis : axis to provide concatenation\n\n    Returns\n    -------\n    a single array, preserving the combined dtypes\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "provide concatenation of an datetimelike array of arrays each of which is a\n    single M8[ns], datetime64[ns, tz] or m8[ns] dtype\n\n    Parameters\n    ----------\n    to_concat : sequence of arrays\n    axis : axis to provide concatenation\n\n    Returns\n    -------\n    a single array, preserving the combined dtypes", "file": "pandas_core_dtypes_concat.py", "function": "_concat_datetime", "project": "pandas", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef checkerboard():\n    \n\n    \n    return load(\"chessboard_RGB.png\")```\n\nUpdated code:\n```python\ndef checkerboard():\n    \n\n    \n    return load(\"chessboard_GRAY_U8.png\")\n```\n\nOriginal documentation:\n```python\nCheckerboard image.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checkerboard image.\n\n    Checkerboards are often used in image calibration, since the\n    corner-points are easy to locate.  Because of the many parallel\n    edges, they also visualise distortions particularly well.", "file": "skimage_data___init__.py", "function": "checkerboard", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_qs_bytes(\n    qs: str, keep_blank_values: bool = False, strict_parsing: bool = False\n) -> Dict[str, List[bytes]]:\n    \n    \n    # This is gross, but python3 doesn't give us another way.\n    # Latin1 is the universal donor of character encodings.\n    result = urllib.parse.parse_qs(\n        qs, keep_blank_values, strict_parsing, encoding=\"latin1\", errors=\"strict\"\n    )\n    encoded = {}\n    for k, v in result.items():\n        encoded[k] = [i.encode(\"latin1\") for i in v]\n    return encoded```\n\nUpdated code:\n```python\ndef parse_qs_bytes(\n    qs: Union[str, bytes], keep_blank_values: bool = False, strict_parsing: bool = False\n) -> Dict[str, List[bytes]]:\n    \n    \n    # This is gross, but python3 doesn't give us another way.\n    # Latin1 is the universal donor of character encodings.\n    if isinstance(qs, bytes):\n        qs = qs.decode(\"latin1\")\n    result = urllib.parse.parse_qs(\n        qs, keep_blank_values, strict_parsing, encoding=\"latin1\", errors=\"strict\"\n    )\n    encoded = {}\n    for k, v in result.items():\n        encoded[k] = [i.encode(\"latin1\") for i in v]\n    return encoded\n```\n\nOriginal documentation:\n```python\nParses a query string like urlparse.parse_qs, but returns the\n    values as byte strings.\n\n    Keys still become type str (interpreted as latin1 in python3!)\n    because it's too painful to keep them as byte strings in\n    python3 and in practice they're nearly always ascii anyway.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parses a query string like urlparse.parse_qs,\n    but takes bytes and returns the values as byte strings.\n\n    Keys still become type str (interpreted as latin1 in python3!)\n    because it's too painful to keep them as byte strings in\n    python3 and in practice they're nearly always ascii anyway.", "file": "tornado_escape.py", "function": "parse_qs_bytes", "project": "tornado", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef select_figure_format(shell, fmt, quality=90):\n    \n    \n    from matplotlib.figure import Figure\n    from IPython.kernel.zmq.pylab import backend_inline\n\n    svg_formatter = shell.display_formatter.formatters['image/svg+xml']\n    png_formatter = shell.display_formatter.formatters['image/png']\n    jpg_formatter = shell.display_formatter.formatters['image/jpeg']\n\n    [ f.type_printers.pop(Figure, None) for f in {svg_formatter, png_formatter, jpg_formatter} ]\n\n    if fmt == 'png':\n        png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png'))\n    elif fmt in ('png2x', 'retina'):\n        png_formatter.for_type(Figure, retina_figure)\n    elif fmt in ('jpg', 'jpeg'):\n        jpg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'jpg', quality))\n    elif fmt == 'svg':\n        svg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'svg'))\n    else:\n        raise ValueError(\"supported formats are: 'png', 'retina', 'svg', 'jpg', not %r\" % fmt)\n\n    # set the format to be used in the backend()\n    backend_inline._figure_format = fmt```\n\nUpdated code:\n```python\ndef select_figure_format(shell, formats, quality=90):\n    \n    \n    from matplotlib.figure import Figure\n    from IPython.kernel.zmq.pylab import backend_inline\n\n    svg_formatter = shell.display_formatter.formatters['image/svg+xml']\n    png_formatter = shell.display_formatter.formatters['image/png']\n    jpg_formatter = shell.display_formatter.formatters['image/jpeg']\n    pdf_formatter = shell.display_formatter.formatters['application/pdf']\n\n    if isinstance(formats, py3compat.string_types):\n        formats = {formats}\n\n    [ f.type_printers.pop(Figure, None) for f in {svg_formatter, png_formatter, jpg_formatter} ]\n\n    for fmt in formats:\n        if fmt == 'png':\n            png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png'))\n        elif fmt in ('png2x', 'retina'):\n            png_formatter.for_type(Figure, retina_figure)\n        elif fmt in ('jpg', 'jpeg'):\n            jpg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'jpg', quality))\n        elif fmt == 'svg':\n            svg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'svg'))\n        elif fmt == 'pdf':\n            pdf_formatter.for_type(Figure, lambda fig: print_figure(fig, 'pdf'))\n        else:\n            raise ValueError(\"supported formats are: 'png', 'retina', 'svg', 'jpg', 'pdf' not %r\" % fmt)\n```\n\nOriginal documentation:\n```python\nSelect figure format for inline backend, can be 'png', 'retina', 'jpg', or 'svg'.\n\n    Using this method ensures only one figure format is active at a time.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Select figure formats for the inline backend.\n\n    Parameters\n    ==========\n    shell : InteractiveShell\n        The main IPython instance\n    formats : list\n        One or a set of figure formats to enable: 'png', 'retina', 'jpeg', 'svg', 'pdf'.", "file": "IPython_core_pylabtools.py", "function": "select_figure_format", "project": "ipython", "change_stats": {"code_changes": 34, "doc_changes": 11, "total_changes": 45}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __decrypt_assertion(self, dom):\n        \n        \n        \n        key = self.__settings.get_sp_key()\n\n        if not key:\n            raise Exception('No private key available, check settings')\n\n        encrypted_assertion_nodes = OneLogin_Saml2_Utils.query(dom, '//saml:EncryptedAssertion')\n        if encrypted_assertion_nodes:\n            encrypted_data_nodes = OneLogin_Saml2_Utils.query(encrypted_assertion_nodes[0], '//saml:EncryptedAssertion/xenc:EncryptedData')\n            if encrypted_data_nodes:\n                encrypted_data = encrypted_data_nodes[0]\n                OneLogin_Saml2_Utils.decrypt_element(encrypted_data, key)\n        return dom```\n\nUpdated code:\n```python\ndef __decrypt_assertion(self, xml):\n        \n        \n        \n        key = self.__settings.get_sp_key()\n\n        if not key:\n            raise Exception('No private key available, check settings')\n\n        encrypted_assertion_nodes = OneLogin_Saml2_XML.query(xml, '//saml:EncryptedAssertion')\n        if encrypted_assertion_nodes:\n            encrypted_data_nodes = OneLogin_Saml2_XML.query(encrypted_assertion_nodes[0], '//saml:EncryptedAssertion/xenc:EncryptedData')\n            if encrypted_data_nodes:\n                keyinfo = OneLogin_Saml2_XML.query(encrypted_assertion_nodes[0], '//saml:EncryptedAssertion/xenc:EncryptedData/ds:KeyInfo')\n                if not keyinfo:\n                    raise Exception('No KeyInfo present, invalid Assertion')\n                keyinfo = keyinfo[0]\n                children = keyinfo.getchildren()\n                if not children:\n                    raise Exception('No child to KeyInfo, invalid Assertion')\n                for child in children:\n                    if 'RetrievalMethod' in child.tag:\n                        if child.attrib['Type'] != 'http://www.w3.org/2001/04/xmlenc#EncryptedKey':\n                            raise Exception('Unsupported Retrieval Method found')\n                        uri  = child.attrib['URI'] \n                        if not uri.startswith('#'):\n                            break\n                        uri = uri.split('#')[1]\n                        encrypted_key = OneLogin_Saml2_XML.query(encrypted_assertion_nodes[0], './xenc:EncryptedKey[@Id=\"'+uri +'\"]')\n                        if encrypted_key:\n                            keyinfo.append(encrypted_key[0])\n\n                encrypted_data = encrypted_data_nodes[0]\n                OneLogin_Saml2_Utils.decrypt_element(encrypted_data, key)\n        return xml\n```\n\nOriginal documentation:\n```python\nDecrypts the Assertion\n\n        :raises: Exception if no private key available\n        :param dom: Encrypted Assertion\n        :type dom: Element\n        :returns: Decrypted Assertion\n        :rtype: Element\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Decrypts the Assertion\n\n        :raises: Exception if no private key available\n        :param xml: Encrypted Assertion\n        :type xml: Element\n        :returns: Decrypted Assertion\n        :rtype: Element", "file": "src_onelogin_saml2_response.py", "function": "OneLogin_Saml2_Response.__decrypt_assertion", "project": "python3-saml", "change_stats": {"code_changes": 29, "doc_changes": 6, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef tz_localize(self, tz, copy=True):\n        \n        \n        \n        from pandas.tseries.index import DatetimeIndex\n\n        if not isinstance(self.index, DatetimeIndex):\n            if len(self.index) > 0:\n                raise Exception('Cannot tz-localize non-time series')\n\n            new_index = DatetimeIndex([], tz=tz)\n        else:\n            new_index = self.index.tz_localize(tz)\n\n        new_values = self.values\n        if copy:\n            new_values = new_values.copy()\n\n        return self._constructor(new_values, index=new_index, name=self.name)```\n\nUpdated code:\n```python\ndef tz_localize(self, tz, copy=True, infer_dst=False):\n        \n        \n        \n        from pandas.tseries.index import DatetimeIndex\n\n        if not isinstance(self.index, DatetimeIndex):\n            if len(self.index) > 0:\n                raise Exception('Cannot tz-localize non-time series')\n\n            new_index = DatetimeIndex([], tz=tz)\n        else:\n            new_index = self.index.tz_localize(tz, infer_dst=infer_dst)\n\n        new_values = self.values\n        if copy:\n            new_values = new_values.copy()\n\n        return self._constructor(new_values, index=new_index, name=self.name)\n```\n\nOriginal documentation:\n```python\nLocalize tz-naive TimeSeries to target time zone\n        Entries will retain their \"naive\" value but will be annotated as\n        being relative to the specified tz.\n\n        After localizing the TimeSeries, you may use tz_convert() to\n        get the Datetime values recomputed to a different tz.\n\n        Parameters\n        ----------\n        tz : string or pytz.timezone object\n        copy : boolean, default True\n            Also make a copy of the underlying data\n\n        Returns\n        -------\n        localized : TimeSeries\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Localize tz-naive TimeSeries to target time zone\n        Entries will retain their \"naive\" value but will be annotated as\n        being relative to the specified tz.\n\n        After localizing the TimeSeries, you may use tz_convert() to\n        get the Datetime values recomputed to a different tz.\n\n        Parameters\n        ----------\n        tz : string or pytz.timezone object\n        copy : boolean, default True\n            Also make a copy of the underlying data\n        infer_dst : boolean, default False\n            Attempt to infer fall dst-transition hours based on order\n\n        Returns\n        -------\n        localized : TimeSeries", "file": "pandas_core_series.py", "function": "Series.tz_localize", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_root_test_conanfile(self, path, tested_reference, profile_host, profile_build,\n                                 update=None, remotes=None, lockfile=None):\n         \n        \n\n        app = ConanApp(self.conan_api.cache_folder)\n        # necessary for correct resolution and update of remote python_requires\n        app.load_remotes(update=update)\n\n        loader = app.loader\n        profile_host.options.scope(tested_reference)\n\n        # do not try apply lock_python_requires for test_package/conanfile.py consumer\n        conanfile = loader.load_consumer(path, user=tested_reference.user,\n                                         channel=tested_reference.channel,\n                                         graph_lock=lockfile, remotes=remotes)\n        initialize_conanfile_profile(conanfile, profile_build, profile_host, CONTEXT_HOST, False)\n        conanfile.display_name = \"%s (test package)\" % str(tested_reference)\n        conanfile.output.scope = conanfile.display_name\n        conanfile.tested_reference_str = repr(tested_reference)\n\n        ref = RecipeReference(conanfile.name, conanfile.version, tested_reference.user,\n                              tested_reference.channel)\n        root_node = Node(ref, conanfile, recipe=RECIPE_CONSUMER, context=CONTEXT_HOST, path=path)\n        return root_node```\n\nUpdated code:\n```python\ndef load_root_test_conanfile(self, path, tested_reference, profile_host, profile_build,\n                                 update=None, remotes=None, lockfile=None,\n                                 tested_python_requires=None):\n         \n        \n\n        app = ConanApp(self.conan_api.cache_folder)\n        # necessary for correct resolution and update of remote python_requires\n        app.load_remotes(update=update)\n\n        loader = app.loader\n        profile_host.options.scope(tested_reference)\n\n        # do not try apply lock_python_requires for test_package/conanfile.py consumer\n        conanfile = loader.load_consumer(path, user=tested_reference.user,\n                                         channel=tested_reference.channel,\n                                         graph_lock=lockfile, remotes=remotes,\n                                         tested_python_requires=tested_python_requires)\n        initialize_conanfile_profile(conanfile, profile_build, profile_host, CONTEXT_HOST, False)\n        conanfile.display_name = \"%s (test package)\" % str(tested_reference)\n        conanfile.output.scope = conanfile.display_name\n        conanfile.tested_reference_str = repr(tested_reference)\n\n        ref = RecipeReference(conanfile.name, conanfile.version, tested_reference.user,\n                              tested_reference.channel)\n        root_node = Node(ref, conanfile, recipe=RECIPE_CONSUMER, context=CONTEXT_HOST, path=path)\n        return root_node\n```\n\nOriginal documentation:\n```python\nCreate and initialize a root node from a test_package/conanfile.py consumer\n\n        :param lockfile: Might be good to lock python-requires, build-requires\n        :param path: The full path to the test_package/conanfile.py being used\n        :param tested_reference: The full RecipeReference of the tested package\n        :param profile_host:\n        :param profile_build:\n        :param update:\n        :param remotes:\n        :return: a graph Node, recipe=RECIPE_CONSUMER\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create and initialize a root node from a test_package/conanfile.py consumer\n\n        :param tested_python_requires: The reference to the tested ``python_requires``\n        :param lockfile: Might be good to lock python-requires, build-requires\n        :param path: The full path to the test_package/conanfile.py being used\n        :param tested_reference: The full RecipeReference of the tested package\n        :param profile_host:\n        :param profile_build:\n        :param update:\n        :param remotes:\n        :return: a graph Node, recipe=RECIPE_CONSUMER", "file": "conan_api_subapi_graph.py", "function": "GraphAPI.load_root_test_conanfile", "project": "conan", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_accept_list(self, request):\n        \n        \n        \n        header = request.META.get('HTTP_ACCEPT', '*/*')\n        header = request.query_params.get(self.settings.URL_ACCEPT_OVERRIDE, header)\n        return [token.strip() for token in header.split(',')]```\n\nUpdated code:\n```python\ndef get_accept_list(self, request):\n        \n        \n        \n        header = request.META.get('HTTP_ACCEPT', '*/*')\n        return [token.strip() for token in header.split(',')]\n```\n\nOriginal documentation:\n```python\nGiven the incoming request, return a tokenised list of media\n        type strings.\n\n        Allows URL style accept override.  eg. \"?accept=application/json\"\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Given the incoming request, return a tokenised list of media\n        type strings.", "file": "rest_framework_negotiation.py", "function": "DefaultContentNegotiation.get_accept_list", "project": "django-rest-framework", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _verify_placeholder_names(gm: torch.fx.GraphModule, sig: ExportGraphSignature):\n    \n    \n    \n    name_to_kind = {\n        spec.arg.name: spec.kind\n        for spec in sig.input_specs\n        if not isinstance(spec.arg, ConstantArgument)\n    }\n    for node in gm.graph.nodes:\n        if node.op == \"placeholder\":\n            if node.name not in name_to_kind:\n                continue\n            node_kind = name_to_kind[node.name]\n            prefix = placeholder_prefixes[node_kind]\n            if not node.name.startswith(prefix):\n                raise SpecViolationError(\n                    f\"Placeholder node name {node.name} does not follow spec for {node_kind}, name should have prefix: {prefix}\"\n                )```\n\nUpdated code:\n```python\ndef _verify_placeholder_names(gm: torch.fx.GraphModule, sig: ExportGraphSignature):\n    \n    \n    \n    name_to_kind = {\n        spec.arg.name: spec.kind\n        for spec in sig.input_specs\n        if not isinstance(spec.arg, ConstantArgument)\n    }\n    for mod in gm.modules():\n        if not isinstance(mod, torch.fx.GraphModule):\n            continue\n        for node in mod.graph.nodes:\n            if node.op == \"placeholder\":\n                if node.name not in name_to_kind:\n                    continue\n                node_kind = name_to_kind[node.name]\n                prefix = placeholder_prefixes[node_kind]\n                if not node.name.startswith(prefix):\n                    raise SpecViolationError(\n                        f\"Placeholder node name {node.name} does not follow spec for {node_kind}, name should have prefix: {prefix}\"\n                    )\n```\n\nOriginal documentation:\n```python\nPerforms a sanity check on the placeholder node names.\n    - User input nodes: no restrictions, should match the original forward() signature\n    - Params/buffers/constants/custom_obj nodes: should start with \"p\", \"b\", \"c\", \"obj\"\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Performs a sanity check on the placeholder node names.\n    - User input nodes: no restrictions, should match the original forward() signature\n    - Params/buffers/constants/custom_obj/token nodes: should start with prefixes defined in <placeholder_prefixes>", "file": "torch_export__trace.py", "function": "_verify_placeholder_names", "project": "pytorch", "change_stats": {"code_changes": 25, "doc_changes": 4, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef std(self, ddof: int = 1):\n        \n        \n        \n        return self._get_cythonized_result(\n            libgroupby.group_var,\n            needs_counts=True,\n            cython_dtype=np.dtype(np.float64),\n            post_processing=lambda vals, inference: np.sqrt(vals),\n            ddof=ddof,\n        )```\n\nUpdated code:\n```python\ndef std(\n        self,\n        ddof: int = 1,\n        engine: str | None = None,\n        engine_kwargs: dict[str, bool] | None = None,\n    ):\n        \n        \n        \n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import sliding_var\n\n            return np.sqrt(\n                self._numba_agg_general(sliding_var, engine_kwargs, \"groupby_std\", ddof)\n            )\n        else:\n            return self._get_cythonized_result(\n                libgroupby.group_var,\n                needs_counts=True,\n                cython_dtype=np.dtype(np.float64),\n                post_processing=lambda vals, inference: np.sqrt(vals),\n                ddof=ddof,\n            )\n```\n\nOriginal documentation:\n```python\nCompute standard deviation of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard deviation of values within each group.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute standard deviation of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard deviation of values within each group.", "file": "pandas_core_groupby_groupby.py", "function": "GroupBy.std", "project": "pandas", "change_stats": {"code_changes": 30, "doc_changes": 19, "total_changes": 49}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef setSyncState(self, service_name: str, state: SyncState) -> None:\n         \n        \n\n        prev_state = self._sync_state\n\n        self._sync_services[service_name] = state\n\n        if any(val == self.SyncState.SYNCING for val in self._sync_services.values()):\n            self._sync_state = self.SyncState.SYNCING\n        elif any(val == self.SyncState.ERROR for val in self._sync_services.values()):\n            self._sync_state = self.SyncState.ERROR\n        else:\n            self._sync_state = self.SyncState.SUCCESS\n\n        if self._sync_state != prev_state:\n            self.syncStateChanged.emit(self._sync_state.value[0])\n\n            if self._sync_state == self.SyncState.SUCCESS:\n                self._last_sync_str = datetime.now().strftime(\"%d/%m/%Y %H:%M\")\n                self.lastSyncDateTimeChanged.emit()\n\n            if self._sync_state != self.SyncState.SYNCING:\n                # schedule new auto update after syncing completed (for whatever reason)\n                if not self._update_timer.isActive():\n                    self._update_timer.start()```\n\nUpdated code:\n```python\ndef setSyncState(self, service_name: str, state: SyncState) -> None:\n         \n        \n\n        prev_state = self._sync_state\n\n        self._sync_services[service_name] = state\n\n        if any(val == SyncState.SYNCING for val in self._sync_services.values()):\n            self._sync_state = SyncState.SYNCING\n        elif any(val == SyncState.ERROR for val in self._sync_services.values()):\n            self._sync_state = SyncState.ERROR\n        else:\n            self._sync_state = SyncState.SUCCESS\n\n        if self._sync_state != prev_state:\n            self.syncStateChanged.emit(self._sync_state)\n\n            if self._sync_state == SyncState.SUCCESS:\n                self._last_sync_str = datetime.now().strftime(\"%d/%m/%Y %H:%M\")\n                self.lastSyncDateTimeChanged.emit()\n\n            if self._sync_state != SyncState.SYNCING:\n                # schedule new auto update after syncing completed (for whatever reason)\n                if not self._update_timer.isActive():\n                    self._update_timer.start()\n```\n\nOriginal documentation:\n```python\nCan be used to register sync services and update account sync states\n\n        Example: `setSyncState(\"PluginSyncService\", Account.SyncState.SYNCING)`\n        :param service_name: A unique name for your service, such as `plugins` or `backups`\n        :param state: One of Account.SyncState\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Can be used to register sync services and update account sync states\n\n        Example: `setSyncState(\"PluginSyncService\", SyncState.SYNCING)`\n        :param service_name: A unique name for your service, such as `plugins` or `backups`\n        :param state: One of SyncState", "file": "cura_API_Account.py", "function": "Account.setSyncState", "project": "Cura", "change_stats": {"code_changes": 18, "doc_changes": 6, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef bot_data(self) -> BD:\n        \n        \n        return self.dispatcher.bot_data```\n\nUpdated code:\n```python\ndef bot_data(self) -> BD:\n        \n        \n        return self.application.bot_data\n```\n\nOriginal documentation:\n```python\n:obj:`dict`: Optional. A dict that can be used to keep any data in. For each\n        update it will be the same ``dict``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":obj:`ContextTypes.bot_data`: Optional. An object that can be used to keep any data in.\n        For each update it will be the same :attr:`ContextTypes.bot_data`. Defaults to :obj:`dict`.", "file": "telegram_ext__callbackcontext.py", "function": "CallbackContext.bot_data", "project": "python-telegram-bot", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_distribution_strategy(distribution_strategy=\"mirrored\",\n                              num_gpus=0,\n                              all_reduce_alg=None,\n                              num_packs=1,\n                              tpu_address=None):\n  \n  \n  if num_gpus < 0:\n    raise ValueError(\"`num_gpus` can not be negative.\")\n\n  distribution_strategy = distribution_strategy.lower()\n  if distribution_strategy == \"off\":\n    if num_gpus > 1:\n      raise ValueError(\"When {} GPUs are specified, distribution_strategy \"\n                       \"flag cannot be set to `off`.\".format(num_gpus))\n    return None\n\n  if distribution_strategy == \"tpu\":\n    # When tpu_address is an empty string, we communicate with local TPUs.\n    cluster_resolver = tpu_initialize(tpu_address)\n    return tf.distribute.experimental.TPUStrategy(cluster_resolver)\n\n  if distribution_strategy == \"multi_worker_mirrored\":\n    return tf.distribute.experimental.MultiWorkerMirroredStrategy(\n        communication=_collective_communication(all_reduce_alg))\n\n  if distribution_strategy == \"one_device\":\n    if num_gpus == 0:\n      return tf.distribute.OneDeviceStrategy(\"device:CPU:0\")\n    if num_gpus > 1:\n      raise ValueError(\"`OneDeviceStrategy` can not be used for more than \"\n                       \"one device.\")\n    return tf.distribute.OneDeviceStrategy(\"device:GPU:0\")\n\n  if distribution_strategy == \"mirrored\":\n    if num_gpus == 0:\n      devices = [\"device:CPU:0\"]\n    else:\n      devices = [\"device:GPU:%d\" % i for i in range(num_gpus)]\n    return tf.distribute.MirroredStrategy(\n        devices=devices,\n        cross_device_ops=_mirrored_cross_device_ops(all_reduce_alg, num_packs))\n\n  if distribution_strategy == \"parameter_server\":\n    return tf.distribute.experimental.ParameterServerStrategy()\n\n  raise ValueError(\"Unrecognized Distribution Strategy: %r\" %\n                   distribution_strategy)```\n\nUpdated code:\n```python\ndef get_distribution_strategy(distribution_strategy=\"mirrored\",\n                              num_gpus=0,\n                              all_reduce_alg=None,\n                              num_packs=1,\n                              tpu_address=None,\n                              **kwargs):\n  \n  \n  del kwargs\n  if num_gpus < 0:\n    raise ValueError(\"`num_gpus` can not be negative.\")\n\n  distribution_strategy = distribution_strategy.lower()\n  if distribution_strategy == \"off\":\n    if num_gpus > 1:\n      raise ValueError(\"When {} GPUs are specified, distribution_strategy \"\n                       \"flag cannot be set to `off`.\".format(num_gpus))\n    return None\n\n  if distribution_strategy == \"tpu\":\n    # When tpu_address is an empty string, we communicate with local TPUs.\n    cluster_resolver = tpu_initialize(tpu_address)\n    return tf.distribute.experimental.TPUStrategy(cluster_resolver)\n\n  if distribution_strategy == \"multi_worker_mirrored\":\n    return tf.distribute.experimental.MultiWorkerMirroredStrategy(\n        communication=_collective_communication(all_reduce_alg))\n\n  if distribution_strategy == \"one_device\":\n    if num_gpus == 0:\n      return tf.distribute.OneDeviceStrategy(\"device:CPU:0\")\n    if num_gpus > 1:\n      raise ValueError(\"`OneDeviceStrategy` can not be used for more than \"\n                       \"one device.\")\n    return tf.distribute.OneDeviceStrategy(\"device:GPU:0\")\n\n  if distribution_strategy == \"mirrored\":\n    if num_gpus == 0:\n      devices = [\"device:CPU:0\"]\n    else:\n      devices = [\"device:GPU:%d\" % i for i in range(num_gpus)]\n    return tf.distribute.MirroredStrategy(\n        devices=devices,\n        cross_device_ops=_mirrored_cross_device_ops(all_reduce_alg, num_packs))\n\n  if distribution_strategy == \"parameter_server\":\n    return tf.distribute.experimental.ParameterServerStrategy()\n\n  raise ValueError(\"Unrecognized Distribution Strategy: %r\" %\n                   distribution_strategy)\n```\n\nOriginal documentation:\n```python\nReturn a DistributionStrategy for running the model.\n\n  Args:\n    distribution_strategy: a string specifying which distribution strategy to\n      use. Accepted values are \"off\", \"one_device\", \"mirrored\",\n      \"parameter_server\", \"multi_worker_mirrored\", and \"tpu\" -- case\n      insensitive. \"off\" means not to use Distribution Strategy; \"tpu\" means to\n      use TPUStrategy using `tpu_address`.\n    num_gpus: Number of GPUs to run this model.\n    all_reduce_alg: Optional. Specifies which algorithm to use when performing\n      all-reduce. For `MirroredStrategy`, valid values are \"nccl\" and\n      \"hierarchical_copy\". For `MultiWorkerMirroredStrategy`, valid values are\n      \"ring\" and \"nccl\".  If None, DistributionStrategy will choose based on\n      device topology.\n    num_packs: Optional.  Sets the `num_packs` in `tf.distribute.NcclAllReduce`\n      or `tf.distribute.HierarchicalCopyAllReduce` for `MirroredStrategy`.\n    tpu_address: Optional. String that represents TPU to connect to. Must not be\n      None if `distribution_strategy` is set to `tpu`.\n\n  Returns:\n    tf.distribute.DistibutionStrategy object.\n  Raises:\n    ValueError: if `distribution_strategy` is \"off\" or \"one_device\" and\n      `num_gpus` is larger than 1; or `num_gpus` is negative or if\n      `distribution_strategy` is `tpu` but `tpu_address` is not specified.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a DistributionStrategy for running the model.\n\n  Args:\n    distribution_strategy: a string specifying which distribution strategy to\n      use. Accepted values are \"off\", \"one_device\", \"mirrored\",\n      \"parameter_server\", \"multi_worker_mirrored\", and \"tpu\" -- case\n      insensitive. \"off\" means not to use Distribution Strategy; \"tpu\" means to\n      use TPUStrategy using `tpu_address`.\n    num_gpus: Number of GPUs to run this model.\n    all_reduce_alg: Optional. Specifies which algorithm to use when performing\n      all-reduce. For `MirroredStrategy`, valid values are \"nccl\" and\n      \"hierarchical_copy\". For `MultiWorkerMirroredStrategy`, valid values are\n      \"ring\" and \"nccl\".  If None, DistributionStrategy will choose based on\n      device topology.\n    num_packs: Optional.  Sets the `num_packs` in `tf.distribute.NcclAllReduce`\n      or `tf.distribute.HierarchicalCopyAllReduce` for `MirroredStrategy`.\n    tpu_address: Optional. String that represents TPU to connect to. Must not be\n      None if `distribution_strategy` is set to `tpu`.\n    **kwargs: Additional kwargs for internal usages.\n\n  Returns:\n    tf.distribute.DistibutionStrategy object.\n  Raises:\n    ValueError: if `distribution_strategy` is \"off\" or \"one_device\" and\n      `num_gpus` is larger than 1; or `num_gpus` is negative or if\n      `distribution_strategy` is `tpu` but `tpu_address` is not specified.", "file": "official_common_distribute_utils.py", "function": "get_distribution_strategy", "project": "models", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_series(self, keep_tz=False):\n        \n        \n        \n        from pandas import Series\n        return Series(self._to_embed(keep_tz),\n                      index=self._shallow_copy(),\n                      name=self.name)```\n\nUpdated code:\n```python\ndef to_series(self, keep_tz=False, index=None, name=None):\n        \n        \n        \n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self._to_embed(keep_tz), index=index, name=name)\n```\n\nOriginal documentation:\n```python\nCreate a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index\n\n        Parameters\n        ----------\n        keep_tz : optional, defaults False.\n            return the data keeping the timezone.\n\n            If keep_tz is True:\n\n              If the timezone is not set, the resulting\n              Series will have a datetime64[ns] dtype.\n\n              Otherwise the Series will have an datetime64[ns, tz] dtype; the\n              tz will be preserved.\n\n            If keep_tz is False:\n\n              Series will have a datetime64[ns] dtype. TZ aware\n              objects will have the tz removed.\n\n        Returns\n        -------\n        Series\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index\n\n        Parameters\n        ----------\n        keep_tz : optional, defaults False.\n            return the data keeping the timezone.\n\n            If keep_tz is True:\n\n              If the timezone is not set, the resulting\n              Series will have a datetime64[ns] dtype.\n\n              Otherwise the Series will have an datetime64[ns, tz] dtype; the\n              tz will be preserved.\n\n            If keep_tz is False:\n\n              Series will have a datetime64[ns] dtype. TZ aware\n              objects will have the tz removed.\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series", "file": "pandas_core_indexes_datetimes.py", "function": "DatetimeIndex.to_series", "project": "pandas", "change_stats": {"code_changes": 14, "doc_changes": 7, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef override_properties(char, new_properties):\n    \n    char.properties.update(new_properties)```\n\nUpdated code:\n```python\ndef override_properties(char, properties=None, valid_values=None):\n    \n    if properties:\n        char.properties.update(properties)\n\n    if valid_values:\n        char.properties['ValidValues'].update(valid_values)\n```\n\nOriginal documentation:\n```python\nOverride characteristic property values.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Override characteristic property values and valid values.", "file": "homeassistant_components_homekit_accessories.py", "function": "override_properties", "project": "core", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _predict_recursive(self, X, x_squared_norms, sample_weight, cluster_node):\n        \n        \n        if cluster_node.left is None:\n            # This cluster has no subcluster. Labels are just the label of the cluster.\n            return np.full(X.shape[0], cluster_node.label, dtype=np.int32)\n\n        # Determine if data points belong to the left or right subcluster\n        centers = np.vstack((cluster_node.left.center, cluster_node.right.center))\n        if hasattr(self, \"_X_mean\"):\n            centers += self._X_mean\n\n        cluster_labels = _labels_inertia_threadpool_limit(\n            X,\n            sample_weight,\n            x_squared_norms,\n            centers,\n            self._n_threads,\n            return_inertia=False,\n        )\n        mask = cluster_labels == 0\n\n        # Compute the labels for each subset of the data points.\n        labels = np.full(X.shape[0], -1, dtype=np.int32)\n\n        labels[mask] = self._predict_recursive(\n            X[mask], x_squared_norms[mask], sample_weight[mask], cluster_node.left\n        )\n\n        labels[~mask] = self._predict_recursive(\n            X[~mask], x_squared_norms[~mask], sample_weight[~mask], cluster_node.right\n        )\n\n        return labels```\n\nUpdated code:\n```python\ndef _predict_recursive(self, X, sample_weight, cluster_node):\n        \n        \n        if cluster_node.left is None:\n            # This cluster has no subcluster. Labels are just the label of the cluster.\n            return np.full(X.shape[0], cluster_node.label, dtype=np.int32)\n\n        # Determine if data points belong to the left or right subcluster\n        centers = np.vstack((cluster_node.left.center, cluster_node.right.center))\n        if hasattr(self, \"_X_mean\"):\n            centers += self._X_mean\n\n        cluster_labels = _labels_inertia_threadpool_limit(\n            X,\n            sample_weight,\n            centers,\n            self._n_threads,\n            return_inertia=False,\n        )\n        mask = cluster_labels == 0\n\n        # Compute the labels for each subset of the data points.\n        labels = np.full(X.shape[0], -1, dtype=np.int32)\n\n        labels[mask] = self._predict_recursive(\n            X[mask], sample_weight[mask], cluster_node.left\n        )\n\n        labels[~mask] = self._predict_recursive(\n            X[~mask], sample_weight[~mask], cluster_node.right\n        )\n\n        return labels\n```\n\nOriginal documentation:\n```python\nPredict recursively by going down the hierarchical tree.\n\n        Parameters\n        ----------\n        X : {ndarray, csr_matrix} of shape (n_samples, n_features)\n            The data points, currently assigned to `cluster_node`, to predict between\n            the subclusters of this node.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point.\n\n        sample_weight : ndarray of shape (n_samples,)\n            The weights for each observation in X.\n\n        cluster_node : _BisectingTree node object\n            The cluster node of the hierarchical tree.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Predict recursively by going down the hierarchical tree.\n\n        Parameters\n        ----------\n        X : {ndarray, csr_matrix} of shape (n_samples, n_features)\n            The data points, currently assigned to `cluster_node`, to predict between\n            the subclusters of this node.\n\n        sample_weight : ndarray of shape (n_samples,)\n            The weights for each observation in X.\n\n        cluster_node : _BisectingTree node object\n            The cluster node of the hierarchical tree.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.", "file": "sklearn_cluster__bisect_k_means.py", "function": "BisectingKMeans._predict_recursive", "project": "scikit-learn", "change_stats": {"code_changes": 9, "doc_changes": 5, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_link_local(address: IPv4Address | IPv6Address) -> bool:\n    \n    return address in LINK_LOCAL_NETWORK```\n\nUpdated code:\n```python\ndef is_link_local(address: IPv4Address | IPv6Address) -> bool:\n    \n    return any(address in network for network in LINK_LOCAL_NETWORKS)\n```\n\nOriginal documentation:\n```python\nCheck if an address is link local.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if an address is link-local (local but not necessarily unique).", "file": "homeassistant_util_network.py", "function": "is_link_local", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _assert_is_current_event_loop(self):\n        \n        \n        try:\n            current = events.get_event_loop()\n        except AssertionError:\n            return\n        if current is not self:\n            raise RuntimeError(\n                \"non-threadsafe operation invoked on an event loop other \"\n                \"than the current one\")```\n\nUpdated code:\n```python\ndef _assert_is_current_event_loop(self):\n        \n        \n        try:\n            current = events.get_event_loop()\n        except AssertionError:\n            return\n        if current is not self:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n```\n\nOriginal documentation:\n```python\nAsserts that this event loop is the current event loop.\n\n        Non-threadsafe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True). The caller is\n        responsible for checking this condition for performance reasons.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Asserts that this event loop is the current event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.", "file": "Lib_asyncio_base_events.py", "function": "BaseEventLoop._assert_is_current_event_loop", "project": "cpython", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef start(self) -> None:\n        \n        with self._lock:\n            if self._started:\n                return\n\n            self.console.show_cursor(False)\n            self._enable_redirect_io()\n            self.console.push_render_hook(self)\n            self._started = True\n\n            if self.auto_refresh:\n                self._refresh_thread = _RefreshThread(self, self.refresh_per_second)\n                self._refresh_thread.start()```\n\nUpdated code:\n```python\ndef start(self, refresh=False) -> None:\n        \n        \n        with self._lock:\n            if self._started:\n                return\n            self.console.set_live(self)\n            self._started = True\n            self.console.show_cursor(False)\n            self._enable_redirect_io()\n            self.console.push_render_hook(self)\n            if refresh:\n                self.refresh()\n            if self.auto_refresh:\n                self._refresh_thread = _RefreshThread(self, self.refresh_per_second)\n                self._refresh_thread.start()\n```\n\nOriginal documentation:\n```python\nStart live rendering display.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Start live rendering display.\n\n        Args:\n            refresh (bool, optional): Also refresh. Defaults to False.", "file": "rich_live.py", "function": "Live.start", "project": "rich", "change_stats": {"code_changes": 12, "doc_changes": 5, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef available_flag(self) -> threading.Event:\n        \n        return self._wrap_event_flag```\n\nUpdated code:\n```python\ndef available_flag(self) -> asyncio.Event:\n        \n        return self._wrap_event_flag\n```\n\nOriginal documentation:\n```python\nReturn threading event flag that indicates if camera's API is responding.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return event flag that indicates if camera's API is responding.", "file": "homeassistant_components_amcrest___init__.py", "function": "AmcrestChecker.available_flag", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_networks(self, epoch):\n        \n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)```\n\nUpdated code:\n```python\ndef load_networks(self, epoch):\n        \n        \n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n```\n\nOriginal documentation:\n```python\nLoad models from the disk\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load all the networks from the disk.\n\n        Parameters:\n            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)", "file": "models_base_model.py", "function": "BaseModel.load_networks", "project": "pytorch-CycleGAN-and-pix2pix", "change_stats": {"code_changes": 3, "doc_changes": 7, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, data, device, session, timeout):\n        \n        super().__init__(data, device)\n        self._timeout = timeout\n        self._session = session\n        self._image_url = None\n        self._image_content = None\n        self._attr_name = f\"{device.device_name} Camera\"\n        self._attr_unique_id = f\"{self._device_id:s}_camera\"```\n\nUpdated code:\n```python\ndef __init__(self, data, device, session, timeout):\n        \n        super().__init__(data, device)\n        self._timeout = timeout\n        self._session = session\n        self._image_url = None\n        self._image_content = None\n        self._attr_unique_id = f\"{self._device_id:s}_camera\"\n```\n\nOriginal documentation:\n```python\nInitialize a August security camera.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize an August security camera.", "file": "homeassistant_components_august_camera.py", "function": "AugustCamera.__init__", "project": "core", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef point_at_angle(self, angle):\n        \n        \n\n        start_angle = angle_of_vector(self.points[0] - self.get_center())\n        return self.point_from_proportion((angle - start_angle) / TAU)```\n\nUpdated code:\n```python\ndef point_at_angle(self, angle: float):\n        \n\n        \n\n        start_angle = angle_of_vector(self.points[0] - self.get_center())\n        return self.point_from_proportion((angle - start_angle) / TAU)\n```\n\nOriginal documentation:\n```python\nReturns the position of a point on the circle.\n\n        Parameters\n        ----------\n        angle : class: `float`\n            The angle of the point along the circle in radians.\n\n        Examples\n        --------\n\n        .. manim:: PointAtAngleExample\n            :save_last_frame:\n\n            class PointAtAngleExample(Scene):\n                def construct(self):\n                    circle = Circle(radius=2.0)\n                    p1 = circle.point_at_angle(PI/2)\n                    p2 = circle.point_at_angle(270*DEGREES)\n\n                    s1 = Square(side_length=0.25).move_to(p1)\n                    s2 = Square(side_length=0.25).move_to(p2)\n                    self.add(circle, s1, s2)\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The location of the point along the circle's circumference.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the position of a point on the circle.\n\n        Parameters\n        ----------\n        angle\n            The angle of the point along the circle in radians.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The location of the point along the circle's circumference.\n\n        Examples\n        --------\n        .. manim:: PointAtAngleExample\n            :save_last_frame:\n\n            class PointAtAngleExample(Scene):\n                def construct(self):\n                    circle = Circle(radius=2.0)\n                    p1 = circle.point_at_angle(PI/2)\n                    p2 = circle.point_at_angle(270*DEGREES)\n\n                    s1 = Square(side_length=0.25).move_to(p1)\n                    s2 = Square(side_length=0.25).move_to(p2)\n                    self.add(circle, s1, s2)", "file": "manim_mobject_geometry_arc.py", "function": "Circle.point_at_angle", "project": "manim", "change_stats": {"code_changes": 5, "doc_changes": 15, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _watch_theme(self, theme: str | None) -> None:\n        \n        self._set_theme(theme)```\n\nUpdated code:\n```python\ndef _watch_theme(self, theme: str) -> None:\n        \n        self._set_theme(theme)\n```\n\nOriginal documentation:\n```python\nWe set the styles on this widget when the theme changes, to ensure that\n        if padding is applied, the colours match.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "We set the styles on this widget when the theme changes, to ensure that\n        if padding is applied, the colors match.", "file": "src_textual_widgets__text_area.py", "function": "TextArea._watch_theme", "project": "textual", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef summary(samples, prob=0.9, group_by_chain=True):\n    \n    \n    \n    if not group_by_chain:\n        samples = {k: v.unsqueeze(0) for k, v in samples.items()}\n\n    row_names = {k: k + '[' + ','.join(map(lambda x: str(x - 1), v.shape[2:])) + ']'\n                 for k, v in samples.items()}\n    max_len = max(max(map(lambda x: len(x), row_names.values())), 10)\n    name_format = '{:>' + str(max_len) + '}'\n    header_format = name_format + ' {:>9} {:>9} {:>9} {:>9} {:>9} {:>9} {:>9}'\n    columns = ['', 'mean', 'std', 'median', '{:.1f}%'.format(50 * (1 - prob)),\n               '{:.1f}%'.format(50 * (1 + prob)), 'n_eff', 'r_hat']\n    print('\\n')\n    print(header_format.format(*columns))\n\n    row_format = name_format + ' {:>9.2f} {:>9.2f} {:>9.2f} {:>9.2f} {:>9.2f} {:>9.2f} {:>9.2f}'\n    for name, value in samples.items():\n        value_flat = torch.reshape(value, (-1,) + value.shape[2:])\n        mean = value_flat.mean(dim=0)\n        sd = value_flat.std(dim=0)\n        median = value_flat.median(dim=0)[0]\n        hpd = stats.hpdi(value_flat, prob=prob)\n        n_eff = _safe(stats.effective_sample_size)(value)\n        r_hat = stats.split_gelman_rubin(value)\n        shape = value_flat.shape[1:]\n        if len(shape) == 0:\n            print(row_format.format(name, mean, sd, median, hpd[0], hpd[1], n_eff, r_hat))\n        else:\n            for idx in product(*map(range, shape)):\n                idx_str = '[{}]'.format(','.join(map(str, idx)))\n                print(row_format.format(name + idx_str, mean[idx], sd[idx], median[idx],\n                                        hpd[0][idx], hpd[1][idx], n_eff[idx], r_hat[idx]))\n    print('\\n')```\n\nUpdated code:\n```python\ndef summary(samples, prob=0.9, group_by_chain=True):\n    \n    \n    \n    if not group_by_chain:\n        samples = {k: v.unsqueeze(0) for k, v in samples.items()}\n\n    summary_dict = {}\n    for name, value in samples.items():\n        value_flat = torch.reshape(value, (-1,) + value.shape[2:])\n        mean = value_flat.mean(dim=0)\n        std = value_flat.std(dim=0)\n        median = value_flat.median(dim=0)[0]\n        hpdi = stats.hpdi(value_flat, prob=prob)\n        n_eff = _safe(stats.effective_sample_size)(value)\n        r_hat = stats.split_gelman_rubin(value)\n        hpd_lower = '{:.1f}%'.format(50 * (1 - prob))\n        hpd_upper = '{:.1f}%'.format(50 * (1 + prob))\n        summary_dict[name] = OrderedDict([(\"mean\", mean), (\"std\", std), (\"median\", median),\n                                          (hpd_lower, hpdi[0]), (hpd_upper, hpdi[1]),\n                                          (\"n_eff\", n_eff), (\"r_hat\", r_hat)])\n    return summary_dict\n```\n\nOriginal documentation:\n```python\nPrints a summary table displaying diagnostics of ``samples`` from the\n    posterior. The diagnostics displayed are mean, standard deviation, median,\n    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,\n    :func:`~pyro.ops.stats.split_gelman_rubin`.\n\n    :param dict samples: dictionary of samples keyed by site name.\n    :param float prob: the probability mass of samples within the credibility interval.\n    :param bool group_by_chain: If True, each variable in `samples`\n        will be treated as having shape `num_chains x num_samples x sample_shape`.\n        Otherwise, the corresponding shape will be `num_samples x sample_shape`\n        (i.e. without chain dimension).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a summary table displaying diagnostics of ``samples`` from the\n    posterior. The diagnostics displayed are mean, standard deviation, median,\n    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,\n    :func:`~pyro.ops.stats.split_gelman_rubin`.\n\n    :param dict samples: dictionary of samples keyed by site name.\n    :param float prob: the probability mass of samples within the credibility interval.\n    :param bool group_by_chain: If True, each variable in `samples`\n        will be treated as having shape `num_chains x num_samples x sample_shape`.\n        Otherwise, the corresponding shape will be `num_samples x sample_shape`\n        (i.e. without chain dimension).", "file": "pyro_infer_mcmc_util.py", "function": "summary", "project": "pyro", "change_stats": {"code_changes": 33, "doc_changes": 4, "total_changes": 37}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_well_known_org(self, ip):\n        \n        supported_orgs = ('google', 'microsoft', 'apple', 'facebook', 'twitter')\n        ip_data = __database__.getIPData(ip)\n        try:\n            ip_asn = ip_data['asn']['asnorg']\n        except (KeyError, TypeError):\n            # No asn data for this ip\n            ip_asn = False\n\n        try:\n            SNI = ip_data['SNI']\n            if type(SNI) == list:\n                # SNI is a list of dicts, each dict contains the 'server_name' and 'port'\n                SNI = SNI[0]\n                if SNI in (None, ''):\n                    SNI = False\n                elif type(SNI) == dict:\n                    SNI = SNI.get('server_name', False)\n        except (KeyError, TypeError):\n            # No SNI data for this ip\n            SNI = False\n\n        try:\n            rdns = ip_data['reverse_dns']\n        except (KeyError, TypeError):\n            # No SNI data for this ip\n            rdns = False\n\n        flow_domain = rdns or SNI\n        for org in supported_orgs:\n            if ip_asn and ip_asn != 'Unknown':\n                org_asn = json.loads(__database__.get_org_info(org, 'asn'))\n                if org.lower() in ip_asn.lower() or ip_asn in org_asn:\n                    return True\n            # remove the asn from ram\n            org_asn = ''\n\n            if flow_domain:\n                # we have the rdns or sni of this flow , now check\n                if org in flow_domain:\n                    # self.print(f\"The domain of this flow ({flow_domain}) belongs to the domains of {org}\")\n                    return True\n\n                org_domains = json.loads(__database__.get_org_info(org, 'domains'))\n\n                flow_TLD = flow_domain.split(\".\")[-1]\n\n                for org_domain in org_domains:\n                    org_domain_TLD = org_domain.split(\".\")[-1]\n                    # make sure the 2 domains have the same same top level domain\n                    if flow_TLD != org_domain_TLD:\n                        continue\n\n                    # match subdomains too\n                    # return true if org has org.com, and the flow_domain is xyz.org.com\n                    # or if org has xyz.org.com, and the flow_domain is org.com return true\n                    if org_domain in flow_domain or flow_domain in org_domain :\n                        return True\n\n                # remove from ram\n                org_domains = ''\n\n            org_ips = json.loads(__database__.get_org_info(org, 'IPs'))\n            if ip in org_ips:\n                return True```\n\nUpdated code:\n```python\ndef is_well_known_org(self, ip):\n        \n        supported_orgs = (\n            'google',\n            'microsoft',\n            'apple',\n            'facebook',\n            'twitter',\n        )\n        ip_data = __database__.getIPData(ip)\n        try:\n            ip_asn = ip_data['asn']['asnorg']\n        except (KeyError, TypeError):\n            # No asn data for this ip\n            ip_asn = False\n\n        try:\n            SNI = ip_data['SNI']\n            if type(SNI) == list:\n                # SNI is a list of dicts, each dict contains the 'server_name' and 'port'\n                SNI = SNI[0]\n                if SNI in (None, ''):\n                    SNI = False\n                elif type(SNI) == dict:\n                    SNI = SNI.get('server_name', False)\n        except (KeyError, TypeError):\n            # No SNI data for this ip\n            SNI = False\n\n        try:\n            rdns = ip_data['reverse_dns']\n        except (KeyError, TypeError):\n            # No SNI data for this ip\n            rdns = False\n\n        flow_domain = rdns or SNI\n        for org in supported_orgs:\n            if ip_asn and ip_asn != 'Unknown':\n                org_asn = json.loads(__database__.get_org_info(org, 'asn'))\n                if org.lower() in ip_asn.lower() or ip_asn in org_asn:\n                    return True\n            # remove the asn from ram\n            org_asn = ''\n\n            if flow_domain:\n                # we have the rdns or sni of this flow , now check\n                if org in flow_domain:\n                    # self.print(f\"The domain of this flow ({flow_domain}) belongs to the domains of {org}\")\n                    return True\n\n                org_domains = json.loads(\n                    __database__.get_org_info(org, 'domains')\n                )\n\n                flow_TLD = flow_domain.split('.')[-1]\n\n                for org_domain in org_domains:\n                    org_domain_TLD = org_domain.split('.')[-1]\n                    # make sure the 2 domains have the same same top level domain\n                    if flow_TLD != org_domain_TLD:\n                        continue\n\n                    # match subdomains too\n                    # return true if org has org.com, and the flow_domain is xyz.org.com\n                    # or if org has xyz.org.com, and the flow_domain is org.com return true\n                    if org_domain in flow_domain or flow_domain in org_domain:\n                        return True\n\n                # remove from ram\n                org_domains = ''\n\n            org_ips = json.loads(__database__.get_org_info(org, 'IPs'))\n            if ip in org_ips:\n                return True\n```\n\nOriginal documentation:\n```python\nget the SNI, ASN, and  rDNS of the IP to check if it belongs\n         to a well-known org\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "get the SNI, ASN, and  rDNS of the IP to check if it belongs\n        to a well-known org", "file": "modules_flowalerts_flowalerts.py", "function": "Module.is_well_known_org", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 20, "doc_changes": 4, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sample_boundaries(\n        blocks: List[ObjectRef[Block]], sort_key: SortKey, num_reducers: int\n    ) -> List[T]:\n        \n        \n        \n        columns = sort_key.get_columns()\n        # TODO(Clark): Support multiple boundary sampling keys.\n        if len(columns) > 1:\n            raise ValueError(\"Multiple boundary sampling keys not supported.\")\n\n        n_samples = int(num_reducers * 10 / len(blocks))\n\n        sample_block = cached_remote_fn(_sample_block)\n\n        sample_results = [\n            sample_block.remote(block, n_samples, sort_key) for block in blocks\n        ]\n        sample_bar = ProgressBar(\n            SortTaskSpec.SORT_SAMPLE_SUB_PROGRESS_BAR_NAME, len(sample_results)\n        )\n        samples = sample_bar.fetch_until_complete(sample_results)\n        sample_bar.close()\n        del sample_results\n        samples = [s for s in samples if len(s) > 0]\n        # The dataset is empty\n        if len(samples) == 0:\n            return [None] * (num_reducers - 1)\n        builder = DelegatingBlockBuilder()\n        for sample in samples:\n            builder.add_block(sample)\n        samples = builder.build()\n        column = columns[0]\n        sample_items = BlockAccessor.for_block(samples).to_numpy(column)\n        sample_items = np.sort(sample_items)\n        ret = [\n            np.quantile(sample_items, q, interpolation=\"nearest\")\n            for q in np.linspace(0, 1, num_reducers)\n        ]\n        return ret[1:]```\n\nUpdated code:\n```python\ndef sample_boundaries(\n        blocks: List[ObjectRef[Block]], sort_key: SortKey, num_reducers: int\n    ) -> List[T]:\n        \n        \n        \n        columns = sort_key.get_columns()\n        n_samples = int(num_reducers * 10 / len(blocks))\n\n        sample_block = cached_remote_fn(_sample_block)\n\n        sample_results = [\n            sample_block.remote(block, n_samples, sort_key) for block in blocks\n        ]\n        sample_bar = ProgressBar(\n            SortTaskSpec.SORT_SAMPLE_SUB_PROGRESS_BAR_NAME, len(sample_results)\n        )\n        samples = sample_bar.fetch_until_complete(sample_results)\n        sample_bar.close()\n        del sample_results\n        samples = [s for s in samples if len(s) > 0]\n        # The dataset is empty\n        if len(samples) == 0:\n            return [None] * (num_reducers - 1)\n        builder = DelegatingBlockBuilder()\n        for sample in samples:\n            builder.add_block(sample)\n        samples = builder.build()\n\n        sample_dict = BlockAccessor.for_block(samples).to_numpy(columns=columns)\n        # Compute sorted indices of the samples. In np.lexsort last key is the\n        # primary key hence have to reverse the order.\n        indices = np.lexsort(list(reversed(list(sample_dict.values()))))\n        # Sort each column by indices, and calculate q-ths quantile items.\n        # Ignore the 1st item as it's not required for the boundary\n        for k, v in sample_dict.items():\n            sorted_v = v[indices]\n            sample_dict[k] = [\n                np.quantile(sorted_v, q, interpolation=\"nearest\")\n                for q in np.linspace(0, 1, num_reducers)\n            ][1:]\n        # Return the list of boundaries as tuples\n        # of a form (col1_value, col2_value, ...)\n        return [\n            tuple(sample_dict[k][i] for k in sample_dict)\n            for i in range(num_reducers - 1)\n        ]\n```\n\nOriginal documentation:\n```python\nReturn (num_reducers - 1) items in ascending order from the blocks that\n        partition the domain into ranges with approximately equally many elements.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return (num_reducers - 1) items in ascending order from the blocks that\n        partition the domain into ranges with approximately equally many elements.\n        Each boundary item is a tuple of a form (col1_value, col2_value, ...).", "file": "python_ray_data__internal_planner_exchange_sort_task_spec.py", "function": "SortTaskSpec.sample_boundaries", "project": "ray", "change_stats": {"code_changes": 31, "doc_changes": 3, "total_changes": 34}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_vector(self, key, use_norm=False):\n        \n\n        \n        index = self.get_index(key)\n        if use_norm:\n            self.fill_norms()\n            result = self.vectors[index] / self.norms[index]\n        else:\n            result = self.vectors[index]\n\n        result.setflags(write=False)  # disallow direct tampering that would invalidate `norms` etc\n        return result```\n\nUpdated code:\n```python\ndef get_vector(self, key, norm=False):\n        \n\n        \n        index = self.get_index(key)\n        if norm:\n            self.fill_norms()\n            result = self.vectors[index] / self.norms[index]\n        else:\n            result = self.vectors[index]\n\n        result.setflags(write=False)  # disallow direct tampering that would invalidate `norms` etc\n        return result\n```\n\nOriginal documentation:\n```python\nGet the key's vector, as a 1D numpy array.\n\n        Parameters\n        ----------\n        key : str or int\n            Key for vector to return, or int slot\n        use_norm : bool, optional\n            If True - resulting vector will be L2-normalized (unit euclidean length).\n\n        Returns\n        -------\n        numpy.ndarray\n            Vector for the specified key.\n\n        Raises\n        ------\n        KeyError\n            If the given key doesn't exist.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the key's vector, as a 1D numpy array.\n\n        Parameters\n        ----------\n        key : str or int\n            Key for vector to return, or int slot\n        norm : bool, optional\n            If True - resulting vector will be L2-normalized (unit euclidean length).\n\n        Returns\n        -------\n        numpy.ndarray\n            Vector for the specified key.\n\n        Raises\n        ------\n        KeyError\n            If the given key doesn't exist.", "file": "gensim_models_keyedvectors.py", "function": "KeyedVectors.get_vector", "project": "gensim", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run_server(\n        self,\n        port=8050,\n        debug=False,\n        dev_tools_ui=None,\n        dev_tools_props_check=None,\n        dev_tools_serve_dev_bundles=None,\n        dev_tools_hot_reload=None,\n        dev_tools_hot_reload_interval=None,\n        dev_tools_hot_reload_watch_interval=None,\n        dev_tools_hot_reload_max_retry=None,\n        dev_tools_silence_routes_logging=None,\n        dev_tools_prune_errors=None,\n        **flask_run_options\n    ):\n        \n        \n        debug = self.enable_dev_tools(\n            debug,\n            dev_tools_ui,\n            dev_tools_props_check,\n            dev_tools_serve_dev_bundles,\n            dev_tools_hot_reload,\n            dev_tools_hot_reload_interval,\n            dev_tools_hot_reload_watch_interval,\n            dev_tools_hot_reload_max_retry,\n            dev_tools_silence_routes_logging,\n            dev_tools_prune_errors,\n        )\n\n        if self._dev_tools.silence_routes_logging:\n            # Since it's silenced, the address doesn't show anymore.\n            host = flask_run_options.get(\"host\", \"127.0.0.1\")\n            ssl_context = flask_run_options.get(\"ssl_context\")\n            self.logger.info(\n                \"Running on %s://%s:%s%s\",\n                \"https\" if ssl_context else \"http\",\n                host,\n                port,\n                self.config.requests_pathname_prefix,\n            )\n\n            # Generate a debugger pin and log it to the screen.\n            debugger_pin = os.environ[\"WERKZEUG_DEBUG_PIN\"] = \"-\".join(\n                itertools.chain(\n                    \"\".join([str(random.randint(0, 9)) for _ in range(3)])\n                    for _ in range(3)\n                )\n            )\n\n            self.logger.info(\"Debugger PIN: %s\", debugger_pin)\n\n        self.server.run(port=port, debug=debug, **flask_run_options)```\n\nUpdated code:\n```python\ndef run_server(\n        self,\n        host=os.getenv(\"HOST\", \"127.0.0.1\"),\n        port=os.getenv(\"PORT\", \"8050\"),\n        debug=False,\n        dev_tools_ui=None,\n        dev_tools_props_check=None,\n        dev_tools_serve_dev_bundles=None,\n        dev_tools_hot_reload=None,\n        dev_tools_hot_reload_interval=None,\n        dev_tools_hot_reload_watch_interval=None,\n        dev_tools_hot_reload_max_retry=None,\n        dev_tools_silence_routes_logging=None,\n        dev_tools_prune_errors=None,\n        **flask_run_options\n    ):\n        \n        \n        debug = self.enable_dev_tools(\n            debug,\n            dev_tools_ui,\n            dev_tools_props_check,\n            dev_tools_serve_dev_bundles,\n            dev_tools_hot_reload,\n            dev_tools_hot_reload_interval,\n            dev_tools_hot_reload_watch_interval,\n            dev_tools_hot_reload_max_retry,\n            dev_tools_silence_routes_logging,\n            dev_tools_prune_errors,\n        )\n\n        # Verify port value\n        try:\n            port = int(port)\n            assert port in range(1, 65536)\n        except Exception as e:\n            e.args = [\n                \"Expecting an integer from 1 to 65535, found port={}\".format(repr(port))\n            ]\n            raise\n\n        if self._dev_tools.silence_routes_logging:\n            # Since it's silenced, the address doesn't show anymore.\n            ssl_context = flask_run_options.get(\"ssl_context\")\n            self.logger.info(\n                \"Running on %s://%s:%s%s\",\n                \"https\" if ssl_context else \"http\",\n                host,\n                port,\n                self.config.requests_pathname_prefix,\n            )\n\n            # Generate a debugger pin and log it to the screen.\n            debugger_pin = os.environ[\"WERKZEUG_DEBUG_PIN\"] = \"-\".join(\n                itertools.chain(\n                    \"\".join([str(random.randint(0, 9)) for _ in range(3)])\n                    for _ in range(3)\n                )\n            )\n\n            self.logger.info(\"Debugger PIN: %s\", debugger_pin)\n\n        self.server.run(host=host, port=port, debug=debug, **flask_run_options)\n```\n\nOriginal documentation:\n```python\nStart the flask server in local mode, you should not run this on a\n        production server, use gunicorn/waitress instead.\n\n        If a parameter can be set by an environment variable, that is listed\n        too. Values provided here take precedence over environment variables.\n\n        :param port: Port used to serve the application\n        :type port: int\n\n        :param debug: Set Flask debug mode and enable dev tools.\n            env: ``DASH_DEBUG``\n        :type debug: bool\n\n        :param debug: Enable/disable all the dev tools unless overridden by the\n            arguments or environment variables. Default is ``True`` when\n            ``enable_dev_tools`` is called directly, and ``False`` when called\n            via ``run_server``. env: ``DASH_DEBUG``\n        :type debug: bool\n\n        :param dev_tools_ui: Show the dev tools UI. env: ``DASH_UI``\n        :type dev_tools_ui: bool\n\n        :param dev_tools_props_check: Validate the types and values of Dash\n            component props. env: ``DASH_PROPS_CHECK``\n        :type dev_tools_props_check: bool\n\n        :param dev_tools_serve_dev_bundles: Serve the dev bundles. Production\n            bundles do not necessarily include all the dev tools code.\n            env: ``DASH_SERVE_DEV_BUNDLES``\n        :type dev_tools_serve_dev_bundles: bool\n\n        :param dev_tools_hot_reload: Activate hot reloading when app, assets,\n            and component files change. env: ``DASH_HOT_RELOAD``\n        :type dev_tools_hot_reload: bool\n\n        :param dev_tools_hot_reload_interval: Interval in seconds for the\n            client to request the reload hash. Default 3.\n            env: ``DASH_HOT_RELOAD_INTERVAL``\n        :type dev_tools_hot_reload_interval: float\n\n        :param dev_tools_hot_reload_watch_interval: Interval in seconds for the\n            server to check asset and component folders for changes.\n            Default 0.5. env: ``DASH_HOT_RELOAD_WATCH_INTERVAL``\n        :type dev_tools_hot_reload_watch_interval: float\n\n        :param dev_tools_hot_reload_max_retry: Maximum number of failed reload\n            hash requests before failing and displaying a pop up. Default 8.\n            env: ``DASH_HOT_RELOAD_MAX_RETRY``\n        :type dev_tools_hot_reload_max_retry: int\n\n        :param dev_tools_silence_routes_logging: Silence the `werkzeug` logger,\n            will remove all routes logging. Enabled with debugging by default\n            because hot reload hash checks generate a lot of requests.\n            env: ``DASH_SILENCE_ROUTES_LOGGING``\n        :type dev_tools_silence_routes_logging: bool\n\n        :param dev_tools_prune_errors: Reduce tracebacks to just user code,\n            stripping out Flask and Dash pieces. Only available with debugging.\n            `True` by default, set to `False` to see the complete traceback.\n            env: ``DASH_PRUNE_ERRORS``\n        :type dev_tools_prune_errors: bool\n\n        :param flask_run_options: Given to `Flask.run`\n\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Start the flask server in local mode, you should not run this on a\n        production server, use gunicorn/waitress instead.\n\n        If a parameter can be set by an environment variable, that is listed\n        too. Values provided here take precedence over environment variables.\n\n        :param host: Host IP used to serve the application\n            env: ``HOST``\n        :type host: string\n\n        :param port: Port used to serve the application\n            env: ``PORT``\n        :type port: int\n\n        :param debug: Set Flask debug mode and enable dev tools.\n            env: ``DASH_DEBUG``\n        :type debug: bool\n\n        :param debug: Enable/disable all the dev tools unless overridden by the\n            arguments or environment variables. Default is ``True`` when\n            ``enable_dev_tools`` is called directly, and ``False`` when called\n            via ``run_server``. env: ``DASH_DEBUG``\n        :type debug: bool\n\n        :param dev_tools_ui: Show the dev tools UI. env: ``DASH_UI``\n        :type dev_tools_ui: bool\n\n        :param dev_tools_props_check: Validate the types and values of Dash\n            component props. env: ``DASH_PROPS_CHECK``\n        :type dev_tools_props_check: bool\n\n        :param dev_tools_serve_dev_bundles: Serve the dev bundles. Production\n            bundles do not necessarily include all the dev tools code.\n            env: ``DASH_SERVE_DEV_BUNDLES``\n        :type dev_tools_serve_dev_bundles: bool\n\n        :param dev_tools_hot_reload: Activate hot reloading when app, assets,\n            and component files change. env: ``DASH_HOT_RELOAD``\n        :type dev_tools_hot_reload: bool\n\n        :param dev_tools_hot_reload_interval: Interval in seconds for the\n            client to request the reload hash. Default 3.\n            env: ``DASH_HOT_RELOAD_INTERVAL``\n        :type dev_tools_hot_reload_interval: float\n\n        :param dev_tools_hot_reload_watch_interval: Interval in seconds for the\n            server to check asset and component folders for changes.\n            Default 0.5. env: ``DASH_HOT_RELOAD_WATCH_INTERVAL``\n        :type dev_tools_hot_reload_watch_interval: float\n\n        :param dev_tools_hot_reload_max_retry: Maximum number of failed reload\n            hash requests before failing and displaying a pop up. Default 8.\n            env: ``DASH_HOT_RELOAD_MAX_RETRY``\n        :type dev_tools_hot_reload_max_retry: int\n\n        :param dev_tools_silence_routes_logging: Silence the `werkzeug` logger,\n            will remove all routes logging. Enabled with debugging by default\n            because hot reload hash checks generate a lot of requests.\n            env: ``DASH_SILENCE_ROUTES_LOGGING``\n        :type dev_tools_silence_routes_logging: bool\n\n        :param dev_tools_prune_errors: Reduce tracebacks to just user code,\n            stripping out Flask and Dash pieces. Only available with debugging.\n            `True` by default, set to `False` to see the complete traceback.\n            env: ``DASH_PRUNE_ERRORS``\n        :type dev_tools_prune_errors: bool\n\n        :param flask_run_options: Given to `Flask.run`\n\n        :return:", "file": "dash_dash.py", "function": "Dash.run_server", "project": "dash", "change_stats": {"code_changes": 18, "doc_changes": 7, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef raise_if_user_provided_url_unsafe(url: str):\n    \n    parsed_url: urllib.parse.ParseResult = urllib.parse.urlparse(url)\n    if not parsed_url.hostname:\n        raise ValueError(\"No hostname\")\n    if parsed_url.scheme == \"http\":\n        port = 80\n    elif parsed_url.scheme == \"https\":\n        port = 443\n    else:\n        raise ValueError(\"Scheme must be either HTTP or HTTPS\")\n    if parsed_url.port is not None and parsed_url.port != port:\n        raise ValueError(\"Port does not match scheme\")\n    # Disallow if hostname resolves to a private (internal) IP address\n    try:\n        addrinfo = socket.getaddrinfo(parsed_url.hostname, port)\n    except socket.gaierror:\n        raise ValueError(\"Invalid hostname\")\n    for _, _, _, _, sockaddr in addrinfo:\n        if ip_address(sockaddr[0]).is_private:  # Prevent addressing internal services\n            raise ValueError(\"Invalid hostname\")```\n\nUpdated code:\n```python\ndef raise_if_user_provided_url_unsafe(url: str):\n    \n    \n    parsed_url: urllib.parse.ParseResult = urllib.parse.urlparse(url)  # urlparse never raises errors\n    if not parsed_url.hostname:\n        raise ValueError(\"No hostname\")\n    if parsed_url.scheme not in (\"http\", \"https\"):\n        raise ValueError(\"Scheme must be either HTTP or HTTPS\")\n    # Disallow if hostname resolves to a private (internal) IP address\n    try:\n        addrinfo = socket.getaddrinfo(parsed_url.hostname, None)\n    except socket.gaierror:\n        raise ValueError(\"Invalid hostname\")\n    for _, _, _, _, sockaddr in addrinfo:\n        if ip_address(sockaddr[0]).is_private:  # Prevent addressing internal services\n            raise ValueError(\"Internal hostname\")\n```\n\nOriginal documentation:\n```python\nRaise if the provided URL seems unsafe, otherwise do nothing.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Raise if the provided URL seems unsafe, otherwise do nothing.\n\n    Equivalent of plugin server raiseIfUserProvidedUrlUnsafe.", "file": "posthog_api_utils.py", "function": "raise_if_user_provided_url_unsafe", "project": "posthog", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ui_click(self, click_button, check_button, appear_button=None, additional_button=None,\n                 offset=(20, 20), retry_wait=10, additional_button_interval=3, skip_first_screenshot=False):\n        \n        \n        \n        logger.hr('UI click')\n        if appear_button is None:\n            appear_button = click_button\n        if not isinstance(additional_button, list):\n            additional_button = [additional_button]\n        click_timer = Timer(retry_wait, count=retry_wait // 0.5)\n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if (isinstance(check_button, Button) and self.appear(check_button, offset=offset)) \\\n                    or (callable(check_button) and check_button()):\n                break\n\n            for button in additional_button:\n                if button is None:\n                    continue\n                if isinstance(button, Button):\n                    self.appear_then_click(button, offset=offset, interval=additional_button_interval)\n                    continue\n                if callable(button) and button():\n                    continue\n\n            if click_timer.reached():\n                if (isinstance(appear_button, Button) and self.appear(appear_button, offset=offset)) \\\n                        or (callable(appear_button) and appear_button()):\n                    self.device.click(click_button)\n                    click_timer.reset()\n                    continue```\n\nUpdated code:\n```python\ndef ui_click(self, click_button, check_button, appear_button=None, additional=None, confirm_wait=2,\n                 offset=(20, 20), retry_wait=10, skip_first_screenshot=False):\n        \n        \n        \n        logger.hr('UI click')\n        if appear_button is None:\n            appear_button = click_button\n\n        click_timer = Timer(retry_wait, count=retry_wait // 0.5)\n        confirm_wait = confirm_wait if additional is not None else 0\n        confirm_timer = Timer(confirm_wait, count=confirm_wait // 0.5).start()\n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if (isinstance(check_button, Button) and self.appear(check_button, offset=offset)) \\\n                    or (callable(check_button) and check_button()):\n                if confirm_timer.reached():\n                    break\n            else:\n                confirm_timer.reset()\n\n            if click_timer.reached():\n                if (isinstance(appear_button, Button) and self.appear(appear_button, offset=offset)) \\\n                        or (callable(appear_button) and appear_button()):\n                    self.device.click(click_button)\n                    click_timer.reset()\n                    continue\n\n            if additional is not None:\n                if additional():\n                    continue\n```\n\nOriginal documentation:\n```python\nArgs:\n            click_button (Button):\n            check_button (Button, callable):\n            appear_button (Button, callable):\n            additional_button (Button, list[Button], callable):\n            additional_button_interval (int, float):\n            offset (bool, int, tuple):\n            retry_wait (int, float):\n            skip_first_screenshot (bool):\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            click_button (Button):\n            check_button (Button, callable):\n            appear_button (Button, callable):\n            additional (callable):\n            confirm_wait (int, float):\n            offset (bool, int, tuple):\n            retry_wait (int, float):\n            skip_first_screenshot (bool):", "file": "module_ui_ui.py", "function": "UI.ui_click", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 29, "doc_changes": 6, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef shift(self, n, freq=None):\n        \n        \n        \n        if freq is not None and freq != self.freq:\n            if isinstance(freq, compat.string_types):\n                freq = frequencies.to_offset(freq)\n            offset = n * freq\n            result = self + offset\n\n            if hasattr(self, 'tz'):\n                result._tz = self.tz\n\n            return result\n\n        if n == 0:\n            # immutable so OK\n            return self.copy()\n\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n\n        start = self[0] + n * self.freq\n        end = self[-1] + n * self.freq\n        attribs = self._get_attributes_dict()\n        return self._generate_range(start=start, end=end, periods=None,\n                                    **attribs)```\n\nUpdated code:\n```python\ndef shift(self, periods, freq=None):\n        \n        \n        \n        if freq is not None and freq != self.freq:\n            if isinstance(freq, compat.string_types):\n                freq = frequencies.to_offset(freq)\n            offset = periods * freq\n            result = self + offset\n            if hasattr(self, 'tz'):\n                result._tz = self.tz\n            return result\n\n        if periods == 0:\n            # immutable so OK\n            return self.copy()\n\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n\n        start = self[0] + periods * self.freq\n        end = self[-1] + periods * self.freq\n        attribs = self._get_attributes_dict()\n        return self._generate_range(start=start, end=end, periods=None,\n                                    **attribs)\n```\n\nOriginal documentation:\n```python\nSpecialized shift which produces a Datetime/Timedelta Array/Index\n\n        Parameters\n        ----------\n        n : int\n            Periods to shift by\n        freq : DateOffset or timedelta-like, optional\n\n        Returns\n        -------\n        shifted : same type as self\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n\n            .. versionchanged:: 0.24.0\n\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.DatetimeIndex\n            Shifted index.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.", "file": "pandas_core_arrays_datetimelike.py", "function": "DatetimeLikeArrayMixin.shift", "project": "pandas", "change_stats": {"code_changes": 14, "doc_changes": 27, "total_changes": 41}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, axes, prop_name, add_name,\n                     valid_types=None, invalid_types=None):\n            \n            \n            \n            self._axes = axes\n            self._prop_name = prop_name\n            self._add_name = add_name\n            self._type_check = lambda artist: (\n                (not valid_types or isinstance(artist, valid_types)) and\n                (not invalid_types or not isinstance(artist, invalid_types))\n            )```\n\nUpdated code:\n```python\ndef __init__(self, axes, prop_name,\n                     valid_types=None, invalid_types=None):\n            \n            \n            \n            self._axes = axes\n            self._prop_name = prop_name\n            self._type_check = lambda artist: (\n                (not valid_types or isinstance(artist, valid_types)) and\n                (not invalid_types or not isinstance(artist, invalid_types))\n            )\n```\n\nOriginal documentation:\n```python\nParameters\n            ----------\n            axes : `~matplotlib.axes.Axes`\n                The Axes from which this sublist will pull the children\n                Artists.\n            prop_name : str\n                The property name used to access this sublist from the Axes;\n                used to generate deprecation warnings.\n            add_name : str\n                The method name used to add Artists of this sublist's type to\n                the Axes; used to generate deprecation warnings.\n            valid_types : list of type, optional\n                A list of types that determine which children will be returned\n                by this sublist. If specified, then the Artists in the sublist\n                must be instances of any of these types. If unspecified, then\n                any type of Artist is valid (unless limited by\n                *invalid_types*.)\n            invalid_types : tuple, optional\n                A list of types that determine which children will *not* be\n                returned by this sublist. If specified, then Artists in the\n                sublist will never be an instance of these types. Otherwise, no\n                types will be excluded.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n            ----------\n            axes : `~matplotlib.axes.Axes`\n                The Axes from which this sublist will pull the children\n                Artists.\n            prop_name : str\n                The property name used to access this sublist from the Axes;\n                used to generate deprecation warnings.\n            valid_types : list of type, optional\n                A list of types that determine which children will be returned\n                by this sublist. If specified, then the Artists in the sublist\n                must be instances of any of these types. If unspecified, then\n                any type of Artist is valid (unless limited by\n                *invalid_types*.)\n            invalid_types : tuple, optional\n                A list of types that determine which children will *not* be\n                returned by this sublist. If specified, then Artists in the\n                sublist will never be an instance of these types. Otherwise, no\n                types will be excluded.", "file": "lib_matplotlib_axes__base.py", "function": "ArtistList.__init__", "project": "matplotlib", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _fastcopy_sendfile(fsrc, fdst):\n    \n    \n    # Note: copyfileobj() is left alone in order to not introduce any\n    # unexpected breakage. Possible risks by using zero-copy calls\n    # in copyfileobj() are:\n    # - fdst cannot be open in \"a\"(ppend) mode\n    # - fsrc and fdst may be open in \"t\"(ext) mode\n    # - fsrc may be a BufferedReader (which hides unread data in a buffer),\n    #   GzipFile (which decompresses data), HTTPResponse (which decodes\n    #   chunks).\n    # - possibly others (e.g. encrypted fs/partition?)\n    global _HAS_SENDFILE\n    try:\n        infd = fsrc.fileno()\n        outfd = fdst.fileno()\n    except Exception as err:\n        raise _GiveupOnFastCopy(err)  # not a regular file\n\n    # Hopefully the whole file will be copied in a single call.\n    # sendfile() is called in a loop 'till EOF is reached (0 return)\n    # so a bufsize smaller or bigger than the actual file size\n    # should not make any difference, also in case the file content\n    # changes while being copied.\n    try:\n        blocksize = max(os.fstat(infd).st_size, 2 ** 23)  # min 8MB\n    except Exception:\n        blocksize = 2 ** 27  # 128MB\n\n    offset = 0\n    while True:\n        try:\n            sent = os.sendfile(outfd, infd, offset, blocksize)\n        except OSError as err:\n            # ...in oder to have a more informative exception.\n            err.filename = fsrc.name\n            err.filename2 = fdst.name\n\n            if err.errno == errno.ENOTSOCK:\n                # sendfile() on this platform (probably Linux < 2.6.33)\n                # does not support copies between regular files (only\n                # sockets).\n                _HAS_SENDFILE = False\n                raise _GiveupOnFastCopy(err)\n\n            if err.errno == errno.ENOSPC:  # filesystem is full\n                raise err from None\n\n            # Give up on first call and if no data was copied.\n            if offset == 0 and os.lseek(outfd, 0, os.SEEK_CUR) == 0:\n                raise _GiveupOnFastCopy(err)\n\n            raise err\n        else:\n            if sent == 0:\n                break  # EOF\n            offset += sent```\n\nUpdated code:\n```python\ndef _fastcopy_sendfile(fsrc, fdst):\n    \n    \n    # Note: copyfileobj() is left alone in order to not introduce any\n    # unexpected breakage. Possible risks by using zero-copy calls\n    # in copyfileobj() are:\n    # - fdst cannot be open in \"a\"(ppend) mode\n    # - fsrc and fdst may be open in \"t\"(ext) mode\n    # - fsrc may be a BufferedReader (which hides unread data in a buffer),\n    #   GzipFile (which decompresses data), HTTPResponse (which decodes\n    #   chunks).\n    # - possibly others (e.g. encrypted fs/partition?)\n    global _USE_CP_SENDFILE\n    try:\n        infd = fsrc.fileno()\n        outfd = fdst.fileno()\n    except Exception as err:\n        raise _GiveupOnFastCopy(err)  # not a regular file\n\n    # Hopefully the whole file will be copied in a single call.\n    # sendfile() is called in a loop 'till EOF is reached (0 return)\n    # so a bufsize smaller or bigger than the actual file size\n    # should not make any difference, also in case the file content\n    # changes while being copied.\n    try:\n        blocksize = max(os.fstat(infd).st_size, 2 ** 23)  # min 8MB\n    except Exception:\n        blocksize = 2 ** 27  # 128MB\n\n    offset = 0\n    while True:\n        try:\n            sent = os.sendfile(outfd, infd, offset, blocksize)\n        except OSError as err:\n            # ...in oder to have a more informative exception.\n            err.filename = fsrc.name\n            err.filename2 = fdst.name\n\n            if err.errno == errno.ENOTSOCK:\n                # sendfile() on this platform (probably Linux < 2.6.33)\n                # does not support copies between regular files (only\n                # sockets).\n                _USE_CP_SENDFILE = False\n                raise _GiveupOnFastCopy(err)\n\n            if err.errno == errno.ENOSPC:  # filesystem is full\n                raise err from None\n\n            # Give up on first call and if no data was copied.\n            if offset == 0 and os.lseek(outfd, 0, os.SEEK_CUR) == 0:\n                raise _GiveupOnFastCopy(err)\n\n            raise err\n        else:\n            if sent == 0:\n                break  # EOF\n            offset += sent\n```\n\nOriginal documentation:\n```python\nCopy data from one regular mmap-like fd to another by using\n    high-performance sendfile(2) syscall.\n    This should work on Linux >= 2.6.33 and Solaris only.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Copy data from one regular mmap-like fd to another by using\n    high-performance sendfile(2) syscall.\n    This should work on Linux >= 2.6.33 only.", "file": "Lib_shutil.py", "function": "_fastcopy_sendfile", "project": "cpython", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, status, query_time=None, context=None):\n        \n        \n\n        self.status      = status\n        self.query_time  = query_time\n        self.context     = context\n\n        return```\n\nUpdated code:\n```python\ndef __init__(self, username, site_name, site_url_user, status,\n                 query_time=None, context=None):\n        \n        \n\n        self.username      = username\n        self.site_name     = site_name\n        self.site_url_user = site_url_user\n        self.status        = status\n        self.query_time    = query_time\n        self.context       = context\n\n        return\n```\n\nOriginal documentation:\n```python\nCreate Query Result Object.\n\n        Contains information about a specific method of detecting usernames on\n        a given type of web sites.\n\n        Keyword Arguments:\n        self                   -- This object.\n        status                 -- Enumeration of type QueryStatus() indicating\n                                  the status of the query.\n        query_time             -- Time (in seconds) required to perform query.\n                                  Default of None.\n        context                -- String indicating any additional context\n                                  about the query.  For example, if there was\n                                  an error, this might indicate the type of\n                                  error that occurred.\n                                  Default of None.\n\n        Return Value:\n        Nothing.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create Query Result Object.\n\n        Contains information about a specific method of detecting usernames on\n        a given type of web sites.\n\n        Keyword Arguments:\n        self                   -- This object.\n        username               -- String indicating username that query result\n                                  was about.\n        site_name              -- String which identifies site.\n        site_url_user          -- String containing URL for username on site.\n                                  NOTE:  The site may or may not exist:  this\n                                         just indicates what the name would\n                                         be, if it existed.\n        status                 -- Enumeration of type QueryStatus() indicating\n                                  the status of the query.\n        query_time             -- Time (in seconds) required to perform query.\n                                  Default of None.\n        context                -- String indicating any additional context\n                                  about the query.  For example, if there was\n                                  an error, this might indicate the type of\n                                  error that occurred.\n                                  Default of None.\n\n        Return Value:\n        Nothing.", "file": "sherlock_result.py", "function": "QueryResult.__init__", "project": "sherlock", "change_stats": {"code_changes": 14, "doc_changes": 9, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _prepare_lab_array(arr, force_copy=True):\n    \n    \n    arr = np.asarray(arr)\n    shape = arr.shape\n    if shape[-1] < 3:\n        raise ValueError('Input array has less than 3 color channels')\n    float_dtype = _supported_float_type(arr.dtype)\n    if float_dtype == np.float32:\n        _func = dtype.img_as_float32\n    else:\n        _func = dtype.img_as_float64\n    return _func(arr, force_copy=force_copy)```\n\nUpdated code:\n```python\ndef _prepare_lab_array(arr, force_copy=True):\n    \n    \n    arr = np.asarray(arr)\n    shape = arr.shape\n    if shape[-1] < 3:\n        raise ValueError('Input image has less than 3 channels.')\n    float_dtype = _supported_float_type(arr.dtype)\n    if float_dtype == np.float32:\n        _func = dtype.img_as_float32\n    else:\n        _func = dtype.img_as_float64\n    return _func(arr, force_copy=force_copy)\n```\n\nOriginal documentation:\n```python\nEnsure input for lab2lch, lch2lab are well-posed.\n\n    Arrays must be in floating point and have at least 3 elements in\n    last dimension.  Return a new array.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure input for lab2lch and lch2lab is well-formed.\n\n    Input array must be in floating point and have at least 3 elements in the\n    last dimension. Returns a new array by default.", "file": "skimage_color_colorconv.py", "function": "_prepare_lab_array", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_order(self, symbol, type, side, amount, price=None, params={}):\n        \n        \n        \n        # if type == 'market':\n        #     raise ExchangeError(self.id + ' createOrder() does not accept market orders')\n        # }\n        self.load_markets()\n        market = self.market(symbol)\n        upperCaseType = type.upper()\n        isMarket = upperCaseType == 'MARKET'\n        isPostOnly = self.is_post_only(isMarket, upperCaseType == 'LIMIT_MAKER', params)\n        if isPostOnly:\n            upperCaseType = 'LIMIT_MAKER'\n            params = self.omit(params, 'postOnly')\n        request = {\n            'symbol': market['id'],\n            'side': side,\n            'type': upperCaseType,\n            # 'timeInForce': timeInForce,\n            # 'accountType': 'SPOT',\n            # 'amount': amount,\n        }\n        if isMarket:\n            if side == 'buy':\n                request['amount'] = self.currency_to_precision(market['quote'], amount)\n            else:\n                request['quantity'] = self.amount_to_precision(symbol, amount)\n        else:\n            request['quantity'] = self.amount_to_precision(symbol, amount)\n            request['price'] = self.price_to_precision(symbol, price)\n        clientOrderId = self.safe_string(params, 'clientOrderId')\n        if clientOrderId is not None:\n            request['clientOrderId'] = clientOrderId\n            params = self.omit(params, 'clientOrderId')\n        # remember the timestamp before issuing the request\n        response = self.privatePostOrders(self.extend(request, params))\n        #\n        #     {\n        #         \"id\" : \"78923648051920896\",\n        #         \"clientOrderId\" : \"\"\n        #     }\n        #\n        response = self.extend(response, {\n            'type': side,\n        })\n        return self.parse_order(response, market)```\n\nUpdated code:\n```python\ndef create_order(self, symbol, type, side, amount, price=None, params={}):\n        \n        \n        \n        # if type == 'market':\n        #     raise ExchangeError(self.id + ' createOrder() does not accept market orders')\n        # }\n        self.load_markets()\n        market = self.market(symbol)\n        if not market['spot']:\n            raise NotSupported(self.id + ' createOrder() does not support ' + market['type'] + ' orders, only spot orders are accepted')\n        request = {\n            'symbol': market['id'],\n            'side': side,\n            # 'timeInForce': timeInForce,\n            # 'accountType': 'SPOT',\n            # 'amount': amount,\n        }\n        orderRequest = self.order_request(symbol, type, side, amount, request, price, params)\n        response = self.privatePostOrders(self.extend(orderRequest[0], orderRequest[1]))\n        #\n        #     {\n        #         \"id\" : \"78923648051920896\",\n        #         \"clientOrderId\" : \"\"\n        #     }\n        #\n        response = self.extend(response, {\n            'type': side,\n        })\n        return self.parse_order(response, market)\n```\n\nOriginal documentation:\n```python\ncreate a trade order\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'market' or 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much of currency you want to trade in units of base currency\n        :param float|None price: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict params: extra parameters specific to the poloniex api endpoint\n        :returns dict: an `order structure <https://docs.ccxt.com/en/latest/manual.html#order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "create a trade order\n        see https://docs.poloniex.com/#authenticated-endpoints-orders-create-order\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'market' or 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much of currency you want to trade in units of base currency\n        :param float|None price: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict params: extra parameters specific to the poloniex api endpoint\n        :returns dict: an `order structure <https://docs.ccxt.com/en/latest/manual.html#order-structure>`", "file": "python_ccxt_poloniex.py", "function": "poloniex.create_order", "project": "ccxt", "change_stats": {"code_changes": 27, "doc_changes": 3, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef head(self, n=5):\n        \n        \n        \n        self._reset_group_selection()\n        mask = self._cumcount_array() < n\n        return self._mask_selected_obj(mask)```\n\nUpdated code:\n```python\ndef head(self, n=5):\n        \n        \n        \n        self._reset_group_selection()\n        mask = self._make_mask_from_positional_indexer(slice(None, n))\n        return self._mask_selected_obj(mask)\n```\n\nOriginal documentation:\n```python\nReturn first n rows of each group.\n\n        Similar to ``.apply(lambda x: x.head(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Does not work for negative values of `n`.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(-1)\n        Empty DataFrame\n        Columns: [A, B]\n        Index: []\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return first n rows of each group.\n\n        Similar to ``.apply(lambda x: x.head(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Parameters\n        ----------\n        n : int\n            If positive: number of entries to include from start of each group.\n            If negative: number of entries to exclude from end of each group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Subset of original Series or DataFrame as determined by n.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(-1)\n           A  B\n        0  1  2", "file": "pandas_core_groupby_groupby.py", "function": "GroupBy.head", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 14, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef as_nested_sql(self):\n        \n        \n        \n        obj = self.clone()\n        obj.bump_prefix()\n        return obj.as_sql()```\n\nUpdated code:\n```python\ndef as_nested_sql(self):\n        \n        \n        \n        obj = self.clone()\n        obj.clear_ordering(True)\n        obj.bump_prefix()\n        return obj.as_sql()\n```\n\nOriginal documentation:\n```python\nPerform the same functionality as the as_sql() method, returning an\n        SQL string and parameters. However, the alias prefixes are bumped\n        beforehand (in a copy -- the current query isn't changed).\n\n        Used when nesting this query inside another.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform the same functionality as the as_sql() method, returning an\n        SQL string and parameters. However, the alias prefixes are bumped\n        beforehand (in a copy -- the current query isn't changed) and any\n        ordering is removed.\n\n        Used when nesting this query inside another.", "file": "django_db_models_sql_query.py", "function": "BaseQuery.as_nested_sql", "project": "django", "change_stats": {"code_changes": 3, "doc_changes": 5, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_rgrids(self, radii, labels=None, angle=None, rpad=None, fmt=None,\n                   **kwargs):\n        \n        \n        \n        radii = np.asarray(radii)\n        rmin = radii.min()\n        if rmin <= 0:\n            raise ValueError('radial grids must be strictly positive')\n\n        self.set_yticks(radii)\n        if labels is not None:\n            self.set_yticklabels(labels)\n        elif fmt is not None:\n            self.yaxis.set_major_formatter(FormatStrFormatter(fmt))\n        if angle is None:\n            angle = self._r_label1_position.to_values()[4]\n        if rpad is not None:\n            self._rpad = rpad\n        self._r_label1_position._t = (angle, self._rpad)\n        self._r_label1_position.invalidate()\n        self._r_label2_position._t = (angle, -self._rpad)\n        self._r_label2_position.invalidate()\n        for t in self.yaxis.get_ticklabels():\n            t.update(kwargs)\n        return self.yaxis.get_gridlines(), self.yaxis.get_ticklabels()```\n\nUpdated code:\n```python\ndef set_rgrids(self, radii, labels=None, angle=None, fmt=None,\n                   **kwargs):\n        \n        \n        \n        # Make sure we take into account unitized data\n        radii = self.convert_xunits(radii)\n        radii = np.asarray(radii)\n        rmin = radii.min()\n        if rmin <= 0:\n            raise ValueError('radial grids must be strictly positive')\n\n        self.set_yticks(radii)\n        if labels is not None:\n            self.set_yticklabels(labels)\n        elif fmt is not None:\n            self.yaxis.set_major_formatter(FormatStrFormatter(fmt))\n        if angle is None:\n            angle = self._r_label_position.to_values()[4]\n        self._r_label_position._t = (angle, 0.0)\n        self._r_label_position.invalidate()\n        for t in self.yaxis.get_ticklabels():\n            t.update(kwargs)\n        return self.yaxis.get_gridlines(), self.yaxis.get_ticklabels()\n```\n\nOriginal documentation:\n```python\nSet the radial locations and labels of the *r* grids.\n\n        The labels will appear at radial distances *radii* at the\n        given *angle* in degrees.\n\n        *labels*, if not None, is a ``len(radii)`` list of strings of the\n        labels to use at each radius.\n\n        If *labels* is None, the built-in formatter will be used.\n\n        *rpad* is a fraction of the max of *radii* which will pad each of\n        the radial labels in the radial direction.\n\n        Return value is a list of tuples (*line*, *label*), where\n        *line* is :class:`~matplotlib.lines.Line2D` instances and the\n        *label* is :class:`~matplotlib.text.Text` instances.\n\n        kwargs are optional text properties for the labels:\n\n        %(Text)s\n\n        ACCEPTS: sequence of floats\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the radial locations and labels of the *r* grids.\n\n        The labels will appear at radial distances *radii* at the\n        given *angle* in degrees.\n\n        *labels*, if not None, is a ``len(radii)`` list of strings of the\n        labels to use at each radius.\n\n        If *labels* is None, the built-in formatter will be used.\n\n        Return value is a list of tuples (*line*, *label*), where\n        *line* is :class:`~matplotlib.lines.Line2D` instances and the\n        *label* is :class:`~matplotlib.text.Text` instances.\n\n        kwargs are optional text properties for the labels:\n\n        %(Text)s\n\n        ACCEPTS: sequence of floats", "file": "lib_matplotlib_projections_polar.py", "function": "PolarAxes.set_rgrids", "project": "matplotlib", "change_stats": {"code_changes": 16, "doc_changes": 5, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _clone_world_group():\n    \n    \n    assert torch.distributed.is_initialized(), \"torch.distributed is not initialized\"\n    global _WORLD_GROUP\n    if _WORLD_GROUP is None:\n        # If not cloned already, clone the world group\n        _WORLD_GROUP = torch.distributed.new_group(\n            ranks=range(torch.distributed.get_world_size()))\n    return _WORLD_GROUP```\n\nUpdated code:\n```python\ndef _clone_world_group():\n    \n    \n    assert dist.is_initialized(), \"dist is not initialized\"\n    global _WORLD_GROUP\n    if _WORLD_GROUP is None:\n        # If not cloned already, clone the world group\n        _WORLD_GROUP = dist.new_group(ranks=range(dist.get_world_size()))\n    return _WORLD_GROUP\n```\n\nOriginal documentation:\n```python\nCreate a clone of the world group\n        Note: We need to clone the torch.distributed world group because we\n        use _get_global_rank() utility function in DeepSpeed at many places.\n        As that function does not work on torch.distributed.group.WORLD, we\n        need to keep a clone of it.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a clone of the world group\n        Note: We need to clone the dist world group because we\n        use dist.get_global_rank() utility function in DeepSpeed at many places.\n        As that function does not work on dist.group.WORLD, we\n        need to keep a clone of it.", "file": "deepspeed_utils_groups.py", "function": "_clone_world_group", "project": "DeepSpeed", "change_stats": {"code_changes": 7, "doc_changes": 8, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        build_context: \"BuildContext\",\n        deploy_context: \"DeployContext\",\n        sync_context: \"SyncContext\",\n        physical_id_mapping: Dict[str, str],\n        log_name: str,\n        stacks: Optional[List[Stack]] = None,\n    ):\n        \n        \n        \n        self._build_context = build_context\n        self._deploy_context = deploy_context\n        self._sync_context = sync_context\n        self._log_name = log_name\n        self._stacks = stacks\n        self._session = None\n        self._physical_id_mapping = physical_id_mapping\n        self._locks = None\n        self._local_sha = None```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        build_context: \"BuildContext\",\n        deploy_context: \"DeployContext\",\n        sync_context: \"SyncContext\",\n        physical_id_mapping: Dict[str, str],\n        log_name: str,\n        stacks: Optional[List[Stack]] = None,\n        application_build_result: Optional[ApplicationBuildResult] = None,\n    ):\n        \n        \n        \n        self._build_context = build_context\n        self._deploy_context = deploy_context\n        self._sync_context = sync_context\n        self._log_name = log_name\n        self._stacks = stacks\n        self._session = None\n        self._physical_id_mapping = physical_id_mapping\n        self._locks = None\n        self._local_sha = None\n        self._application_build_result = application_build_result\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        build_context : BuildContext\n            BuildContext used for build related parameters\n        deploy_context : BuildContext\n            DeployContext used for this deploy related parameters\n        sync_context: SyncContext\n            SyncContext object that obtains sync information.\n        physical_id_mapping : Dict[str, str]\n            Mapping between resource logical identifier and physical identifier\n        log_name : str\n            Name to be used for logging purposes\n        stacks : List[Stack], optional\n            List of stacks containing a root stack and optional nested stacks\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        build_context : BuildContext\n            BuildContext used for build related parameters\n        deploy_context : BuildContext\n            DeployContext used for this deploy related parameters\n        sync_context: SyncContext\n            SyncContext object that obtains sync information.\n        physical_id_mapping : Dict[str, str]\n            Mapping between resource logical identifier and physical identifier\n        log_name : str\n            Name to be used for logging purposes\n        stacks : List[Stack], optional\n            List of stacks containing a root stack and optional nested stacks\n         application_build_result: Optional[ApplicationBuildResult]\n            Pre-build ApplicationBuildResult which can be re-used during SyncFlows", "file": "samcli_lib_sync_sync_flow.py", "function": "SyncFlow.__init__", "project": "aws-sam-cli", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_hash(self, salt, plain_password):\n        \n        return hashlib.sha256(salt.encode() + plain_password.encode()).hexdigest()```\n\nUpdated code:\n```python\ndef get_hash(self, plain_password, salt=''):\n        \n        return to_hex(hashlib.pbkdf2_hmac('sha256', plain_password.encode(), salt.encode(), 100000, dklen=128))\n```\n\nOriginal documentation:\n```python\nReturn the hashed password, salt + SHA-256.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the hashed password, salt + pbkdf2_hmac.", "file": "glances_password.py", "function": "GlancesPassword.get_hash", "project": "glances", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef handle_exception(self, e):\n        \n        \n        exc_type, exc_value, tb = sys.exc_info()\n        got_request_exception.send(self, exception=e)\n\n        if self.propagate_exceptions:\n            # if we want to repropagate the exception, we can attempt to\n            # raise it with the whole traceback in case we can do that\n            # (the function was actually called from the except part)\n            # otherwise, we just raise the error again\n            if exc_value is e:\n                reraise(exc_type, exc_value, tb)\n            else:\n                raise e\n\n        self.log_exception((exc_type, exc_value, tb))\n        handler = self._find_error_handler(InternalServerError())\n        if handler is None:\n            return InternalServerError()\n        return self.finalize_request(handler(e), from_error_handler=True)```\n\nUpdated code:\n```python\ndef handle_exception(self, e):\n        \n        \n        exc_type, exc_value, tb = sys.exc_info()\n        got_request_exception.send(self, exception=e)\n\n        if self.propagate_exceptions:\n            # if we want to repropagate the exception, we can attempt to\n            # raise it with the whole traceback in case we can do that\n            # (the function was actually called from the except part)\n            # otherwise, we just raise the error again\n            if exc_value is e:\n                reraise(exc_type, exc_value, tb)\n            else:\n                raise e\n\n        self.log_exception((exc_type, exc_value, tb))\n        server_error = InternalServerError()\n        # TODO: pass as param when Werkzeug>=1.0.0 is required\n        # TODO: also remove note about this from docstring and docs\n        server_error.original_exception = e\n        handler = self._find_error_handler(server_error)\n\n        if handler is not None:\n            server_error = handler(server_error)\n\n        return self.finalize_request(server_error, from_error_handler=True)\n```\n\nOriginal documentation:\n```python\nDefault exception handling that kicks in when an exception\n        occurs that is not caught.  In debug mode the exception will\n        be re-raised immediately, otherwise it is logged and the handler\n        for a 500 internal server error is used.  If no such handler\n        exists, a default 500 internal server error message is displayed.\n\n        .. versionadded:: 0.3\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Handle an exception that did not have an error handler\n        associated with it, or that was raised from an error handler.\n        This always causes a 500 ``InternalServerError``.\n\n        Always sends the :data:`got_request_exception` signal.\n\n        If :attr:`propagate_exceptions` is ``True``, such as in debug\n        mode, the error will be re-raised so that the debugger can\n        display it. Otherwise, the original exception is logged, and\n        an :exc:`~werkzeug.exceptions.InternalServerError` is returned.\n\n        If an error handler is registered for ``InternalServerError`` or\n        ``500``, it will be used. For consistency, the handler will\n        always receive the ``InternalServerError``. The original\n        unhandled exception is available as ``e.original_exception``.\n\n        .. note::\n            Prior to Werkzeug 1.0.0, ``InternalServerError`` will not\n            always have a ``original_exception`` attribute. Use\n            ``getattr(e, \"original_exception\", None)`` to simulate the\n            behavior for compatibility.\n\n        .. versionchanged:: 1.1.0\n            Always passes the ``InternalServerError`` instance to the\n            handler, setting ``original_exception`` to the unhandled\n            error.\n\n        .. versionchanged:: 1.1.0\n            ``after_request`` functions and other finalization is done\n            even for the default 500 response when there is no handler.\n\n        .. versionadded:: 0.3", "file": "src_flask_app.py", "function": "Flask.handle_exception", "project": "flask", "change_stats": {"code_changes": 16, "doc_changes": 37, "total_changes": 53}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef call(\n        self,\n        hidden_states,\n        attention_mask: Optional[tf.Tensor] = None,\n        encoder_hidden_states: Optional[tf.Tensor] = None,\n        encoder_attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        encoder_layer_head_mask: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[tf.Tensor]] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n        \n        \n        \n        residual = hidden_states\n\n        # Self Attention\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        # add present self-attn cache to positions 1,2 of present_key_value tuple\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n        )\n        hidden_states = self.dropout(hidden_states, training=training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Cross-Attention Block\n        cross_attn_present_key_value = None\n        if encoder_hidden_states is not None:\n            residual = hidden_states\n\n            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            hidden_states, _, cross_attn_present_key_value = self.encoder_attn(\n                hidden_states=hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                layer_head_mask=encoder_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n            )\n            hidden_states = self.dropout(hidden_states, training=training)\n            hidden_states = residual + hidden_states\n            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n\n            # add cross-attn to positions 3,4 of present_key_value tuple\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = self.activation_dropout(hidden_states, training=training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = self.dropout(hidden_states, training=training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\n        return (\n            hidden_states,\n            self_attn_weights,\n            present_key_value,\n        )```\n\nUpdated code:\n```python\ndef call(\n        self,\n        hidden_states,\n        attention_mask: Optional[tf.Tensor] = None,\n        encoder_hidden_states: Optional[tf.Tensor] = None,\n        encoder_attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        cross_attn_layer_head_mask: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[tf.Tensor]] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n        \n        \n        \n        residual = hidden_states\n\n        # Self Attention\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        # add present self-attn cache to positions 1,2 of present_key_value tuple\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n        )\n        hidden_states = self.dropout(hidden_states, training=training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Cross-Attention Block\n        cross_attn_present_key_value = None\n        cross_attn_weights = None\n        if encoder_hidden_states is not None:\n            residual = hidden_states\n\n            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n                hidden_states=hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n            )\n            hidden_states = self.dropout(hidden_states, training=training)\n            hidden_states = residual + hidden_states\n            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n\n            # add cross-attn to positions 3,4 of present_key_value tuple\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = self.activation_dropout(hidden_states, training=training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = self.dropout(hidden_states, training=training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\n        return (\n            hidden_states,\n            self_attn_weights,\n            cross_attn_weights,\n            present_key_value,\n        )\n```\n\nOriginal documentation:\n```python\nArgs:\n            hidden_states (:obj:`tf.Tensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n            attention_mask (:obj:`tf.Tensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            encoder_hidden_states (:obj:`tf.Tensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_attention_mask (:obj:`tf.Tensor`): encoder attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (:obj:`tf.Tensor`): mask for attention heads in a given layer of size\n                `(decoder_attention_heads,)`\n            encoder_layer_head_mask (:obj:`tf.Tensor`): mask for encoder attention heads in a given layer of size\n                `(encoder_attention_heads,)`\n            past_key_value (:obj:`Tuple(tf.Tensor)`): cached past key and value projection states\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            hidden_states (:obj:`tf.Tensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n            attention_mask (:obj:`tf.Tensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            encoder_hidden_states (:obj:`tf.Tensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_attention_mask (:obj:`tf.Tensor`): encoder attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (:obj:`tf.Tensor`): mask for attention heads in a given layer of size\n                `(decoder_attention_heads,)`\n            cross_attn_layer_head_mask (:obj:`tf.Tensor`): mask for heads of the cross-attention module.\n                `(decoder_attention_heads,)`\n            past_key_value (:obj:`Tuple(tf.Tensor)`): cached past key and value projection states", "file": "src_transformers_models_marian_modeling_tf_marian.py", "function": "TFMarianDecoderLayer.call", "project": "transformers", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _select_recursive(parent_paths, dir_only, follow_symlinks):\n    \n    if follow_symlinks is None:\n        follow_symlinks = False\n    for parent_path in parent_paths:\n        paths = [parent_path._make_child_relpath('')]\n        while paths:\n            path = paths.pop()\n            yield path\n            try:\n                # We must close the scandir() object before proceeding to\n                # avoid exhausting file descriptors when globbing deep trees.\n                with path._scandir() as scandir_it:\n                    entries = list(scandir_it)\n            except OSError:\n                pass\n            else:\n                for entry in entries:\n                    try:\n                        if entry.is_dir(follow_symlinks=follow_symlinks):\n                            paths.append(path._make_child_entry(entry))\n                            continue\n                    except OSError:\n                        pass\n                    if not dir_only:\n                        yield path._make_child_entry(entry)```\n\nUpdated code:\n```python\ndef _select_recursive(parent_paths, dir_only, follow_symlinks, match):\n    \n    \n    if follow_symlinks is None:\n        follow_symlinks = False\n    for parent_path in parent_paths:\n        if match is not None:\n            # If we're filtering paths through a regex, record the length of\n            # the parent path. We'll pass it to match(path, pos=...) later.\n            parent_len = len(str(parent_path._make_child_relpath('_'))) - 1\n        paths = [parent_path._make_child_relpath('')]\n        while paths:\n            path = paths.pop()\n            if match is None or match(str(path), parent_len):\n                # Yield *directory* path that matches pattern (if any).\n                yield path\n            try:\n                # We must close the scandir() object before proceeding to\n                # avoid exhausting file descriptors when globbing deep trees.\n                with path._scandir() as scandir_it:\n                    entries = list(scandir_it)\n            except OSError:\n                pass\n            else:\n                for entry in entries:\n                    # Handle directory entry.\n                    try:\n                        if entry.is_dir(follow_symlinks=follow_symlinks):\n                            # Recurse into this directory.\n                            paths.append(path._make_child_direntry(entry))\n                            continue\n                    except OSError:\n                        pass\n\n                    # Handle file entry.\n                    if not dir_only:\n                        # Avoid cost of making a path object for non-matching\n                        # files by matching against the os.DirEntry object.\n                        if match is None or match(path._direntry_str(entry), parent_len):\n                            # Yield *file* path that matches pattern (if any).\n                            yield path._make_child_direntry(entry)\n```\n\nOriginal documentation:\n```python\nYield given paths and all their subdirectories, recursively.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Yield given paths and all their children, recursively, filtering by\n    string and type.", "file": "Lib_pathlib__abc.py", "function": "_select_recursive", "project": "cpython", "change_stats": {"code_changes": 25, "doc_changes": 5, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_ticklines(self):\n        ''\n        lines = []\n        ticks = self.get_major_ticks()        \n        for tick in ticks:\n            lines.append(tick.tick1line)\n            lines.append(tick.tick2line)\n        return silent_list('Line2D ticklines', lines)```\n\nUpdated code:\n```python\ndef get_ticklines(self, minor=False):\n        ''\n        if minor:\n            return self.get_minorticklines()\n        return self.get_majorticklines()\n```\n\nOriginal documentation:\n```python\nReturn the ticklines lines as a list of Line2D instance\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the tick lines as a list of Line2D instances", "file": "lib_matplotlib_axis.py", "function": "Axis.get_ticklines", "project": "matplotlib", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef enumerate_support(self):\n        \n        \n        \n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef enumerate_support(self, expand=True):\n        \n        \n        \n        raise NotImplementedError\n```\n\nOriginal documentation:\n```python\nReturns tensor containing all values supported by a discrete\n        distribution. The result will enumerate over dimension 0, so the shape\n        of the result will be `(cardinality,) + batch_shape + event_shape`\n        (where `event_shape = ()` for univariate distributions).\n\n        Note that this enumerates over all batched tensors in lock-step\n        `[[0, 0], [1, 1], ...]`. To iterate over the full Cartesian product\n        use `itertools.product(m.enumerate_support())`.\n\n        Returns:\n            Tensor iterating over dimension 0.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns tensor containing all values supported by a discrete\n        distribution. The result will enumerate over dimension 0, so the shape\n        of the result will be `(cardinality,) + batch_shape + event_shape`\n        (where `event_shape = ()` for univariate distributions).\n\n        Note that this enumerates over all batched tensors in lock-step\n        `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n        along dim 0, but with the remaining batch dimensions being\n        singleton dimensions, `[[0], [1], ..`.\n\n        To iterate over the full Cartesian product use\n        `itertools.product(m.enumerate_support())`.\n\n        Args:\n            expand (bool): whether to expand the support over the\n                batch dims to match the distribution's `batch_shape`.\n\n        Returns:\n            Tensor iterating over dimension 0.", "file": "torch_distributions_distribution.py", "function": "Distribution.enumerate_support", "project": "pytorch", "change_stats": {"code_changes": 4, "doc_changes": 14, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_signed_elements(self, signed_elements):\n        \n        \n        \n        if len(signed_elements) > 2:\n            return False\n\n        response_tag = '{%s}Response' % OneLogin_Saml2_Constants.NS_SAMLP\n        assertion_tag = '{%s}Assertion' % OneLogin_Saml2_Constants.NS_SAML\n\n        if (response_tag in signed_elements and signed_elements.count(response_tag) > 1) or \\\n           (assertion_tag in signed_elements and signed_elements.count(assertion_tag) > 1) or \\\n           (response_tag not in signed_elements and assertion_tag not in signed_elements):\n            return False\n\n        # Check that the signed elements found here, are the ones that will be verified\n        # by OneLogin_Saml2_Utils.validate_sign\n        if response_tag in signed_elements:\n            expected_signature_nodes = OneLogin_Saml2_XML.query(self.document, OneLogin_Saml2_Utils.RESPONSE_SIGNATURE_XPATH)\n            if len(expected_signature_nodes) != 1:\n                raise Exception('Unexpected number of Response signatures found. SAML Response rejected.')\n\n        if assertion_tag in signed_elements:\n            expected_signature_nodes = self.__query(OneLogin_Saml2_Utils.ASSERTION_SIGNATURE_XPATH)\n            if len(expected_signature_nodes) != 1:\n                raise Exception('Unexpected number of Assertion signatures found. SAML Response rejected.')\n\n        return True```\n\nUpdated code:\n```python\ndef validate_signed_elements(self, signed_elements):\n        \n        \n        \n        if len(signed_elements) > 2:\n            return False\n\n        response_tag = '{%s}Response' % OneLogin_Saml2_Constants.NS_SAMLP\n        assertion_tag = '{%s}Assertion' % OneLogin_Saml2_Constants.NS_SAML\n\n        if (response_tag in signed_elements and signed_elements.count(response_tag) > 1) or \\\n           (assertion_tag in signed_elements and signed_elements.count(assertion_tag) > 1) or \\\n           (response_tag not in signed_elements and assertion_tag not in signed_elements):\n            return False\n\n        # Check that the signed elements found here, are the ones that will be verified\n        # by OneLogin_Saml2_Utils.validate_sign\n        if response_tag in signed_elements:\n            expected_signature_nodes = OneLogin_Saml2_XML.query(self.document, OneLogin_Saml2_Utils.RESPONSE_SIGNATURE_XPATH)\n            if len(expected_signature_nodes) != 1:\n                raise OneLogin_Saml2_ValidationError(\n                    'Unexpected number of Response signatures found. SAML Response rejected.',\n                    OneLogin_Saml2_ValidationError.WRONG_NUMBER_OF_SIGNATURES_IN_RESPONSE\n                )\n\n        if assertion_tag in signed_elements:\n            expected_signature_nodes = self.__query(OneLogin_Saml2_Utils.ASSERTION_SIGNATURE_XPATH)\n            if len(expected_signature_nodes) != 1:\n                raise OneLogin_Saml2_ValidationError(\n                    'Unexpected number of Assertion signatures found. SAML Response rejected.',\n                    OneLogin_Saml2_ValidationError.WRONG_NUMBER_OF_SIGNATURES_IN_ASSERTION\n                )\n\n        return True\n```\n\nOriginal documentation:\n```python\nVerifies that the document has the expected signed nodes.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Verifies that the document has the expected signed nodes.\n\n        :param signed_elements: The signed elements to be checked\n        :type signed_elements: list\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean", "file": "src_onelogin_saml2_response.py", "function": "OneLogin_Saml2_Response.validate_signed_elements", "project": "python3-saml", "change_stats": {"code_changes": 12, "doc_changes": 7, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"warn\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):\n    \n    \n    est = KMeans(\n        n_clusters=n_clusters,\n        init=init,\n        n_init=n_init,\n        max_iter=max_iter,\n        verbose=verbose,\n        tol=tol,\n        random_state=random_state,\n        copy_x=copy_x,\n        algorithm=algorithm,\n    ).fit(X, sample_weight=sample_weight)\n    if return_n_iter:\n        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n    else:\n        return est.cluster_centers_, est.labels_, est.inertia_```\n\nUpdated code:\n```python\ndef k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"auto\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):\n    \n    \n    est = KMeans(\n        n_clusters=n_clusters,\n        init=init,\n        n_init=n_init,\n        max_iter=max_iter,\n        verbose=verbose,\n        tol=tol,\n        random_state=random_state,\n        copy_x=copy_x,\n        algorithm=algorithm,\n    ).fit(X, sample_weight=sample_weight)\n    if return_n_iter:\n        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n    else:\n        return est.cluster_centers_, est.labels_, est.inertia_\n```\n\nOriginal documentation:\n```python\nPerform K-means clustering algorithm.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. It must be noted that the data\n        will be converted to C ordering, which will cause a memory copy\n        if the given data is not C-contiguous.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is not used during\n        initialization if `init` is a callable or a user provided array.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        - `'k-means++'` : selects initial cluster centers for k-mean\n          clustering in a smart way to speed up convergence. See section\n          Notes in k_init for more details.\n        - `'random'`: choose `n_clusters` observations (rows) at random from data\n          for the initial centroids.\n        - If an array is passed, it should be of shape `(n_clusters, n_features)`\n          and gives the initial centers.\n        - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n          random state and return an initialization.\n\n    n_init : 'auto' or int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` will change from 10 to `'auto'` in version 1.4.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If `copy_x` is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        `copy_x` is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if `copy_x` is False.\n\n    algorithm : {\"lloyd\", \"elkan\", \"auto\", \"full\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        `\"auto\"` and `\"full\"` are deprecated and they will be removed in\n        Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        The `label[i]` is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform K-means clustering algorithm.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. It must be noted that the data\n        will be converted to C ordering, which will cause a memory copy\n        if the given data is not C-contiguous.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is not used during\n        initialization if `init` is a callable or a user provided array.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        - `'k-means++'` : selects initial cluster centers for k-mean\n          clustering in a smart way to speed up convergence. See section\n          Notes in k_init for more details.\n        - `'random'`: choose `n_clusters` observations (rows) at random from data\n          for the initial centroids.\n        - If an array is passed, it should be of shape `(n_clusters, n_features)`\n          and gives the initial centers.\n        - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n          random state and return an initialization.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If `copy_x` is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        `copy_x` is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if `copy_x` is False.\n\n    algorithm : {\"lloyd\", \"elkan\", \"auto\", \"full\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        `\"auto\"` and `\"full\"` are deprecated and they will be removed in\n        Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        The `label[i]` is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.", "file": "sklearn_cluster__kmeans.py", "function": "k_means", "project": "scikit-learn", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef execute_flow(\n    flow_file: Path,\n    working_dir: Path,\n    output_dir: Path,\n    connections: dict,\n    inputs: Mapping[str, Any],\n    *,\n    run_id: str = None,\n    run_aggregation: bool = True,\n    enable_stream_output: bool = False,\n    allow_generator_output: bool = False,  # TODO: remove this\n    **kwargs,\n) -> LineResult:\n    \n    \n    flow_executor = FlowExecutor.create(flow_file, connections, working_dir, raise_ex=False, **kwargs)\n    flow_executor.enable_streaming_for_llm_flow(lambda: enable_stream_output)\n    with _change_working_dir(working_dir), _force_flush_tracer_provider():\n        # Execute nodes in the flow except the aggregation nodes\n        # TODO: remove index=0 after UX no longer requires a run id similar to batch runs\n        # (run_id_index, eg. xxx_0) for displaying the interface\n        line_result = flow_executor.exec_line(\n            inputs, index=0, allow_generator_output=allow_generator_output, run_id=run_id\n        )\n        # persist the output to the output directory\n        line_result.output = flow_executor._multimedia_processor.persist_multimedia_data(\n            line_result.output, base_dir=working_dir, sub_dir=output_dir\n        )\n        if run_aggregation and line_result.aggregation_inputs:\n            # Convert inputs of aggregation to list type\n            flow_inputs = {k: [v] for k, v in inputs.items()}\n            aggregation_inputs = {k: [v] for k, v in line_result.aggregation_inputs.items()}\n            aggregation_results = flow_executor.exec_aggregation(\n                flow_inputs, aggregation_inputs=aggregation_inputs, run_id=run_id\n            )\n            line_result.node_run_infos = {**line_result.node_run_infos, **aggregation_results.node_run_infos}\n            line_result.run_info.metrics = aggregation_results.metrics\n            # The aggregation inputs of line results is not utilized in the flow test. So we set it into None.\n            line_result.aggregation_inputs = None\n        if isinstance(line_result.output, dict):\n            # remove line_number from output\n            line_result.output.pop(LINE_NUMBER_KEY, None)\n        return line_result```\n\nUpdated code:\n```python\ndef execute_flow(\n    flow_file: Path,\n    working_dir: Path,\n    output_dir: Path,\n    connections: dict,\n    inputs: Mapping[str, Any],\n    *,\n    run_id: str = None,\n    run_aggregation: bool = True,\n    enable_stream_output: bool = False,\n    allow_generator_output: bool = False,  # TODO: remove this\n    init_kwargs: Optional[dict] = None,\n    **kwargs,\n) -> LineResult:\n    \n    \n    flow_executor = FlowExecutor.create(\n        flow_file, connections, working_dir, raise_ex=False, init_kwargs=init_kwargs, **kwargs\n    )\n    flow_executor.enable_streaming_for_llm_flow(lambda: enable_stream_output)\n    with _change_working_dir(working_dir), _force_flush_tracer_provider():\n        # Execute nodes in the flow except the aggregation nodes\n        # TODO: remove index=0 after UX no longer requires a run id similar to batch runs\n        # (run_id_index, eg. xxx_0) for displaying the interface\n        line_result = flow_executor.exec_line(\n            inputs, index=0, allow_generator_output=allow_generator_output, run_id=run_id\n        )\n        # persist the output to the output directory\n        line_result.output = flow_executor._multimedia_processor.persist_multimedia_data(\n            line_result.output, base_dir=working_dir, sub_dir=output_dir\n        )\n        if run_aggregation and line_result.aggregation_inputs:\n            # Convert inputs of aggregation to list type\n            flow_inputs = {k: [v] for k, v in inputs.items()}\n            aggregation_inputs = {k: [v] for k, v in line_result.aggregation_inputs.items()}\n            aggregation_results = flow_executor.exec_aggregation(\n                flow_inputs, aggregation_inputs=aggregation_inputs, run_id=run_id\n            )\n            line_result.node_run_infos = {**line_result.node_run_infos, **aggregation_results.node_run_infos}\n            line_result.run_info.metrics = aggregation_results.metrics\n            # The aggregation inputs of line results is not utilized in the flow test. So we set it into None.\n            line_result.aggregation_inputs = None\n        if isinstance(line_result.output, dict):\n            # remove line_number from output\n            line_result.output.pop(LINE_NUMBER_KEY, None)\n        return line_result\n```\n\nOriginal documentation:\n```python\nExecute the flow, including aggregation nodes.\n\n    :param flow_file: The path to the flow file.\n    :type flow_file: Path\n    :param working_dir: The working directory of the flow.\n    :type working_dir: Path\n    :param output_dir: Relative path relative to working_dir.\n    :type output_dir: Path\n    :param connections: A dictionary containing connection information.\n    :type connections: dict\n    :param inputs: A dictionary containing the input values for the flow.\n    :type inputs: Mapping[str, Any]\n    :param enable_stream_output: Whether to allow stream (generator) output for flow output. Default is False.\n    :type enable_stream_output: Optional[bool]\n    :param run_id: Run id will be set in operation context and used for session.\n    :type run_id: Optional[str]\n    :param kwargs: Other keyword arguments to create flow executor.\n    :type kwargs: Any\n    :return: The line result of executing the flow.\n    :rtype: ~promptflow.executor._result.LineResult\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Execute the flow, including aggregation nodes.\n\n    :param flow_file: The path to the flow file.\n    :type flow_file: Path\n    :param working_dir: The working directory of the flow.\n    :type working_dir: Path\n    :param output_dir: Relative path relative to working_dir.\n    :type output_dir: Path\n    :param connections: A dictionary containing connection information.\n    :type connections: dict\n    :param inputs: A dictionary containing the input values for the flow.\n    :type inputs: Mapping[str, Any]\n    :param enable_stream_output: Whether to allow stream (generator) output for flow output. Default is False.\n    :type enable_stream_output: Optional[bool]\n    :param run_id: Run id will be set in operation context and used for session.\n    :type run_id: Optional[str]\n    :param init_kwargs: Initialization parameters for flex flow, only supported when flow is callable class.\n    :type init_kwargs: dict\n    :param kwargs: Other keyword arguments to create flow executor.\n    :type kwargs: Any\n    :return: The line result of executing the flow.\n    :rtype: ~promptflow.executor._result.LineResult", "file": "src_promptflow-core_promptflow_executor_flow_executor.py", "function": "execute_flow", "project": "promptflow", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef properties_to_Q(properties: List[Property], team_id: int, is_person_query: bool = False) -> Q:\n    \n    \n    \n    filters = Q()\n\n    if len(properties) == 0:\n        return filters\n\n    if is_person_query:\n        for property in properties:\n            filters &= property.property_to_Q()\n        return filters\n\n    person_properties = [prop for prop in properties if prop.type == \"person\"]\n    if len(person_properties) > 0:\n        person_Q = Q()\n        for property in person_properties:\n            person_Q &= property.property_to_Q()\n        filters &= Q(Exists(Person.objects.filter(person_Q, id=OuterRef(\"person_id\"),).only(\"pk\")))\n\n    for property in [prop for prop in properties if prop.type == \"event\"]:\n        filters &= property.property_to_Q()\n\n    # importing from .event and .cohort below to avoid importing from partially initialized modules\n\n    element_properties = [prop for prop in properties if prop.type == \"element\"]\n    if len(element_properties) > 0:\n        from posthog.models.event import Event\n\n        filters &= Q(\n            Exists(\n                Event.objects.filter(pk=OuterRef(\"id\"))\n                .filter(\n                    **Event.objects.filter_by_element(\n                        {item.key: item.value for item in element_properties}, team_id=team_id,\n                    )\n                )\n                .only(\"id\")\n            )\n        )\n\n    cohort_properties = [prop for prop in properties if prop.type == \"cohort\"]\n    if len(cohort_properties) > 0:\n        from posthog.models.cohort import CohortPeople\n\n        for item in cohort_properties:\n            if item.key == \"id\":\n                cohort_id = int(cast(Union[str, int], item.value))\n                filters &= Q(\n                    Exists(\n                        CohortPeople.objects.filter(cohort_id=cohort_id, person_id=OuterRef(\"person_id\"),).only(\"id\")\n                    )\n                )\n\n    return filters```\n\nUpdated code:\n```python\ndef properties_to_Q(properties: List[Property], team_id: int, is_direct_query: bool = False) -> Q:\n    \n    \n    \n    filters = Q()\n\n    if len(properties) == 0:\n        return filters\n\n    if is_direct_query:\n        for property in properties:\n            filters &= property.property_to_Q()\n        return filters\n\n    person_properties = [prop for prop in properties if prop.type == \"person\"]\n    if len(person_properties) > 0:\n        person_Q = Q()\n        for property in person_properties:\n            person_Q &= property.property_to_Q()\n        filters &= Q(Exists(Person.objects.filter(person_Q, id=OuterRef(\"person_id\"),).only(\"pk\")))\n\n    for property in [prop for prop in properties if prop.type == \"event\"]:\n        filters &= property.property_to_Q()\n\n    # importing from .event and .cohort below to avoid importing from partially initialized modules\n\n    element_properties = [prop for prop in properties if prop.type == \"element\"]\n    if len(element_properties) > 0:\n        from posthog.models.event import Event\n\n        filters &= Q(\n            Exists(\n                Event.objects.filter(pk=OuterRef(\"id\"))\n                .filter(\n                    **Event.objects.filter_by_element(\n                        {item.key: item.value for item in element_properties}, team_id=team_id,\n                    )\n                )\n                .only(\"id\")\n            )\n        )\n\n    cohort_properties = [prop for prop in properties if prop.type == \"cohort\"]\n    if len(cohort_properties) > 0:\n        from posthog.models.cohort import CohortPeople\n\n        for item in cohort_properties:\n            if item.key == \"id\":\n                cohort_id = int(cast(Union[str, int], item.value))\n                filters &= Q(\n                    Exists(\n                        CohortPeople.objects.filter(cohort_id=cohort_id, person_id=OuterRef(\"person_id\"),).only(\"id\")\n                    )\n                )\n\n    if len([prop for prop in properties if prop.type == \"group\"]):\n        raise ValueError(\"Group properties are not supported for indirect filtering via postgres\")\n\n    return filters\n```\n\nOriginal documentation:\n```python\nConverts a filter to Q, for use in Django ORM .filter()\n    If you're filtering a Person QuerySet, use is_person_query to avoid doing an unnecessary nested loop\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Converts a filter to Q, for use in Django ORM .filter()\n    If you're filtering a Person/Group QuerySet, use is_direct_query to avoid doing an unnecessary nested loop", "file": "posthog_queries_base.py", "function": "properties_to_Q", "project": "posthog", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _parse_section_content(self, tag):\n        \n        contents = []\n        next_tag = tag\n        while next_tag and not self._is_section(next_tag):\n            if self._is_code_section(next_tag):\n                content = self._parse_code_section(next_tag)\n            else:\n                content = self._parse_content(next_tag.text())\n\n            if content:\n                contents.append(content)\n            next_tag = next_tag.next\n        return ' '.join(contents)```\n\nUpdated code:\n```python\ndef _parse_section_content(self, tag, *, depth=0):\n        \n        \n        \n        contents = []\n        section_found = False\n\n        next_tag = tag\n        while next_tag:\n            if section_found or self._is_section(next_tag):\n                section_found = True\n                break\n\n            if self._is_code_section(next_tag):\n                content = self._parse_code_section(next_tag)\n            elif depth <= 0 or not next_tag.child:\n                content = self._parse_content(next_tag.text())\n            else:\n                content, section_found = self._parse_section_content(\n                    tag=next_tag.child,\n                    depth=depth - 1\n                )\n\n            if content:\n                contents.append(content)\n            next_tag = next_tag.next\n\n        return ' '.join(contents), section_found\n```\n\nOriginal documentation:\n```python\nGets the content from tag till before a new section.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets the content from tag till before a new section.\n\n        if depth > 0, recursively check for sections in all tag's children.\n\n        Returns a tuple with: the parsed content,\n        and a boolean indicating if a section was found.", "file": "readthedocs_search_parsers.py", "function": "BaseParser._parse_section_content", "project": "readthedocs.org", "change_stats": {"code_changes": 24, "doc_changes": 7, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, camera):\n        \n        \n        \n        self.camera = camera\n        self.file_name_to_pixel_array_map = {}\n        self.pixel_array = np.array(camera.pixel_array)\n        self.reset_pixel_array()```\n\nUpdated code:\n```python\ndef __init__(self, camera: Camera):\n        \n        \n        \n        self.camera = camera\n        self.file_name_to_pixel_array_map = {}\n        self.pixel_array = np.array(camera.pixel_array)\n        self.reset_pixel_array()\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        camera : Camera\n            Camera object to use.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        camera\n            Camera object to use.", "file": "manim_camera_camera.py", "function": "BackgroundColoredVMobjectDisplayer.__init__", "project": "manim", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef iglob(pathname):\n    \n\n    \n    dirname, basename = os.path.split(pathname)\n    if not has_magic(pathname):\n        if basename:\n            if os.path.lexists(pathname):\n                yield pathname\n        else:\n            # Patterns ending with a slash should match only directories\n            if os.path.isdir(dirname):\n                yield pathname\n        return\n    if not dirname:\n        yield from glob1(None, basename)\n        return\n    # `os.path.split()` returns the argument itself as a dirname if it is a\n    # drive or UNC path.  Prevent an infinite recursion if a drive or UNC path\n    # contains magic characters (i.e. r'\\\\?\\C:').\n    if dirname != pathname and has_magic(dirname):\n        dirs = iglob(dirname)\n    else:\n        dirs = [dirname]\n    if has_magic(basename):\n        glob_in_dir = glob1\n    else:\n        glob_in_dir = glob0\n    for dirname in dirs:\n        for name in glob_in_dir(dirname, basename):\n            yield os.path.join(dirname, name)```\n\nUpdated code:\n```python\ndef iglob(pathname, *, recursive=False):\n    \n    \n    dirname, basename = os.path.split(pathname)\n    if not has_magic(pathname):\n        if basename:\n            if os.path.lexists(pathname):\n                yield pathname\n        else:\n            # Patterns ending with a slash should match only directories\n            if os.path.isdir(dirname):\n                yield pathname\n        return\n    if not dirname:\n        if recursive and _isrecursive(basename):\n            yield from glob2(dirname, basename)\n        else:\n            yield from glob1(dirname, basename)\n        return\n    # `os.path.split()` returns the argument itself as a dirname if it is a\n    # drive or UNC path.  Prevent an infinite recursion if a drive or UNC path\n    # contains magic characters (i.e. r'\\\\?\\C:').\n    if dirname != pathname and has_magic(dirname):\n        dirs = iglob(dirname, recursive=recursive)\n    else:\n        dirs = [dirname]\n    if has_magic(basename):\n        if recursive and _isrecursive(basename):\n            glob_in_dir = glob2\n        else:\n            glob_in_dir = glob1\n    else:\n        glob_in_dir = glob0\n    for dirname in dirs:\n        for name in glob_in_dir(dirname, basename):\n            yield os.path.join(dirname, name)\n```\n\nOriginal documentation:\n```python\nReturn an iterator which yields the paths matching a pathname pattern.\n\n    The pattern may contain simple shell-style wildcards a la\n    fnmatch. However, unlike fnmatch, filenames starting with a\n    dot are special cases that are not matched by '*' and '?'\n    patterns.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return an iterator which yields the paths matching a pathname pattern.\n\n    The pattern may contain simple shell-style wildcards a la\n    fnmatch. However, unlike fnmatch, filenames starting with a\n    dot are special cases that are not matched by '*' and '?'\n    patterns.\n\n    If recursive is true, the pattern '**' will match any files and\n    zero or more directories and subdirectories.", "file": "Lib_glob.py", "function": "iglob", "project": "cpython", "change_stats": {"code_changes": 17, "doc_changes": 5, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self,\n                styles,\n                conditions,\n                input_is_latent=False,\n                noise=None,\n                randomize_noise=True,\n                truncation=1,\n                truncation_latent=None,\n                inject_index=None,\n                return_latents=False):\n        \n        \n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n            styles = style_truncation\n        # get style latent with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n                                                        noise[2::2], self.to_rgbs):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None```\n\nUpdated code:\n```python\ndef forward(self,\n                styles,\n                conditions,\n                input_is_latent=False,\n                noise=None,\n                randomize_noise=True,\n                truncation=1,\n                truncation_latent=None,\n                inject_index=None,\n                return_latents=False):\n        \n        \n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n                                                        noise[2::2], self.to_rgbs):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:  # only apply SFT to half of the channels\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:  # apply SFT to all the channels\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n```\n\nOriginal documentation:\n```python\nForward function for StyleGAN2Generator.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            input_is_latent (bool): Whether input is latent style.\n                Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is\n                False. Default: True.\n            truncation (float): TODO. Default: 1.\n            truncation_latent (Tensor | None): TODO. Default: None.\n            inject_index (int | None): The injection index for mixing noise.\n                Default: None.\n            return_latents (bool): Whether to return style latents.\n                Default: False.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward function for StyleGAN2GeneratorCSFT.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            conditions (list[Tensor]): SFT conditions to generators.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.", "file": "gfpgan_archs_gfpganv1_clean_arch.py", "function": "StyleGAN2GeneratorCSFT.forward", "project": "GFPGAN", "change_stats": {"code_changes": 10, "doc_changes": 21, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_multiple_bases(self):\n        \n        A = ModelState(\"app\", \"A\", [(\"a_id\", models.AutoField(primary_key=True))])\n        B = ModelState(\"app\", \"B\", [(\"b_id\", models.AutoField(primary_key=True))])\n        C = ModelState(\"app\", \"C\", [], bases=(\"app.A\", \"app.B\"))\n        D = ModelState(\"app\", \"D\", [], bases=(\"app.A\", \"app.B\"))\n        E = ModelState(\"app\", \"E\", [], bases=(\"app.A\", \"app.B\"))\n        changes = self.get_changes([], [A, B, C, D, E])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"app\",\n            0,\n            [\"CreateModel\", \"CreateModel\", \"CreateModel\", \"CreateModel\", \"CreateModel\"],\n        )\n        self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"A\")\n        self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"B\")\n        self.assertOperationAttributes(changes, \"app\", 0, 2, name=\"C\")\n        self.assertOperationAttributes(changes, \"app\", 0, 3, name=\"D\")\n        self.assertOperationAttributes(changes, \"app\", 0, 4, name=\"E\")```\n\nUpdated code:\n```python\ndef test_multiple_bases(self):\n        \n        \n        \n        A = ModelState(\"app\", \"A\", [(\"a_id\", models.AutoField(primary_key=True))])\n        B = ModelState(\"app\", \"B\", [(\"b_id\", models.AutoField(primary_key=True))])\n        C = ModelState(\"app\", \"C\", [], bases=(\"app.A\", \"app.B\"))\n        D = ModelState(\"app\", \"D\", [], bases=(\"app.A\", \"app.B\"))\n        E = ModelState(\"app\", \"E\", [], bases=(\"app.A\", \"app.B\"))\n        changes = self.get_changes([], [A, B, C, D, E])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"app\",\n            0,\n            [\"CreateModel\", \"CreateModel\", \"CreateModel\", \"CreateModel\", \"CreateModel\"],\n        )\n        self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"A\")\n        self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"B\")\n        self.assertOperationAttributes(changes, \"app\", 0, 2, name=\"C\")\n        self.assertOperationAttributes(changes, \"app\", 0, 3, name=\"D\")\n        self.assertOperationAttributes(changes, \"app\", 0, 4, name=\"E\")\n```\n\nOriginal documentation:\n```python\n#23956 - Inheriting models doesn't move *_ptr fields into AddField operations.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inheriting models doesn't move *_ptr fields into AddField operations.", "file": "tests_migrations_test_autodetector.py", "function": "AutodetectorTests.test_multiple_bases", "project": "django", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_x509_key_descriptors(metadata, cert=None):\n        \n        \n        \n        if cert is None or cert == '':\n            return metadata\n        try:\n            root = OneLogin_Saml2_XML.to_etree(metadata)\n        except Exception as e:\n            raise Exception('Error parsing metadata. ' + str(e))\n\n        assert root.tag == '{%s}EntityDescriptor' % OneLogin_Saml2_Constants.NS_MD\n        try:\n            sp_sso_descriptor = next(root.iterfind('.//md:SPSSODescriptor', namespaces=OneLogin_Saml2_Constants.NSMAP))\n        except StopIteration:\n            raise Exception('Malformed metadata.')\n\n        OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, False)\n        OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, True)\n        return OneLogin_Saml2_XML.to_string(root)```\n\nUpdated code:\n```python\ndef add_x509_key_descriptors(metadata, cert=None, add_encryption=True):\n        \n        \n        \n        if cert is None or cert == '':\n            return metadata\n        try:\n            root = OneLogin_Saml2_XML.to_etree(metadata)\n        except Exception as e:\n            raise Exception('Error parsing metadata. ' + str(e))\n\n        assert root.tag == '{%s}EntityDescriptor' % OneLogin_Saml2_Constants.NS_MD\n        try:\n            sp_sso_descriptor = next(root.iterfind('.//md:SPSSODescriptor', namespaces=OneLogin_Saml2_Constants.NSMAP))\n        except StopIteration:\n            raise Exception('Malformed metadata.')\n\n        if add_encryption:\n            OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, False)\n        OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, True)\n        return OneLogin_Saml2_XML.to_string(root)\n```\n\nOriginal documentation:\n```python\nAdds the x509 descriptors (sign/encryption) to the metadata\n        The same cert will be used for sign/encrypt\n\n        :param metadata: SAML Metadata XML\n        :type metadata: string\n\n        :param cert: x509 cert\n        :type cert: string\n\n        :returns: Metadata with KeyDescriptors\n        :rtype: string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adds the x509 descriptors (sign/encryption) to the metadata\n        The same cert will be used for sign/encrypt\n\n        :param metadata: SAML Metadata XML\n        :type metadata: string\n\n        :param cert: x509 cert\n        :type cert: string\n\n        :param add_encryption: Determines if the KeyDescriptor[use=\"encryption\"] should be added.\n        :type add_encryption: boolean\n\n        :returns: Metadata with KeyDescriptors\n        :rtype: string", "file": "src_onelogin_saml2_metadata.py", "function": "OneLogin_Saml2_Metadata.add_x509_key_descriptors", "project": "python3-saml", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef node_name(self) -> str:\n        \n        return f\"{self.sketch_name} {self.node_id}\"```\n\nUpdated code:\n```python\ndef node_name(self) -> str:\n        \n        \n        return f\"{self.sketch_name} {self.node_id}\"\n```\n\nOriginal documentation:\n```python\nName of the whole node (will be the same for several entities!).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Name of the whole node.\n\n        The name will be the same for several entities.", "file": "homeassistant_components_mysensors_device.py", "function": "MySensorsDevice.node_name", "project": "core", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef action_point_get_cost(zone, pinned):\n        \n        \n        \n        if pinned == 'DANGEROUS':\n            cost = ACTION_POINTS_COST[zone.hazard_level] * 2\n        elif pinned == 'SAFE':\n            cost = ACTION_POINTS_COST[zone.hazard_level]\n        elif pinned == 'OBSCURE':\n            cost = ACTION_POINTS_COST_OBSCURE[zone.hazard_level]\n        elif pinned == 'LOGGER':\n            cost = ACTION_POINTS_COST_LOGGER[zone.hazard_level]\n        elif pinned == 'STRONGHOLD':\n            cost = 200\n        else:\n            logger.warning(f'Unable to get AP cost from zone={zone}, pinned={pinned}, assume it costs 40.')\n            cost = 40\n\n        if zone.is_port:\n            cost = 0\n\n        return cost```\n\nUpdated code:\n```python\ndef action_point_get_cost(zone, pinned):\n        \n        \n        \n        if pinned == 'DANGEROUS':\n            cost = ACTION_POINTS_COST[zone.hazard_level] * 2\n        elif pinned == 'SAFE':\n            cost = ACTION_POINTS_COST[zone.hazard_level]\n        elif pinned == 'OBSCURE':\n            cost = ACTION_POINTS_COST_OBSCURE[zone.hazard_level]\n        elif pinned == 'ABYSSAL':\n            cost = ACTION_POINTS_COST_ABYSSAL[zone.hazard_level]\n        elif pinned == 'STRONGHOLD':\n            cost = 200\n        else:\n            logger.warning(f'Unable to get AP cost from zone={zone}, pinned={pinned}, assume it costs 40.')\n            cost = 40\n\n        if zone.is_port:\n            cost = 0\n\n        return cost\n```\n\nOriginal documentation:\n```python\nArgs:\n            zone (Zone): Zone to enter.\n            pinned (str): Zone type. Available types: DANGEROUS, SAFE, OBSCURE, LOGGER, STRONGHOLD.\n\n        Returns:\n            int: Action points that will cost.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            zone (Zone): Zone to enter.\n            pinned (str): Zone type. Available types: DANGEROUS, SAFE, OBSCURE, ABYSSAL, STRONGHOLD.\n\n        Returns:\n            int: Action points that will cost.", "file": "module_os_handler_action_point.py", "function": "ActionPointHandler.action_point_get_cost", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_monkey_upload_path(self, url_to_monkey):\n        \n        \n        \n        if not url_to_monkey or (\"linux\" not in url_to_monkey and \"windows\" not in url_to_monkey):\n            logger.error(\n                \"Can't get destination path because source path %s is invalid.\", url_to_monkey\n            )\n            return False\n        try:\n            if \"linux\" in url_to_monkey:\n                return self.monkey_target_paths[\"linux\"]\n            elif \"windows-32\" in url_to_monkey:\n                return self.monkey_target_paths[\"win32\"]\n            elif \"windows-64\" in url_to_monkey:\n                return self.monkey_target_paths[\"win64\"]\n            else:\n                logger.error(\n                    \"Could not figure out what type of monkey server was trying to upload, \"\n                    \"thus destination path can not be chosen.\"\n                )\n                return False\n        except KeyError:\n            logger.error(\n                'Unknown key was found. Please use \"linux\", \"win32\" and \"win64\" keys to '\n                \"initialize \"\n                \"custom dict of monkey's destination paths\"\n            )\n            return False```\n\nUpdated code:\n```python\ndef get_monkey_upload_path(self, url_to_monkey):\n        \n        \n        \n        if not url_to_monkey or (\"linux\" not in url_to_monkey and \"windows\" not in url_to_monkey):\n            logger.error(\n                \"Can't get destination path because source path %s is invalid.\", url_to_monkey\n            )\n            return False\n        try:\n            if \"linux\" in url_to_monkey:\n                return self.monkey_target_paths[\"linux\"]\n            elif \"windows\" in url_to_monkey:\n                return self.monkey_target_paths[\"win64\"]\n            else:\n                logger.error(\n                    \"Could not figure out what type of monkey server was trying to upload, \"\n                    \"thus destination path can not be chosen.\"\n                )\n                return False\n        except KeyError:\n            logger.error(\n                'Unknown key was found. Please use \"linux\" and \"win64\" keys to '\n                \"initialize \"\n                \"custom dict of monkey's destination paths\"\n            )\n            return False\n```\n\nOriginal documentation:\n```python\nGets destination path from one of WEB_RCE predetermined paths(self.monkey_target_paths).\n        :param url_to_monkey: Hosted monkey's url. egz :\n        http://localserver:9999/monkey/windows-32.exe\n        :return: Corresponding monkey path from self.monkey_target_paths\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets destination path from one of WEB_RCE predetermined paths(self.monkey_target_paths).\n        :param url_to_monkey: Hosted monkey's url. egz :\n        http://localserver:9999/monkey/windows-64.exe\n        :return: Corresponding monkey path from self.monkey_target_paths", "file": "monkey_infection_monkey_exploit_web_rce.py", "function": "WebRCE.get_monkey_upload_path", "project": "monkey", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_active_params(self, tags=None):\n        \n        \n        \n        if tags is None:  # return all active params\n            return self._active_params\n        elif isinstance(tags, str) and tags not in self._param_tags:\n            # return empty set, since tag doesn't exist; XXX raise warning?\n            return set()\n        elif isinstance(tags, str):  # only return active params in the tag\n            return self._active_params.intersection(self._param_tags[tags])\n        elif isinstance(tags, list) or isinstance(tags, tuple):\n            params_to_return = set()\n            for tag in tags:\n                assert isinstance(tag, str)\n                if tag in self._param_tags:\n                    params_to_return.update(self._param_tags[tag])\n            return params_to_return.intersection(self._active_params)\n        else:\n            raise TypeError```\n\nUpdated code:\n```python\ndef get_active_params(self):\n        \n        \n        \n        return self._active_params\n```\n\nOriginal documentation:\n```python\n:param tag: optional argument specifying that only active params carrying a particular\n            tag or any of several tags should be returned\n        :type tags: string or iterable over strings\n        :returns: all active params in the ParamStore, possibly filtered to a particular tag or tags\n        :rtype: set\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":returns: all active params in the ParamStore\n        :rtype: set", "file": "pyro_params_param_store.py", "function": "ParamStoreDict.get_active_params", "project": "pyro", "change_stats": {"code_changes": 21, "doc_changes": 7, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef has_binding(api):\n    \n    \n    # we can't import an incomplete pyside and pyqt4\n    # this will cause a crash in sip (#1431)\n    # check for complete presence before importing\n    module_name = api_to_module[api]\n\n    import imp\n    try:\n        #importing top level PyQt4/PySide module is ok...\n        mod = import_module(module_name)\n        #...importing submodules is not\n        imp.find_module('QtCore', mod.__path__)\n        imp.find_module('QtGui', mod.__path__)\n        imp.find_module('QtSvg', mod.__path__)\n        if api == QT_API_PYQT5:\n            # QT5 requires QtWidgets too\n            imp.find_module('QtWidgets', mod.__path__)\n\n        #we can also safely check PySide version\n        if api == QT_API_PYSIDE:\n            return check_version(mod.__version__, '1.0.3')\n        else:\n            return True\n    except ImportError:\n        return False```\n\nUpdated code:\n```python\ndef has_binding(api):\n    \n    \n    # we can't import an incomplete pyside and pyqt4\n    # this will cause a crash in sip (#1431)\n    # check for complete presence before importing\n    module_name = api_to_module[api]\n\n    import imp\n    try:\n        #importing top level PyQt4/PySide module is ok...\n        mod = import_module(module_name)\n        #...importing submodules is not\n        imp.find_module('QtCore', mod.__path__)\n        imp.find_module('QtGui', mod.__path__)\n        imp.find_module('QtSvg', mod.__path__)\n        if api in (QT_API_PYQT5, QT_API_PYSIDE2):\n            # QT5 requires QtWidgets too\n            imp.find_module('QtWidgets', mod.__path__)\n\n        #we can also safely check PySide version\n        if api == QT_API_PYSIDE:\n            return check_version(mod.__version__, '1.0.3')\n        else:\n            return True\n    except ImportError:\n        return False\n```\n\nOriginal documentation:\n```python\nSafely check for PyQt4/5 or PySide, without importing submodules\n\n    Supports Python <= 3.3\n\n       Parameters\n       ----------\n       api : str [ 'pyqtv1' | 'pyqt' | 'pyqt5' | 'pyside' | 'pyqtdefault']\n            Which module to check for\n\n       Returns\n       -------\n       True if the relevant module appears to be importable\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Safely check for PyQt4/5, PySide or PySide2, without importing submodules\n\n    Supports Python <= 3.3\n\n       Parameters\n       ----------\n       api : str [ 'pyqtv1' | 'pyqt' | 'pyqt5' | 'pyside' | 'pyside2' | 'pyqtdefault']\n            Which module to check for\n\n       Returns\n       -------\n       True if the relevant module appears to be importable", "file": "IPython_external_qt_loaders.py", "function": "has_binding", "project": "ipython", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _run_param_hooks(self):\n        \n\n        \n        for hook in self.param_hooks:\n            #print('running parameter hook: ', hook)\n            (unique_array_index, param_attribute_location) = hook\n            (params_list_index, attribute_location) = param_attribute_location\n            param = self.params_list[params_list_index]\n            schedule_grad_var = self.grad_var_list[params_list_index]\n            #was_grad_var = self.debug_grad_var_list[params_list_index]\n            if schedule_grad_var is not None:\n                if param.grad_var is None:\n                    if self.verbosity_level >= 2:\n                        print('Somebody removed grad_var.')\n                    if schedule_grad_var.data is not None:\n                        if param.data.dtype != schedule_grad_var.data.dtype:\n                            raise RuntimeError('It is not allowed to change the parameter dtype in a static chain!')\n                        schedule_grad_var.data.fill(0)\n                        param.grad_var = schedule_grad_var\n\n            if attribute_location == 'data':\n                # This is the corresponding parameter array, which might\n                # have had its reference changed to a different array or set\n                # to None.\n                possibly_modified_parameter_array = \\\n                    self.params_list[params_list_index].data\n                if self.unique_arrays[unique_array_index] is not possibly_modified_parameter_array:\n                    if self.verbosity_level >= 2:\n                        print('The parameter data attribute has changed: ', self.params_list[params_list_index])\n                    if self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is not None:\n\n\n                        # Set the schedule's array to refer to the 'data'\n                        # attribute of the parameter.\n                        if self.verbosity_level >= 2:\n                            print('Setting schedule array equal to data attribute reference.')\n                        self.unique_arrays[unique_array_index] = possibly_modified_parameter_array\n                    elif self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is None:\n                        # The data attribute was set to None by outside code.\n                        # Assume that it is safe to replace a None-valued\n                        # array with 0.\n                        self.unique_arrays[unique_array_index].fill(0)\n                        if self.verbosity_level >= 2:\n                            print('Zero-ing and updating parameter data attribute reference.')\n                        self.params_list[params_list_index].data = self.unique_arrays[unique_array_index]\n            elif attribute_location == 'grad':\n                # This is the corresponding parameter array, which might\n                # have had its reference changed to a different array or set\n                # to None.\n                possibly_modified_parameter_array = \\\n                    self.params_list[params_list_index].grad\n                if self.unique_arrays[unique_array_index] is not possibly_modified_parameter_array:\n                    if self.verbosity_level >= 2:\n                        print('The parameter grad attribute has changed: ', self.params_list[params_list_index])\n                    if self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is not None:\n                        # Set the schedule's array to refer to the 'grad'\n                        # attribute of the parameter.\n                        if self.verbosity_level >= 2:\n                            print('Setting schedule array equal to grad attribute reference.')\n                        self.unique_arrays[unique_array_index] = possibly_modified_parameter_array\n                    elif self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is None:\n                        # The grad attribute was set to None by outside code.\n                        # Assume that it is safe to replace a None-valued\n                        # array with 0.\n                        self.unique_arrays[unique_array_index].fill(0)\n                        if self.verbosity_level >= 2:\n                            print('Zero-ing and updating parameter grad attribute reference.')\n                        self.params_list[params_list_index].grad = self.unique_arrays[unique_array_index]```\n\nUpdated code:\n```python\ndef _run_param_hooks(self):\n        \n\n        \n        for hook in self.param_hooks:\n            #print('running parameter hook: ', hook)\n            (unique_array_index, param_attribute_location) = hook\n            (params_list_index, attribute_location) = param_attribute_location\n            param = self.params_list[params_list_index]\n            schedule_grad_var = self.grad_var_list[params_list_index]\n            if schedule_grad_var is not None:\n                if param.grad_var is None:\n                    if self.verbosity_level >= 2:\n                        print('Somebody removed grad_var.')\n                    if schedule_grad_var.data is not None:\n                        if param.data.dtype != schedule_grad_var.data.dtype:\n                            raise RuntimeError('It is not allowed to change the parameter dtype in a static chain!')\n                        param.grad_var = schedule_grad_var\n\n            if attribute_location == 'data':\n                # This is the corresponding parameter array, which might\n                # have had its reference changed to a different array or set\n                # to None.\n                possibly_modified_parameter_array = \\\n                    self.params_list[params_list_index].data\n                if self.unique_arrays[unique_array_index] is not possibly_modified_parameter_array:\n                    if self.verbosity_level >= 2:\n                        print('The parameter data attribute has changed: ', self.params_list[params_list_index])\n                    if self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is not None:\n                        # Set the schedule's array to refer to the 'data'\n                        # attribute of the parameter.\n                        if self.verbosity_level >= 2:\n                            print('Setting schedule array equal to data attribute reference.')\n                        if self.unique_arrays[unique_array_index].dtype != possibly_modified_parameter_array.dtype:\n                            raise RuntimeError('It is not allowed to change the parameter dtype in a static chain!')\n                        self.unique_arrays[unique_array_index] = possibly_modified_parameter_array\n                    elif self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is None:\n                        # The data attribute was set to None by outside code.\n                        if self.verbosity_level >= 2:\n                            print('Zero-ing and updating parameter data attribute reference.')\n                        self.params_list[params_list_index].data = self.unique_arrays[unique_array_index]\n            elif attribute_location == 'grad':\n                # This is the corresponding parameter array, which might\n                # have had its reference changed to a different array or set\n                # to None.\n                possibly_modified_parameter_array = \\\n                    self.params_list[params_list_index].grad\n                if self.unique_arrays[unique_array_index] is not possibly_modified_parameter_array:\n                    if self.verbosity_level >= 2:\n                        print('The parameter grad attribute has changed: ', self.params_list[params_list_index])\n                    if self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is not None:\n                        # Set the schedule's array to refer to the 'grad'\n                        # attribute of the parameter.\n                        if self.verbosity_level >= 2:\n                            print('Setting schedule array equal to grad attribute reference.')\n                        if self.unique_arrays[unique_array_index].dtype != possibly_modified_parameter_array:\n                            raise RuntimeError('It is not allowed to change the parameter dtype in a static chain!')\n                        self.unique_arrays[unique_array_index] = possibly_modified_parameter_array\n                    elif self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is None:\n                        # The grad attribute was set to None by outside code.\n                        if self.verbosity_level >= 2:\n                            print('Zero-ing and updating parameter grad attribute reference.')\n                        self.params_list[params_list_index].grad = self.unique_arrays[unique_array_index]\n```\n\nOriginal documentation:\n```python\nRun parameter reference updater hooks.\n\n        This updates the 'data' and/or 'grad' attributes of the parameters,\n        if necessary. It also handles the case where the 'grad' attribute\n        was set to 'None' by outside Chainer code.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run parameter reference updater hooks.\n\n        This updates fixme.\n        It also handles the case where the 'grad' attribute\n        was set to 'None' by outside Chainer code.", "file": "chainer_graph_optimizations_static_graph.py", "function": "StaticScheduleFunction._run_param_hooks", "project": "chainer", "change_stats": {"code_changes": 16, "doc_changes": 6, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef Event(type, _fields=None, **fields):\n    \n\n    \n    event = dict(_fields or {}, type=type, **fields)\n    if 'timestamp' not in event:\n        event['timestamp'] = time.time()\n    return event```\n\nUpdated code:\n```python\ndef Event(type, _fields=None, __dict__=dict, __now__=monotonic, **fields):\n    \n\n    \n    event = __dict__(_fields, **fields) if _fields else fields\n    if 'timestamp' not in event:\n        event.update(timestamp=__now__(), type=type)\n    else:\n        event['type'] = type\n    return event\n```\n\nOriginal documentation:\n```python\nCreate an event.\n\n    An event is a dictionary, the only required field is ``type``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create an event.\n\n    An event is a dictionary, the only required field is ``type``.\n    A ``timestamp`` field will be set to the current time if not provided.", "file": "celery_events___init__.py", "function": "Event", "project": "celery", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef shift(self, periods, offset=None, time_rule=None, **kwds):\n        \n        \n        \n        if periods == 0:\n            return self.copy()\n\n        time_rule = kwds.get('timeRule', time_rule)\n        if time_rule is not None and offset is None:\n            offset = datetools.getOffset(time_rule)\n\n        if offset is None:\n            new_values = np.empty(len(self), dtype=self.dtype)\n\n            if periods > 0:\n                new_values[periods:] = self.values[:-periods]\n                new_values[:periods] = nan\n            elif periods < 0:\n                new_values[:periods] = self.values[-periods:]\n                new_values[periods:] = nan\n\n            return Series(new_values, index=self.index)\n        else:\n            return Series(self, index=self.index.shift(periods, offset))```\n\nUpdated code:\n```python\ndef shift(self, periods, offset=None, **kwds):\n        \n        \n        \n        if periods == 0:\n            return self.copy()\n\n        offset = kwds.get('timeRule', offset)\n        if isinstance(offset, basestring):\n            offset = datetools.getOffset(offset)\n\n        if offset is None:\n            new_values = np.empty(len(self), dtype=self.dtype)\n\n            if periods > 0:\n                new_values[periods:] = self.values[:-periods]\n                new_values[:periods] = nan\n            elif periods < 0:\n                new_values[:periods] = self.values[-periods:]\n                new_values[periods:] = nan\n\n            return Series(new_values, index=self.index)\n        else:\n            return Series(self, index=self.index.shift(periods, offset))\n```\n\nOriginal documentation:\n```python\nShift the index of the Series by desired number of periods with an\n        optional time offset\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n        offset : DateOffset or timedelta, optional\n            Increment to use from datetools module\n        time_rule : string, optional\n            time rule name to use by name (e.g. 'WEEKDAY')\n\n        Returns\n        -------\n        shifted : Series\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Shift the index of the Series by desired number of periods with an\n        optional time offset\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n        offset : DateOffset, timedelta, or time rule string, optional\n            Increment to use from datetools module or time rule (e.g. 'EOM')\n\n        Returns\n        -------\n        shifted : Series", "file": "pandas_core_series.py", "function": "Series.shift", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 8, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_number_of_depreciations(self):\n\t\t\n\n\t\tasset = create_asset(\n\t\t\titem_code = \"Macbook Pro\",\n\t\t\tcalculate_depreciation = 1,\n\t\t\tavailable_for_use_date = \"2019-12-31\",\n\t\t\ttotal_number_of_depreciations = 3,\n\t\t\texpected_value_after_useful_life = 10000,\n\t\t\tdepreciation_start_date = \"2020-07-01\",\n\t\t\topening_accumulated_depreciation = 10000,\n\t\t\tnumber_of_depreciations_booked = 5,\n\t\t\tdo_not_save = 1\n\t\t)\n\n\t\tself.assertRaises(frappe.ValidationError, asset.save)```\n\nUpdated code:\n```python\ndef test_number_of_depreciations(self):\n\t\t\n\n\t\t# number_of_depreciations_booked > total_number_of_depreciations\n\t\tasset = create_asset(\n\t\t\titem_code = \"Macbook Pro\",\n\t\t\tcalculate_depreciation = 1,\n\t\t\tavailable_for_use_date = \"2019-12-31\",\n\t\t\ttotal_number_of_depreciations = 3,\n\t\t\texpected_value_after_useful_life = 10000,\n\t\t\tdepreciation_start_date = \"2020-07-01\",\n\t\t\topening_accumulated_depreciation = 10000,\n\t\t\tnumber_of_depreciations_booked = 5,\n\t\t\tdo_not_save = 1\n\t\t)\n\n\t\tself.assertRaises(frappe.ValidationError, asset.save)\n\n\t\t# number_of_depreciations_booked = total_number_of_depreciations\n\t\tasset_2 = create_asset(\n\t\t\titem_code = \"Macbook Pro\",\n\t\t\tcalculate_depreciation = 1,\n\t\t\tavailable_for_use_date = \"2019-12-31\",\n\t\t\ttotal_number_of_depreciations = 5,\n\t\t\texpected_value_after_useful_life = 10000,\n\t\t\tdepreciation_start_date = \"2020-07-01\",\n\t\t\topening_accumulated_depreciation = 10000,\n\t\t\tnumber_of_depreciations_booked = 5,\n\t\t\tdo_not_save = 1\n\t\t)\n\n\t\tself.assertRaises(frappe.ValidationError, asset_2.save)\n```\n\nOriginal documentation:\n```python\nTests if an error is raised when number_of_depreciations_booked > total_number_of_depreciations.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tests if an error is raised when number_of_depreciations_booked >= total_number_of_depreciations.", "file": "erpnext_assets_doctype_asset_test_asset.py", "function": "TestDepreciationBasics.test_number_of_depreciations", "project": "erpnext", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_help(btntype):\n          \n        logger.debug(\"Setting help\")\n        hlp = \"\"\n        if btntype == \"reset\":\n            hlp = \"Load/Refresh stats for the currently training session\"\n        elif btntype == \"clear\":\n            hlp = \"Clear currently displayed session stats\"\n        elif btntype == \"save\":\n            hlp = \"Save session stats to csv\"\n        elif btntype == \"load\":\n            hlp = \"Load saved session stats\"\n        return hlp```\n\nUpdated code:\n```python\ndef set_help(btntype):\n          \n        logger.debug(\"Setting help\")\n        hlp = \"\"\n        if btntype == \"reload\":\n            hlp = \"Load/Refresh stats for the currently training session\"\n        elif btntype == \"clear\":\n            hlp = \"Clear currently displayed session stats\"\n        elif btntype == \"save\":\n            hlp = \"Save session stats to csv\"\n        elif btntype == \"load\":\n            hlp = \"Load saved session stats\"\n        return hlp\n```\n\nOriginal documentation:\n```python\nSet the helptext for option buttons\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the help text for option buttons", "file": "lib_gui_display_analysis.py", "function": "Options.set_help", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef inference(self, x, inference_args, *args, **kwargs):\n        \n        \n        pass```\n\nUpdated code:\n```python\ndef inference(self, x, inference_args, *args, **kwargs):\n        \n        \n        # get options\n        threshold = inference_args.threshold\n        minlenratio = inference_args.minlenratio\n        maxlenratio = inference_args.maxlenratio\n\n        # forward encoder\n        xs = x.unsqueeze(0)\n        hs, _ = self.encoder(xs, None)\n\n        # set limits of length\n        maxlen = int(hs.size(1) * maxlenratio)\n        minlen = int(hs.size(1) * minlenratio)\n\n        # initialize\n        idx = 0\n        ys = hs.new_zeros(1, 1, self.odim)\n        outs, probs = [], []\n\n        # forward decoder step-by-step\n        while True:\n            # update index\n            idx += 1\n\n            # calculate output and stop prob at idx-th step\n            y_masks = subsequent_mask(idx).unsqueeze(0)\n            z = self.decoder.recognize(ys, y_masks, hs)  # (B, adim)\n            outs += [self.feat_out(z)]  # [(1, odim, 1), ...]\n            probs += [torch.sigmoid(self.prob_out(z))[0]]\n\n            # update next inputs\n            ys = torch.cat((ys, outs[-1].unsqueeze(0)), dim=1)  # (1, idx + 1, odim)\n\n            # check whether to finish generation\n            if int(sum(probs[-1] >= threshold)) > 0 or idx >= maxlen:\n                # check mininum length\n                if idx < minlen:\n                    continue\n                outs = torch.cat(outs, dim=0).unsqueeze(0).transpose(1, 2)  # (L, odim) -> (1, L, odim) -> (1, odim, L)\n                if self.postnet is not None:\n                    outs = outs + self.postnet(outs)  # (1, odim, L)\n                outs = outs.transpose(2, 1).squeeze(0)  # (L, odim)\n                probs = torch.cat(probs, dim=0)\n                break\n\n        return outs, probs\n```\n\nOriginal documentation:\n```python\nGenerates the sequence of features given the sequences of characters\n\n        :param torch.Tensor x: the sequence of characters (T)\n        :param Namespace inference_args: argments containing following attributes\n            (float) threshold: threshold in inference\n            (float) minlenratio: minimum length ratio in inference\n            (float) maxlenratio: maximum length ratio in inference\n        :rtype: torch.Tensor\n        :return: the sequence of stop probabilities (L)\n        :rtype: torch.Tensor\n        :return: the sequence of attention weight (L, T)\n        :rtype: torch.Tensor\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates the sequence of features given the sequences of characters\n\n        :param torch.Tensor x: the sequence of characters (T)\n        :param Namespace inference_args: argments containing following attributes\n            (float) threshold: threshold in inference\n            (float) minlenratio: minimum length ratio in inference\n            (float) maxlenratio: maximum length ratio in inference\n        :rtype: torch.Tensor\n        :return: the sequence of stop probabilities (L)\n        :rtype: torch.Tensor", "file": "espnet_nets_pytorch_backend_e2e_tts_transformer.py", "function": "Transformer.inference", "project": "espnet", "change_stats": {"code_changes": 48, "doc_changes": 4, "total_changes": 52}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef repeat(self, reps):\n        \n        \n        \n        new_index = self.index.repeat(reps)\n        new_values = self._values.repeat(reps)\n        return self._constructor(new_values,\n                                 index=new_index).__finalize__(self)```\n\nUpdated code:\n```python\ndef repeat(self, reps, *args, **kwargs):\n        \n        \n        \n        nv.validate_repeat(args, kwargs)\n        new_index = self.index.repeat(reps)\n        new_values = self._values.repeat(reps)\n        return self._constructor(new_values,\n                                 index=new_index).__finalize__(self)\n```\n\nOriginal documentation:\n```python\nreturn a new Series with the values repeated reps times\n\n        See also\n        --------\n        numpy.ndarray.repeat\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Repeat elements of an Series. Refer to `numpy.ndarray.repeat`\n        for more information about the `reps` argument.\n\n        See also\n        --------\n        numpy.ndarray.repeat", "file": "pandas_core_series.py", "function": "Series.repeat", "project": "pandas", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __exit__(self, *args):\n        \n        \n        \n\n        if self._log_file_handle:\n            self._log_file_handle.close()\n            self._log_file_handle = None```\n\nUpdated code:\n```python\ndef __exit__(self, *args):\n        \n        \n        \n\n        if self._log_file_handle:\n            self._log_file_handle.close()\n            self._log_file_handle = None\n\n        if self._containers_mode == ContainersMode.WARM:\n            self._clean_running_containers_and_related_resources()\n```\n\nOriginal documentation:\n```python\nCleanup any necessary opened files\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Cleanup any necessary opened resources", "file": "samcli_commands_local_cli_common_invoke_context.py", "function": "InvokeContext.__exit__", "project": "aws-sam-cli", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(self, source):\n        \n        \n\n        \n        if isinstance(source, pd.DataFrame):\n            assert isinstance(source.index, pd.tseries.index.DatetimeIndex)\n            source = DataFrameSource(source, sids=self.sids)\n\n        # create transforms and zipline\n        simulated_trading = self._create_simulator(source)\n\n        # loop through simulated_trading, each iteration returns a\n        # perf ndict\n        perfs = list(simulated_trading)\n\n        # convert perf ndict to pandas dataframe\n        daily_stats = self._create_daily_stats(perfs)\n\n        return daily_stats```\n\nUpdated code:\n```python\ndef run(self, source, start=None, end=None):\n        \n\n        \n        if isinstance(source, (list, tuple)):\n            assert start is not None and end is not None, \\\n            \"When providing a list of sources, start and end date have to be specified.\"\n        elif isinstance(source, pd.DataFrame):\n            assert isinstance(source.index, pd.tseries.index.DatetimeIndex)\n            # if DataFrame provided, wrap in DataFrameSource\n            source = DataFrameSource(source, sids=self.sids)\n\n        # If values not set, try to extract from source.\n        if start is None:\n            start = source.start\n        if end is None:\n            end = source.end\n\n        if not isinstance(source, (list, tuple)):\n            self.sources = [source]\n        else:\n            self.sources = source\n\n        # create transforms and zipline\n        self.simulated_trading = self._create_simulator(start=start, end=end)\n\n        # loop through simulated_trading, each iteration returns a\n        # perf ndict\n        perfs = list(self.simulated_trading)\n\n        # convert perf ndict to pandas dataframe\n        daily_stats = self._create_daily_stats(perfs)\n\n        return daily_stats\n```\n\nOriginal documentation:\n```python\nRun the algorithm.\n\n        :Arguments:\n            data : zipline source or pandas.DataFrame\n               pandas.DataFrame must have the following structure:\n               * column names must consist of ints representing the different sids\n               * index must be TimeStamps\n               * array contents should be price\n\n        :Returns:\n            daily_stats : pandas.DataFrame\n              Daily performance metrics such as returns, alpha etc.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run the algorithm.\n\n        :Arguments:\n            source : can be either:\n                     - pandas.DataFrame\n                     - zipline source\n                     - list of zipline sources\n\n               If pandas.DataFrame is provided, it must have the\n               following structure:\n               * column names must consist of ints representing the\n                 different sids\n               * index must be DatetimeIndex\n               * array contents should be price info.\n\n        :Returns:\n            daily_stats : pandas.DataFrame\n              Daily performance metrics such as returns, alpha etc.", "file": "zipline_algorithm.py", "function": "TradingAlgorithm.run", "project": "zipline", "change_stats": {"code_changes": 26, "doc_changes": 18, "total_changes": 44}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_version(cls, settings, path):\n        \n        \n        abs_path = cls.get_absolute_path(settings, path)\n        with cls._lock:\n            hashes = cls._static_hashes\n            if abs_path not in hashes:\n                try:\n                    data = cls.get_content(abs_path)\n                    hashes[abs_path] = hashlib.md5(data).hexdigest()\n                except Exception:\n                    gen_log.error(\"Could not open static file %r\", path)\n                    hashes[abs_path] = None\n            hsh = hashes.get(abs_path)\n            if hsh:\n                return hsh\n        return None```\n\nUpdated code:\n```python\ndef get_version(cls, settings, path):\n        \n        \n        abs_path = cls.get_absolute_path(settings, path)\n        return cls._get_cached_version(abs_path)\n```\n\nOriginal documentation:\n```python\nGenerate the version string to be used in static URLs.\n\n        This method may be overridden in subclasses (but note that it\n        is a class method rather than a static method).  The default\n        implementation uses a hash of the file's contents.\n\n        ``settings`` is the `Application.settings` dictionary and ``path``\n        is the relative location of the requested asset on the filesystem.\n        The returned value should be a string, or ``None`` if no version\n        could be determined.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate the version string to be used in static URLs.\n\n        ``settings`` is the `Application.settings` dictionary and ``path``\n        is the relative location of the requested asset on the filesystem.\n        The returned value should be a string, or ``None`` if no version\n        could be determined.\n\n        This method was previously recommended for subclasses to override;\n        `get_content_version` is now preferred as it allows the base\n        class to handle caching of the result.", "file": "tornado_web.py", "function": "StaticFileHandler.get_version", "project": "tornado", "change_stats": {"code_changes": 16, "doc_changes": 10, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef failure(self, exc):\n        \n        \n        \n        if isinstance(exc, str):\n            exc = CatchResponseError(exc)\n        self._manual_result = exc```\n\nUpdated code:\n```python\ndef failure(self, exc):\n        \n        \n        \n        if not isinstance(exc, Exception):\n            exc = CatchResponseError(exc)\n        self._manual_result = exc\n```\n\nOriginal documentation:\n```python\nReport the response as a failure.\n        \n        exc can be either a python exception, or a string in which case it will\n        be wrapped inside a CatchResponseError. \n        \n        Example::\n        \n            with self.client.get(\"/\", catch_response=True) as response:\n                if response.content == \"\":\n                    response.failure(\"No data\")\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Report the response as a failure.\n        \n        if exc is anything other than a python exception (like a string) it will\n        be wrapped inside a CatchResponseError. \n        \n        Example::\n        \n            with self.client.get(\"/\", catch_response=True) as response:\n                if response.content == \"\":\n                    response.failure(\"No data\")", "file": "locust_contrib_fasthttp.py", "function": "ResponseContextManager.failure", "project": "locust", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef app_install(self, url, installing_callback=None):\n        \n        \n        \n        r = self._reqsess.post(self.path2url('/install'), data={'url': url})\n        if r.status_code != 200:\n            raise RuntimeError(\"app install error:\", r.text)\n        id = r.text.strip()\n        print(time.strftime('%H:%M:%S'), \"id:\", id)\n        return self._wait_install_finished(id, installing_callback)```\n\nUpdated code:\n```python\ndef app_install(self, url, installing_callback=None, server=None):\n        \n        \n        \n        r = self._reqsess.post(self.path2url('/install'), data={'url': url})\n        if r.status_code != 200:\n            raise RuntimeError(\"app install error:\", r.text)\n        id = r.text.strip()\n        print(time.strftime('%H:%M:%S'), \"id:\", id)\n        return self._wait_install_finished(id, installing_callback)\n```\n\nOriginal documentation:\n```python\n{u'message': u'downloading', \"progress\": {u'titalSize': 407992690, u'copiedSize': 49152}}\n\n        Returns:\n            packageName\n\n        Raises:\n            RuntimeError\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "{u'message': u'downloading', \"progress\": {u'totalSize': 407992690, u'copiedSize': 49152}}\n\n        Returns:\n            packageName\n\n        Raises:\n            RuntimeError", "file": "uiautomator2___init__.py", "function": "UIAutomatorServer.app_install", "project": "uiautomator2", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fit(self, X, Y):\n        \n        \n        super().fit(X, Y)\n        # expose the fitted attributes `x_scores_` and `y_scores_`\n        self.x_scores_ = self._x_scores\n        self.y_scores_ = self._y_scores\n        return self```\n\nUpdated code:\n```python\ndef fit(self, X, y=None, Y=None):\n        \n        \n        y = _deprecate_Y_when_required(y, Y)\n\n        super().fit(X, y)\n        # expose the fitted attributes `x_scores_` and `y_scores_`\n        self.x_scores_ = self._x_scores\n        self.y_scores_ = self._y_scores\n        return self\n```\n\nOriginal documentation:\n```python\nFit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of predictors.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n        Returns\n        -------\n        self : object\n            Fitted model.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of predictors.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target vectors, where `n_samples` is the number of samples and\n            `n_targets` is the number of response variables.\n\n            .. deprecated:: 1.5\n               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.\n\n        Returns\n        -------\n        self : object\n            Fitted model.", "file": "sklearn_cross_decomposition__pls.py", "function": "PLSRegression.fit", "project": "scikit-learn", "change_stats": {"code_changes": 8, "doc_changes": 9, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _reload_with_unavailable_videos(self, item_id, data, ytcfg):\n        \n        \n        \n        browse_id = params = None\n        renderer = self._extract_sidebar_info_renderer(data, 'playlistSidebarPrimaryInfoRenderer')\n        if not renderer:\n            return\n        menu_renderer = try_get(\n            renderer, lambda x: x['menu']['menuRenderer']['items'], list) or []\n        for menu_item in menu_renderer:\n            if not isinstance(menu_item, dict):\n                continue\n            nav_item_renderer = menu_item.get('menuNavigationItemRenderer')\n            text = try_get(\n                nav_item_renderer, lambda x: x['text']['simpleText'], str)\n            if not text or text.lower() != 'show unavailable videos':\n                continue\n            browse_endpoint = try_get(\n                nav_item_renderer, lambda x: x['navigationEndpoint']['browseEndpoint'], dict) or {}\n            browse_id = browse_endpoint.get('browseId')\n            params = browse_endpoint.get('params')\n            break\n\n        headers = self.generate_api_headers(\n            ytcfg=ytcfg, account_syncid=self._extract_account_syncid(ytcfg, data),\n            visitor_data=self._extract_visitor_data(data, ytcfg))\n        query = {\n            'params': params or 'wgYCCAA=',\n            'browseId': browse_id or 'VL%s' % item_id\n        }\n        return self._extract_response(\n            item_id=item_id, headers=headers, query=query,\n            check_get_keys='contents', fatal=False, ytcfg=ytcfg,\n            note='Downloading API JSON with unavailable videos')```\n\nUpdated code:\n```python\ndef _reload_with_unavailable_videos(self, item_id, data, ytcfg):\n        \n        \n        \n        is_playlist = bool(traverse_obj(\n            data, ('metadata', 'playlistMetadataRenderer'), ('header', 'playlistHeaderRenderer')))\n        if not is_playlist:\n            return\n        headers = self.generate_api_headers(\n            ytcfg=ytcfg, account_syncid=self._extract_account_syncid(ytcfg, data),\n            visitor_data=self._extract_visitor_data(data, ytcfg))\n        query = {\n            'params': 'wgYCCAA=',\n            'browseId': f'VL{item_id}'\n        }\n        return self._extract_response(\n            item_id=item_id, headers=headers, query=query,\n            check_get_keys='contents', fatal=False, ytcfg=ytcfg,\n            note='Redownloading playlist API JSON with unavailable videos')\n```\n\nOriginal documentation:\n```python\nGet playlist with unavailable videos if the 'show unavailable videos' button exists.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Reload playlists with unavailable videos (e.g. private videos, region blocked, etc.)", "file": "yt_dlp_extractor_youtube.py", "function": "YoutubeTabBaseInfoExtractor._reload_with_unavailable_videos", "project": "yt-dlp", "change_stats": {"code_changes": 30, "doc_changes": 4, "total_changes": 34}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef urljoin_rfc(base, ref):\n    \n    \n    \n    if ref.startswith('?'):\n        fpart = urlparse.urlsplit(str(base))[2].rsplit('/', 1)[-1]\n        ref = ''.join([fpart, ref])\n    # convert ref to a string. This should already\n    # be the case, however, many spiders do not convert.\n    return urlparse.urljoin(base, str(ref))```\n\nUpdated code:\n```python\ndef urljoin_rfc(base, ref, encoding='utf-8'):\n    \n    \n    # XXX: this code was commented out because its purpose is unknown and\n    # there's no test or documentation that specifies its behaviour. please\n    # don't restore this code without adding unittests for it\n    #if ref.startswith('?'):\n    #    fpart = urlparse.urlsplit(unicode_to_str(base, encoding))[2].rsplit('/', 1)[-1]\n    #    ref = ''.join([fpart, ref])\n    return urlparse.urljoin(unicode_to_str(base, encoding), unicode_to_str(ref, encoding))\n```\n\nOriginal documentation:\n```python\nFixed urlparse.urljoin version that handles\n    relative query string as RFC states.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Same as urlparse.urljoin but supports unicode values in base and ref\n    parameters (in which case they will be converted to str using the given\n    encoding).\n\n    Always returns a str.", "file": "scrapy_utils_url.py", "function": "urljoin_rfc", "project": "scrapy", "change_stats": {"code_changes": 18, "doc_changes": 9, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cleanup():\n    \n    \n    \n    compiledir = theano.config.compiledir\n    for directory in os.listdir(compiledir):\n        file = None\n        try:\n            try:\n                filename = os.path.join(compiledir, directory, \"key.pkl\")\n                file = open(filename, 'rb')\n                #print file\n                try:\n                    keydata = cPickle.load(file)\n                    for key in list(keydata.keys):\n                        have_npy_abi_version = False\n                        for obj in flatten(key):\n                            if isinstance(obj, numpy.ndarray):\n                                keydata.remove_key(key)\n                                break\n                            if (isinstance(obj, basestring) and\n                                obj.startswith('NPY_ABI_VERSION=0x')):\n                                have_npy_abi_version = True\n\n                        if not have_npy_abi_version:\n                            keydata.remove_key(key)\n                    if len(keydata.keys) == 0:\n                        shutil.rmtree(os.path.join(compiledir, directory))\n\n                except EOFError:\n                    print (\"ERROR while reading this key file '%s'.\"\n                           \" Delete its directory\" % filename)\n            except IOError:\n                pass\n        finally:\n            if file is not None:\n                file.close()```\n\nUpdated code:\n```python\ndef cleanup():\n    \n    \n    \n    compiledir = theano.config.compiledir\n    for directory in os.listdir(compiledir):\n        file = None\n        try:\n            try:\n                filename = os.path.join(compiledir, directory, \"key.pkl\")\n                file = open(filename, 'rb')\n                #print file\n                try:\n                    keydata = cPickle.load(file)\n                    for key in list(keydata.keys):\n                        have_npy_abi_version = False\n                        have_c_compiler = False\n                        for obj in flatten(key):\n                            if isinstance(obj, numpy.ndarray):\n                                keydata.remove_key(key)\n                                break\n                            elif isinstance(obj, basestring):\n                                if obj.startswith('NPY_ABI_VERSION=0x'):\n                                    have_npy_abi_version = True\n                                elif obj.startswith('c_compiler_str='):\n                                    have_c_compiler = True\n\n                        if not have_npy_abi_version or not have_c_compiler:\n                            keydata.remove_key(key)\n                    if len(keydata.keys) == 0:\n                        shutil.rmtree(os.path.join(compiledir, directory))\n\n                except EOFError:\n                    print (\"ERROR while reading this key file '%s'.\"\n                           \" Delete its directory\" % filename)\n            except IOError:\n                pass\n        finally:\n            if file is not None:\n                file.close()\n```\n\nOriginal documentation:\n```python\nDelete keys in old format from the compiledir.\n\n    Old clean up include key in old format:\n    1) keys that have an ndarray in them.\n       Now we use a hash in the keys of the constant data.\n    2) key that don't have the numpy ABI version in them\n\n    If there is no key left for a compiled module, we delete the module.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Delete keys in old format from the compiledir.\n\n    Old clean up include key in old format:\n    1) keys that have an ndarray in them.\n       Now we use a hash in the keys of the constant data.\n    2) key that don't have the numpy ABI version in them\n    3) They do not have a compile version string\n\n    If there is no key left for a compiled module, we delete the module.", "file": "theano_gof_compiledir.py", "function": "cleanup", "project": "Theano", "change_stats": {"code_changes": 13, "doc_changes": 3, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef flip_horizontal(keypoints, flip_point, flip_permutation, scope=None):\n  \n  \n  with tf.name_scope(scope, 'FlipHorizontal'):\n    keypoints = tf.transpose(keypoints, [1, 0, 2])\n    keypoints = tf.gather(keypoints, flip_permutation)\n    v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)\n    u = flip_point * 2.0 - u\n    new_keypoints = tf.concat([v, u], 2)\n    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])\n    return new_keypoints```\n\nUpdated code:\n```python\ndef flip_horizontal(keypoints, flip_point, flip_permutation=None, scope=None):\n  \n  \n  with tf.name_scope(scope, 'FlipHorizontal'):\n    keypoints = tf.transpose(keypoints, [1, 0, 2])\n    if flip_permutation:\n      keypoints = tf.gather(keypoints, flip_permutation)\n    v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)\n    u = flip_point * 2.0 - u\n    new_keypoints = tf.concat([v, u], 2)\n    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])\n    return new_keypoints\n```\n\nOriginal documentation:\n```python\nFlips the keypoints horizontally around the flip_point.\n\n  This operation flips the x coordinate for each keypoint around the flip_point\n  and also permutes the keypoints in a manner specified by flip_permutation.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    flip_point:  (float) scalar tensor representing the x coordinate to flip the\n      keypoints around.\n    flip_permutation: rank 1 int32 tensor containing the keypoint flip\n      permutation. This specifies the mapping from original keypoint indices\n      to the flipped keypoint indices. This is used primarily for keypoints\n      that are not reflection invariant. E.g. Suppose there are 3 keypoints\n      representing ['head', 'right_eye', 'left_eye'], then a logical choice for\n      flip_permutation might be [0, 2, 1] since we want to swap the 'left_eye'\n      and 'right_eye' after a horizontal flip.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Flips the keypoints horizontally around the flip_point.\n\n  This operation flips the x coordinate for each keypoint around the flip_point\n  and also permutes the keypoints in a manner specified by flip_permutation.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    flip_point:  (float) scalar tensor representing the x coordinate to flip the\n      keypoints around.\n    flip_permutation: integer list or rank 1 int32 tensor containing the\n      keypoint flip permutation. This specifies the mapping from original\n      keypoint indices to the flipped keypoint indices. This is used primarily\n      for keypoints that are not reflection invariant. E.g. Suppose there are 3\n      keypoints representing ['head', 'right_eye', 'left_eye'], then a logical\n      choice for flip_permutation might be [0, 2, 1] since we want to swap the\n      'left_eye' and 'right_eye' after a horizontal flip.\n      Default to None or empty list to keep the original order after flip.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]", "file": "research_object_detection_core_keypoint_ops.py", "function": "flip_horizontal", "project": "models", "change_stats": {"code_changes": 7, "doc_changes": 17, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_connection(self) -> \"mariadb.Connection\":\n\t\t\n\t\t\n\t\tglobal _SITE_POOLS\n\n\t\t# don't pool root connections\n\t\tif self.user == \"root\":\n\t\t\treturn self.create_connection()\n\n\t\tif not is_connection_pooling_enabled():\n\t\t\tself.close_connection_pools()\n\t\t\treturn self.create_connection()\n\n\t\tif frappe.local.site not in _SITE_POOLS:\n\t\t\tsite_pool = self.create_connection_pool()\n\t\telse:\n\t\t\tsite_pool = self.get_connection_pool()\n\n\t\ttry:\n\t\t\tconn = site_pool.get_connection()\n\t\texcept mariadb.PoolError:\n\t\t\t# PoolError is raised when the pool is exhausted\n\t\t\tconn = self.create_connection()\n\t\t\ttry:\n\t\t\t\tsite_pool.add_connection(conn)\n\t\t\t\t# log this via frappe.logger & continue - site needs bigger pool...over _POOL_SIZE\n\t\t\texcept mariadb.PoolError:\n\t\t\t\t# PoolError is raised when size limit is reached\n\t\t\t\t# log this via frappe.logger & continue - site needs a much bigger pool...over _MAX_POOL_SIZE\n\t\t\t\tpass\n\n\t\treturn conn```\n\nUpdated code:\n```python\ndef _get_connection(self):\n\t\t\n\t\treturn self.create_connection()\n```\n\nOriginal documentation:\n```python\nReturn MariaDB connection object.\n\n\t\tIf frappe.conf.disable_database_connection_pooling is set, return a new connection\n\t\tobject and close existing pool if exists. Else, return a connection from the pool.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return MariaDB connection object.", "file": "frappe_database_mariadb_database.py", "function": "MariaDBConnectionUtil._get_connection", "project": "frappe", "change_stats": {"code_changes": 35, "doc_changes": 5, "total_changes": 40}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, xs_pad, ilens, prev_states=None):\n        \n\n        \n        if prev_states is None:\n            prev_states = [None] * len(self.enc)\n        assert len(prev_states) == len(self.enc)\n\n        current_states = []\n        for module, prev_state in zip(self.enc, prev_states):\n            xs_pad, ilens, states = module(\n                xs_pad,\n                ilens,\n                prev_state=prev_state,\n            )\n            current_states.append(states)\n\n        if isinstance(xs_pad, tuple):\n            final_xs_pad, aux_xs_list = xs_pad[0], xs_pad[1]\n\n            mask = to_device(final_xs_pad, make_pad_mask(ilens).unsqueeze(-1))\n\n            aux_xs_list = [layer.masked_fill(mask, 0.0) for layer in aux_xs_list]\n\n            return (\n                (\n                    final_xs_pad.masked_fill(mask, 0.0),\n                    aux_xs_list,\n                ),\n                ilens,\n                current_states,\n            )\n        else:\n            mask = to_device(xs_pad, make_pad_mask(ilens).unsqueeze(-1))\n\n            return xs_pad.masked_fill(mask, 0.0), ilens, current_states```\n\nUpdated code:\n```python\ndef forward(\n        self,\n        feats: torch.Tensor,\n        feats_len: torch.Tensor,\n        prev_states: Optional[List[torch.Tensor]] = None,\n    ):\n        \n\n        \n        if prev_states is None:\n            prev_states = [None] * len(self.enc)\n        assert len(prev_states) == len(self.enc)\n\n        _enc_out = feats\n        _enc_out_len = feats_len\n        current_states = []\n        for rnn_module, prev_state in zip(self.enc, prev_states):\n            _enc_out, _enc_out_len, states = rnn_module(\n                _enc_out,\n                _enc_out_len,\n                prev_states=prev_state,\n            )\n            current_states.append(states)\n\n        if isinstance(_enc_out, tuple):\n            enc_out, aux_enc_out = _enc_out[0], _enc_out[1]\n            enc_out_len, aux_enc_out_len = _enc_out_len[0], _enc_out_len[1]\n\n            enc_out_mask = to_device(enc_out, make_pad_mask(enc_out_len).unsqueeze(-1))\n            enc_out = enc_out.masked_fill(enc_out_mask, 0.0)\n\n            for i in range(len(aux_enc_out)):\n                aux_mask = to_device(\n                    aux_enc_out[i], make_pad_mask(aux_enc_out_len[i]).unsqueeze(-1)\n                )\n                aux_enc_out[i] = aux_enc_out[i].masked_fill(aux_mask, 0.0)\n\n            return (\n                (enc_out, aux_enc_out),\n                (enc_out_len, aux_enc_out_len),\n                current_states,\n            )\n        else:\n            enc_out_mask = to_device(\n                _enc_out, make_pad_mask(_enc_out_len).unsqueeze(-1)\n            )\n\n            return _enc_out.masked_fill(enc_out_mask, 0.0), _enc_out_len, current_states\n```\n\nOriginal documentation:\n```python\nForward encoder.\n\n        Args:\n            xs_pad: Batch of padded input sequences (B, Tmax, idim)\n            ilens: Batch of lengths of input sequences (B)\n            prev_state: Batch of previous encoder hidden states (B, ??)\n\n        Returns:\n            : Batch of padded output sequences (B, Tmax, hdim)\n                    or tuple w/ aux outputs ((B, Tmax, hdim), [L x (B, Tmax, hdim)])\n            : Batch of lengths of output sequences (B)\n            : Batch of hidden state sequences (B, Tmax, hdim)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward encoder.\n\n        Args:\n            feats: Feature sequences. (B, F, D_feats)\n            feats_len: Feature sequences lengths. (B,)\n            prev_states: Previous encoder hidden states. [N x (B, T, D_enc)]\n\n        Returns:\n            enc_out: Encoder output sequences. (B, T, D_enc)\n                   with or without encoder intermediate output sequences.\n                   ((B, T, D_enc), [N x (B, T, D_enc)])\n            enc_out_len: Encoder output sequences lengths. (B,)\n            current_states: Encoder hidden states. [N x (B, T, D_enc)]", "file": "espnet_nets_pytorch_backend_transducer_rnn_encoder.py", "function": "Encoder.forward", "project": "espnet", "change_stats": {"code_changes": 48, "doc_changes": 17, "total_changes": 65}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef duplicated(values, keep='first'):\n    \n    \n    \n\n    dtype = values.dtype\n\n    # no need to revert to original type\n    if needs_i8_conversion(dtype):\n        values = values.view(np.int64)\n    elif is_period_arraylike(values):\n        from pandas.tseries.period import PeriodIndex\n        values = PeriodIndex(values).asi8\n    elif is_categorical_dtype(dtype):\n        values = values.values.codes\n    elif isinstance(values, (ABCSeries, ABCIndex)):\n        values = values.values\n\n    if is_integer_dtype(dtype):\n        values = _ensure_int64(values)\n        duplicated = htable.duplicated_int64(values, keep=keep)\n    elif is_float_dtype(dtype):\n        values = _ensure_float64(values)\n        duplicated = htable.duplicated_float64(values, keep=keep)\n    else:\n        values = _ensure_object(values)\n        duplicated = htable.duplicated_object(values, keep=keep)\n\n    return duplicated```\n\nUpdated code:\n```python\ndef duplicated(values, keep='first'):\n    \n    \n    \n\n    dtype = values.dtype\n\n    # no need to revert to original type\n    if needs_i8_conversion(dtype):\n        values = values.view(np.int64)\n    elif is_period_arraylike(values):\n        from pandas.tseries.period import PeriodIndex\n        values = PeriodIndex(values).asi8\n    elif is_categorical_dtype(dtype):\n        values = values.values.codes\n    elif isinstance(values, (ABCSeries, ABCIndex)):\n        values = values.values\n\n    if is_signed_integer_dtype(dtype):\n        values = _ensure_int64(values)\n        duplicated = htable.duplicated_int64(values, keep=keep)\n    elif is_unsigned_integer_dtype(dtype):\n        values = _ensure_uint64(values)\n        duplicated = htable.duplicated_uint64(values, keep=keep)\n    elif is_float_dtype(dtype):\n        values = _ensure_float64(values)\n        duplicated = htable.duplicated_float64(values, keep=keep)\n    else:\n        values = _ensure_object(values)\n        duplicated = htable.duplicated_object(values, keep=keep)\n\n    return duplicated\n```\n\nOriginal documentation:\n```python\nReturn boolean ndarray denoting duplicate values\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    keep : {'first', 'last', False}, default 'first'\n        - ``first`` : Mark duplicates as ``True`` except for the first\n          occurrence.\n        - ``last`` : Mark duplicates as ``True`` except for the last\n          occurrence.\n        - False : Mark all duplicates as ``True``.\n\n    Returns\n    -------\n    duplicated : ndarray\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return boolean ndarray denoting duplicate values.\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    values : ndarray-like\n        Array over which to check for duplicate values.\n    keep : {'first', 'last', False}, default 'first'\n        - ``first`` : Mark duplicates as ``True`` except for the first\n          occurrence.\n        - ``last`` : Mark duplicates as ``True`` except for the last\n          occurrence.\n        - False : Mark all duplicates as ``True``.\n\n    Returns\n    -------\n    duplicated : ndarray", "file": "pandas_core_algorithms.py", "function": "duplicated", "project": "pandas", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef post_process_masks(\n        self, masks, original_sizes, reshaped_input_sizes, mask_threshold=0.0, binarize=True, pad_size=None\n    ):\n        \n        \n        \n        requires_backends(self, [\"torch\"])\n        pad_size = self.pad_size if pad_size is None else pad_size\n        target_image_size = (pad_size[\"height\"], pad_size[\"width\"])\n\n        output_masks = []\n        for i, original_size in enumerate(original_sizes):\n            interpolated_mask = F.interpolate(masks[i], target_image_size, mode=\"bilinear\", align_corners=False)\n            interpolated_mask = interpolated_mask[..., : reshaped_input_sizes[i][0], : reshaped_input_sizes[i][1]]\n            interpolated_mask = F.interpolate(interpolated_mask, original_size, mode=\"bilinear\", align_corners=False)\n            if binarize:\n                interpolated_mask = interpolated_mask > mask_threshold\n            output_masks.append(interpolated_mask)\n\n        return output_masks```\n\nUpdated code:\n```python\ndef post_process_masks(\n        self, masks, original_sizes, reshaped_input_sizes, mask_threshold=0.0, binarize=True, pad_size=None\n    ):\n        \n        \n        \n        requires_backends(self, [\"torch\"])\n        pad_size = self.pad_size if pad_size is None else pad_size\n        target_image_size = (pad_size[\"height\"], pad_size[\"width\"])\n        if isinstance(original_sizes, (torch.Tensor, np.ndarray)):\n            original_sizes = original_sizes.tolist()\n        if isinstance(reshaped_input_sizes, (torch.Tensor, np.ndarray)):\n            reshaped_input_sizes = reshaped_input_sizes.tolist()\n        output_masks = []\n        for i, original_size in enumerate(original_sizes):\n            if isinstance(masks[i], np.ndarray):\n                masks[i] = torch.from_numpy(masks[i])\n            elif not isinstance(masks[i], torch.Tensor):\n                raise ValueError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n            interpolated_mask = F.interpolate(masks[i], target_image_size, mode=\"bilinear\", align_corners=False)\n            interpolated_mask = interpolated_mask[..., : reshaped_input_sizes[i][0], : reshaped_input_sizes[i][1]]\n            interpolated_mask = F.interpolate(interpolated_mask, original_size, mode=\"bilinear\", align_corners=False)\n            if binarize:\n                interpolated_mask = interpolated_mask > mask_threshold\n            output_masks.append(interpolated_mask)\n\n        return output_masks\n```\n\nOriginal documentation:\n```python\nRemove padding and upscale masks to the original image size.\n\n        Args:\n            masks (`torch.Tensor`):\n                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n            original_sizes (`torch.Tensor`):\n                The original size of the images before resizing for input to the model, in (height, width) format.\n            reshaped_input_sizes (`torch.Tensor`):\n                The size of the image input to the model, in (height, width) format. Used to remove padding.\n            mask_threshold (`float`, *optional*, defaults to 0.0):\n                The threshold to use for binarizing the masks.\n            binarize (`bool`, *optional*, defaults to `True`):\n                Whether to binarize the masks.\n            pad_size (`int`, *optional*, defaults to `self.pad_size`):\n                The target size the images were padded to before being passed to the model. If None, the target size is\n                assumed to be the processor's `pad_size`.\n        Returns:\n            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n            is given by original_size.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Remove padding and upscale masks to the original image size.\n\n        Args:\n            masks (`Union[List[torch.Tensor], List[np.ndarray]]`):\n                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n                width) format.\n            reshaped_input_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n                The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.\n            mask_threshold (`float`, *optional*, defaults to 0.0):\n                The threshold to use for binarizing the masks.\n            binarize (`bool`, *optional*, defaults to `True`):\n                Whether to binarize the masks.\n            pad_size (`int`, *optional*, defaults to `self.pad_size`):\n                The target size the images were padded to before being passed to the model. If None, the target size is\n                assumed to be the processor's `pad_size`.\n        Returns:\n            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n            is given by original_size.", "file": "src_transformers_models_sam_image_processing_sam.py", "function": "SamImageProcessor.post_process_masks", "project": "transformers", "change_stats": {"code_changes": 11, "doc_changes": 13, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save(self, path: Union[str, Path], state: Dict[str, Union[nn.Module, Optimizer, Any]]) -> None:\n        \n        \n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state))\n        self.barrier()```\n\nUpdated code:\n```python\ndef save(\n        self,\n        path: Union[str, Path],\n        state: Dict[str, Union[nn.Module, Optimizer, Any]],\n        filter: Optional[Dict[str, Callable[[str, Any], bool]]] = None,\n    ) -> None:\n        \n        \n        if filter is not None:\n            if not isinstance(filter, dict):\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\n            if not set(filter).issubset(state):\n                raise ValueError(\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\n                )\n            for k, v in filter.items():\n                if not callable(v):\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\n        self.barrier()\n```\n\nOriginal documentation:\n```python\nSave checkpoint contents to a file.\n\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\n        This method must be called on all processes!\n\n        Args:\n            path: A path to where the file(s) should be saved\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\n                state-dict will be retrieved and converted automatically.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save checkpoint contents to a file.\n\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\n        This method must be called on all processes!\n\n        Args:\n            path: A path to where the file(s) should be saved\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\n                state-dict will be retrieved and converted automatically.\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\n                state key, where its filter will be applied to the ``state_dict`` generated.", "file": "src_lightning_fabric_fabric.py", "function": "Fabric.save", "project": "pytorch-lightning", "change_stats": {"code_changes": 21, "doc_changes": 5, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_WPD_filter(\n    Phi: ComplexTensor,\n    Rf: ComplexTensor,\n    reference_vector: torch.Tensor,\n    eps: float = 1e-8,\n) -> ComplexTensor:\n    \n    \n    B, F = Rf.shape[:2]\n    Cbtaps = Rf.size(-1)\n    eye = torch.eye(Cbtaps, dtype=Rf.dtype, device=Rf.device)\n    shape = [1 for _ in range(Rf.dim() - 2)] + [Cbtaps, Cbtaps]\n    eye = eye.view(*shape).repeat(B, F, 1, 1)\n    with torch.no_grad():\n        epsilon = FC.trace(Rf).real.abs()[..., None, None] * eps\n        # in case that correlation_matrix is all-zero\n        epsilon = epsilon + eps\n\n    # numerator: (..., C_1, C_2) x (..., C_2, C_3) -> (..., C_1, C_3)\n    if is_torch_1_1_plus:\n        # torch.solve is required, which is only available after pytorch 1.1.0+\n        numerator = FC.solve(Phi, Rf + epsilon * eye)[0]\n    else:\n        numerator = FC.matmul((Rf + epsilon * eye).inverse2(), Phi)\n    # ws: (..., C, C) / (...,) -> (..., C, C)\n    ws = numerator / (FC.trace(numerator)[..., None, None] + eps)\n    # h: (..., F, C_1, C_2) x (..., C_2) -> (..., F, C_1)\n    beamform_vector = FC.einsum(\"...fec,...c->...fe\", [ws, reference_vector])\n    # (B, F, (btaps + 1) * C)\n    return beamform_vector```\n\nUpdated code:\n```python\ndef get_WPD_filter(\n    Phi: ComplexTensor,\n    Rf: ComplexTensor,\n    reference_vector: torch.Tensor,\n    use_torch_solver: bool = True,\n    diagonal_loading: bool = True,\n    diag_eps: float = 1e-7,\n    eps: float = 1e-8,\n) -> ComplexTensor:\n    \n    \n    if diagonal_loading:\n        Rf = tik_reg(Rf, reg=diag_eps, eps=eps)\n\n    # numerator: (..., C_1, C_2) x (..., C_2, C_3) -> (..., C_1, C_3)\n    if use_torch_solver and is_torch_1_1_plus:\n        # torch.solve is required, which is only available after pytorch 1.1.0+\n        numerator = FC.solve(Phi, Rf)[0]\n    else:\n        numerator = FC.matmul(Rf.inverse2(), Phi)\n    # ws: (..., C, C) / (...,) -> (..., C, C)\n    ws = numerator / (FC.trace(numerator)[..., None, None] + eps)\n    # h: (..., F, C_1, C_2) x (..., C_2) -> (..., F, C_1)\n    beamform_vector = FC.einsum(\"...fec,...c->...fe\", [ws, reference_vector])\n    # (B, F, (btaps + 1) * C)\n    return beamform_vector\n```\n\nOriginal documentation:\n```python\nReturn the WPD vector.\n\n        WPD is the Weighted Power minimization Distortionless response\n        convolutional beamformer. As follows:\n\n        h = (Rf^-1 @ Phi_{xx}) / tr[(Rf^-1) @ Phi_{xx}] @ u\n\n    Reference:\n        T. Nakatani and K. Kinoshita, \"A Unified Convolutional Beamformer\n        for Simultaneous Denoising and Dereverberation,\" in IEEE Signal\n        Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi:\n        10.1109/LSP.2019.2911179.\n        https://ieeexplore.ieee.org/document/8691481\n\n    Args:\n        Phi (ComplexTensor): (B, F, (btaps+1) * C, (btaps+1) * C)\n            is the PSD of zero-padded speech [x^T(t,f) 0 ... 0]^T.\n        Rf (ComplexTensor): (B, F, (btaps+1) * C, (btaps+1) * C)\n            is the power normalized spatio-temporal covariance matrix.\n        reference_vector (torch.Tensor): (B, (btaps+1) * C)\n            is the reference_vector.\n        eps (float):\n\n    Returns:\n        filter_matrix (ComplexTensor): (B, F, (btaps + 1) * C)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the WPD vector.\n\n        WPD is the Weighted Power minimization Distortionless response\n        convolutional beamformer. As follows:\n\n        h = (Rf^-1 @ Phi_{xx}) / tr[(Rf^-1) @ Phi_{xx}] @ u\n\n    Reference:\n        T. Nakatani and K. Kinoshita, \"A Unified Convolutional Beamformer\n        for Simultaneous Denoising and Dereverberation,\" in IEEE Signal\n        Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi:\n        10.1109/LSP.2019.2911179.\n        https://ieeexplore.ieee.org/document/8691481\n\n    Args:\n        Phi (ComplexTensor): (B, F, (btaps+1) * C, (btaps+1) * C)\n            is the PSD of zero-padded speech [x^T(t,f) 0 ... 0]^T.\n        Rf (ComplexTensor): (B, F, (btaps+1) * C, (btaps+1) * C)\n            is the power normalized spatio-temporal covariance matrix.\n        reference_vector (torch.Tensor): (B, (btaps+1) * C)\n            is the reference_vector.\n        use_torch_solver (bool): Whether to use `solve` instead of `inverse`\n        diagonal_loading (bool): Whether to add a tiny term to the diagonal of psd_n\n        diag_eps (float):\n        eps (float):\n\n    Returns:\n        filter_matrix (ComplexTensor): (B, F, (btaps + 1) * C)", "file": "espnet2_enh_layers_beamformer.py", "function": "get_WPD_filter", "project": "espnet", "change_stats": {"code_changes": 22, "doc_changes": 5, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef summon_full_params(\n        module,\n        recurse: bool = True,\n        writeback: bool = True,\n        rank0_only: bool = False,\n        offload_to_cpu: bool = False,\n    ) -> Generator:\n        \n        \n        # Note that we specify root_only as FSDP roots will handle summoning\n        # child FSDP instances based on recurse argument.\n        root_fsdp_modules = FullyShardedDataParallel.fsdp_modules(\n            module, root_only=True\n        )\n        # Summon all params for all FSDP instances\n        with contextlib.ExitStack() as stack:\n            for module in root_fsdp_modules:\n                stack.enter_context(\n                    module._summon_full_params(\n                        recurse=recurse,\n                        writeback=writeback,\n                        rank0_only=rank0_only,\n                        offload_to_cpu=offload_to_cpu,\n                    )\n                )\n            # Yield to the caller, with full params in all FSDP instances.\n            yield\n        # Exiting from the ExitStack will reshard all params.\n        return```\n\nUpdated code:\n```python\ndef summon_full_params(\n        module,\n        recurse: bool = True,\n        writeback: bool = True,\n        rank0_only: bool = False,\n        offload_to_cpu: bool = False,\n        with_grads: bool = False,\n    ) -> Generator:\n        \n        \n        # Note that we specify root_only as FSDP roots will handle summoning\n        # child FSDP instances based on recurse argument.\n        root_fsdp_modules = FullyShardedDataParallel.fsdp_modules(\n            module, root_only=True\n        )\n        # Summon all params for all FSDP instances\n        with contextlib.ExitStack() as stack:\n            for module in root_fsdp_modules:\n                stack.enter_context(\n                    module._summon_full_params(\n                        recurse=recurse,\n                        writeback=writeback,\n                        rank0_only=rank0_only,\n                        offload_to_cpu=offload_to_cpu,\n                        with_grads=with_grads,\n                    )\n                )\n            # Yield to the caller, with full params in all FSDP instances.\n            yield\n        # Exiting from the ExitStack will reshard all params.\n        return\n```\n\nOriginal documentation:\n```python\nr\"\"\" A context manager to expose full params for FSDP instances.\n        Can be useful *after* forward/backward for a model to get\n        the params for additional processing or checking. It can take a non-FSDP\n        module and will summon full params for all contained FSDP modules as\n        well as their children, depending on the ``recurse`` argument.\n\n        .. note:: This can be used on inner FSDPs.\n        .. note:: This can *not* be used within a forward or backward pass. Nor\n            can forward and backward be started from within this context.\n        .. note:: Parameters will revert to their local shards after the context\n            manager exits, storage behavior is the same as forward.\n        .. note:: The full parameters can be modified, but only the portion\n            corresponding to the local param shard will persist after the\n            context manager exits (unless ``writeback=False``, in which case\n            changes will be discarded). In the case where FSDP does not shard\n            the parameters, currently only when ``world_size == 1``, or ``NO_SHARD``\n            config, the modification is persisted regardless of ``writeback``.\n        .. note:: This method works on modules which are not FSDP themselves but\n            may contain multiple independent FSDP units. In that case, the given\n            arguments will apply to all contained FSDP units.\n\n        .. warning:: Note that ``rank0_only=True`` in conjunction with\n            ``writeback=True`` is not currently supported and will raise an\n            error. This is because model parameter shapes would be different\n            across ranks within the context, and writing to them can lead to\n            inconsistency across ranks when the context is exited.\n\n        .. warning:: Note that ``offload_to_cpu`` and ``rank0_only=False`` will\n            result in full parameters being redundantly copied to CPU memory for\n            GPUs that reside on the same machine, which may incur the risk of\n            CPU OOM. It is recommended to use ``offload_to_cpu`` with\n            ``rank0_only=True``.\n\n        Args:\n            recurse (bool, Optional): recursively summon all params for nested\n                FSDP instances (default: True).\n            writeback (bool, Optional): if ``False``, modifications to params are\n                discarded after the context manager exits;\n                disabling this can be slightly more efficient (default: True)\n            rank0_only (bool, Optional): if ``True``, full parameters are\n                materialized on only global rank 0. This means that within the\n                context, only rank 0 will have full parameters and the other\n                ranks will have sharded parameters. Note that setting\n                ``rank0_only=True`` with ``writeback=True`` is not supported,\n                as model parameter shapes will be different across ranks\n                within the context, and writing to them can lead to\n                inconsistency across ranks when the context is exited.\n            offload_to_cpu (bool, Optional): If ``True``, full parameters are\n                offloaded to CPU. Note that this offloading currently only\n                occurs if the parameter is sharded (which is only not the case\n                for world_size = 1 or ``NO_SHARD`` config). It is recommended\n                to use ``offload_to_cpu`` with ``rank0_only=True`` to avoid\n                redundant copies of model parameters being offloaded to the same CPU memory.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\" A context manager to expose full params for FSDP instances.\n        Can be useful *after* forward/backward for a model to get\n        the params for additional processing or checking. It can take a non-FSDP\n        module and will summon full params for all contained FSDP modules as\n        well as their children, depending on the ``recurse`` argument.\n\n        .. note:: This can be used on inner FSDPs.\n        .. note:: This can *not* be used within a forward or backward pass. Nor\n            can forward and backward be started from within this context.\n        .. note:: Parameters will revert to their local shards after the context\n            manager exits, storage behavior is the same as forward.\n        .. note:: The full parameters can be modified, but only the portion\n            corresponding to the local param shard will persist after the\n            context manager exits (unless ``writeback=False``, in which case\n            changes will be discarded). In the case where FSDP does not shard\n            the parameters, currently only when ``world_size == 1``, or ``NO_SHARD``\n            config, the modification is persisted regardless of ``writeback``.\n        .. note:: This method works on modules which are not FSDP themselves but\n            may contain multiple independent FSDP units. In that case, the given\n            arguments will apply to all contained FSDP units.\n\n        .. warning:: Note that ``rank0_only=True`` in conjunction with\n            ``writeback=True`` is not currently supported and will raise an\n            error. This is because model parameter shapes would be different\n            across ranks within the context, and writing to them can lead to\n            inconsistency across ranks when the context is exited.\n\n        .. warning:: Note that ``offload_to_cpu`` and ``rank0_only=False`` will\n            result in full parameters being redundantly copied to CPU memory for\n            GPUs that reside on the same machine, which may incur the risk of\n            CPU OOM. It is recommended to use ``offload_to_cpu`` with\n            ``rank0_only=True``.\n\n        Args:\n            recurse (bool, Optional): recursively summon all params for nested\n                FSDP instances (default: True).\n            writeback (bool, Optional): if ``False``, modifications to params are\n                discarded after the context manager exits;\n                disabling this can be slightly more efficient (default: True)\n            rank0_only (bool, Optional): if ``True``, full parameters are\n                materialized on only global rank 0. This means that within the\n                context, only rank 0 will have full parameters and the other\n                ranks will have sharded parameters. Note that setting\n                ``rank0_only=True`` with ``writeback=True`` is not supported,\n                as model parameter shapes will be different across ranks\n                within the context, and writing to them can lead to\n                inconsistency across ranks when the context is exited.\n            offload_to_cpu (bool, Optional): If ``True``, full parameters are\n                offloaded to CPU. Note that this offloading currently only\n                occurs if the parameter is sharded (which is only not the case\n                for world_size = 1 or ``NO_SHARD`` config). It is recommended\n                to use ``offload_to_cpu`` with ``rank0_only=True`` to avoid\n                redundant copies of model parameters being offloaded to the same CPU memory.\n            with_grads (bool, Optional): If ``True``, gradients are also\n                unsharded with the parameters. Currently, this is only\n                supported when passing ``use_orig_params=True`` to the FSDP\n                constructor and ``offload_to_cpu=False`` to this method.\n                (Default: ``False``)", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel.summon_full_params", "project": "pytorch", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef column_or_1d(y, *, warn=False):\n    \n    \n    xp, _ = get_namespace(y)\n    y = xp.asarray(y)\n    shape = y.shape\n    if len(shape) == 1:\n        return _asarray_with_order(xp.reshape(y, -1), order=\"C\", xp=xp)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn(\n                \"A column-vector y was passed when a 1d array was\"\n                \" expected. Please change the shape of y to \"\n                \"(n_samples, ), for example using ravel().\",\n                DataConversionWarning,\n                stacklevel=2,\n            )\n        return _asarray_with_order(xp.reshape(y, -1), order=\"C\", xp=xp)\n\n    raise ValueError(\n        \"y should be a 1d array, got an array of shape {} instead.\".format(shape)\n    )```\n\nUpdated code:\n```python\ndef column_or_1d(y, *, dtype=None, warn=False):\n    \n    \n    xp, _ = get_namespace(y)\n    y = xp.asarray(y, dtype=dtype)\n    shape = y.shape\n    if len(shape) == 1:\n        return _asarray_with_order(xp.reshape(y, -1), order=\"C\", xp=xp)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn(\n                \"A column-vector y was passed when a 1d array was\"\n                \" expected. Please change the shape of y to \"\n                \"(n_samples, ), for example using ravel().\",\n                DataConversionWarning,\n                stacklevel=2,\n            )\n        return _asarray_with_order(xp.reshape(y, -1), order=\"C\", xp=xp)\n\n    raise ValueError(\n        \"y should be a 1d array, got an array of shape {} instead.\".format(shape)\n    )\n```\n\nOriginal documentation:\n```python\nRavel column or 1d numpy array, else raises an error.\n\n    Parameters\n    ----------\n    y : array-like\n       Input data.\n\n    warn : bool, default=False\n       To control display of warnings.\n\n    Returns\n    -------\n    y : ndarray\n       Output data.\n\n    Raises\n    ------\n    ValueError\n        If `y` is not a 1D array or a 2D array with a single row or column.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ravel column or 1d numpy array, else raises an error.\n\n    Parameters\n    ----------\n    y : array-like\n       Input data.\n\n    dtype : data-type, default=None\n        Data type for `y`.\n\n        .. versionadded:: 1.2\n\n    warn : bool, default=False\n       To control display of warnings.\n\n    Returns\n    -------\n    y : ndarray\n       Output data.\n\n    Raises\n    ------\n    ValueError\n        If `y` is not a 1D array or a 2D array with a single row or column.", "file": "sklearn_utils_validation.py", "function": "column_or_1d", "project": "scikit-learn", "change_stats": {"code_changes": 6, "doc_changes": 7, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _print_api_warning(self, resource: StateResource, api_response: dict):\n        \n        \n        # Print warnings if anything was given.\n        warning_msgs = api_response.get(\"partial_failure_warning\", None)\n        if warning_msgs:\n            warnings.warn(warning_msgs)\n\n        # Print warnings if data is truncated.\n        data = api_response[\"result\"]\n        total = api_response[\"total\"]\n        if total > len(data):\n            warnings.warn(\n                (\n                    f\"{len(data)} ({total} total) {resource.value} \"\n                    f\"are returned. {total - len(data)} entries have been truncated. \"\n                    \"Use `--filter` to reduce the amount of data to return \"\n                    \"or increase the limit by specifying`--limit`.\"\n                ),\n            )\n\n        # Print the additional warnings.\n        warnings_to_print = api_response.get(\"warnings\", [])\n        if warnings_to_print:\n            for warning_to_print in warnings_to_print:\n                warnings.warn(warning_to_print)```\n\nUpdated code:\n```python\ndef _print_api_warning(self, resource: StateResource, api_response: dict):\n        \n        \n        # Print warnings if anything was given.\n        warning_msgs = api_response.get(\"partial_failure_warning\", None)\n        if warning_msgs:\n            warnings.warn(warning_msgs)\n\n        # Print warnings if data is truncated at the data source.\n        num_after_truncation = api_response[\"num_after_truncation\"]\n        total = api_response[\"total\"]\n        if total > num_after_truncation:\n            # NOTE(rickyyx): For now, there's not much users could do (neither can we),\n            # with hard truncation. Unless we allow users to set a higher\n            # `RAY_MAX_LIMIT_FROM_DATA_SOURCE`, the data will always be truncated at the\n            # data source.\n            warnings.warn(\n                (\n                    f\"{num_after_truncation} ({total} total) {resource.value} \"\n                    \"are retrieved from the data source. \"\n                    f\"{total - num_after_truncation} entries have been truncated. \"\n                    f\"Max of {num_after_truncation} entries are retrieved from data \"\n                    \"source to prevent over-sized payloads.\"\n                ),\n            )\n\n        # Print warnings if return data is limited at the API server due to\n        # limit enforced at the server side\n        num_filtered = api_response[\"num_filtered\"]\n        data = api_response[\"result\"]\n        if num_filtered > len(data):\n            warnings.warn(\n                (\n                    f\"{len(data)}/{num_filtered} {resource.value} returned. \"\n                    \"Use `--filter` to reduce the amount of data to return or \"\n                    \"setting a higher limit with `--limit` to see all data. \"\n                ),\n            )\n\n        # Print the additional warnings.\n        warnings_to_print = api_response.get(\"warnings\", [])\n        if warnings_to_print:\n            for warning_to_print in warnings_to_print:\n                warnings.warn(warning_to_print)\n```\n\nOriginal documentation:\n```python\nPrint the API warnings.\n\n        Args:\n            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',\n                see `StateResource` for details.\n            api_response: The dictionarified `ListApiResponse` or `SummaryApiResponse`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Print the API warnings.\n\n        We print warnings for users:\n            1. when some data sources are not available\n            2. when results were truncated at the data source\n            3. when results were limited\n            4. when callsites not enabled for listing objects\n\n        Args:\n            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',\n                see `StateResource` for details.\n            api_response: The dictionarified `ListApiResponse` or `SummaryApiResponse`.", "file": "python_ray_experimental_state_api.py", "function": "StateApiClient._print_api_warning", "project": "ray", "change_stats": {"code_changes": 34, "doc_changes": 8, "total_changes": 42}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef send_alert(self, type: str = \"I\", message: str = \"\", browser_notification: t.Optional[bool] = None):\n        \n        \n        self._send_ws_alert(\n            type,\n            message,\n            self._get_app_config(\"browser_notification\", True)\n            if browser_notification is None\n            else browser_notification,\n        )```\n\nUpdated code:\n```python\ndef send_alert(\n        self,\n        type: str = \"I\",\n        message: str = \"\",\n        browser_notification: t.Optional[bool] = None,\n        duration: t.Optional[int] = None,\n    ):\n        \n        \n        self._send_ws_alert(\n            type,\n            message,\n            self._get_app_config(\"browser_notification\", True)\n            if browser_notification is None\n            else browser_notification,\n            self._get_app_config(\"notification_duration\", 3000) if duration is None else duration,\n        )\n```\n\nOriginal documentation:\n```python\nSends a notification alert to the UI.\n\n        Arguments:\n        type -- One of success, info, warning, error (or first letter of), empty string removes the alert (default: I)\n        message -- text message to display (default: empty string)\n        browser_notification -- Ask the browser to also show the notification (default: app_config[browser_notification])\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sends a notification alert to the UI.\n\n        Arguments:\n        type -- One of success, info, warning, error (or first letter of), empty string removes the alert (default: I)\n        message -- text message to display (default: empty string)\n        browser_notification -- Ask the browser to also show the notification (default: app_config[browser_notification])\n        duration -- time during which the notification is shown in ms (default: app_config[notification_duration])", "file": "taipy_gui_gui.py", "function": "Gui.send_alert", "project": "taipy", "change_stats": {"code_changes": 11, "doc_changes": 3, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_open(self):\n        \n        \n        \n        dut = self.dm_o.deembed(self.raw_1f)\n        res_calc = np.real(1/dut.y[0,0,0])\n        ind_calc = 1e9*np.imag(1/dut.y[0,0,0])/2/np.pi/dut.f\n        self.assertTrue(np.isclose(res_calc, 4, rtol=self.rtol))\n        self.assertTrue(np.isclose(ind_calc, 1, rtol=self.rtol))```\n\nUpdated code:\n```python\ndef test_open(self):\n        \n        \n        \n        dut = self.dm_o.deembed(self.raw7_1f)\n        ind_calc = 1e9*np.imag(1/dut.y[0,0,0])/2/np.pi/dut.f\n        self.assertTrue(np.isclose(ind_calc, 1, rtol=self.rtol))\n```\n\nOriginal documentation:\n```python\nAfter open de-embedding, the network is a R-L-R network with 2ohm-1nH-2ohm.\n        Test that this is true at a spot frequency.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "After de-embedding, the network is a pure inductor of 1nH.\n        Test that this is true at a spot frequency.", "file": "skrf_calibration_tests_test_deembedding.py", "function": "DeembeddingTestCase.test_open", "project": "scikit-rf", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _load_tts(self, tts_checkpoint: str, tts_config_path: str, use_cuda: bool) -> None:\n        \n        \n        # pylint: disable=global-statement\n\n        self.tts_config = load_config(tts_config_path)\n        self.use_phonemes = self.tts_config.use_phonemes\n        self.ap = AudioProcessor(verbose=False, **self.tts_config.audio)\n\n        self.tts_model = setup_tts_model(config=self.tts_config)\n        self.tts_model.load_checkpoint(self.tts_config, tts_checkpoint, eval=True)\n        if use_cuda:\n            self.tts_model.cuda()\n        self._set_tts_speaker_file()```\n\nUpdated code:\n```python\ndef _load_tts(self, tts_checkpoint: str, tts_config_path: str, use_cuda: bool) -> None:\n        \n        \n        # pylint: disable=global-statement\n\n        self.tts_config = load_config(tts_config_path)\n        self.use_phonemes = self.tts_config.use_phonemes\n        self.ap = AudioProcessor(verbose=False, **self.tts_config.audio)\n\n        self.tts_model = setup_tts_model(config=self.tts_config)\n        self.tts_model.load_checkpoint(self.tts_config, tts_checkpoint, eval=True)\n        if use_cuda:\n            self.tts_model.cuda()\n        speaker_manager = self._init_speaker_manager()\n        self.tts_model.speaker_manager = speaker_manager\n```\n\nOriginal documentation:\n```python\nLoad the TTS model.\n\n        Args:\n            tts_checkpoint (str): path to the model checkpoint.\n            tts_config_path (str): path to the model config file.\n            use_cuda (bool): enable/disable CUDA use.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load the TTS model.\n\n        1. Load the model config.\n        2. Init the AudioProcessor.\n        3. Init the model from the config.\n        4. Move the model to the GPU if CUDA is enabled.\n        5. Init the speaker manager for the model.\n\n        Args:\n            tts_checkpoint (str): path to the model checkpoint.\n            tts_config_path (str): path to the model config file.\n            use_cuda (bool): enable/disable CUDA use.", "file": "TTS_utils_synthesizer.py", "function": "Synthesizer._load_tts", "project": "TTS", "change_stats": {"code_changes": 5, "doc_changes": 8, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef icon(self):\n        \n        return ZODIAC_ICONS.get(self._state)```\n\nUpdated code:\n```python\ndef icon(self) -> str | None:\n        \n        return ZODIAC_ICONS.get(self._state)\n```\n\nOriginal documentation:\n```python\nIcon to use in the frontend, if any.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Icon to use in the frontend.", "file": "homeassistant_components_zodiac_sensor.py", "function": "ZodiacSensor.icon", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_accelerator_set_when_using_tpu(tpu_cores):\n    \n    assert isinstance(Trainer(accelerator=\"tpu\", devices=tpu_cores).accelerator, TPUAccelerator)```\n\nUpdated code:\n```python\ndef test_accelerator_set_when_using_tpu(devices):\n    \n    assert isinstance(Trainer(accelerator=\"tpu\", devices=devices).accelerator, TPUAccelerator)\n```\n\nOriginal documentation:\n```python\nTest if the accelerator is set to `tpu` when tpu_cores is not None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test if the accelerator is set to `tpu` when devices is not None.", "file": "tests_tests_pytorch_models_test_tpu.py", "function": "test_accelerator_set_when_using_tpu", "project": "pytorch-lightning", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_tf_version() -> Tuple[int, int]:\n     \n    \n    global _TF_VERS  # pylint:disable=global-statement\n    if _TF_VERS is None:\n        import tensorflow as tf  # pylint:disable=import-outside-toplevel\n        split = tf.__version__.split(\".\")[:2]\n        _TF_VERS = (int(split[0]), int(split[1]))\n    return _TF_VERS```\n\nUpdated code:\n```python\ndef get_tf_version() -> tuple[int, int]:\n     \n    \n    global _TF_VERS  # pylint:disable=global-statement\n    if _TF_VERS is None:\n        import tensorflow as tf  # pylint:disable=import-outside-toplevel\n        split = tf.__version__.split(\".\")[:2]\n        _TF_VERS = (int(split[0]), int(split[1]))\n    return _TF_VERS\n```\n\nOriginal documentation:\n```python\nObtain the major. minor version of currently installed Tensorflow.\n\n    Returns\n    -------\n    tuple[int, int]\n        A tuple of the form (major, minor) representing the version of TensorFlow that is installed\n\n    Example\n    -------\n    >>> from lib.utils import get_tf_version\n    >>> get_tf_version()\n    (2, 9)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Obtain the major. minor version of currently installed Tensorflow.\n\n    Returns\n    -------\n    tuple[int, int]\n        A tuple of the form (major, minor) representing the version of TensorFlow that is installed\n\n    Example\n    -------\n    >>> from lib.utils import get_tf_version\n    >>> get_tf_version()\n    (2, 10)", "file": "lib_utils.py", "function": "get_tf_version", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef nll(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        max_length: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        \n        assert max_length is None\n\n        batch_size = text.size(0)\n        # For data parallel\n        if max_length is None:\n            text = text[:, : text_lengths.max()]\n        else:\n            text = text[:, :max_length]\n\n        # NOTE(yifan): The first token is space when using bpe\n        text = text[:, 1:]\n        text_lengths = text_lengths - 1\n\n        # 1. Create a sentence pair like '<sos> w1 w2 w3' and 'w1 w2 w3 <eos>'\n        # text: (Batch, Length) -> x, y: (Batch, Length + 1)\n        x, x_lengths = text, text_lengths   # text already has <sos>\n        t = F.pad(text, [0, 1], \"constant\", self.ignore_id)\n        for i, l in enumerate(text_lengths):\n            t[i, l] = self.eos_id\n        t = t[:, 1:]    # remove <sos>\n\n        # 2. Forward Language model\n        # x: (Batch, Length) -> y: (Batch, Length, NVocab)\n        y, _ = self.lm(x, None)\n\n        # 3. Calc negative log likelihood\n        # nll: (BxL,)\n        nll = F.cross_entropy(y.view(-1, y.shape[-1]), t.view(-1), reduction=\"none\")\n        # nll: (BxL,) -> (BxL,)\n        if max_length is None:\n            nll.masked_fill_(make_pad_mask(x_lengths).to(nll.device).view(-1), 0.0)\n        else:\n            # nll.masked_fill_(\n            # make_pad_mask(x_lengths, maxlen=max_length + 1).to(nll.device).view(-1),\n            #     0.0,\n            # )\n            raise NotImplementedError\n        # nll: (BxL,) -> (B, L)\n        nll = nll.view(batch_size, -1)\n        return nll, x_lengths```\n\nUpdated code:\n```python\ndef nll(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        max_length: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        \n        batch_size = text.size(0)\n        # For data parallel\n        if max_length is None:\n            text = text[:, : text_lengths.max()]\n        else:\n            text = text[:, :max_length]\n\n        # 1. Create a sentence pair like '<sos> w1 w2 w3' and 'w1 w2 w3 <eos>'\n        # text: (Batch, Length) -> x, y: (Batch, Length + 1)\n        x = F.pad(text, [1, 0], \"constant\", self.eos)\n        t = F.pad(text, [0, 1], \"constant\", self.ignore_id)\n        for i, l in enumerate(text_lengths):\n            t[i, l] = self.sos\n        x_lengths = text_lengths + 1\n\n        # 2. Forward Language model\n        # x: (Batch, Length) -> y: (Batch, Length, NVocab)\n        y, _ = self.lm(x, None)\n\n        # 3. Calc negative log likelihood\n        # nll: (BxL,)\n        nll = F.cross_entropy(y.view(-1, y.shape[-1]), t.view(-1), reduction=\"none\")\n        # nll: (BxL,) -> (BxL,)\n        if max_length is None:\n            nll.masked_fill_(make_pad_mask(x_lengths).to(nll.device).view(-1), 0.0)\n        else:\n            nll.masked_fill_(\n                make_pad_mask(x_lengths, maxlen=max_length + 1).to(nll.device).view(-1),\n                0.0,\n            )\n        # nll: (BxL,) -> (B, L)\n        nll = nll.view(batch_size, -1)\n        return nll, x_lengths\n```\n\nOriginal documentation:\n```python\nCompute negative log likelihood (nll)\n        NOTE(yifan): We only use nll to calculate perplexity,\n            so there is no condition in each sentence.\n\n        Normally, this function is called in batchify_nll.\n        Args:\n            text: (Batch, Length)\n            text_lengths: (Batch,)\n            max_lengths: int\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute negative log likelihood(nll)\n\n        Normally, this function is called in batchify_nll.\n        Args:\n            text: (Batch, Length)\n            text_lengths: (Batch,)\n            max_lengths: int", "file": "espnet2_lm_espnet_model.py", "function": "ESPnetLanguageModel.nll", "project": "espnet", "change_stats": {"code_changes": 23, "doc_changes": 6, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(self, source=None, stream=False, **kwargs):\n        \n        \n        \n        return self.predict(source, stream, **kwargs)```\n\nUpdated code:\n```python\ndef __call__(\n        self,\n        source: Union[str, Path, int, list, tuple, PIL.Image.Image, np.ndarray, torch.Tensor] = None,\n        stream: bool = False,\n        **kwargs,\n    ) -> list:\n        \n        \n        \n        return self.predict(source, stream, **kwargs)\n```\n\nOriginal documentation:\n```python\nAn alias for the predict method, enabling the model instance to be callable.\n\n        This method simplifies the process of making predictions by allowing the model instance to be called directly\n        with the required arguments for prediction.\n\n        Args:\n            source (str | int | PIL.Image | np.ndarray, optional): The source of the image for making predictions.\n                Accepts various types, including file paths, URLs, PIL images, and numpy arrays. Defaults to None.\n            stream (bool, optional): If True, treats the input source as a continuous stream for predictions.\n                Defaults to False.\n            **kwargs (dict): Additional keyword arguments for configuring the prediction process.\n\n        Returns:\n            (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "An alias for the predict method, enabling the model instance to be callable.\n\n        This method simplifies the process of making predictions by allowing the model instance to be called directly\n        with the required arguments for prediction.\n\n        Args:\n            source (str | Path | int | PIL.Image | np.ndarray, optional): The source of the image for making\n                predictions. Accepts various types, including file paths, URLs, PIL images, and numpy arrays.\n                Defaults to None.\n            stream (bool, optional): If True, treats the input source as a continuous stream for predictions.\n                Defaults to False.\n            **kwargs (dict): Additional keyword arguments for configuring the prediction process.\n\n        Returns:\n            (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.", "file": "ultralytics_engine_model.py", "function": "Model.__call__", "project": "ultralytics", "change_stats": {"code_changes": 9, "doc_changes": 7, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _guild_operations_boss_preparation(self, skip_first_screenshot=True):\n        \n        \n        \n        # Ensure in dispatch for Guild Raid Boss\n        self.ui_click(GUILD_BOSS_ENTER, check_button=GUILD_DISPATCH_RECOMMEND_2, skip_first_screenshot=True)\n\n        # If configured, auto recommend fleet composition\n        if self.config.ENABLE_GUILD_OPERATIONS_BOSS_RECOMMEND:\n            self.device.click(GUILD_DISPATCH_RECOMMEND_2)\n\n        is_loading = False\n        empty_timeout = Timer(3, count=6)\n        dispatch_count = 0\n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if self.appear(GUILD_DISPATCH_EMPTY_2):\n                # Account for loading lag especially if using\n                # guild support\n                if not empty_timeout.started():\n                    empty_timeout.reset()\n                    continue\n                elif empty_timeout.reached():\n                    logger.warning('Fleet composition is empty, cannot auto-battle Guild Raid Boss')\n                    return False\n\n            if self.appear(GUILD_DISPATCH_FLEET, interval=3):\n                # Button does not appear greyed out even\n                # when empty fleet composition\n                if not self.appear(GUILD_DISPATCH_EMPTY_2):\n                    if dispatch_count < 3:\n                        self.device.click(GUILD_DISPATCH_FLEET)\n                        dispatch_count += 1\n                    else:\n                        logger.warning('Fleet cannot be dispatched for auto-battle Guild Raid Boss, verify composition manually')\n                        return False\n                continue\n\n            # Only print once when detected\n            if not is_loading:\n                if self.is_combat_loading():\n                    is_loading = True\n                continue\n\n            if self.handle_combat_automation_confirm():\n                continue\n\n            # End\n            if self.is_combat_executing():\n                return True```\n\nUpdated code:\n```python\ndef _guild_operations_boss_preparation(self, az, skip_first_screenshot=True):\n        \n        \n        \n        # Ensure in dispatch for Guild Raid Boss\n        self.ui_click(GUILD_BOSS_ENTER, check_button=GUILD_DISPATCH_RECOMMEND_2, skip_first_screenshot=True)\n\n        # If configured, auto recommend fleet composition\n        if self.config.ENABLE_GUILD_OPERATIONS_BOSS_RECOMMEND:\n            self.device.click(GUILD_DISPATCH_RECOMMEND_2)\n\n        is_loading = False\n        empty_timeout = Timer(3, count=6)\n        dispatch_count = 0\n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if self.appear(GUILD_DISPATCH_EMPTY_2):\n                # Account for loading lag especially if using\n                # guild support\n                if not empty_timeout.started():\n                    empty_timeout.reset()\n                    continue\n                elif empty_timeout.reached():\n                    logger.warning('Fleet composition is empty, cannot auto-battle Guild Raid Boss')\n                    return False\n\n            if self.appear(GUILD_DISPATCH_FLEET, interval=3):\n                # Button does not appear greyed out even\n                # when empty fleet composition\n                if not self.appear(GUILD_DISPATCH_EMPTY_2):\n                    if dispatch_count < 3:\n                        self.device.click(GUILD_DISPATCH_FLEET)\n                        dispatch_count += 1\n                    else:\n                        logger.warning(\n                            'Fleet cannot be dispatched for auto-battle Guild Raid Boss, verify composition manually')\n                        return False\n                continue\n\n            # Only print once when detected\n            if not is_loading:\n                if az.is_combat_loading():\n                    is_loading = True\n                continue\n\n            if az.handle_combat_automation_confirm():\n                continue\n\n            # End\n            if az.is_combat_executing():\n                return True\n```\n\nOriginal documentation:\n```python\nExecute preperation sequence for guild raid boss\n\n        Pages:\n            in: GUILD_OPERATIONS_BOSS\n            out: IN_BATTLE\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Execute preparation sequence for guild raid boss\n\n        az is a GuildCombat instance to handle combat various\n        interfaces. Independently created to avoid conflicts\n        or override methods of parent/child objects\n\n        Pages:\n            in: GUILD_OPERATIONS_BOSS\n            out: IN_BATTLE", "file": "module_guild_operations.py", "function": "GuildOperations._guild_operations_boss_preparation", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 13, "doc_changes": 8, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef make_pad_mask(lengths, xs=None, length_dim=-1):\n    \n\n    \n    if length_dim == 0:\n        raise ValueError(\"length_dim cannot be 0: {}\".format(length_dim))\n\n    if not isinstance(lengths, list):\n        lengths = lengths.tolist()\n    bs = int(len(lengths))\n    if xs is None:\n        maxlen = int(max(lengths))\n    else:\n        maxlen = xs.size(length_dim)\n\n    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n\n    if xs is not None:\n        assert xs.size(0) == bs, (xs.size(0), bs)\n\n        if length_dim < 0:\n            length_dim = xs.dim() + length_dim\n        # ind = (:, None, ..., None, :, , None, ..., None)\n        ind = tuple(\n            slice(None) if i in (0, length_dim) else None for i in range(xs.dim())\n        )\n        mask = mask[ind].expand_as(xs).to(xs.device)\n    return mask```\n\nUpdated code:\n```python\ndef make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None):\n    \n\n    \n    if length_dim == 0:\n        raise ValueError(\"length_dim cannot be 0: {}\".format(length_dim))\n\n    if not isinstance(lengths, list):\n        lengths = lengths.tolist()\n    bs = int(len(lengths))\n    if maxlen is None:\n        if xs is None:\n            maxlen = int(max(lengths))\n        else:\n            maxlen = xs.size(length_dim)\n    else:\n        assert xs is None\n        assert maxlen >= int(max(lengths))\n\n    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n\n    if xs is not None:\n        assert xs.size(0) == bs, (xs.size(0), bs)\n\n        if length_dim < 0:\n            length_dim = xs.dim() + length_dim\n        # ind = (:, None, ..., None, :, , None, ..., None)\n        ind = tuple(\n            slice(None) if i in (0, length_dim) else None for i in range(xs.dim())\n        )\n        mask = mask[ind].expand_as(xs).to(xs.device)\n    return mask\n```\n\nOriginal documentation:\n```python\nMake mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Make mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)", "file": "espnet_nets_pytorch_backend_nets_utils.py", "function": "make_pad_mask", "project": "espnet", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sample_n(self, n):\n        \n        \n        \n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef sample_n(self, n):\n        \n        \n        \n        warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n        return self.sample((n,))\n```\n\nOriginal documentation:\n```python\nGenerates n samples or n batches of samples if the distribution parameters\n        are batched.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates n samples or n batches of samples if the distribution\n        parameters are batched.", "file": "torch_distributions_distribution.py", "function": "Distribution.sample_n", "project": "pytorch", "change_stats": {"code_changes": 5, "doc_changes": 6, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef format_finger_print(fingerprint):\n        \n        \n        \n        formated_fingerprint = fingerprint.replace(':', '')\n        return formated_fingerprint.lower()```\n\nUpdated code:\n```python\ndef format_finger_print(fingerprint):\n        \n        \n        \n        formatted_fingerprint = fingerprint.replace(':', '')\n        return formatted_fingerprint.lower()\n```\n\nOriginal documentation:\n```python\nFormates a fingerprint.\n\n        :param fingerprint: fingerprint\n        :type: string\n\n        :returns: Formated fingerprint\n        :rtype: string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Formats a fingerprint.\n\n        :param fingerprint: fingerprint\n        :type: string\n\n        :returns: Formatted fingerprint\n        :rtype: string", "file": "src_onelogin_saml2_utils.py", "function": "OneLogin_Saml2_Utils.format_finger_print", "project": "python3-saml", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef emit_Flatten(self, IR_node):\n        \n        \n        \n        input_name, output_name = self._get_in_out_names(IR_node)\n\n        # blob_order == 0 if the input blob needs not be rearranged\n        # blob_order == 1 if the input blob needs to be rearranged\n        # Kit TODO: set blob_order\n        blob_order = 1\n        '''\n        # using keras_layer.input.shape have a \"?\" (Dimension[None] at the front),\n        # making a 3D tensor with unknown batch size 4D\n        if len(keras_layer.input.shape) == 4:\n            blob_order = 1\n        '''\n\n        self.builder.add_flatten(name = IR_node.name, mode = blob_order, input_name = input_name, output_name = output_name)```\n\nUpdated code:\n```python\ndef emit_Flatten(self, IR_node):\n        \n        \n        \n        # Get input and output names\n        input_name = self.IR_graph.get_node(IR_node.in_edges[0]).real_name\n        output_name = IR_node.out_edges[0]\n\n        \n        # blob_order == 0 if the input blob needs not be rearranged\n        # blob_order == 1 if the input blob needs to be rearranged\n        blob_order = 0\n\n        # using keras_layer.input.shape have a \"?\" (Dimension[None] at the front),\n        # making a 3D tensor with unknown batch size 4D\n        if len(keras_layer.input.shape) == 4:\n            blob_order = 1\n        \n\n        self.builder.add_flatten(name=IR_node.name, mode=1,\n                                 input_name=input_name, output_name=output_name)\n```\n\nOriginal documentation:\n```python\nConvert a flatten layer from keras to coreml.\n        ----------\n        Parameters\n        keras_layer: layer\n            A keras layer object.\n\n        builder: NeuralNetworkBuilder\n            A neural network builder object.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert a flatten layer from keras to coreml.", "file": "mmdnn_conversion_coreml_coreml_emitter.py", "function": "CoreMLEmitter.emit_Flatten", "project": "MMdnn", "change_stats": {"code_changes": 17, "doc_changes": 9, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _invoke_action(action_db, runnertype_db, params, context=None, additional_contexts=None):\n        \n        \n        \n        # TODO: Re-use the same code path as we use in action executions API endpoint\n        action_ref = action_db.ref\n        runnertype_db = action_utils.get_runnertype_by_name(action_db.runner_type['name'])\n\n        liveaction_db = LiveActionDB(action=action_ref, context=context, parameters=params)\n\n        try:\n            liveaction_db.parameters = param_utils.render_live_params(\n                runner_parameters=runnertype_db.runner_parameters,\n                action_parameters=action_db.parameters,\n                params=liveaction_db.parameters,\n                action_context=liveaction_db.context,\n                additional_contexts=additional_contexts)\n        except param_exc.ParamException as e:\n            # We still need to create a request, so liveaction_db is assigned an ID\n            liveaction_db, execution_db = action_service.create_request(liveaction_db)\n\n            # By this point the execution is already in the DB therefore need to mark it failed.\n            _, e, tb = sys.exc_info()\n            action_service.update_status(\n                liveaction=liveaction_db,\n                new_status=action_constants.LIVEACTION_STATUS_FAILED,\n                result={'error': str(e), 'traceback': ''.join(traceback.format_tb(tb, 20))})\n\n            # Might be a good idea to return the actual ActionExecution rather than bubble up\n            # the exception.\n            raise validation_exc.ValueValidationException(str(e))\n\n        liveaction_db, execution_db = action_service.request(liveaction_db)\n\n        return execution_db```\n\nUpdated code:\n```python\ndef _invoke_action(self, action_db, runnertype_db, params, context=None,\n                       additional_contexts=None):\n        \n        \n        \n        action_ref = action_db.ref\n        runnertype_db = action_utils.get_runnertype_by_name(action_db.runner_type['name'])\n\n        liveaction_db = LiveActionDB(action=action_ref, context=context, parameters=params)\n\n        try:\n            liveaction_db.parameters = self.get_resolved_parameters(\n                runnertype_db=runnertype_db,\n                action_db=action_db,\n                params=liveaction_db.parameters,\n                context=liveaction_db.context,\n                additional_contexts=additional_contexts)\n        except param_exc.ParamException as e:\n            # We still need to create a request, so liveaction_db is assigned an ID\n            liveaction_db, execution_db = action_service.create_request(liveaction_db)\n\n            # By this point the execution is already in the DB therefore need to mark it failed.\n            _, e, tb = sys.exc_info()\n            action_service.update_status(\n                liveaction=liveaction_db,\n                new_status=action_constants.LIVEACTION_STATUS_FAILED,\n                result={'error': str(e), 'traceback': ''.join(traceback.format_tb(tb, 20))})\n\n            # Might be a good idea to return the actual ActionExecution rather than bubble up\n            # the exception.\n            raise validation_exc.ValueValidationException(str(e))\n\n        liveaction_db, execution_db = action_service.request(liveaction_db)\n\n        return execution_db\n```\n\nOriginal documentation:\n```python\nSchedule an action execution.\n\n        :type action_exec_spec: :class:`ActionExecutionSpecDB`\n\n        :param params: Partially rendered parameters to execute the action with.\n        :type params: ``dict``\n\n        :rtype: :class:`LiveActionDB` on successful schedueling, None otherwise.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Schedule an action execution.\n\n        :type action_exec_spec: :class:`ActionExecutionSpecDB`\n\n        :param params: Partially rendered parameters to execute the action with.\n        :type params: ``dict``\n\n        :rtype: :class:`LiveActionDB` on successful scheduling, None otherwise.", "file": "st2reactor_st2reactor_rules_enforcer.py", "function": "RuleEnforcer._invoke_action", "project": "st2", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef crossover(\n        self,\n        parents_params: np.ndarray,\n        rng: np.random.RandomState,\n        study: Study,\n        search_space: Dict[str, BaseDistribution],\n    ) -> np.ndarray:\n        \n        \n\n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef crossover(\n        self,\n        parents_params: np.ndarray,\n        rng: np.random.RandomState,\n        study: Study,\n        search_space_bounds: np.ndarray,\n    ) -> np.ndarray:\n        \n        \n\n        raise NotImplementedError\n```\n\nOriginal documentation:\n```python\nPerform crossover of selected parent individuals.\n\n        This method is called in :func:`~optuna.samplers.NSGAIISampler.sample_relative`.\n\n        Args:\n            parents_params:\n                A ``numpy.ndarray`` with dimensions ``num_parents x num_parameters``.\n                Represents a parameter space for each parent individual. This space is\n                continuous for numerical parameters and discrete for categorical parameters.\n            rng:\n                An instance of ``numpy.random.RandomState``.\n            study:\n                Target study object.\n            search_space:\n                The subspace of original search space returned by\n                :func:`~optuna.samplers.BaseSampler.infer_relative_search_space`,\n                containing either only categorical or numerical distributions.\n\n        Returns:\n            A 1-dimensional ``numpy.ndarray`` containing new parameter combination.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform crossover of selected parent individuals.\n\n        This method is called in :func:`~optuna.samplers.NSGAIISampler.sample_relative`.\n\n        Args:\n            parents_params:\n                A ``numpy.ndarray`` with dimensions ``num_parents x num_parameters``.\n                Represents a parameter space for each parent individual. This space is\n                continuous for numerical parameters.\n            rng:\n                An instance of ``numpy.random.RandomState``.\n            study:\n                Target study object.\n            search_space_bounds:\n                A ``numpy.ndarray`` with dimensions ``len_search_space x 2`` representing\n                numerical distribution bounds constructed from transformed search space.\n\n        Returns:\n            A 1-dimensional ``numpy.ndarray`` containing new parameter combination.", "file": "optuna_samplers_nsgaii__crossovers__base.py", "function": "BaseCrossover.crossover", "project": "optuna", "change_stats": {"code_changes": 4, "doc_changes": 11, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_m2m_with_unicode_reference(self):\n        \n        \n        \n        m1 = UnicodeReferenceModel.objects.create()\n        m2 = UnicodeReferenceModel.objects.create()\n        m2.others.add(m1)  # used to cause an error (see ticket #6045)\n        m2.save()\n        list(m2.others.all())```\n\nUpdated code:\n```python\ndef test_m2m_with_unicode_reference(self):\n        \n        \n        \n        m1 = StringReferenceModel.objects.create()\n        m2 = StringReferenceModel.objects.create()\n        m2.others.add(m1)  # used to cause an error (see ticket #6045)\n        m2.save()\n        list(m2.others.all())\n```\n\nOriginal documentation:\n```python\nRegression test for #6045: references to other models can be unicode\n        strings, providing they are directly convertible to ASCII.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Regression test for #6045: references to other models can be\n        strings, providing they are directly convertible to ASCII.", "file": "tests_m2m_and_m2o_tests.py", "function": "RelatedObjectUnicodeTests.test_m2m_with_unicode_reference", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_event(self, t: dict):\n        \n        \n        \n        if not isinstance(t, dict):\n            raise ValueError('Event must be a dictionary')\n        self.thoughts.append(t)```\n\nUpdated code:\n```python\ndef add_event(self, t: dict):\n        \n        \n        \n        if not isinstance(t, dict):\n            raise AgentEventTypeError()\n        self.thoughts.append(t)\n```\n\nOriginal documentation:\n```python\nAdds an event to memory if it is a valid event.\n\n        Parameters:\n        - t (dict): The thought that we want to add to memory\n\n        Raises:\n        - ValueError: If t is not a dict\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adds an event to memory if it is a valid event.\n\n        Parameters:\n        - t (dict): The thought that we want to add to memory\n\n        Raises:\n        - AgentEventTypeError: If t is not a dict", "file": "agenthub_monologue_agent_utils_monologue.py", "function": "Monologue.add_event", "project": "OpenDevin", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _match(self, similarity_matrix, **params):\n    \n    \n    pass```\n\nUpdated code:\n```python\ndef _match(self, similarity_matrix, valid_rows):\n    \n    \n    pass\n```\n\nOriginal documentation:\n```python\nMethod to be overridden by implementations.\n\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n        where higher value means more similar.\n      **params: Additional keyword arguments for specific implementations of\n        the Matcher.\n\n    Returns:\n      match_results: Integer tensor of shape [M]: match_results[i]>=0 means\n        that column i is matched to row match_results[i], match_results[i]=-1\n        means that the column is not matched. match_results[i]=-2 means that\n        the column is ignored (usually this happens when there is a very weak\n        match which one neither wants as positive nor negative example).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Method to be overridden by implementations.\n\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n        where higher value means more similar.\n      valid_rows: A boolean tensor of shape [N] indicating the rows that are\n        valid for matching.\n    Returns:\n      match_results: Integer tensor of shape [M]: match_results[i]>=0 means\n        that column i is matched to row match_results[i], match_results[i]=-1\n        means that the column is not matched. match_results[i]=-2 means that\n        the column is ignored (usually this happens when there is a very weak\n        match which one neither wants as positive nor negative example).", "file": "research_object_detection_core_matcher.py", "function": "Matcher._match", "project": "models", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_kubeconfig(token, ca, master_ip, filename, user):\n    \n    \n    \n    snap_path = os.environ.get(\"SNAP\")\n    config_template = \"{}/{}\".format(snap_path, \"kubelet.config.template\")\n    config = \"{}/credentials/{}\".format(snapdata_path, filename)\n    shutil.copyfile(config, \"{}.backup\".format(config))\n    try_set_file_permissions(\"{}.backup\".format(config))\n    ca_line = ca_one_line(ca)\n    with open(config_template, \"r\") as tfp:\n        with open(config, \"w+\") as fp:\n            config_txt = tfp.read()\n            config_txt = config_txt.replace(\"CADATA\", ca_line)\n            config_txt = config_txt.replace(\"NAME\", user)\n            config_txt = config_txt.replace(\"TOKEN\", token)\n            config_txt = config_txt.replace(\"127.0.0.1\", master_ip)\n            fp.write(config_txt)\n        try_set_file_permissions(config)```\n\nUpdated code:\n```python\ndef create_kubeconfig(token, ca, master_ip, api_port, filename, user):\n    \n    \n    \n    snap_path = os.environ.get(\"SNAP\")\n    config_template = \"{}/{}\".format(snap_path, \"kubelet.config.template\")\n    config = \"{}/credentials/{}\".format(snapdata_path, filename)\n    shutil.copyfile(config, \"{}.backup\".format(config))\n    try_set_file_permissions(\"{}.backup\".format(config))\n    ca_line = ca_one_line(ca)\n    with open(config_template, \"r\") as tfp:\n        with open(config, \"w+\") as fp:\n            config_txt = tfp.read()\n            config_txt = config_txt.replace(\"CADATA\", ca_line)\n            config_txt = config_txt.replace(\"NAME\", user)\n            config_txt = config_txt.replace(\"TOKEN\", token)\n            config_txt = config_txt.replace(\"127.0.0.1\", master_ip)\n            config_txt = config_txt.replace(\"16443\", api_port)\n            fp.write(config_txt)\n        try_set_file_permissions(config)\n```\n\nOriginal documentation:\n```python\nCreate a kubeconfig file. The file in stored under credentials named after the user\n\n    :param token: the token to be in the kubeconfig\n    :param ca: the ca\n    :param master_ip: the master node IP\n    :param filename: the name of the config file\n    :param user: the user to use al login\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a kubeconfig file. The file in stored under credentials named after the user\n\n    :param token: the token to be in the kubeconfig\n    :param ca: the ca\n    :param master_ip: the master node IP\n    :param api_port: the API server port\n    :param filename: the name of the config file\n    :param user: the user to use al login", "file": "scripts_wrappers_join.py", "function": "create_kubeconfig", "project": "microk8s", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sample(\n    obj_len: int,\n    size: int,\n    replace: bool,\n    weights: np.ndarray | None,\n    random_state: np.random.RandomState,\n) -> np.ndarray:\n    \n    \n    \n    if weights is not None:\n        weight_sum = weights.sum()\n        if weight_sum != 0:\n            weights = weights / weight_sum\n        else:\n            raise ValueError(\"Invalid weights: weights sum to zero\")\n\n    return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n        np.intp, copy=False\n    )```\n\nUpdated code:\n```python\ndef sample(\n    obj_len: int,\n    size: int,\n    replace: bool,\n    weights: np.ndarray | None,\n    random_state: np.random.RandomState | np.random.Generator,\n) -> np.ndarray:\n    \n    \n    \n    if weights is not None:\n        weight_sum = weights.sum()\n        if weight_sum != 0:\n            weights = weights / weight_sum\n        else:\n            raise ValueError(\"Invalid weights: weights sum to zero\")\n\n    return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n        np.intp, copy=False\n    )\n```\n\nOriginal documentation:\n```python\nRandomly sample `size` indices in `np.arange(obj_len)`\n\n    Parameters\n    ----------\n    obj_len : int\n        The length of the indices being considered\n    size : int\n        The number of values to choose\n    replace : bool\n        Allow or disallow sampling of the same row more than once.\n    weights : np.ndarray[np.float64] or None\n        If None, equal probability weighting, otherwise weights according\n        to the vector normalized\n    random_state: np.random.RandomState\n        State used for the random sampling\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Randomly sample `size` indices in `np.arange(obj_len)`\n\n    Parameters\n    ----------\n    obj_len : int\n        The length of the indices being considered\n    size : int\n        The number of values to choose\n    replace : bool\n        Allow or disallow sampling of the same row more than once.\n    weights : np.ndarray[np.float64] or None\n        If None, equal probability weighting, otherwise weights according\n        to the vector normalized\n    random_state: np.random.RandomState or np.random.Generator\n        State used for the random sampling\n\n    Returns\n    -------\n    np.ndarray[np.intp]", "file": "pandas_core_sample.py", "function": "sample", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _ixs(self, i, axis=0):\n        \n        \n        \n        try:\n\n            # dispatch to the values if we need\n            values = self._values\n            if isinstance(values, np.ndarray):\n                return libindex.get_value_at(values, i)\n            else:\n                return values[i]\n        except IndexError:\n            raise\n        except Exception:\n            if isinstance(i, slice):\n                indexer = self.index._convert_slice_indexer(i, kind=\"iloc\")\n                return self._get_values(indexer)\n            else:\n                label = self.index[i]\n                if isinstance(label, Index):\n                    return self.take(i, axis=axis, convert=True)\n                else:\n                    return libindex.get_value_at(self, i)```\n\nUpdated code:\n```python\ndef _ixs(self, i: int, axis: int = 0):\n        \n        \n        \n\n        # dispatch to the values if we need\n        values = self._values\n        if isinstance(values, np.ndarray):\n            return libindex.get_value_at(values, i)\n        else:\n            return values[i]\n```\n\nOriginal documentation:\n```python\nReturn the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int, slice, or sequence of integers\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)", "file": "pandas_core_series.py", "function": "Series._ixs", "project": "pandas", "change_stats": {"code_changes": 29, "doc_changes": 4, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_title_case(text: str) -> str:\n    \n    \n    return \"\".join(word.capitalize() for word in text.split(\"_\"))```\n\nUpdated code:\n```python\ndef to_title_case(text: str, sep: str = \"\") -> str:\n    \n    \n    return sep.join(word.title() for word in text.split(\"_\"))\n```\n\nOriginal documentation:\n```python\nConvert a string from snake case to title case.\n\n    Args:\n        text: The string to convert.\n\n    Returns:\n        The title case string.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert a string from snake case to title case.\n\n    Args:\n        text: The string to convert.\n        sep: The separator to use to join the words.\n\n    Returns:\n        The title case string.", "file": "reflex_utils_format.py", "function": "to_title_case", "project": "reflex", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef execute_sell(self, trade: Trade, limit: float) -> None:\n        \n        \n        \n        # Execute sell and update trade record\n        order_id = self.exchange.sell(str(trade.pair), limit, trade.amount)['id']\n        trade.open_order_id = order_id\n        trade.close_rate_requested = limit\n\n        profit_trade = trade.calc_profit(rate=limit)\n        current_rate = self.exchange.get_ticker(trade.pair)['bid']\n        profit_percent = trade.calc_profit_percent(limit)\n        pair_url = self.exchange.get_pair_detail_url(trade.pair)\n        gain = \"profit\" if profit_percent > 0 else \"loss\"\n\n        msg = {\n            'type': RPCMessageType.SELL_NOTIFICATION,\n            'exchange': trade.exchange.capitalize(),\n            'pair': trade.pair,\n            'gain': gain,\n            'market_url': pair_url,\n            'limit': limit,\n            'amount': trade.amount,\n            'open_rate': trade.open_rate,\n            'current_rate': current_rate,\n            'profit_amount': profit_trade,\n            'profit_percent': profit_percent,\n        }\n\n        # For regular case, when the configuration exists\n        if 'stake_currency' in self.config and 'fiat_display_currency' in self.config:\n            stake_currency = self.config['stake_currency']\n            fiat_currency = self.config['fiat_display_currency']\n            msg.update({\n                'stake_currency': stake_currency,\n                'fiat_currency': fiat_currency,\n            })\n\n        # Send the message\n        self.rpc.send_msg(msg)\n        Trade.session.flush()```\n\nUpdated code:\n```python\ndef execute_sell(self, trade: Trade, limit: float, sell_reason: SellType) -> None:\n        \n        \n        \n        # Execute sell and update trade record\n        order_id = self.exchange.sell(str(trade.pair), limit, trade.amount)['id']\n        trade.open_order_id = order_id\n        trade.close_rate_requested = limit\n        trade.sell_reason = sell_reason.value\n\n        profit_trade = trade.calc_profit(rate=limit)\n        current_rate = self.exchange.get_ticker(trade.pair)['bid']\n        profit_percent = trade.calc_profit_percent(limit)\n        pair_url = self.exchange.get_pair_detail_url(trade.pair)\n        gain = \"profit\" if profit_percent > 0 else \"loss\"\n\n        msg = {\n            'type': RPCMessageType.SELL_NOTIFICATION,\n            'exchange': trade.exchange.capitalize(),\n            'pair': trade.pair,\n            'gain': gain,\n            'market_url': pair_url,\n            'limit': limit,\n            'amount': trade.amount,\n            'open_rate': trade.open_rate,\n            'current_rate': current_rate,\n            'profit_amount': profit_trade,\n            'profit_percent': profit_percent,\n        }\n\n        # For regular case, when the configuration exists\n        if 'stake_currency' in self.config and 'fiat_display_currency' in self.config:\n            stake_currency = self.config['stake_currency']\n            fiat_currency = self.config['fiat_display_currency']\n            msg.update({\n                'stake_currency': stake_currency,\n                'fiat_currency': fiat_currency,\n            })\n\n        # Send the message\n        self.rpc.send_msg(msg)\n        Trade.session.flush()\n```\n\nOriginal documentation:\n```python\nExecutes a limit sell for the given trade and limit\n        :param trade: Trade instance\n        :param limit: limit rate for the sell order\n        :return: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Executes a limit sell for the given trade and limit\n        :param trade: Trade instance\n        :param limit: limit rate for the sell order\n        :param sellreason: Reason the sell was triggered\n        :return: None", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.execute_sell", "project": "freqtrade", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(self, gradients):\n        \n        if not self._gradients:\n            self._gradients.extend(\n                [\n                    tf.Variable(tf.zeros_like(gradient), trainable=False) if gradient is not None else gradient\n                    for gradient in gradients\n                ]\n            )\n\n        if len(gradients) != len(self._gradients):\n            raise ValueError(\"Expected %s gradients, but got %d\" % (len(self._gradients), len(gradients)))\n\n        for accum_gradient, gradient in zip(self._get_replica_gradients(), gradients):\n            if accum_gradient is not None and gradient is not None:\n                accum_gradient.assign_add(gradient)\n\n        self._accum_steps.assign_add(1)```\n\nUpdated code:\n```python\ndef __call__(self, gradients):\n        \n        if not self._gradients:\n            _ = self.step  # Create the step variable.\n            self._gradients.extend(\n                [\n                    tf.Variable(\n                        tf.zeros_like(gradient), trainable=False, synchronization=tf.VariableSynchronization.ON_READ,\n                    )\n                    for gradient in gradients\n                ]\n            )\n        if len(gradients) != len(self._gradients):\n            raise ValueError(\"Expected %s gradients, but got %d\" % (len(self._gradients), len(gradients)))\n\n        for accum_gradient, gradient in zip(self._gradients, gradients):\n            accum_gradient.assign_add(gradient)\n\n        self._accum_steps.assign_add(1)\n```\n\nOriginal documentation:\n```python\nAccumulates :obj:`gradients`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Accumulates :obj:`gradients` on the current replica.", "file": "src_transformers_optimization_tf.py", "function": "GradientAccumulator.__call__", "project": "transformers", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fix_message_rendered_content(data: TableData, field: TableName) -> None:\n    \n    \n    \n    for message in data[field]:\n        message_object = Message.objects.get(id=message['id'])\n        try:\n            rendered_content = save_message_rendered_content(message_object, message['content'])  # type: Optional[str]\n        except Exception:\n            rendered_content = None\n\n        if rendered_content is None:\n            # This can happen with two possible causes:\n            # * rendering markdown failing with the exception being caught in bugdown\n            # * The explicit None clause from an exception escaping\n            logging.warning(\"Error in markdown rendering for message ID %s; continuing\" % (message['id']))```\n\nUpdated code:\n```python\ndef fix_message_rendered_content(data: TableData, field: TableName) -> None:\n    \n    \n    \n    for message in data[field]:\n        message_object = Message.objects.get(id=message['id'])\n        if message_object.rendered_content is not None:\n            # For Zulip->Zulip imports, we use the original rendered markdown.\n            continue\n\n        try:\n            rendered_content = save_message_rendered_content(message_object, message['content'])  # type: Optional[str]\n        except Exception:\n            rendered_content = None\n\n        if rendered_content is None:\n            # This can happen with two possible causes:\n            # * rendering markdown failing with the exception being caught in bugdown\n            # * The explicit None clause from an exception escaping\n            logging.warning(\"Error in markdown rendering for message ID %s; continuing\" % (message['id']))\n```\n\nOriginal documentation:\n```python\nThis function sets the rendered_content of all the messages\n    after the messages have been imported.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This function sets the rendered_content of all the messages\n    after the messages have been imported from a non-Zulip platform.", "file": "zerver_lib_import_realm.py", "function": "fix_message_rendered_content", "project": "zulip", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fsdp_modules(\n        module: nn.Module, root_only: bool = False\n    ) -> List[\"FullyShardedDataParallel\"]:\n        \n        \n        \n        fsdp_modules = []\n        for sub_module in module.modules():\n            if isinstance(sub_module, FullyShardedDataParallel):\n                if not root_only or sub_module.check_is_root():\n                    fsdp_modules.append(sub_module)\n\n        return fsdp_modules```\n\nUpdated code:\n```python\ndef fsdp_modules(\n        module: nn.Module,\n        root_only: bool = False,\n    ) -> List[\"FullyShardedDataParallel\"]:\n        \n        \n        \n        return [\n            submodule for submodule in module.modules()\n            if isinstance(submodule, FullyShardedDataParallel) and\n            (not root_only or submodule.check_is_root())\n        ]\n```\n\nOriginal documentation:\n```python\nHelper function to return all nested FSDP instances, including self.\n\n        Args:\n            module: the root module. This module does not have to be a FSDP module.\n            root_only: whether to return only root FSDP modules (default: False).\n\n        Returns:\n            fsdp_modules: the FSDP modules that are nested in the input module.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns all nested FSDP instances, possibly including ``module`` itself\n        and only including FSDP root modules if ``root_only=True``.\n\n        Args:\n            module (torch.nn.Module): Root module, which may or may not be an\n                ``FSDP`` module.\n            root_only (bool): Whether to return only FSDP root modules.\n                (Default: ``False``)\n\n        Returns:\n            List[FullyShardedDataParallel]: FSDP modules that are nested in\n            the input ``module``.", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel.fsdp_modules", "project": "pytorch", "change_stats": {"code_changes": 17, "doc_changes": 14, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self):\n                \n        \n\n        self.root_api_url = \"https://api.github.com/repos/danielmiessler/fabric/contents/patterns?ref=main\"\n        self.config_directory = os.path.expanduser(\"~/.config/fabric\")\n        self.pattern_directory = os.path.join(\n            self.config_directory, \"patterns\")\n        os.makedirs(self.pattern_directory, exist_ok=True)\n        self.update_patterns()```\n\nUpdated code:\n```python\ndef __init__(self):\n        \n        self.repo_zip_url = \"https://github.com/danielmiessler/fabric/archive/refs/heads/main.zip\"\n        self.config_directory = os.path.expanduser(\"~/.config/fabric\")\n        self.pattern_directory = os.path.join(\n            self.config_directory, \"patterns\")\n        os.makedirs(self.pattern_directory, exist_ok=True)\n        self.update_patterns()\n```\n\nOriginal documentation:\n```python\nInitialize the object with default values and update patterns.\n\n        This method initializes the object with default values for root_api_url, config_directory, and pattern_directory.\n        It then creates the pattern_directory if it does not exist and calls the update_patterns method to update the patterns.\n\n        Raises:\n            OSError: If there is an issue creating the pattern_directory.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the object with default values.", "file": "installer_client_cli_utils.py", "function": "Update.__init__", "project": "fabric", "change_stats": {"code_changes": 6, "doc_changes": 10, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef has_same_digits(num1: int, num2: int) -> bool:\n    \n    \n    \n    digits = [0] * 10\n\n    while num1 > 0 and num2 > 0:\n        digits[num1 % 10] += 1\n        digits[num2 % 10] -= 1\n        num1 //= 10\n        num2 //= 10\n\n    for digit in digits:\n        if digit != 0:\n            return False\n\n    return True```\n\nUpdated code:\n```python\ndef has_same_digits(num1: int, num2: int) -> bool:\n    \n    \n    \n    return sorted(str(num1)) == sorted(str(num2))\n```\n\nOriginal documentation:\n```python\nReturn True if num1 and num2 have the same frequency of every digit, False\n    otherwise.\n\n    digits[] is a frequency table where the index represents the digit from\n    0-9, and the element stores the number of appearances. Increment the\n    respective index every time you see the digit in num1, and decrement if in\n    num2. At the end, if the numbers have the same digits, every index must\n    contain 0.\n\n    >>> has_same_digits(123456789, 987654321)\n    True\n\n    >>> has_same_digits(123, 12)\n    False\n\n    >>> has_same_digits(1234566, 123456)\n    False\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return True if num1 and num2 have the same frequency of every digit, False\n    otherwise.\n\n    >>> has_same_digits(123456789, 987654321)\n    True\n\n    >>> has_same_digits(123, 23)\n    False\n\n    >>> has_same_digits(1234566, 123456)\n    False", "file": "project_euler_problem_070_sol1.py", "function": "has_same_digits", "project": "Python", "change_stats": {"code_changes": 16, "doc_changes": 10, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef permutation_importance(\n    estimator,\n    X,\n    y,\n    *,\n    scoring=None,\n    n_repeats=5,\n    n_jobs=None,\n    random_state=None,\n    sample_weight=None,\n):\n    \n    \n    if not hasattr(X, \"iloc\"):\n        X = check_array(X, force_all_finite=\"allow-nan\", dtype=None)\n\n    # Precompute random seed from the random state to be used\n    # to get a fresh independent RandomState instance for each\n    # parallel call to _calculate_permutation_scores, irrespective of\n    # the fact that variables are shared or not depending on the active\n    # joblib backend (sequential, thread-based or process-based).\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(**scorers_dict)\n\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n\n    scores = Parallel(n_jobs=n_jobs)(\n        delayed(_calculate_permutation_scores)(\n            estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer\n        )\n        for col_idx in range(X.shape[1])\n    )\n\n    if isinstance(baseline_score, dict):\n        return {\n            name: _create_importances_bunch(\n                baseline_score[name],\n                # unpack the permuted scores\n                np.array([scores[col_idx][name] for col_idx in range(X.shape[1])]),\n            )\n            for name in baseline_score\n        }\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))```\n\nUpdated code:\n```python\ndef permutation_importance(\n    estimator,\n    X,\n    y,\n    *,\n    scoring=None,\n    n_repeats=5,\n    n_jobs=None,\n    random_state=None,\n    sample_weight=None,\n    max_samples=1.0,\n):\n    \n    \n    if not hasattr(X, \"iloc\"):\n        X = check_array(X, force_all_finite=\"allow-nan\", dtype=None)\n\n    # Precompute random seed from the random state to be used\n    # to get a fresh independent RandomState instance for each\n    # parallel call to _calculate_permutation_scores, irrespective of\n    # the fact that variables are shared or not depending on the active\n    # joblib backend (sequential, thread-based or process-based).\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n\n    if not isinstance(max_samples, numbers.Integral):\n        max_samples = int(max_samples * X.shape[0])\n    elif not (0 < max_samples <= X.shape[0]):\n        raise ValueError(\"max_samples must be in (0, n_samples]\")\n\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(**scorers_dict)\n\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n\n    scores = Parallel(n_jobs=n_jobs)(\n        delayed(_calculate_permutation_scores)(\n            estimator,\n            X,\n            y,\n            sample_weight,\n            col_idx,\n            random_seed,\n            n_repeats,\n            scorer,\n            max_samples,\n        )\n        for col_idx in range(X.shape[1])\n    )\n\n    if isinstance(baseline_score, dict):\n        return {\n            name: _create_importances_bunch(\n                baseline_score[name],\n                # unpack the permuted scores\n                np.array([scores[col_idx][name] for col_idx in range(X.shape[1])]),\n            )\n            for name in baseline_score\n        }\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))\n```\n\nOriginal documentation:\n```python\nPermutation importance for feature evaluation [BRE]_.\n\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\n    data set used to train the estimator or a hold-out set. The permutation\n    importance of a feature is calculated as follows. First, a baseline metric,\n    defined by :term:`scoring`, is evaluated on a (potentially different)\n    dataset defined by the `X`. Next, a feature column from the validation set\n    is permuted and the metric is evaluated again. The permutation importance\n    is defined to be the difference between the baseline metric and metric from\n    permutating the feature column.\n\n    Read more in the :ref:`User Guide <permutation_importance>`.\n\n    Parameters\n    ----------\n    estimator : object\n        An estimator that has already been :term:`fitted` and is compatible\n        with :term:`scorer`.\n\n    X : ndarray or DataFrame, shape (n_samples, n_features)\n        Data on which permutation importance will be computed.\n\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n        Targets for supervised or `None` for unsupervised.\n\n    scoring : str, callable, list, tuple, or dict, default=None\n        Scorer to use.\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        Passing multiple scores to `scoring` is more efficient than calling\n        `permutation_importance` for each of the scores as it reuses\n        predictions to avoid redundant computation.\n\n        If None, the estimator's default scorer is used.\n\n    n_repeats : int, default=5\n        Number of times to permute a feature.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel. The computation is done by computing\n        permutation score for each columns and parallelized over the columns.\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        Pseudo-random number generator to control the permutations of each\n        feature.\n        Pass an int to get reproducible results across function calls.\n        See :term: `Glossary <random_state>`.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights used in scoring.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\n        Dictionary-like object, with the following attributes.\n\n        importances_mean : ndarray of shape (n_features, )\n            Mean of feature importance over `n_repeats`.\n        importances_std : ndarray of shape (n_features, )\n            Standard deviation over `n_repeats`.\n        importances : ndarray of shape (n_features, n_repeats)\n            Raw permutation importance scores.\n\n        If there are multiple scoring metrics in the scoring parameter\n        `result` is a dict with scorer names as keys (e.g. 'roc_auc') and\n        `Bunch` objects like above as values.\n\n    References\n    ----------\n    .. [BRE] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n             2001. https://doi.org/10.1023/A:1010933404324\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.inspection import permutation_importance\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n    >>> y = [1, 1, 1, 0, 0, 0]\n    >>> clf = LogisticRegression().fit(X, y)\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\n    ...                                 random_state=0)\n    >>> result.importances_mean\n    array([0.4666..., 0.       , 0.       ])\n    >>> result.importances_std\n    array([0.2211..., 0.       , 0.       ])\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Permutation importance for feature evaluation [BRE]_.\n\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\n    data set used to train the estimator or a hold-out set. The permutation\n    importance of a feature is calculated as follows. First, a baseline metric,\n    defined by :term:`scoring`, is evaluated on a (potentially different)\n    dataset defined by the `X`. Next, a feature column from the validation set\n    is permuted and the metric is evaluated again. The permutation importance\n    is defined to be the difference between the baseline metric and metric from\n    permutating the feature column.\n\n    Read more in the :ref:`User Guide <permutation_importance>`.\n\n    Parameters\n    ----------\n    estimator : object\n        An estimator that has already been :term:`fitted` and is compatible\n        with :term:`scorer`.\n\n    X : ndarray or DataFrame, shape (n_samples, n_features)\n        Data on which permutation importance will be computed.\n\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n        Targets for supervised or `None` for unsupervised.\n\n    scoring : str, callable, list, tuple, or dict, default=None\n        Scorer to use.\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        Passing multiple scores to `scoring` is more efficient than calling\n        `permutation_importance` for each of the scores as it reuses\n        predictions to avoid redundant computation.\n\n        If None, the estimator's default scorer is used.\n\n    n_repeats : int, default=5\n        Number of times to permute a feature.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel. The computation is done by computing\n        permutation score for each columns and parallelized over the columns.\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        Pseudo-random number generator to control the permutations of each\n        feature.\n        Pass an int to get reproducible results across function calls.\n        See :term: `Glossary <random_state>`.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights used in scoring.\n\n        .. versionadded:: 0.24\n\n    max_samples : int or float, default=1.0\n        The number of samples to draw from X to compute feature importance\n        in each repeat (without replacement).\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\n          will be used.\n\n        While using this option may provide less accurate importance estimates,\n        it keeps the method tractable when evaluating feature importance on\n        large datasets. In combination with `n_repeats`, this allows to control\n        the computational speed vs statistical accuracy trade-off of this method.\n\n        .. versionadded:: 1.0\n\n    Returns\n    -------\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\n        Dictionary-like object, with the following attributes.\n\n        importances_mean : ndarray of shape (n_features, )\n            Mean of feature importance over `n_repeats`.\n        importances_std : ndarray of shape (n_features, )\n            Standard deviation over `n_repeats`.\n        importances : ndarray of shape (n_features, n_repeats)\n            Raw permutation importance scores.\n\n        If there are multiple scoring metrics in the scoring parameter\n        `result` is a dict with scorer names as keys (e.g. 'roc_auc') and\n        `Bunch` objects like above as values.\n\n    References\n    ----------\n    .. [BRE] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n             2001. https://doi.org/10.1023/A:1010933404324\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.inspection import permutation_importance\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n    >>> y = [1, 1, 1, 0, 0, 0]\n    >>> clf = LogisticRegression().fit(X, y)\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\n    ...                                 random_state=0)\n    >>> result.importances_mean\n    array([0.4666..., 0.       , 0.       ])\n    >>> result.importances_std\n    array([0.2211..., 0.       , 0.       ])", "file": "sklearn_inspection__permutation_importance.py", "function": "permutation_importance", "project": "scikit-learn", "change_stats": {"code_changes": 18, "doc_changes": 18, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef make_layout_margins(layoutgrids, fig, renderer, *, w_pad=0, h_pad=0,\n                        hspace=0, wspace=0):\n    \n    \n    \n    for sfig in fig.subfigs:  # recursively make child panel margins\n        ss = sfig._subplotspec\n        gs = ss.get_gridspec()\n\n        make_layout_margins(layoutgrids, sfig, renderer,\n                            w_pad=w_pad, h_pad=h_pad,\n                            hspace=hspace, wspace=wspace)\n\n        margins = get_margin_from_padding(sfig, w_pad=0, h_pad=0,\n                                          hspace=hspace, wspace=wspace)\n        layoutgrids[gs].edit_outer_margin_mins(margins, ss)\n\n    for ax in fig._localaxes:\n        if not ax.get_subplotspec() or not ax.get_in_layout():\n            continue\n\n        ss = ax.get_subplotspec()\n        gs = ss.get_gridspec()\n\n        if gs not in layoutgrids:\n            return\n\n        margin = get_margin_from_padding(ax, w_pad=w_pad, h_pad=h_pad,\n                                         hspace=hspace, wspace=wspace)\n        pos, bbox = get_pos_and_bbox(ax, renderer)\n        # the margin is the distance between the bounding box of the axes\n        # and its position (plus the padding from above)\n        margin['left'] += pos.x0 - bbox.x0\n        margin['right'] += bbox.x1 - pos.x1\n        # remember that rows are ordered from top:\n        margin['bottom'] += pos.y0 - bbox.y0\n        margin['top'] += bbox.y1 - pos.y1\n\n        # make margin for colorbars.  These margins go in the\n        # padding margin, versus the margin for axes decorators.\n        for cbax in ax._colorbars:\n            # note pad is a fraction of the parent width...\n            pad = colorbar_get_pad(layoutgrids, cbax)\n            # colorbars can be child of more than one subplot spec:\n            cbp_rspan, cbp_cspan = get_cb_parent_spans(cbax)\n            loc = cbax._colorbar_info['location']\n            cbpos, cbbbox = get_pos_and_bbox(cbax, renderer)\n            if loc == 'right':\n                if cbp_cspan.stop == ss.colspan.stop:\n                    # only increase if the colorbar is on the right edge\n                    margin['rightcb'] += cbbbox.width + pad\n            elif loc == 'left':\n                if cbp_cspan.start == ss.colspan.start:\n                    # only increase if the colorbar is on the left edge\n                    margin['leftcb'] += cbbbox.width + pad\n            elif loc == 'top':\n                if cbp_rspan.start == ss.rowspan.start:\n                    margin['topcb'] += cbbbox.height + pad\n            else:\n                if cbp_rspan.stop == ss.rowspan.stop:\n                    margin['bottomcb'] += cbbbox.height + pad\n            # If the colorbars are wider than the parent box in the\n            # cross direction\n            if loc in ['top', 'bottom']:\n                if (cbp_cspan.start == ss.colspan.start and\n                        cbbbox.x0 < bbox.x0):\n                    margin['left'] += bbox.x0 - cbbbox.x0\n                if (cbp_cspan.stop == ss.colspan.stop and\n                        cbbbox.x1 > bbox.x1):\n                    margin['right'] += cbbbox.x1 - bbox.x1\n            # or taller:\n            if loc in ['left', 'right']:\n                if (cbp_rspan.stop == ss.rowspan.stop and\n                        cbbbox.y0 < bbox.y0):\n                    margin['bottom'] += bbox.y0 - cbbbox.y0\n                if (cbp_rspan.start == ss.rowspan.start and\n                        cbbbox.y1 > bbox.y1):\n                    margin['top'] += cbbbox.y1 - bbox.y1\n        # pass the new margins down to the layout grid for the solution...\n        layoutgrids[gs].edit_outer_margin_mins(margin, ss)\n\n    # make margins for figure-level legends:\n    for leg in fig.legends:\n        inv_trans_fig = None\n        if leg._outside_loc and leg._bbox_to_anchor is None:\n            if inv_trans_fig is None:\n                inv_trans_fig = fig.transFigure.inverted().transform_bbox\n            bbox = inv_trans_fig(leg.get_tightbbox(renderer))\n            w = bbox.width + 2 * w_pad\n            h = bbox.height + 2 * h_pad\n            legendloc = leg._outside_loc\n            if legendloc == 'lower':\n                layoutgrids[fig].edit_margin_min('bottom', h)\n            elif legendloc == 'upper':\n                layoutgrids[fig].edit_margin_min('top', h)\n            if legendloc == 'right':\n                layoutgrids[fig].edit_margin_min('right', w)\n            elif legendloc == 'left':\n                layoutgrids[fig].edit_margin_min('left', w)```\n\nUpdated code:\n```python\ndef make_layout_margins(layoutgrids, fig, renderer, *, w_pad=0, h_pad=0,\n                        hspace=0, wspace=0):\n    \n    \n    \n    for sfig in fig.subfigs:  # recursively make child panel margins\n        ss = sfig._subplotspec\n        gs = ss.get_gridspec()\n\n        make_layout_margins(layoutgrids, sfig, renderer,\n                            w_pad=w_pad, h_pad=h_pad,\n                            hspace=hspace, wspace=wspace)\n\n        margins = get_margin_from_padding(sfig, w_pad=0, h_pad=0,\n                                          hspace=hspace, wspace=wspace)\n        layoutgrids[gs].edit_outer_margin_mins(margins, ss)\n\n    for ax in fig._localaxes:\n        if not ax.get_subplotspec() or not ax.get_in_layout():\n            continue\n\n        ss = ax.get_subplotspec()\n        gs = ss.get_gridspec()\n\n        if gs not in layoutgrids:\n            return\n\n        margin = get_margin_from_padding(ax, w_pad=w_pad, h_pad=h_pad,\n                                         hspace=hspace, wspace=wspace)\n        pos, bbox = get_pos_and_bbox(ax, renderer)\n        # the margin is the distance between the bounding box of the Axes\n        # and its position (plus the padding from above)\n        margin['left'] += pos.x0 - bbox.x0\n        margin['right'] += bbox.x1 - pos.x1\n        # remember that rows are ordered from top:\n        margin['bottom'] += pos.y0 - bbox.y0\n        margin['top'] += bbox.y1 - pos.y1\n\n        # make margin for colorbars.  These margins go in the\n        # padding margin, versus the margin for Axes decorators.\n        for cbax in ax._colorbars:\n            # note pad is a fraction of the parent width...\n            pad = colorbar_get_pad(layoutgrids, cbax)\n            # colorbars can be child of more than one subplot spec:\n            cbp_rspan, cbp_cspan = get_cb_parent_spans(cbax)\n            loc = cbax._colorbar_info['location']\n            cbpos, cbbbox = get_pos_and_bbox(cbax, renderer)\n            if loc == 'right':\n                if cbp_cspan.stop == ss.colspan.stop:\n                    # only increase if the colorbar is on the right edge\n                    margin['rightcb'] += cbbbox.width + pad\n            elif loc == 'left':\n                if cbp_cspan.start == ss.colspan.start:\n                    # only increase if the colorbar is on the left edge\n                    margin['leftcb'] += cbbbox.width + pad\n            elif loc == 'top':\n                if cbp_rspan.start == ss.rowspan.start:\n                    margin['topcb'] += cbbbox.height + pad\n            else:\n                if cbp_rspan.stop == ss.rowspan.stop:\n                    margin['bottomcb'] += cbbbox.height + pad\n            # If the colorbars are wider than the parent box in the\n            # cross direction\n            if loc in ['top', 'bottom']:\n                if (cbp_cspan.start == ss.colspan.start and\n                        cbbbox.x0 < bbox.x0):\n                    margin['left'] += bbox.x0 - cbbbox.x0\n                if (cbp_cspan.stop == ss.colspan.stop and\n                        cbbbox.x1 > bbox.x1):\n                    margin['right'] += cbbbox.x1 - bbox.x1\n            # or taller:\n            if loc in ['left', 'right']:\n                if (cbp_rspan.stop == ss.rowspan.stop and\n                        cbbbox.y0 < bbox.y0):\n                    margin['bottom'] += bbox.y0 - cbbbox.y0\n                if (cbp_rspan.start == ss.rowspan.start and\n                        cbbbox.y1 > bbox.y1):\n                    margin['top'] += cbbbox.y1 - bbox.y1\n        # pass the new margins down to the layout grid for the solution...\n        layoutgrids[gs].edit_outer_margin_mins(margin, ss)\n\n    # make margins for figure-level legends:\n    for leg in fig.legends:\n        inv_trans_fig = None\n        if leg._outside_loc and leg._bbox_to_anchor is None:\n            if inv_trans_fig is None:\n                inv_trans_fig = fig.transFigure.inverted().transform_bbox\n            bbox = inv_trans_fig(leg.get_tightbbox(renderer))\n            w = bbox.width + 2 * w_pad\n            h = bbox.height + 2 * h_pad\n            legendloc = leg._outside_loc\n            if legendloc == 'lower':\n                layoutgrids[fig].edit_margin_min('bottom', h)\n            elif legendloc == 'upper':\n                layoutgrids[fig].edit_margin_min('top', h)\n            if legendloc == 'right':\n                layoutgrids[fig].edit_margin_min('right', w)\n            elif legendloc == 'left':\n                layoutgrids[fig].edit_margin_min('left', w)\n```\n\nOriginal documentation:\n```python\nFor each axes, make a margin between the *pos* layoutbox and the\n    *axes* layoutbox be a minimum size that can accommodate the\n    decorations on the axis.\n\n    Then make room for colorbars.\n\n    Parameters\n    ----------\n    layoutgrids : dict\n    fig : `~matplotlib.figure.Figure`\n        `.Figure` instance to do the layout in.\n    renderer : `~matplotlib.backend_bases.RendererBase` subclass.\n        The renderer to use.\n    w_pad, h_pad : float, default: 0\n        Width and height padding (in fraction of figure).\n    hspace, wspace : float, default: 0\n        Width and height padding as fraction of figure size divided by\n        number of columns or rows.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "For each Axes, make a margin between the *pos* layoutbox and the\n    *axes* layoutbox be a minimum size that can accommodate the\n    decorations on the axis.\n\n    Then make room for colorbars.\n\n    Parameters\n    ----------\n    layoutgrids : dict\n    fig : `~matplotlib.figure.Figure`\n        `.Figure` instance to do the layout in.\n    renderer : `~matplotlib.backend_bases.RendererBase` subclass.\n        The renderer to use.\n    w_pad, h_pad : float, default: 0\n        Width and height padding (in fraction of figure).\n    hspace, wspace : float, default: 0\n        Width and height padding as fraction of figure size divided by\n        number of columns or rows.", "file": "lib_matplotlib__constrained_layout.py", "function": "make_layout_margins", "project": "matplotlib", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _forward_generator(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        feats: torch.Tensor,\n        feats_lengths: torch.Tensor,\n        singing: torch.Tensor,\n        singing_lengths: torch.Tensor,\n        duration: torch.Tensor,\n        label: Optional[Dict[str, torch.Tensor]] = None,\n        label_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        melody: Optional[Dict[str, torch.Tensor]] = None,\n        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        tempo: Optional[Dict[str, torch.Tensor]] = None,\n        tempo_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        beat: Optional[Dict[str, torch.Tensor]] = None,\n        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        pitch: Optional[torch.Tensor] = None,\n        pitch_lengths: Optional[torch.Tensor] = None,\n        sids: Optional[torch.Tensor] = None,\n        spembs: Optional[torch.Tensor] = None,\n        lids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, Any]:\n        \n\n        \n        # setup\n        batch_size = text.size(0)\n        feats = feats.transpose(1, 2)\n        singing = singing.unsqueeze(1)\n\n        # calculate generator outputs\n        reuse_cache = True\n        if not self.cache_generator_outputs or self._cache is None:\n            reuse_cache = False\n            outs = self.generator(\n                text=text,\n                text_lengths=text_lengths,\n                feats=feats,\n                feats_lengths=feats_lengths,\n                duration=duration,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                melody_lengths=melody_lengths,\n                tempo=tempo,\n                tempo_lengths=tempo_lengths,\n                beat=beat,\n                beat_lengths=beat_lengths,\n                pitch=pitch,\n                pitch_lengths=pitch_lengths,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n            )\n        else:\n            outs = self._cache\n\n        # store cache\n        if self.training and self.cache_generator_outputs and not reuse_cache:\n            self._cache = outs\n\n        # parse outputs\n        singing_hat_, start_idxs, _, z_mask, outs_ = outs\n        if not self.use_visinger:\n            _, z_p, m_p, logs_p, _, logs_q = outs_\n        else:\n            if self.use_dp:\n                (\n                    _,\n                    z_p,\n                    m_p,\n                    logs_p,\n                    _,\n                    logs_q,\n                    pred_pitch,\n                    gt_pitch,\n                    logw,\n                    logw_gt,\n                    log_probs,\n                ) = outs_\n            else:\n                _, z_p, m_p, logs_p, _, logs_q, pred_pitch, gt_pitch = outs_\n        singing_ = get_segments(\n            x=singing,\n            start_idxs=start_idxs * self.generator.upsample_factor,\n            segment_size=self.generator.segment_size * self.generator.upsample_factor,\n        )\n\n        # calculate discriminator outputs\n        p_hat = self.discriminator(singing_hat_)\n        with torch.no_grad():\n            # do not store discriminator gradient in generator turn\n            p = self.discriminator(singing_)\n\n        # calculate losses\n        with autocast(enabled=False):\n            mel_loss = self.mel_loss(singing_hat_, singing_)\n            kl_loss = self.kl_loss(z_p, logs_q, m_p, logs_p, z_mask)\n            adv_loss = self.generator_adv_loss(p_hat)\n            feat_match_loss = self.feat_match_loss(p_hat, p)\n\n            if self.use_visinger:\n                pitch_loss = self.mse_loss(pred_pitch, gt_pitch)\n                if self.use_dp:\n                    ctc_loss = self.ctc_loss(\n                        log_probs, label, feats_lengths, label_lengths\n                    )\n                    dur_loss = self.mse_loss(logw, logw_gt)\n\n            mel_loss = mel_loss * self.lambda_mel\n            kl_loss = kl_loss * self.lambda_kl\n\n            adv_loss = adv_loss * self.lambda_adv\n            feat_match_loss = feat_match_loss * self.lambda_feat_match\n            if self.use_visinger:\n                pitch_loss = pitch_loss * self.lambda_pitch\n                if self.use_dp:\n                    ctc_loss = ctc_loss * self.lambda_phoneme\n                    dur_loss = dur_loss * self.lambda_dur\n            # loss = mel_loss + kl_loss + dur_loss + adv_loss + feat_match_loss\n\n            loss = mel_loss + kl_loss + adv_loss + feat_match_loss\n            if self.use_visinger:\n                loss += pitch_loss\n            if self.use_dp:\n                loss += dur_loss\n                loss += ctc_loss\n\n        if self.use_visinger:\n            if not self.use_dp:\n                stats = dict(\n                    generator_loss=loss.item(),\n                    generator_mel_loss=mel_loss.item(),\n                    generator_kl_loss=kl_loss.item(),\n                    generator_adv_loss=adv_loss.item(),\n                    generator_feat_match_loss=feat_match_loss.item(),\n                    pitch_loss=pitch_loss.item(),\n                )\n            else:\n                stats = dict(\n                    generator_loss=loss.item(),\n                    generator_mel_loss=mel_loss.item(),\n                    generator_kl_loss=kl_loss.item(),\n                    generator_dur_loss=dur_loss.item(),\n                    generator_adv_loss=adv_loss.item(),\n                    generator_feat_match_loss=feat_match_loss.item(),\n                    pitch_loss=pitch_loss.item(),\n                    ctc_loss=ctc_loss.item(),\n                )\n        else:\n            stats = dict(\n                generator_loss=loss.item(),\n                generator_mel_loss=mel_loss.item(),\n                generator_kl_loss=kl_loss.item(),\n                generator_adv_loss=adv_loss.item(),\n                generator_feat_match_loss=feat_match_loss.item(),\n            )\n\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n\n        # reset cache\n        if reuse_cache or not self.training:\n            self._cache = None\n\n        return {\n            \"loss\": loss,\n            \"stats\": stats,\n            \"weight\": weight,\n            \"optim_idx\": 0,  # needed for trainer\n        }```\n\nUpdated code:\n```python\ndef _forward_generator(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        feats: torch.Tensor,\n        feats_lengths: torch.Tensor,\n        singing: torch.Tensor,\n        singing_lengths: torch.Tensor,\n        duration: torch.Tensor,\n        label: Optional[Dict[str, torch.Tensor]] = None,\n        label_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        melody: Optional[Dict[str, torch.Tensor]] = None,\n        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        tempo: Optional[Dict[str, torch.Tensor]] = None,\n        tempo_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        beat: Optional[Dict[str, torch.Tensor]] = None,\n        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        pitch: Optional[torch.Tensor] = None,\n        pitch_lengths: Optional[torch.Tensor] = None,\n        sids: Optional[torch.Tensor] = None,\n        spembs: Optional[torch.Tensor] = None,\n        lids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, Any]:\n        \n\n        \n        # setup\n        batch_size = text.size(0)\n        feats = feats.transpose(1, 2)\n        singing = singing.unsqueeze(1)\n\n        # calculate generator outputs\n        reuse_cache = True\n        if not self.cache_generator_outputs or self._cache is None:\n            reuse_cache = False\n            outs = self.generator(\n                text=text,\n                text_lengths=text_lengths,\n                feats=feats,\n                feats_lengths=feats_lengths,\n                duration=duration,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                melody_lengths=melody_lengths,\n                tempo=tempo,\n                tempo_lengths=tempo_lengths,\n                beat=beat,\n                beat_lengths=beat_lengths,\n                pitch=pitch,\n                pitch_lengths=pitch_lengths,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n            )\n        else:\n            outs = self._cache\n\n        # store cache\n        if self.training and self.cache_generator_outputs and not reuse_cache:\n            self._cache = outs\n\n        # parse outputs\n        singing_hat_, start_idxs, _, z_mask, outs_ = outs\n        if not self.use_visinger:\n            _, z_p, m_p, logs_p, _, logs_q = outs_\n        else:\n            if self.use_dp:\n                (\n                    _,\n                    z_p,\n                    m_p,\n                    logs_p,\n                    _,\n                    logs_q,\n                    pred_pitch,\n                    gt_pitch,\n                    logw,\n                    logw_gt,\n                    log_probs,\n                ) = outs_\n            else:\n                _, z_p, m_p, logs_p, _, logs_q, pred_pitch, gt_pitch = outs_\n        singing_ = get_segments(\n            x=singing,\n            start_idxs=start_idxs * self.generator.upsample_factor,\n            segment_size=self.generator.segment_size * self.generator.upsample_factor,\n        )\n\n        # calculate discriminator outputs\n        p_hat = self.discriminator(singing_hat_)\n        with torch.no_grad():\n            # do not store discriminator gradient in generator turn\n            p = self.discriminator(singing_)\n\n        # calculate losses\n        with autocast(enabled=False):\n            mel_loss = self.mel_loss(singing_hat_, singing_)\n            kl_loss = self.kl_loss(z_p, logs_q, m_p, logs_p, z_mask)\n            adv_loss = self.generator_adv_loss(p_hat)\n            feat_match_loss = self.feat_match_loss(p_hat, p)\n\n            if self.use_visinger:\n                pitch_loss = self.mse_loss(pred_pitch, gt_pitch)\n                if self.use_dp:\n                    ctc_loss = self.ctc_loss(\n                        log_probs, label, feats_lengths, label_lengths\n                    )\n                    dur_loss = self.mse_loss(logw, logw_gt)\n\n            mel_loss = mel_loss * self.lambda_mel\n            kl_loss = kl_loss * self.lambda_kl\n\n            adv_loss = adv_loss * self.lambda_adv\n            feat_match_loss = feat_match_loss * self.lambda_feat_match\n            if self.use_visinger:\n                pitch_loss = pitch_loss * self.lambda_pitch\n                if self.use_dp:\n                    ctc_loss = ctc_loss * self.lambda_phoneme\n                    dur_loss = dur_loss * self.lambda_dur\n\n            loss = mel_loss + kl_loss + adv_loss + feat_match_loss\n            if self.use_visinger:\n                loss += pitch_loss\n            if self.use_dp:\n                loss += dur_loss\n                loss += ctc_loss\n\n        if self.use_visinger:\n            if not self.use_dp:\n                stats = dict(\n                    generator_loss=loss.item(),\n                    generator_mel_loss=mel_loss.item(),\n                    generator_kl_loss=kl_loss.item(),\n                    generator_adv_loss=adv_loss.item(),\n                    generator_feat_match_loss=feat_match_loss.item(),\n                    pitch_loss=pitch_loss.item(),\n                )\n            else:\n                stats = dict(\n                    generator_loss=loss.item(),\n                    generator_mel_loss=mel_loss.item(),\n                    generator_kl_loss=kl_loss.item(),\n                    generator_dur_loss=dur_loss.item(),\n                    generator_adv_loss=adv_loss.item(),\n                    generator_feat_match_loss=feat_match_loss.item(),\n                    pitch_loss=pitch_loss.item(),\n                    ctc_loss=ctc_loss.item(),\n                )\n        else:\n            stats = dict(\n                generator_loss=loss.item(),\n                generator_mel_loss=mel_loss.item(),\n                generator_kl_loss=kl_loss.item(),\n                generator_adv_loss=adv_loss.item(),\n                generator_feat_match_loss=feat_match_loss.item(),\n            )\n\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n\n        # reset cache\n        if reuse_cache or not self.training:\n            self._cache = None\n\n        return {\n            \"loss\": loss,\n            \"stats\": stats,\n            \"weight\": weight,\n            \"optim_idx\": 0,  # needed for trainer\n        }\n```\n\nOriginal documentation:\n```python\nPerform generator forward.\n\n        Args:\n            text (Tensor): Text index tensor (B, T_text).\n            text_lengths (Tensor): Text length tensor (B,).\n            feats (Tensor): Feature tensor (B, T_feats, aux_channels).\n            feats_lengths (Tensor): Feature length tensor (B,).\n            singing (Tensor): Singing waveform tensor (B, T_wav).\n            singing_lengths (Tensor): Singing length tensor (B,).\n            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).\n            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).\n            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).\n\n        Returns:\n            Dict[str, Any]:\n                * loss (Tensor): Loss scalar tensor.\n                * stats (Dict[str, float]): Statistics to be monitored.\n                * weight (Tensor): Weight tensor to summarize losses.\n                * optim_idx (int): Optimizer index (0 for G and 1 for D).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform generator forward.\n\n        Args:\n            text (Tensor): Text index tensor (B, T_text).\n            text_lengths (Tensor): Text length tensor (B,).\n            feats (Tensor): Feature tensor (B, T_feats, aux_channels).\n            feats_lengths (Tensor): Feature length tensor (B,).\n            singing (Tensor): Singing waveform tensor (B, T_wav).\n            singing_lengths (Tensor): Singing length tensor (B,).\n            duration (Optional[Dict]): key is \"phn\", \"syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            label (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded label ids (B, Tmax).\n            label_lengths (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of the lengths of padded label ids (B, ).\n            melody (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded melody (B, Tmax).\n            melody_lengths (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of the lengths of padded melody (B, ).\n            tempo (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded tempo (B, Tmax).\n            tempo_lengths (Optional[Dict]):  key is \"lab\" or \"score\";\n                value (LongTensor): Batch of the lengths of padded tempo (B, ).\n            beat (Optional[Dict]): key is \"lab\", \"score_phn\" or \"score_syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            beat_length (Optional[Dict]): key is \"lab\", \"score_phn\" or \"score_syb\";\n                value (LongTensor): Batch of the lengths of padded beat (B, ).\n            pitch (FloatTensor): Batch of padded f0 (B, Tmax).\n            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).\n            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).\n            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).\n            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).\n\n        Returns:\n            Dict[str, Any]:\n                * loss (Tensor): Loss scalar tensor.\n                * stats (Dict[str, float]): Statistics to be monitored.\n                * weight (Tensor): Weight tensor to summarize losses.\n                * optim_idx (int): Optimizer index (0 for G and 1 for D).", "file": "espnet2_gan_svs_vits_vits.py", "function": "VITS._forward_generator", "project": "espnet", "change_stats": {"code_changes": 3, "doc_changes": 22, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_value(self, label, value):\n        \n        \n        \n        self.index._engine.set_value(self, label, value)```\n\nUpdated code:\n```python\ndef set_value(self, label, value):\n        \n        \n        \n        try:\n            self.index._engine.set_value(self, label, value)\n            return self\n        except KeyError:\n            new_index = np.concatenate([self.index.values, [label]])\n            new_values = np.concatenate([self.values, [value]])\n            return Series(new_values, index=new_index, name=self.name)\n```\n\nOriginal documentation:\n```python\nQuickly set single value at passed label\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed\n\n        Returns\n        -------\n        element : scalar value\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Quickly set single value at passed label\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed", "file": "pandas_core_series.py", "function": "Series.set_value", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef paragraphs(cls, nb=3, ext_word_list=None):\n        \n        \n        \n        if ext_word_list:\n            return [cls.paragraph(ext_word_list=ext_word_list) for _ in range(0, nb)]\n\n        return [cls.paragraph() for _ in range(0, nb)]```\n\nUpdated code:\n```python\ndef paragraphs(cls, nb=3, ext_word_list=None):\n        \n        \n        \n\n        return [cls.paragraph(ext_word_list=ext_word_list) for _ in range(0, nb)]\n```\n\nOriginal documentation:\n```python\nGenerate an array of paragraphs\n        :example array($paragraph1, $paragraph2, $paragraph3)\n        :param nb how many paragraphs to return\n        :param 'ext_word_list' a list of word you would like to have \n                 instead of 'Lorem ipsum'\n        :return array\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate an array of paragraphs\n        :example array($paragraph1, $paragraph2, $paragraph3)\n        :param nb how many paragraphs to return\n        :param 'ext_word_list' a list of word you would like to have \n                 instead of 'Lorem ipsum'\n        :return list", "file": "faker_providers_lorem___init__.py", "function": "Provider.paragraphs", "project": "faker", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef nav_scale_callback(self, *args, reset_progress=True):  # pylint:disable=unused-argument\n         \n        \n        self._display_frame.pack_threshold_slider()\n        if reset_progress:\n            self.stop_playback()\n        frame_count = self._det_faces.filter.count\n        if self._current_nav_frame_count == frame_count:\n            logger.trace(\"Filtered count has not changed. Returning\")\n            return False\n        if self._globals.tk_filter_mode.get() == \"Misaligned Faces\":\n            self._det_faces.tk_face_count_changed.set(True)\n        max_frame = max(0, frame_count - 1)\n        logger.debug(\"Filtered frame count has changed. Updating from %s to %s\",\n                     self._current_nav_frame_count, frame_count)\n        self._nav[\"scale\"].config(to=max_frame)\n        self._nav[\"label\"].config(text=\"/{}\".format(max_frame))\n        state = \"disabled\" if max_frame == 0 else \"normal\"\n        self._nav[\"entry\"].config(state=state)\n        if reset_progress:\n            self._globals.tk_transport_index.set(0)\n        return True```\n\nUpdated code:\n```python\ndef nav_scale_callback(self, *args, reset_progress=True):  # pylint:disable=unused-argument\n         \n        \n        self._display_frame.pack_threshold_slider()\n        if reset_progress:\n            self.stop_playback()\n        frame_count = self._det_faces.filter.count\n        if self._current_nav_frame_count == frame_count:\n            logger.trace(\"Filtered count has not changed. Returning\")\n        if self._globals.tk_filter_mode.get() == \"Misaligned Faces\":\n            self._det_faces.tk_face_count_changed.set(True)\n        self._update_total_frame_count()\n        if reset_progress:\n            self._globals.tk_transport_index.set(0)\n```\n\nOriginal documentation:\n```python\nAdjust transport slider scale for different filters. Hide or display optional filter\n        controls.\n\n        Returns\n        -------\n        bool\n            ``True`` if the navigation scale has been updated otherwise ``False``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adjust transport slider scale for different filters. Hide or display optional filter\n        controls.", "file": "tools_manual_frameviewer_control.py", "function": "Navigation.nav_scale_callback", "project": "faceswap", "change_stats": {"code_changes": 12, "doc_changes": 7, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef matmul(a, b, transa=False, transb=False):\n    \n\n    \n    return MatMul(transa=transa, transb=transb).apply((a, b))[0]```\n\nUpdated code:\n```python\ndef matmul(a, b, transa=False, transb=False):\n    \n\n    \n    if backend.get_array_module(a, b) is chainerx:\n        # TODO(sonots): Support transa and transb in ChainerX\n        # TODO(sonots): Support dtype promotion in ChainerX\n        # TODO(sonots): Support ndim > 2 in ChainerX\n        if (not transa and not transb\n                and a.dtype == b.dtype\n                and a.ndim == 2 and b.ndim == 2):\n            return function._chainerx_op(chainerx.dot, a, b)\n\n    return MatMul(transa=transa, transb=transb).apply((a, b))[0]\n```\n\nOriginal documentation:\n```python\nComputes the matrix multiplication of two arrays.\n\n    Args:\n        a (Variable): The left operand of the matrix multiplication.\n            If ``a`` and ``b`` are both 1-D arrays, ``matmul`` returns a dot\n            product of vector `a` and vector `b`. If 2-D arrays, ``matmul``\n            returns matrix product of ``a`` and ``b``. If either's dimension is\n            larger than 2, they are treated as a stack of matrices residing in\n            the last two indexes. ``matmul`` returns a stack of each two\n            arrays. In this case, ``a`` and ``b`` are broadcasted along axes\n            except the last two.\n        b (Variable): The right operand of the matrix multiplication.\n            Its array is treated as a matrix in the same way as ``a``'s array.\n        transa (bool): If ``True``, each matrices in ``a`` will be transposed.\n            If ``a.ndim == 1``, do nothing.\n        transb (bool): If ``True``, each matrices in ``b`` will be transposed.\n            If ``b.ndim == 1``, do nothing.\n\n    Returns:\n        ~chainer.Variable: The result of the matrix multiplication.\n\n    .. admonition:: Example\n\n        >>> a = np.array([[1, 0], [0, 1]], np.float32)\n        >>> b = np.array([[4, 1], [2, 2]], np.float32)\n        >>> F.matmul(a, b).array\n        array([[4., 1.],\n               [2., 2.]], dtype=float32)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes the matrix multiplication of two arrays.\n\n    Args:\n        a (Variable): The left operand of the matrix multiplication.\n            If ``a`` and ``b`` are both 1-D arrays, ``matmul`` returns a dot\n            product of vector `a` and vector `b`. If 2-D arrays, ``matmul``\n            returns matrix product of ``a`` and ``b``. If either's dimension is\n            larger than 2, they are treated as a stack of matrices residing in\n            the last two indexes. ``matmul`` returns a stack of each two\n            arrays. In this case, ``a`` and ``b`` are broadcasted along axes\n            except the last two.\n        b (Variable): The right operand of the matrix multiplication.\n            Its array is treated as a matrix in the same way as ``a``'s array.\n        transa (bool): If ``True``, each matrices in ``a`` will be transposed.\n            If ``a.ndim == 1``, do nothing.\n        transb (bool): If ``True``, each matrices in ``b`` will be transposed.\n            If ``b.ndim == 1``, do nothing.\n\n    Returns:\n        ~chainer.Variable: The result of the matrix multiplication.\n\n    .. admonition:: Example\n\n        >>> a = np.array([[1, 0], [0, 1]], np.float32)\n        >>> b = np.array([[4, 1], [2, 2]], np.float32)\n        >>> F.matmul(a, b).data\n        array([[4., 1.],\n               [2., 2.]], dtype=float32)", "file": "chainer_", "function": "matmul", "project": "chainer", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        nlp: Language,\n        name: str = \"span_ruler\",\n        *,\n        spans_key: Optional[str] = DEFAULT_SPANS_KEY,\n        spans_filter: Optional[\n            Callable[[Iterable[Span], Iterable[Span]], Iterable[Span]]\n        ] = None,\n        annotate_ents: bool = False,\n        ents_filter: Callable[\n            [Iterable[Span], Iterable[Span]], Iterable[Span]\n        ] = util.filter_chain_spans,\n        phrase_matcher_attr: Optional[Union[int, str]] = None,\n        validate: bool = False,\n        overwrite: bool = False,\n        scorer: Optional[Callable] = partial(\n            overlapping_labeled_spans_score, spans_key=DEFAULT_SPANS_KEY\n        ),\n    ) -> None:\n        \n        \n        self.nlp = nlp\n        self.name = name\n        self.spans_key = spans_key\n        self.annotate_ents = annotate_ents\n        self.phrase_matcher_attr = phrase_matcher_attr\n        self.validate = validate\n        self.overwrite = overwrite\n        self.spans_filter = spans_filter\n        self.ents_filter = ents_filter\n        self.scorer = scorer\n        self._match_label_id_map: Dict[int, Dict[str, str]] = {}\n        self.clear()```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        nlp: Language,\n        name: str = \"span_ruler\",\n        *,\n        spans_key: Optional[str] = DEFAULT_SPANS_KEY,\n        spans_filter: Optional[\n            Callable[[Iterable[Span], Iterable[Span]], Iterable[Span]]\n        ] = None,\n        annotate_ents: bool = False,\n        ents_filter: Callable[\n            [Iterable[Span], Iterable[Span]], Iterable[Span]\n        ] = util.filter_chain_spans,\n        phrase_matcher_attr: Optional[Union[int, str]] = None,\n        matcher_fuzzy_compare: Callable = levenshtein_compare,\n        validate: bool = False,\n        overwrite: bool = False,\n        scorer: Optional[Callable] = partial(\n            overlapping_labeled_spans_score, spans_key=DEFAULT_SPANS_KEY\n        ),\n    ) -> None:\n        \n        \n        self.nlp = nlp\n        self.name = name\n        self.spans_key = spans_key\n        self.annotate_ents = annotate_ents\n        self.phrase_matcher_attr = phrase_matcher_attr\n        self.validate = validate\n        self.overwrite = overwrite\n        self.spans_filter = spans_filter\n        self.ents_filter = ents_filter\n        self.scorer = scorer\n        self.matcher_fuzzy_compare = matcher_fuzzy_compare\n        self._match_label_id_map: Dict[int, Dict[str, str]] = {}\n        self.clear()\n```\n\nOriginal documentation:\n```python\nInitialize the span ruler. If patterns are supplied here, they\n        need to be a list of dictionaries with a `\"label\"` and `\"pattern\"`\n        key. A pattern can either be a token pattern (list) or a phrase pattern\n        (string). For example: `{'label': 'ORG', 'pattern': 'Apple'}`.\n\n        nlp (Language): The shared nlp object to pass the vocab to the matchers\n            and process phrase patterns.\n        name (str): Instance name of the current pipeline component. Typically\n            passed in automatically from the factory when the component is\n            added. Used to disable the current span ruler while creating\n            phrase patterns with the nlp object.\n        spans_key (Optional[str]): The spans key to save the spans under. If\n            `None`, no spans are saved. Defaults to \"ruler\".\n        spans_filter (Optional[Callable[[Iterable[Span], Iterable[Span]], List[Span]]):\n            The optional method to filter spans before they are assigned to\n            doc.spans. Defaults to `None`.\n        annotate_ents (bool): Whether to save spans to doc.ents. Defaults to\n            `False`.\n        ents_filter (Callable[[Iterable[Span], Iterable[Span]], List[Span]]):\n            The method to filter spans before they are assigned to doc.ents.\n            Defaults to `util.filter_chain_spans`.\n        phrase_matcher_attr (Optional[Union[int, str]]): Token attribute to\n            match on, passed to the internal PhraseMatcher as `attr`. Defaults\n            to `None`.\n        validate (bool): Whether patterns should be validated, passed to\n            Matcher and PhraseMatcher as `validate`.\n        overwrite (bool): Whether to remove any existing spans under this spans\n            key if `spans_key` is set, and/or to remove any ents under `doc.ents` if\n            `annotate_ents` is set. Defaults to `True`.\n        scorer (Optional[Callable]): The scoring method. Defaults to\n            spacy.pipeline.span_ruler.overlapping_labeled_spans_score.\n\n        DOCS: https://spacy.io/api/spanruler#init\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the span ruler. If patterns are supplied here, they\n        need to be a list of dictionaries with a `\"label\"` and `\"pattern\"`\n        key. A pattern can either be a token pattern (list) or a phrase pattern\n        (string). For example: `{'label': 'ORG', 'pattern': 'Apple'}`.\n\n        nlp (Language): The shared nlp object to pass the vocab to the matchers\n            and process phrase patterns.\n        name (str): Instance name of the current pipeline component. Typically\n            passed in automatically from the factory when the component is\n            added. Used to disable the current span ruler while creating\n            phrase patterns with the nlp object.\n        spans_key (Optional[str]): The spans key to save the spans under. If\n            `None`, no spans are saved. Defaults to \"ruler\".\n        spans_filter (Optional[Callable[[Iterable[Span], Iterable[Span]], List[Span]]):\n            The optional method to filter spans before they are assigned to\n            doc.spans. Defaults to `None`.\n        annotate_ents (bool): Whether to save spans to doc.ents. Defaults to\n            `False`.\n        ents_filter (Callable[[Iterable[Span], Iterable[Span]], List[Span]]):\n            The method to filter spans before they are assigned to doc.ents.\n            Defaults to `util.filter_chain_spans`.\n        phrase_matcher_attr (Optional[Union[int, str]]): Token attribute to\n            match on, passed to the internal PhraseMatcher as `attr`. Defaults\n            to `None`.\n        matcher_fuzzy_compare (Callable): The fuzzy comparison method for the\n            internal Matcher. Defaults to\n            spacy.matcher.levenshtein.levenshtein_compare.\n        validate (bool): Whether patterns should be validated, passed to\n            Matcher and PhraseMatcher as `validate`.\n        overwrite (bool): Whether to remove any existing spans under this spans\n            key if `spans_key` is set, and/or to remove any ents under `doc.ents` if\n            `annotate_ents` is set. Defaults to `True`.\n        scorer (Optional[Callable]): The scoring method. Defaults to\n            spacy.pipeline.span_ruler.overlapping_labeled_spans_score.\n\n        DOCS: https://spacy.io/api/spanruler#init", "file": "spacy_pipeline_span_ruler.py", "function": "SpanRuler.__init__", "project": "spaCy", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_data(self):\n        \n        return self._values, self._edges, self._baseline```\n\nUpdated code:\n```python\ndef get_data(self):\n        \n        StairData = namedtuple('StairData', 'values edges baseline')\n        return StairData(self._values, self._edges, self._baseline)\n```\n\nOriginal documentation:\n```python\nGet `.StepPatch` values, edges and baseline.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get `.StepPatch` values, edges and baseline as namedtuple.", "file": "lib_matplotlib_patches.py", "function": "StepPatch.get_data", "project": "matplotlib", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef deep_copy(self, preserve_uuid: bool = False) -> \"ExecutionPlan\":\n        \n        \n        dataset_uuid = None\n        if preserve_uuid:\n            dataset_uuid = self._dataset_uuid\n        in_blocks = self._in_blocks\n        if isinstance(in_blocks, BlockList):\n            in_blocks = in_blocks.copy()\n        plan_copy = ExecutionPlan(\n            in_blocks,\n            copy.copy(self._in_stats),\n            dataset_uuid=dataset_uuid,\n            run_by_consumer=self._run_by_consumer,\n        )\n        if self._snapshot_blocks:\n            # Copy over the existing snapshot.\n            plan_copy._snapshot_blocks = self._snapshot_blocks.copy()\n            plan_copy._snapshot_stats = copy.copy(self._snapshot_stats)\n        plan_copy._stages_before_snapshot = self._stages_before_snapshot.copy()\n        plan_copy._stages_after_snapshot = self._stages_after_snapshot.copy()\n        return plan_copy```\n\nUpdated code:\n```python\ndef deep_copy(self) -> \"ExecutionPlan\":\n        \n        \n        in_blocks = self._in_blocks\n        if isinstance(in_blocks, BlockList):\n            in_blocks = in_blocks.copy()\n        plan_copy = ExecutionPlan(\n            in_blocks,\n            copy.copy(self._in_stats),\n            run_by_consumer=self._run_by_consumer,\n        )\n        if self._snapshot_blocks:\n            # Copy over the existing snapshot.\n            plan_copy._snapshot_blocks = self._snapshot_blocks.copy()\n            plan_copy._snapshot_stats = copy.copy(self._snapshot_stats)\n        plan_copy._stages_before_snapshot = self._stages_before_snapshot.copy()\n        plan_copy._stages_after_snapshot = self._stages_after_snapshot.copy()\n        return plan_copy\n```\n\nOriginal documentation:\n```python\nCreate a deep copy of this execution plan.\n\n        This copy can be executed AND cleared without mutating the original.\n\n        Args:\n            preserve_uuid: Whether to preserve the original UUID in the copy.\n\n        Returns:\n            A deep copy of this execution plan.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a deep copy of this execution plan.\n\n        This copy can be executed AND cleared without mutating the original.\n\n        Returns:\n            A deep copy of this execution plan.", "file": "python_ray_data__internal_plan.py", "function": "ExecutionPlan.deep_copy", "project": "ray", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_http_if_no_scheme(url):\n    \n    parser = parse_url(url)\n    if url.startswith('//'):\n        url = 'http:' + url\n    elif not parser.scheme or not parser.netloc:\n        url = 'http://' + url\n    return url```\n\nUpdated code:\n```python\ndef add_http_if_no_scheme(url):\n    \n    if url.startswith('//'):\n        url = 'http:' + url\n        return url\n    parser = parse_url(url)\n    if not parser.scheme or not parser.netloc:\n        url = 'http://' + url\n    return url\n```\n\nOriginal documentation:\n```python\nAdds http as the default scheme if it is missing from the url\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add http as the default scheme if it is missing from the url.", "file": "scrapy_utils_url.py", "function": "add_http_if_no_scheme", "project": "scrapy", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _build_param_buckets(self) -> None:\n        \n        \n        if not self.parameters_as_bucket_view:\n            return\n\n        # Bucket B_{i,j}: parameters stored on dev_i assigned to rank j\n        num_devices = len(self._device_to_per_rank_params)\n        self._buckets = [[] for _ in range(num_devices)]\n\n        for dev_i, (device, param_lists) in enumerate(self._device_to_per_rank_params.items()):\n            for params in param_lists:\n                bucket_size = 0\n                dtype = None\n                trainable_params = []\n                for param in params:\n                    if not _is_trainable(param):\n                        # Clone in case the parameter was previously part of\n                        # a bucket to avoid the data from being destroyed\n                        param.data = param.data.detach().clone()\n                    else:\n                        bucket_size += param.numel()\n                        trainable_params.append(param)\n                    dtype = param.dtype  # assumes all same dtype\n\n                if bucket_size == 0:\n                    # Create a dummy bucket if there are no parameters\n                    bucket = torch.zeros(1, device=device)\n                else:\n                    # Construct the bucket (assuming all dense and same dtype)\n                    bucket = torch.empty(bucket_size, dtype=dtype, device=device)\n                    offset = 0\n                    for param in trainable_params:\n                        offset_next = offset + param.numel()\n                        bucket[offset:offset_next].copy_(param.data.flatten())\n                        param.data = bucket[offset:offset_next].view_as(param.data)\n                        offset = offset_next\n                self._buckets[dev_i].append(bucket)```\n\nUpdated code:\n```python\ndef _build_param_buckets(self) -> None:\n        \n        \n        if not self.parameters_as_bucket_view or self._overlap_with_ddp:\n            return\n\n        # `self._buckets[i][j]` are the parameters stored on device i and\n        # assigned to rank j\n        num_devices = len(self._device_to_params_per_rank)\n        self._buckets = [[] for _ in range(num_devices)]  # type: ignore[assignment]\n\n        for dev_i, (device, params_per_rank) in enumerate(self._device_to_params_per_rank.items()):\n            for params in params_per_rank:\n                bucket_size = 0\n                dtype = None\n                trainable_params = []\n                for param in params:\n                    if not _is_trainable(param):\n                        # Clone in case the parameter was previously part of\n                        # a bucket to avoid the data from being destroyed\n                        param.data = param.data.detach().clone()\n                    else:\n                        bucket_size += param.numel()\n                        trainable_params.append(param)\n                    dtype = param.dtype  # assumes all same dtype\n\n                if bucket_size == 0:\n                    # Create a dummy bucket if there are no parameters\n                    bucket = torch.zeros(1, device=device)\n                else:\n                    # Construct the bucket (assuming all dense and same dtype)\n                    bucket = torch.empty(bucket_size, dtype=dtype, device=device)\n                    offset = 0\n                    for param in trainable_params:\n                        offset_next = offset + param.numel()\n                        bucket[offset:offset_next].copy_(param.data.flatten())\n                        param.data = bucket[offset:offset_next].view_as(param.data)\n                        offset = offset_next\n                self._buckets[dev_i].append(bucket)\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Builds parameter buckets if ``parameters_as_bucket_view`` is enabled so\n        that for each device that stores this rank's parameters, there is a\n        bucket (represented as a tensor) containing all of the parameters on\n        that device that are assigned to a given rank in the parameter update\n        partition.\n\n        This function is called in the constructor and any time parameter\n        trainability is changed.\n\n        .. warning::\n            The current implementation assumes that all of the parameters in a\n            bucket are of the same dense type when allocating the bucket's\n            tensor.\n\n        .. warning::\n            If the model parameters are stored across more than one device,\n            then the storage partitioning must be the same across all\n            processes in order for parameter synchronization to work.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Builds parameter buckets if ``parameters_as_bucket_view=True`` so\n        that for each device that stores this rank's parameters, there is a\n        bucket (represented as a tensor) containing all of the parameters on\n        that device that are assigned to a given rank in the parameter update\n        partition.\n\n        This method is called in the constructor and any time parameter\n        trainability is changed.\n\n        .. warning::\n            The current implementation assumes that all of the parameters in a\n            bucket are of the same dense type when allocating the bucket's\n            tensor.\n\n        .. warning::\n            If the model parameters are stored across more than one device,\n            then the storage partitioning must be the same across all\n            processes in order for parameter synchronization to work.", "file": "torch_distributed_optim_zero_redundancy_optimizer.py", "function": "ZeroRedundancyOptimizer._build_param_buckets", "project": "pytorch", "change_stats": {"code_changes": 15, "doc_changes": 6, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mobilenetv2_110d(pretrained=False, **kwargs):\n      \n    model = _gen_mobilenet_v2(\n        'mobilenetv2_100d', 1.1, depth_multiplier=1.2, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model```\n\nUpdated code:\n```python\ndef mobilenetv2_110d(pretrained=False, **kwargs):\n     \n    model = _gen_mobilenet_v2(\n        'mobilenetv2_110d', 1.1, depth_multiplier=1.2, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model\n```\n\nOriginal documentation:\n```python\nMobileNet V2\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "MobileNet V2 w/ 1.1 channel, 1.2 depth multipliers", "file": "timm_models_efficientnet.py", "function": "mobilenetv2_110d", "project": "pytorch-image-models", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef write_data(self,\n                   engine=None,\n                   equities_data={},\n                   futures_data={},\n                   exchanges_data={},\n                   root_symbols_data={},\n                   equities_identifiers=[],\n                   futures_identifiers=[],\n                   exchanges_identifiers=[],\n                   root_symbols_identifiers=[]):\n         \n        \n\n        if engine:\n            self.engine = engine\n\n        if (equities_data or futures_data or exchanges_data or\n                root_symbols_data):\n            self._write_data_dicts(equities_data, futures_data,\n                                   exchanges_data, root_symbols_data)\n\n        if (equities_identifiers or futures_identifiers or\n                exchanges_identifiers or root_symbols_identifiers):\n            self._write_data_lists(equities_identifiers,\n                                   futures_identifiers,\n                                   exchanges_identifiers,\n                                   root_symbols_identifiers)```\n\nUpdated code:\n```python\ndef write_data(self,\n                   engine=None,\n                   equities_data={},\n                   futures_data={},\n                   exchanges_data={},\n                   root_symbols_data={},\n                   equities_df=pd.DataFrame(),\n                   futures_df=pd.DataFrame(),\n                   exchanges_df=pd.DataFrame(),\n                   root_symbols_df=pd.DataFrame(),\n                   equities_identifiers=[],\n                   futures_identifiers=[],\n                   exchanges_identifiers=[],\n                   root_symbols_identifiers=[],\n                   allow_sid_assignment=True):\n         \n        \n        if engine:\n            self.engine = engine\n\n        # If any pandas.DataFrame data has been provided,\n        # write it to the database.\n        if not(equities_df.empty and futures_df.empty and\n               exchanges_df.empty and root_symbols_df.empty):\n            self._write_data_dataframes(equities_df, futures_df,\n                                        exchanges_df, root_symbols_df)\n\n        if (equities_data or futures_data or exchanges_data or\n                root_symbols_data):\n            self._write_data_dicts(equities_data, futures_data,\n                                   exchanges_data, root_symbols_data)\n\n        # These could be lists or other iterables such as a pandas.Index.\n        # For simplicity, don't check whether data has been provided.\n        self._write_data_lists(equities_identifiers,\n                               futures_identifiers,\n                               exchanges_identifiers,\n                               root_symbols_identifiers,\n                               allow_sid_assignment=allow_sid_assignment)\n```\n\nOriginal documentation:\n```python\nWrite the supplied data to the database.\n\n        Parameters\n        ----------\n        equities_data: dict\n            A dictionary of equity metadata\n        futures_data: dict\n            A dictionary of futures metadata\n        exchanges_data: dict\n            A dictionary of exchanges metadata\n        root_symbols_data: dict\n            A dictionary of root symbols metadata\n        equities_identifiers: list\n            A list of equities identifiers (sids or symbols)\n        futures_identifiers: list\n            A list of futures identifiers (sids or symbols)\n        exchanges_identifiers: list\n            A list of exchanges identifiers (ids or names)\n        root_symbols_identifiers: list\n            A list of root symbols identifiers (ids or symbols)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write the supplied data to the database.\n\n        Parameters\n        ----------\n        equities_data: dict, optional\n            A dictionary of equity metadata\n        futures_data: dict, optional\n            A dictionary of futures metadata\n        exchanges_data: dict, optional\n            A dictionary of exchanges metadata\n        root_symbols_data: dict, optional\n            A dictionary of root symbols metadata\n        equities_df: pandas.DataFrame, optional\n            A pandas.DataFrame of equity metadata\n        futures_df: pandas.DataFrame, optional\n            A pandas.DataFrame of futures metadata\n        exchanges_df: pandas.DataFrame, optional\n            A pandas.DataFrame of exchanges metadata\n        root_symbols_df: pandas.DataFrame, optional\n            A pandas.DataFrame of root symbols metadata\n        equities_identifiers: list, optional\n            A list of equities identifiers (sids, symbols, Assets)\n        futures_identifiers: list, optional\n            A list of futures identifiers (sids, symbols, Assets)\n        exchanges_identifiers: list, optional\n            A list of exchanges identifiers (ids or names)\n        root_symbols_identifiers: list, optional\n            A list of root symbols identifiers (ids or symbols)", "file": "zipline_finance_trading.py", "function": "TradingEnvironment.write_data", "project": "zipline", "change_stats": {"code_changes": 30, "doc_changes": 30, "total_changes": 60}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef format_usage(self, ctx, formatter):\n        \n        pieces = self.collect_usage_pieces(ctx)\n        formatter.write_usage(ctx.command_path, ' '.join(pieces))```\n\nUpdated code:\n```python\ndef format_usage(self, ctx, formatter):\n        \n        \n        pieces = self.collect_usage_pieces(ctx)\n        formatter.write_usage(ctx.command_path, \" \".join(pieces))\n```\n\nOriginal documentation:\n```python\nWrites the usage line into the formatter.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Writes the usage line into the formatter.\n\n        This is a low-level method called by :meth:`get_usage`.", "file": "pipenv_vendor_click_core.py", "function": "Command.format_usage", "project": "pipenv", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef tuple_positional_schema(self, schema: core_schema.TuplePositionalSchema) -> JsonSchemaValue:\n        \n        \n        json_schema: JsonSchemaValue = {'type': 'array'}\n        json_schema['minItems'] = len(schema['items_schema'])\n        prefixItems = [self.generate_inner(item) for item in schema['items_schema']]\n        if prefixItems:\n            json_schema['prefixItems'] = prefixItems\n        if 'extras_schema' in schema:\n            json_schema['items'] = self.generate_inner(schema['extras_schema'])\n        else:\n            json_schema['maxItems'] = len(schema['items_schema'])\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema```\n\nUpdated code:\n```python\ndef tuple_positional_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        \n        warnings.warn(\n            '`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.',\n            PydanticDeprecatedSince26,\n            stacklevel=2,\n        )\n        return self.tuple_schema(schema)\n```\n\nOriginal documentation:\n```python\nGenerates a JSON schema that matches a positional tuple schema e.g. `Tuple[int, str, bool]`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replaced by `tuple_schema`.", "file": "pydantic_json_schema.py", "function": "GenerateJsonSchema.tuple_positional_schema", "project": "pydantic", "change_stats": {"code_changes": 22, "doc_changes": 10, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _show(self, image: np.ndarray, name: str = \"\") -> None:\n         \n        \n        logger.debug(\"Updating preview: (name: %s)\", name)\n        try:\n            scriptpath = os.path.realpath(os.path.dirname(sys.argv[0]))\n            if self._args.write_image:\n                logger.debug(\"Saving preview to disk\")\n                img = \"training_preview.jpg\"\n                imgfile = os.path.join(scriptpath, img)\n                cv2.imwrite(imgfile, image)  # pylint: disable=no-member\n                logger.debug(\"Saved preview to: '%s'\", img)\n            if self._args.redirect_gui:\n                logger.debug(\"Generating preview for GUI\")\n                img = \".gui_training_preview.jpg\"\n                imgfile = os.path.join(scriptpath, \"lib\", \"gui\",\n                                       \".cache\", \"preview\", img)\n                cv2.imwrite(imgfile, image)  # pylint: disable=no-member\n                logger.debug(\"Generated preview for GUI: '%s'\", imgfile)\n            if self._args.preview:\n                logger.debug(\"Generating preview for display: '%s'\", name)\n                self._preview.buffer.add_image(name, image)\n                logger.debug(\"Generated preview for display: '%s'\", name)\n        except Exception as err:\n            logging.error(\"could not preview sample\")\n            raise err\n        logger.debug(\"Updated preview: (name: %s)\", name)```\n\nUpdated code:\n```python\ndef _show(self, image: np.ndarray, name: str = \"\") -> None:\n         \n        \n        logger.debug(\"Updating preview: (name: %s)\", name)\n        try:\n            scriptpath = os.path.realpath(os.path.dirname(sys.argv[0]))\n            if self._args.write_image:\n                logger.debug(\"Saving preview to disk\")\n                img = \"training_preview.png\"\n                imgfile = os.path.join(scriptpath, img)\n                cv2.imwrite(imgfile, image)  # pylint: disable=no-member\n                logger.debug(\"Saved preview to: '%s'\", img)\n            if self._args.redirect_gui:\n                logger.debug(\"Generating preview for GUI\")\n                img = \".gui_training_preview.png\"\n                imgfile = os.path.join(scriptpath, \"lib\", \"gui\", \".cache\", \"preview\", img)\n                cv2.imwrite(imgfile, image)  # pylint: disable=no-member\n                logger.debug(\"Generated preview for GUI: '%s'\", imgfile)\n            if self._args.preview:\n                logger.debug(\"Generating preview for display: '%s'\", name)\n                self._preview.buffer.add_image(name, image)\n                logger.debug(\"Generated preview for display: '%s'\", name)\n        except Exception as err:\n            logging.error(\"could not preview sample\")\n            raise err\n        logger.debug(\"Updated preview: (name: %s)\", name)\n```\n\nOriginal documentation:\n```python\nGenerate the preview and write preview file output.\n\n        Handles the output and display of preview images.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The preview image to be displayed and/or written out\n        name: str, optional\n            The name of the image for saving or display purposes. If an empty string is passed\n            then it will automatically be names. Default: \"\"\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate the preview and write preview file output.\n\n        Handles the output and display of preview images.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The preview image to be displayed and/or written out\n        name: str, optional\n            The name of the image for saving or display purposes. If an empty string is passed\n            then it will automatically be named. Default: \"\"", "file": "scripts_train.py", "function": "Train._show", "project": "faceswap", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _coerce_to_type(x):\n    \n    \n    \n    dtype = None\n\n    if is_datetime64tz_dtype(x):\n        dtype = x.dtype\n    elif is_datetime64_dtype(x):\n        x = to_datetime(x)\n        dtype = np.dtype(\"datetime64[ns]\")\n    elif is_timedelta64_dtype(x):\n        x = to_timedelta(x)\n        dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        # GH 20303\n        x = x.astype(np.int64)\n\n    if dtype is not None:\n        # GH 19768: force NaT to NaN during integer conversion\n        x = np.where(x.notna(), x.view(np.int64), np.nan)\n\n    return x, dtype```\n\nUpdated code:\n```python\ndef _coerce_to_type(x):\n    \n    \n    \n    dtype = None\n\n    if is_datetime64tz_dtype(x):\n        dtype = x.dtype\n    elif is_datetime64_dtype(x):\n        x = to_datetime(x)\n        dtype = np.dtype(\"datetime64[ns]\")\n    elif is_timedelta64_dtype(x):\n        x = to_timedelta(x)\n        dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        # GH 20303\n        x = x.astype(np.int64)\n    # To support cut and qcut for IntegerArray we convert to float dtype.\n    # Will properly support in the future.\n    # https://github.com/pandas-dev/pandas/pull/31290\n    # https://github.com/pandas-dev/pandas/issues/31389\n    elif is_extension_array_dtype(x) and is_integer_dtype(x):\n        x = x.to_numpy(dtype=np.float64, na_value=np.nan)\n\n    if dtype is not None:\n        # GH 19768: force NaT to NaN during integer conversion\n        x = np.where(x.notna(), x.view(np.int64), np.nan)\n\n    return x, dtype\n```\n\nOriginal documentation:\n```python\nif the passed data is of datetime/timedelta or bool type,\n    this method converts it to numeric so that cut or qcut method can\n    handle it\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "if the passed data is of datetime/timedelta, bool or nullable int type,\n    this method converts it to numeric so that cut or qcut method can\n    handle it", "file": "pandas_core_reshape_tile.py", "function": "_coerce_to_type", "project": "pandas", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fromFile(cls, pathOrFile):\n        \n        rootObject = readPlist(pathOrFile)\n        plist = cls()\n        plist.update(rootObject)\n        return plist```\n\nUpdated code:\n```python\ndef fromFile(cls, pathOrFile):\n        \n        with maybe_open(pathOrFile, 'rb') as fp:\n            value = load(fp)\n        plist = cls()\n        plist.update(value)\n        return plist\n```\n\nOriginal documentation:\n```python\nDeprecated. Use the readPlist() function instead.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Deprecated. Use the load() function instead.", "file": "Lib_plistlib.py", "function": "Plist.fromFile", "project": "cpython", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef eventplot(self, positions, orientation='horizontal', lineoffsets=1,\n                  linelengths=1, linewidths=None, colors=None,\n                  linestyles='solid', **kwargs):\n        \n        \n        \n\n        lineoffsets, linelengths = self._process_unit_info(\n                [(\"y\", lineoffsets), (\"y\", linelengths)], kwargs)\n\n        # fix positions, noting that it can be a list of lists:\n        if not np.iterable(positions):\n            positions = [positions]\n        elif any(np.iterable(position) for position in positions):\n            positions = [np.asanyarray(position) for position in positions]\n        else:\n            positions = [np.asanyarray(positions)]\n\n        if len(positions) == 0:\n            return []\n\n        poss = []\n        for position in positions:\n            poss += self._process_unit_info([(\"x\", position)], kwargs)\n        positions = poss\n\n        # prevent 'singular' keys from **kwargs dict from overriding the effect\n        # of 'plural' keyword arguments (e.g. 'color' overriding 'colors')\n        colors = cbook._local_over_kwdict(colors, kwargs, 'color')\n        linewidths = cbook._local_over_kwdict(linewidths, kwargs, 'linewidth')\n        linestyles = cbook._local_over_kwdict(linestyles, kwargs, 'linestyle')\n\n        if not np.iterable(lineoffsets):\n            lineoffsets = [lineoffsets]\n        if not np.iterable(linelengths):\n            linelengths = [linelengths]\n        if not np.iterable(linewidths):\n            linewidths = [linewidths]\n        if not np.iterable(colors):\n            colors = [colors]\n        if hasattr(linestyles, 'lower') or not np.iterable(linestyles):\n            linestyles = [linestyles]\n\n        lineoffsets = np.asarray(lineoffsets)\n        linelengths = np.asarray(linelengths)\n        linewidths = np.asarray(linewidths)\n\n        if len(lineoffsets) == 0:\n            lineoffsets = [None]\n        if len(linelengths) == 0:\n            linelengths = [None]\n        if len(linewidths) == 0:\n            lineoffsets = [None]\n        if len(linewidths) == 0:\n            lineoffsets = [None]\n        if len(colors) == 0:\n            colors = [None]\n        try:\n            # Early conversion of the colors into RGBA values to take care\n            # of cases like colors='0.5' or colors='C1'.  (Issue #8193)\n            colors = mcolors.to_rgba_array(colors)\n        except ValueError:\n            # Will fail if any element of *colors* is None. But as long\n            # as len(colors) == 1 or len(positions), the rest of the\n            # code should process *colors* properly.\n            pass\n\n        if len(lineoffsets) == 1 and len(positions) != 1:\n            lineoffsets = np.tile(lineoffsets, len(positions))\n            lineoffsets[0] = 0\n            lineoffsets = np.cumsum(lineoffsets)\n        if len(linelengths) == 1:\n            linelengths = np.tile(linelengths, len(positions))\n        if len(linewidths) == 1:\n            linewidths = np.tile(linewidths, len(positions))\n        if len(colors) == 1:\n            colors = list(colors)\n            colors = colors * len(positions)\n        if len(linestyles) == 1:\n            linestyles = [linestyles] * len(positions)\n\n        if len(lineoffsets) != len(positions):\n            raise ValueError('lineoffsets and positions are unequal sized '\n                             'sequences')\n        if len(linelengths) != len(positions):\n            raise ValueError('linelengths and positions are unequal sized '\n                             'sequences')\n        if len(linewidths) != len(positions):\n            raise ValueError('linewidths and positions are unequal sized '\n                             'sequences')\n        if len(colors) != len(positions):\n            raise ValueError('colors and positions are unequal sized '\n                             'sequences')\n        if len(linestyles) != len(positions):\n            raise ValueError('linestyles and positions are unequal sized '\n                             'sequences')\n\n        colls = []\n        for position, lineoffset, linelength, linewidth, color, linestyle in \\\n                zip(positions, lineoffsets, linelengths, linewidths,\n                    colors, linestyles):\n            coll = mcoll.EventCollection(position,\n                                         orientation=orientation,\n                                         lineoffset=lineoffset,\n                                         linelength=linelength,\n                                         linewidth=linewidth,\n                                         color=color,\n                                         linestyle=linestyle)\n            self.add_collection(coll, autolim=False)\n            coll._internal_update(kwargs)\n            colls.append(coll)\n\n        if len(positions) > 0:\n            # try to get min/max\n            min_max = [(np.min(_p), np.max(_p)) for _p in positions\n                       if len(_p) > 0]\n            # if we have any non-empty positions, try to autoscale\n            if len(min_max) > 0:\n                mins, maxes = zip(*min_max)\n                minpos = np.min(mins)\n                maxpos = np.max(maxes)\n\n                minline = (lineoffsets - linelengths).min()\n                maxline = (lineoffsets + linelengths).max()\n\n                if orientation == \"vertical\":\n                    corners = (minline, minpos), (maxline, maxpos)\n                else:  # \"horizontal\"\n                    corners = (minpos, minline), (maxpos, maxline)\n                self.update_datalim(corners)\n                self._request_autoscale_view()\n\n        return colls```\n\nUpdated code:\n```python\ndef eventplot(self, positions, orientation='horizontal', lineoffsets=1,\n                  linelengths=1, linewidths=None, colors=None, alpha=None,\n                  linestyles='solid', **kwargs):\n        \n        \n        \n\n        lineoffsets, linelengths = self._process_unit_info(\n                [(\"y\", lineoffsets), (\"y\", linelengths)], kwargs)\n\n        # fix positions, noting that it can be a list of lists:\n        if not np.iterable(positions):\n            positions = [positions]\n        elif any(np.iterable(position) for position in positions):\n            positions = [np.asanyarray(position) for position in positions]\n        else:\n            positions = [np.asanyarray(positions)]\n\n        if len(positions) == 0:\n            return []\n\n        poss = []\n        for position in positions:\n            poss += self._process_unit_info([(\"x\", position)], kwargs)\n        positions = poss\n\n        # prevent 'singular' keys from **kwargs dict from overriding the effect\n        # of 'plural' keyword arguments (e.g. 'color' overriding 'colors')\n        colors = cbook._local_over_kwdict(colors, kwargs, 'color')\n        linewidths = cbook._local_over_kwdict(linewidths, kwargs, 'linewidth')\n        linestyles = cbook._local_over_kwdict(linestyles, kwargs, 'linestyle')\n\n        if not np.iterable(lineoffsets):\n            lineoffsets = [lineoffsets]\n        if not np.iterable(linelengths):\n            linelengths = [linelengths]\n        if not np.iterable(linewidths):\n            linewidths = [linewidths]\n        if not np.iterable(colors):\n            colors = [colors]\n        if not np.iterable(alpha):\n            alpha = [alpha]\n        if hasattr(linestyles, 'lower') or not np.iterable(linestyles):\n            linestyles = [linestyles]\n\n        lineoffsets = np.asarray(lineoffsets)\n        linelengths = np.asarray(linelengths)\n        linewidths = np.asarray(linewidths)\n\n        if len(lineoffsets) == 0:\n            lineoffsets = [None]\n        if len(linelengths) == 0:\n            linelengths = [None]\n        if len(linewidths) == 0:\n            lineoffsets = [None]\n        if len(linewidths) == 0:\n            lineoffsets = [None]\n        if len(colors) == 0:\n            colors = [None]\n        try:\n            # Early conversion of the colors into RGBA values to take care\n            # of cases like colors='0.5' or colors='C1'.  (Issue #8193)\n            colors = mcolors.to_rgba_array(colors)\n        except ValueError:\n            # Will fail if any element of *colors* is None. But as long\n            # as len(colors) == 1 or len(positions), the rest of the\n            # code should process *colors* properly.\n            pass\n\n        if len(lineoffsets) == 1 and len(positions) != 1:\n            lineoffsets = np.tile(lineoffsets, len(positions))\n            lineoffsets[0] = 0\n            lineoffsets = np.cumsum(lineoffsets)\n        if len(linelengths) == 1:\n            linelengths = np.tile(linelengths, len(positions))\n        if len(linewidths) == 1:\n            linewidths = np.tile(linewidths, len(positions))\n        if len(colors) == 1:\n            colors = list(colors) * len(positions)\n        if len(alpha) == 1:\n            alpha = list(alpha) * len(positions)\n        if len(linestyles) == 1:\n            linestyles = [linestyles] * len(positions)\n\n        if len(lineoffsets) != len(positions):\n            raise ValueError('lineoffsets and positions are unequal sized '\n                             'sequences')\n        if len(linelengths) != len(positions):\n            raise ValueError('linelengths and positions are unequal sized '\n                             'sequences')\n        if len(linewidths) != len(positions):\n            raise ValueError('linewidths and positions are unequal sized '\n                             'sequences')\n        if len(colors) != len(positions):\n            raise ValueError('colors and positions are unequal sized '\n                             'sequences')\n        if len(alpha) != len(positions):\n            raise ValueError('alpha and positions are unequal sized '\n                             'sequences')\n        if len(linestyles) != len(positions):\n            raise ValueError('linestyles and positions are unequal sized '\n                             'sequences')\n\n        colls = []\n        for position, lineoffset, linelength, linewidth, color, alpha_, \\\n            linestyle in \\\n                zip(positions, lineoffsets, linelengths, linewidths,\n                    colors, alpha, linestyles):\n            coll = mcoll.EventCollection(position,\n                                         orientation=orientation,\n                                         lineoffset=lineoffset,\n                                         linelength=linelength,\n                                         linewidth=linewidth,\n                                         color=color,\n                                         alpha=alpha_,\n                                         linestyle=linestyle)\n            self.add_collection(coll, autolim=False)\n            coll._internal_update(kwargs)\n            colls.append(coll)\n\n        if len(positions) > 0:\n            # try to get min/max\n            min_max = [(np.min(_p), np.max(_p)) for _p in positions\n                       if len(_p) > 0]\n            # if we have any non-empty positions, try to autoscale\n            if len(min_max) > 0:\n                mins, maxes = zip(*min_max)\n                minpos = np.min(mins)\n                maxpos = np.max(maxes)\n\n                minline = (lineoffsets - linelengths).min()\n                maxline = (lineoffsets + linelengths).max()\n\n                if orientation == \"vertical\":\n                    corners = (minline, minpos), (maxline, maxpos)\n                else:  # \"horizontal\"\n                    corners = (minpos, minline), (maxpos, maxline)\n                self.update_datalim(corners)\n                self._request_autoscale_view()\n\n        return colls\n```\n\nOriginal documentation:\n```python\nPlot identical parallel lines at the given positions.\n\n        This type of plot is commonly used in neuroscience for representing\n        neural events, where it is usually called a spike raster, dot raster,\n        or raster plot.\n\n        However, it is useful in any situation where you wish to show the\n        timing or position of multiple sets of discrete events, such as the\n        arrival times of people to a business on each day of the month or the\n        date of hurricanes each year of the last century.\n\n        Parameters\n        ----------\n        positions : array-like or list of array-like\n            A 1D array-like defines the positions of one sequence of events.\n\n            Multiple groups of events may be passed as a list of array-likes.\n            Each group can be styled independently by passing lists of values\n            to *lineoffsets*, *linelengths*, *linewidths*, *colors* and\n            *linestyles*.\n\n            Note that *positions* can be a 2D array, but in practice different\n            event groups usually have different counts so that one will use a\n            list of different-length arrays rather than a 2D array.\n\n        orientation : {'horizontal', 'vertical'}, default: 'horizontal'\n            The direction of the event sequence:\n\n            - 'horizontal': the events are arranged horizontally.\n              The indicator lines are vertical.\n            - 'vertical': the events are arranged vertically.\n              The indicator lines are horizontal.\n\n        lineoffsets : float or array-like, default: 1\n            The offset of the center of the lines from the origin, in the\n            direction orthogonal to *orientation*.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        linelengths : float or array-like, default: 1\n            The total height of the lines (i.e. the lines stretches from\n            ``lineoffset - linelength/2`` to ``lineoffset + linelength/2``).\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        linewidths : float or array-like, default: :rc:`lines.linewidth`\n            The line width(s) of the event lines, in points.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        colors : color or list of colors, default: :rc:`lines.color`\n            The color(s) of the event lines.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        linestyles : str or tuple or list of such values, default: 'solid'\n            Default is 'solid'. Valid strings are ['solid', 'dashed',\n            'dashdot', 'dotted', '-', '--', '-.', ':']. Dash tuples\n            should be of the form::\n\n                (offset, onoffseq),\n\n            where *onoffseq* is an even length tuple of on and off ink\n            in points.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n\n        **kwargs\n            Other keyword arguments are line collection properties.  See\n            `.LineCollection` for a list of the valid properties.\n\n        Returns\n        -------\n        list of `.EventCollection`\n            The `.EventCollection` that were added.\n\n        Notes\n        -----\n        For *linelengths*, *linewidths*, *colors*, and *linestyles*, if only\n        a single value is given, that value is applied to all lines.  If an\n        array-like is given, it must have the same length as *positions*, and\n        each value will be applied to the corresponding row of the array.\n\n        Examples\n        --------\n        .. plot:: gallery/lines_bars_and_markers/eventplot_demo.py\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Plot identical parallel lines at the given positions.\n\n        This type of plot is commonly used in neuroscience for representing\n        neural events, where it is usually called a spike raster, dot raster,\n        or raster plot.\n\n        However, it is useful in any situation where you wish to show the\n        timing or position of multiple sets of discrete events, such as the\n        arrival times of people to a business on each day of the month or the\n        date of hurricanes each year of the last century.\n\n        Parameters\n        ----------\n        positions : array-like or list of array-like\n            A 1D array-like defines the positions of one sequence of events.\n\n            Multiple groups of events may be passed as a list of array-likes.\n            Each group can be styled independently by passing lists of values\n            to *lineoffsets*, *linelengths*, *linewidths*, *colors* and\n            *linestyles*.\n\n            Note that *positions* can be a 2D array, but in practice different\n            event groups usually have different counts so that one will use a\n            list of different-length arrays rather than a 2D array.\n\n        orientation : {'horizontal', 'vertical'}, default: 'horizontal'\n            The direction of the event sequence:\n\n            - 'horizontal': the events are arranged horizontally.\n              The indicator lines are vertical.\n            - 'vertical': the events are arranged vertically.\n              The indicator lines are horizontal.\n\n        lineoffsets : float or array-like, default: 1\n            The offset of the center of the lines from the origin, in the\n            direction orthogonal to *orientation*.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        linelengths : float or array-like, default: 1\n            The total height of the lines (i.e. the lines stretches from\n            ``lineoffset - linelength/2`` to ``lineoffset + linelength/2``).\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        linewidths : float or array-like, default: :rc:`lines.linewidth`\n            The line width(s) of the event lines, in points.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        colors : color or list of colors, default: :rc:`lines.color`\n            The color(s) of the event lines.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        alpha : float or array-like, default: 1\n            The alpha blending value(s), between 0 (transparent) and 1\n            (opaque).\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        linestyles : str or tuple or list of such values, default: 'solid'\n            Default is 'solid'. Valid strings are ['solid', 'dashed',\n            'dashdot', 'dotted', '-', '--', '-.', ':']. Dash tuples\n            should be of the form::\n\n                (offset, onoffseq),\n\n            where *onoffseq* is an even length tuple of on and off ink\n            in points.\n\n            If *positions* is 2D, this can be a sequence with length matching\n            the length of *positions*.\n\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n\n        **kwargs\n            Other keyword arguments are line collection properties.  See\n            `.LineCollection` for a list of the valid properties.\n\n        Returns\n        -------\n        list of `.EventCollection`\n            The `.EventCollection` that were added.\n\n        Notes\n        -----\n        For *linelengths*, *linewidths*, *colors*, *alpha* and *linestyles*, if\n        only a single value is given, that value is applied to all lines. If an\n        array-like is given, it must have the same length as *positions*, and\n        each value will be applied to the corresponding row of the array.\n\n        Examples\n        --------\n        .. plot:: gallery/lines_bars_and_markers/eventplot_demo.py", "file": "lib_matplotlib_axes__axes.py", "function": "eventplot", "project": "matplotlib", "change_stats": {"code_changes": 20, "doc_changes": 13, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_insert_content(self):\n        \n        \n        \n        self.panel.process_request(self.request)\n        self.panel.process_response(self.request, self.response)\n        # ensure the panel does not have content yet.\n        self.assertNotIn(\n            \"django.contrib.staticfiles.finders.\" \"AppDirectoriesFinder\",\n            self.panel.content,\n        )\n        self.panel.generate_stats(self.request, self.response)\n        # ensure the panel renders correctly.\n        self.assertIn(\n            \"django.contrib.staticfiles.finders.\" \"AppDirectoriesFinder\",\n            self.panel.content,\n        )\n        self.assertValidHTML(self.panel.content)```\n\nUpdated code:\n```python\ndef test_insert_content(self):\n        \n        \n        \n        response = self.panel.process_request(self.request)\n        # ensure the panel does not have content yet.\n        self.assertNotIn(\n            \"django.contrib.staticfiles.finders.\" \"AppDirectoriesFinder\",\n            self.panel.content,\n        )\n        self.panel.generate_stats(self.request, response)\n        # ensure the panel renders correctly.\n        self.assertIn(\n            \"django.contrib.staticfiles.finders.\" \"AppDirectoriesFinder\",\n            self.panel.content,\n        )\n        self.assertValidHTML(self.panel.content)\n```\n\nOriginal documentation:\n```python\nTest that the panel only inserts content after generate_stats and\n        not the process_response.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test that the panel only inserts content after generate_stats and\n        not the process_request.", "file": "tests_panels_test_staticfiles.py", "function": "StaticFilesPanelTestCase.test_insert_content", "project": "django-debug-toolbar", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef exec_cmd(cmd: List[str],\n             throw_on_error: bool = True,\n             stream_output: bool = False) -> Union[int, Tuple[int, str, str]]:\n    \n    \n    \n    if stream_output:\n        child = subprocess.Popen(\n            cmd, universal_newlines=True, stdin=subprocess.PIPE)\n        child.communicate()\n        exit_code = child.wait()\n        if throw_on_error and exit_code != 0:\n            raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\n        return exit_code\n    else:\n        child = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True)\n        (stdout, stderr) = child.communicate()\n        exit_code = child.wait()\n        if throw_on_error and exit_code != 0:\n            raise ShellCommandException(\n                \"Non-zero exit code: %s\\n\\nSTDOUT:\\n%s\\n\\nSTDERR:%s\" %\n                (exit_code, stdout, stderr))\n        return exit_code, stdout, stderr```\n\nUpdated code:\n```python\ndef exec_cmd(cmd: List[str],\n             throw_on_error: bool = True,\n             logger: Optional[logging.Logger] = None\n             ) -> Union[int, Tuple[int, str, str]]:\n    \n    \n    \n    child = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stdin=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True)\n    (stdout, stderr) = child.communicate()\n    exit_code = child.wait()\n    if throw_on_error and exit_code != 0:\n        raise ShellCommandException(\n            \"Non-zero exit code: %s\\n\\nSTDOUT:\\n%s\\n\\nSTDERR:%s\" %\n            (exit_code, stdout, stderr))\n    return exit_code, stdout, stderr\n```\n\nOriginal documentation:\n```python\nRuns a command as a child process.\n\n    A convenience wrapper for running a command from a Python script.\n\n    Note on the return value: If stream_output is true, then only the exit code\n    is returned. If stream_output is false, then a tuple of the exit code,\n    standard output and standard error is returned.\n\n    Args:\n        cmd: the command to run, as a list of strings\n        throw_on_error: if true, raises an Exception if the exit code of the\n            program is nonzero\n        stream_output: if true, does not capture standard output and error; if\n            false, captures these, streams and returns them\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Runs a command as a child process.\n\n    A convenience wrapper for running a command from a Python script.\n\n    Note on the return value: A tuple of the exit code,\n    standard output and standard error is returned.\n\n    Args:\n        cmd: the command to run, as a list of strings\n        throw_on_error: if true, raises an Exception if the exit code of the\n            program is nonzero", "file": "python_ray__private_runtime_env_conda_utils.py", "function": "exec_cmd", "project": "ray", "change_stats": {"code_changes": 40, "doc_changes": 7, "total_changes": 47}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_eos_threshold(self, log_probs):\n        \n        \n        \n        max_probs, _ = torch.max(log_probs, dim=-1)\n        eos_probs = log_probs[:, self.eos_index]\n        cond = eos_probs > (self.eos_threshold * max_probs)\n        return cond```\n\nUpdated code:\n```python\ndef _check_eos_threshold(self, log_probs):\n        \n        \n        max_probs, _ = torch.max(log_probs, dim=-1)\n        eos_probs = log_probs[:, self.eos_index]\n        cond = eos_probs > (self.eos_threshold * max_probs)\n        return cond\n```\n\nOriginal documentation:\n```python\nThis method checks whether eos log-probabilities exceed threshold.\n\n        Arguments\n        ---------\n        log_probs : torch.Tensor\n            The log-probabilities.\n\n        Return\n        ------\n        cond : torch.BoolTensor\n            Each element represents whether the eos log-probabilities will be kept.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method checks whether eos log-probabilities exceed threshold.\n\n        Arguments\n        ---------\n        log_probs : torch.Tensor\n            The log-probabilities.\n\n        Returns\n        ------\n        cond : torch.BoolTensor\n            Each element represents whether the eos log-probabilities will be kept.", "file": "speechbrain_decoders_seq2seq.py", "function": "S2SBeamSearcher._check_eos_threshold", "project": "speechbrain", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef success(self, message):\n        \n        \n        \n        self.logger.log(LogVerbosity.SUCCESS.value, message)```\n\nUpdated code:\n```python\ndef success(self, *args, **kwargs):\n        \n        \n        \n        with ColorContext(color='green'):\n            self.logger.info(*args, **kwargs)\n```\n\nOriginal documentation:\n```python\nPrints messages as success\n\n        :param message: Message to log\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Prints messages as success\n\n        .. #noqa: DAR101", "file": "jina_logging_logger.py", "function": "JinaLogger.success", "project": "jina", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef should_retrain(\n    new_fingerprint: Fingerprint, old_model: Text, train_path: Text\n) -> FingerprintComparisonResult:\n    \n\n    \n    fingerprint_comparison = FingerprintComparisonResult()\n\n    if old_model is None or not os.path.exists(old_model):\n        return fingerprint_comparison\n\n    with unpack_model(old_model) as unpacked:\n        last_fingerprint = fingerprint_from_path(unpacked)\n        old_core, old_nlu = get_model_subdirectories(unpacked)\n\n        fingerprint_comparison = FingerprintComparisonResult(\n            core=did_section_fingerprint_change(\n                last_fingerprint, new_fingerprint, SECTION_CORE\n            ),\n            nlu=did_section_fingerprint_change(\n                last_fingerprint, new_fingerprint, SECTION_NLU\n            ),\n            nlg=did_section_fingerprint_change(\n                last_fingerprint, new_fingerprint, SECTION_NLG\n            ),\n        )\n\n        core_merge_failed = False\n        if not fingerprint_comparison.should_retrain_core():\n            target_path = os.path.join(train_path, DEFAULT_CORE_SUBDIRECTORY_NAME)\n            core_merge_failed = not move_model(old_core, target_path)\n            fingerprint_comparison.core = core_merge_failed\n\n        if not fingerprint_comparison.should_retrain_nlg() and core_merge_failed:\n            # If moving the Core model failed, we should also retrain NLG\n            fingerprint_comparison.nlg = True\n\n        if not fingerprint_comparison.should_retrain_nlu():\n            target_path = os.path.join(train_path, \"nlu\")\n            fingerprint_comparison.nlu = not move_model(old_nlu, target_path)\n\n        return fingerprint_comparison```\n\nUpdated code:\n```python\ndef should_retrain(\n    new_fingerprint: Fingerprint,\n    old_model: Text,\n    train_path: Text,\n    has_e2e_examples: bool = False,\n    force_training: bool = False,\n) -> FingerprintComparisonResult:\n    \n\n    \n    fingerprint_comparison = FingerprintComparisonResult()\n\n    if old_model is None or not os.path.exists(old_model):\n        return fingerprint_comparison\n\n    with unpack_model(old_model) as unpacked:\n        last_fingerprint = fingerprint_from_path(unpacked)\n        old_core, old_nlu = get_model_subdirectories(unpacked)\n\n        fingerprint_comparison = FingerprintComparisonResult(\n            core=did_section_fingerprint_change(\n                last_fingerprint, new_fingerprint, SECTION_CORE\n            ),\n            nlu=did_section_fingerprint_change(\n                last_fingerprint, new_fingerprint, SECTION_NLU\n            ),\n            nlg=did_section_fingerprint_change(\n                last_fingerprint, new_fingerprint, SECTION_NLG\n            ),\n            force_training=force_training,\n        )\n\n        # We should retrain core if nlu data changes and there are e2e stories.\n        if has_e2e_examples and fingerprint_comparison.should_retrain_nlu():\n            fingerprint_comparison.core = True\n\n        core_merge_failed = False\n        if not fingerprint_comparison.should_retrain_core():\n            target_path = os.path.join(train_path, DEFAULT_CORE_SUBDIRECTORY_NAME)\n            core_merge_failed = not move_model(old_core, target_path)\n            fingerprint_comparison.core = core_merge_failed\n\n        if not fingerprint_comparison.should_retrain_nlg() and core_merge_failed:\n            # If moving the Core model failed, we should also retrain NLG\n            fingerprint_comparison.nlg = True\n\n        if not fingerprint_comparison.should_retrain_nlu():\n            target_path = os.path.join(train_path, \"nlu\")\n            fingerprint_comparison.nlu = not move_model(old_nlu, target_path)\n\n        return fingerprint_comparison\n```\n\nOriginal documentation:\n```python\nCheck which components of a model should be retrained.\n\n    Args:\n        new_fingerprint: The fingerprint of the new model to be trained.\n        old_model: Path to the old zipped model file.\n        train_path: Path to the directory in which the new model will be trained.\n\n    Returns:\n        A FingerprintComparisonResult object indicating whether Rasa Core and/or Rasa NLU needs\n        to be retrained or not.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check which components of a model should be retrained.\n\n    Args:\n        new_fingerprint: The fingerprint of the new model to be trained.\n        old_model: Path to the old zipped model file.\n        train_path: Path to the directory in which the new model will be trained.\n        has_e2e_examples: Whether the new training data contains e2e examples.\n        force_training: Indicates if the model needs to be retrained even if the data has not changed.\n\n    Returns:\n        A FingerprintComparisonResult object indicating whether Rasa Core and/or Rasa NLU needs\n        to be retrained or not.", "file": "rasa_model.py", "function": "should_retrain", "project": "rasa", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef progress_bar_callback(self) -> Optional[ProgressBarBase]:\n        \n        for c in self.callbacks:\n            if isinstance(c, ProgressBarBase):\n                return c\n        return None```\n\nUpdated code:\n```python\ndef progress_bar_callback(self) -> Optional[ProgressBar]:\n        \n        for c in self.callbacks:\n            if isinstance(c, ProgressBar):\n                return c\n        return None\n```\n\nOriginal documentation:\n```python\nAn instance of :class:`~lightning.pytorch.callbacks.progress.base.ProgressBarBase` found in the\n        Trainer.callbacks list, or ``None`` if one doesn't exist.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the\n        Trainer.callbacks list, or ``None`` if one doesn't exist.", "file": "src_lightning_pytorch_trainer_trainer.py", "function": "Trainer.progress_bar_callback", "project": "pytorch-lightning", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef clear_sessions(user=None, keep_current=False, device=None, force=False):\n\t\n\t\n\n\treason = \"Logged In From Another Session\"\n\tif force:\n\t\treason = \"Force Logged out by the user\"\n\n\tfor sid in get_sessions_to_clear(user, keep_current, device):\n\t\tdelete_session(sid, reason=reason)```\n\nUpdated code:\n```python\ndef clear_sessions(user=None, keep_current=False, force=False):\n\t\n\t\n\n\treason = \"Logged In From Another Session\"\n\tif force:\n\t\treason = \"Force Logged out by the user\"\n\n\tfor sid in get_sessions_to_clear(user, keep_current):\n\t\tdelete_session(sid, reason=reason)\n```\n\nOriginal documentation:\n```python\nClear other sessions of the current user. Called at login / logout\n\n\t:param user: user name (default: current user)\n\t:param keep_current: keep current session (default: false)\n\t:param device: delete sessions of this device (default: desktop, mobile)\n\t:param force: triggered by the user (default false)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Clear other sessions of the current user. Called at login / logout\n\n\t:param user: user name (default: current user)\n\t:param keep_current: keep current session (default: false)\n\t:param force: triggered by the user (default false)", "file": "frappe_sessions.py", "function": "clear_sessions", "project": "frappe", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef peak_signal_noise_ratio(im_true, im_test, data_range=None):\n    \n    \n\n    \n    check_shape_equality(im_true, im_test)\n\n    if data_range is None:\n        if im_true.dtype != im_test.dtype:\n            warn(\"Inputs have mismatched dtype.  Setting data_range based on \"\n                 \"im_true.\")\n        dmin, dmax = dtype_range[im_true.dtype.type]\n        true_min, true_max = np.min(im_true), np.max(im_true)\n        if true_max > dmax or true_min < dmin:\n            raise ValueError(\n                \"im_true has intensity values outside the range expected for \"\n                \"its data type.  Please manually specify the data_range\")\n        if true_min >= 0:\n            # most common case (255 for uint8, 1 for float)\n            data_range = dmax\n        else:\n            data_range = dmax - dmin\n\n    im_true, im_test = _as_floats(im_true, im_test)\n\n    err = mean_squared_error(im_true, im_test)\n    return 10 * np.log10((data_range ** 2) / err)```\n\nUpdated code:\n```python\ndef peak_signal_noise_ratio(im_true, im_test, data_range=None):\n    \n    \n\n    \n    check_shape_equality(im_true, im_test)\n\n    if data_range is None:\n        if im_true.dtype != im_test.dtype:\n            warn(\"Inputs have mismatched dtype.  Setting data_range based on \"\n                 \"im_true.\", stacklevel=2)\n        dmin, dmax = dtype_range[im_true.dtype.type]\n        true_min, true_max = np.min(im_true), np.max(im_true)\n        if true_max > dmax or true_min < dmin:\n            raise ValueError(\n                \"im_true has intensity values outside the range expected for \"\n                \"its data type.  Please manually specify the data_range\")\n        if true_min >= 0:\n            # most common case (255 for uint8, 1 for float)\n            data_range = dmax\n        else:\n            data_range = dmax - dmin\n\n    im_true, im_test = _as_floats(im_true, im_test)\n\n    err = mean_squared_error(im_true, im_test)\n    return 10 * np.log10((data_range ** 2) / err)\n```\n\nOriginal documentation:\n```python\nCompute the peak signal to noise ratio (PSNR) for an image.\n\n    Parameters\n    ----------\n    im_true : ndarray\n        Ground-truth image, same shape as im_test.\n    im_test : ndarray\n        Test image.\n    data_range : int, optional\n        The data range of the input image (distance between minimum and\n        maximum possible values).  By default, this is estimated from the image\n        data-type.\n\n    Returns\n    -------\n    psnr : float\n        The PSNR metric.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute the peak signal to noise ratio (PSNR) for an image.\n\n    Parameters\n    ----------\n    im_true : ndarray\n        Ground-truth image, same shape as im_test.\n    im_test : ndarray\n        Test image.\n    data_range : int, optional\n        The data range of the input image (distance between minimum and\n        maximum possible values).  By default, this is estimated from the image\n        data-type.\n\n    Returns\n    -------\n    psnr : float\n        The PSNR metric.\n\n    Notes\n    -----\n    .. versionchanged:: 0.16\n        This function was renamed from ``skimage.measure.compare_psnr`` to\n        ``skimage.metrics.peak_singal_noise_ratio``.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio", "file": "skimage_metrics_simple_metrics.py", "function": "peak_signal_noise_ratio", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef main(idea: str = DEFAULT_PLAYER_SETUP, investment: float = 3.0, n_round: int = 100):\n    \n    \n    \n    asyncio.run(start_game(idea, investment, n_round))```\n\nUpdated code:\n```python\ndef main(investment: float = 3.0, n_round: int = 100):\n    \n    \n    \n    asyncio.run(start_game(investment, n_round))\n```\n\nOriginal documentation:\n```python\n:param idea: game config instructions\n    :param investment: contribute a certain dollar amount to watch the debate\n    :param n_round: maximum rounds of the debate\n    :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":param investment: contribute a certain dollar amount to watch the debate\n    :param n_round: maximum rounds of the debate\n    :return:", "file": "examples_werewolf_game_start_game.py", "function": "main", "project": "MetaGPT", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update_cache(self, soco: SoCo) -> set[Alarm]:\n        \n        \n        soco_alarms = get_alarms(soco)\n        new_alarms = set()\n\n        for alarm in soco_alarms:\n            if alarm.alarm_id not in self._alarms:\n                new_alarms.add(alarm)\n                self._alarms[alarm.alarm_id] = alarm\n\n        for alarm_id, alarm in list(self._alarms.items()):\n            if alarm not in soco_alarms:\n                self._alarms.pop(alarm_id)\n\n        return new_alarms```\n\nUpdated code:\n```python\ndef update_cache(self, soco: SoCo, update_id: int | None = None) -> bool:\n        \n        try:\n            self.alarms.update(soco)\n        except (OSError, SoCoException) as err:\n            _LOGGER.error(\"Could not update alarms using %s: %s\", soco, err)\n            return False\n\n        if update_id and self.alarms.last_id < update_id:\n            # Skip updates if latest query result is outdated or lagging\n            return False\n\n        if (\n            self.last_processed_event_id\n            and self.alarms.last_id <= self.last_processed_event_id\n        ):\n            # Skip updates already processed\n            return False\n\n        _LOGGER.debug(\n            \"Updating processed event %s from %s (was %s)\",\n            self.alarms.last_id,\n            soco,\n            self.last_processed_event_id,\n        )\n        self.last_processed_event_id = self.alarms.last_id\n        return True\n```\n\nOriginal documentation:\n```python\nPopulate cache of known alarms.\n\n        Prune deleted alarms and return new alarms.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update cache of known alarms and return if cache has changed.", "file": "homeassistant_components_sonos_alarms.py", "function": "SonosAlarms.update_cache", "project": "core", "change_stats": {"code_changes": 37, "doc_changes": 6, "total_changes": 43}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef market_is_tradable(self, market: Dict[str, Any]) -> bool:\n        \n        \n        \n        symbol_parts = market['symbol'].split('/')\n        return (len(symbol_parts) == 2 and\n                len(symbol_parts[0]) > 0 and\n                len(symbol_parts[1]) > 0 and\n                symbol_parts[0] == market.get('base') and\n                symbol_parts[1] == market.get('quote')\n                )```\n\nUpdated code:\n```python\ndef market_is_tradable(self, market: Dict[str, Any]) -> bool:\n        \n        \n        \n        return (\n            market.get('quote', None) is not None\n            and market.get('base', None) is not None\n            and (self.trading_mode == TradingMode.SPOT and self.market_is_spot(market))\n            or (self.trading_mode == TradingMode.MARGIN and self.market_is_margin(market))\n            or (self.trading_mode == TradingMode.FUTURES and self.market_is_future(market))\n        )\n```\n\nOriginal documentation:\n```python\nCheck if the market symbol is tradable by Freqtrade.\n        By default, checks if it's splittable by `/` and both sides correspond to base / quote\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if the market symbol is tradable by Freqtrade.\n        Ensures that Configured mode aligns to", "file": "freqtrade_exchange_exchange.py", "function": "Exchange.market_is_tradable", "project": "freqtrade", "change_stats": {"code_changes": 16, "doc_changes": 4, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _syscmd_ver(system='', release='', version='',\n\n               supported_platforms=('win32','win16','dos','os2')):\n\n     \n\n    \n    if sys.platform not in supported_platforms:\n        return system,release,version\n\n    # Try some common cmd strings\n    for cmd in ('ver','command /c ver','cmd /c ver'):\n        try:\n            pipe = popen(cmd)\n            info = pipe.read()\n            if pipe.close():\n                raise os.error('command failed')\n            # XXX How can I suppress shell errors from being written\n            #     to stderr ?\n        except os.error as why:\n            #print 'Command %s failed: %s' % (cmd,why)\n            continue\n        except IOError as why:\n            #print 'Command %s failed: %s' % (cmd,why)\n            continue\n        else:\n            break\n    else:\n        return system,release,version\n\n    # Parse the output\n    info = info.strip()\n    m = _ver_output.match(info)\n    if m is not None:\n        system,release,version = m.groups()\n        # Strip trailing dots from version and release\n        if release[-1] == '.':\n            release = release[:-1]\n        if version[-1] == '.':\n            version = version[:-1]\n        # Normalize the version and build strings (eliminating additional\n        # zeros)\n        version = _norm_version(version)\n    return system,release,version```\n\nUpdated code:\n```python\ndef _syscmd_ver(system='', release='', version='',\n\n               supported_platforms=('win32', 'win16', 'dos')):\n\n     \n\n    \n    if sys.platform not in supported_platforms:\n        return system, release, version\n\n    # Try some common cmd strings\n    for cmd in ('ver', 'command /c ver', 'cmd /c ver'):\n        try:\n            pipe = popen(cmd)\n            info = pipe.read()\n            if pipe.close():\n                raise OSError('command failed')\n            # XXX How can I suppress shell errors from being written\n            #     to stderr ?\n        except OSError as why:\n            #print 'Command %s failed: %s' % (cmd, why)\n            continue\n        else:\n            break\n    else:\n        return system, release, version\n\n    # Parse the output\n    info = info.strip()\n    m = _ver_output.match(info)\n    if m is not None:\n        system, release, version = m.groups()\n        # Strip trailing dots from version and release\n        if release[-1] == '.':\n            release = release[:-1]\n        if version[-1] == '.':\n            version = version[:-1]\n        # Normalize the version and build strings (eliminating additional\n        # zeros)\n        version = _norm_version(version)\n    return system, release, version\n```\n\nOriginal documentation:\n```python\nTries to figure out the OS version used and returns\n        a tuple (system,release,version).\n\n        It uses the \"ver\" shell command for this which is known\n        to exists on Windows, DOS and OS/2. XXX Others too ?\n\n        In case this fails, the given parameters are used as\n        defaults.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tries to figure out the OS version used and returns\n        a tuple (system, release, version).\n\n        It uses the \"ver\" shell command for this which is known\n        to exists on Windows, DOS. XXX Others too ?\n\n        In case this fails, the given parameters are used as\n        defaults.", "file": "Lib_platform.py", "function": "_syscmd_ver", "project": "cpython", "change_stats": {"code_changes": 23, "doc_changes": 6, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extract_info(self, url, download=True, ie_key=None, extra_info={},\n                     process=True, force_generic_extractor=False):\n        '''\n        \n        '''\n\n        if not ie_key and force_generic_extractor:\n            ie_key = 'Generic'\n\n        if ie_key:\n            ies = [self.get_info_extractor(ie_key)]\n        else:\n            ies = self._ies\n\n        for ie in ies:\n            if not ie.suitable(url):\n                continue\n\n            ie = self.get_info_extractor(ie.ie_key())\n            if not ie.working():\n                self.report_warning('The program functionality for this site has been marked as broken, '\n                                    'and will probably not work.')\n\n            return self.__extract_info(url, ie, download, extra_info, process)\n        else:\n            self.report_error('no suitable InfoExtractor for URL %s' % url)```\n\nUpdated code:\n```python\ndef extract_info(self, url, download=True, ie_key=None, extra_info={},\n                     process=True, force_generic_extractor=False):\n        \n        \n        \n\n        if not ie_key and force_generic_extractor:\n            ie_key = 'Generic'\n\n        if ie_key:\n            ies = [self.get_info_extractor(ie_key)]\n        else:\n            ies = self._ies\n\n        for ie in ies:\n            if not ie.suitable(url):\n                continue\n\n            ie = self.get_info_extractor(ie.ie_key())\n            if not ie.working():\n                self.report_warning('The program functionality for this site has been marked as broken, '\n                                    'and will probably not work.')\n\n            return self.__extract_info(url, ie, download, extra_info, process)\n        else:\n            self.report_error('no suitable InfoExtractor for URL %s' % url)\n```\n\nOriginal documentation:\n```python\nReturns a list with a dictionary for each video we find.\n        If 'download', also downloads the videos.\n        extra_info is a dict containing the extra values to add to each result\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a list with a dictionary for each video extracted.\n\n        Arguments:\n        url -- URL to extract\n\n        Keyword arguments:\n        download -- whether to download videos during extraction\n        ie_key -- extractor key hint\n        extra_info -- dictionary containing the extra values to add to each result\n        process -- whether to resolve all unresolved references (URLs, playlist items),\n            must be True for download to work.\n        force_generic_extractor -- force using the generic extractor", "file": "youtube_dl_YoutubeDL.py", "function": "YoutubeDL.extract_info", "project": "youtube-dl", "change_stats": {"code_changes": 6, "doc_changes": 17, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_discovery_info(component_setup, groups, controller_id):\n    \n    discovery_data = {}\n    for group in groups:\n        groupname = group.attrib[\"name\"]\n        for product_cfg in component_setup:\n            products = group.findall(product_cfg[CONF_XPATH])\n            for product in products:\n                nodes = product.findall(product_cfg[CONF_NODE])\n                for node in nodes:\n                    if \"setting\" in node.attrib and node.attrib[\"setting\"] == \"yes\":\n                        continue\n                    ihc_id = int(node.attrib[\"id\"].strip(\"_\"), 0)\n                    name = f\"{groupname}_{ihc_id}\"\n                    device = {\n                        \"ihc_id\": ihc_id,\n                        \"ctrl_id\": controller_id,\n                        \"product\": {\n                            \"name\": product.get(\"name\") or \"\",\n                            \"note\": product.get(\"note\") or \"\",\n                            \"position\": product.get(\"position\") or \"\",\n                        },\n                        \"product_cfg\": product_cfg,\n                    }\n                    discovery_data[name] = device\n    return discovery_data```\n\nUpdated code:\n```python\ndef get_discovery_info(platform_setup, groups, controller_id):\n    \n    discovery_data = {}\n    for group in groups:\n        groupname = group.attrib[\"name\"]\n        for product_cfg in platform_setup:\n            products = group.findall(product_cfg[CONF_XPATH])\n            for product in products:\n                nodes = product.findall(product_cfg[CONF_NODE])\n                for node in nodes:\n                    if \"setting\" in node.attrib and node.attrib[\"setting\"] == \"yes\":\n                        continue\n                    ihc_id = int(node.attrib[\"id\"].strip(\"_\"), 0)\n                    name = f\"{groupname}_{ihc_id}\"\n                    device = {\n                        \"ihc_id\": ihc_id,\n                        \"ctrl_id\": controller_id,\n                        \"product\": {\n                            \"name\": product.get(\"name\") or \"\",\n                            \"note\": product.get(\"note\") or \"\",\n                            \"position\": product.get(\"position\") or \"\",\n                        },\n                        \"product_cfg\": product_cfg,\n                    }\n                    discovery_data[name] = device\n    return discovery_data\n```\n\nOriginal documentation:\n```python\nGet discovery info for specified IHC component.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get discovery info for specified IHC platform.", "file": "homeassistant_components_ihc___init__.py", "function": "get_discovery_info", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rgb2gray(image):\n    \n    \n    \n    r, g, b = cv2.split(image)\n    return cv2.add(\n        cv2.multiply(cv2.max(cv2.max(r, g), b), 0.5),\n        cv2.multiply(cv2.min(cv2.min(r, g), b), 0.5)\n    )```\n\nUpdated code:\n```python\ndef rgb2gray(image):\n    \n    \n    \n    # r, g, b = cv2.split(image)\n    # return cv2.add(\n    #     cv2.multiply(cv2.max(cv2.max(r, g), b), 0.5),\n    #     cv2.multiply(cv2.min(cv2.min(r, g), b), 0.5)\n    # )\n    r, g, b = cv2.split(image)\n    maximum = cv2.max(r, g)\n    cv2.max(maximum, b, dst=maximum)\n    cv2.multiply(maximum, 0.5, dst=maximum)\n    cv2.min(r, g, dst=r)\n    cv2.min(r, b, dst=r)\n    cv2.multiply(r, 0.5, dst=r)\n    # minimum = r\n    cv2.add(maximum, r, dst=maximum)\n    return maximum\n```\n\nOriginal documentation:\n```python\nArgs:\n        image (np.ndarray): Shape (height, width, channel)\n\n    Returns:\n        np.ndarray: Shape (height, width)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "gray = ( MAX(r, g, b) + MIN(r, g, b)) / 2\n\n    Args:\n        image (np.ndarray): Shape (height, width, channel)\n\n    Returns:\n        np.ndarray: Shape (height, width)", "file": "module_base_utils.py", "function": "rgb2gray", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 20, "doc_changes": 6, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_transaction_fees(self, codes=None, params={}):\n        \n        \n        \n        self.load_markets()\n        response = self.privateWalletGetWithdrawStatus(params)\n        #\n        #    {\n        #        \"currency\": \"MTN\",\n        #        \"name\": \"Medicalchain\",\n        #        \"name_cn\": \"Medicalchain\",\n        #        \"deposit\": \"0\",\n        #        \"withdraw_percent\": \"0%\",\n        #        \"withdraw_fix\": \"900\",\n        #        \"withdraw_day_limit\": \"500000\",\n        #        \"withdraw_day_limit_remain\": \"500000\",\n        #        \"withdraw_amount_mini\": \"900.1\",\n        #        \"withdraw_eachtime_limit\": \"90000000000\",\n        #        \"withdraw_fix_on_chains\": {\n        #            \"ETH\": \"900\"\n        #        }\n        #    }\n        #\n        withdrawFees = {}\n        for i in range(0, len(response)):\n            entry = response[i]\n            currencyId = self.safe_string(entry, 'currency')\n            code = self.safe_currency_code(currencyId)\n            withdrawFees[code] = {}\n            withdrawFix = self.safe_value(entry, 'withdraw_fix_on_chains')\n            if withdrawFix is None:\n                withdrawFix = {}\n                withdrawFix[code] = self.safe_number(entry, 'withdraw_fix')\n            keys = list(withdrawFix.keys())\n            for i in range(0, len(keys)):\n                key = keys[i]\n                withdrawFees[code][key] = self.parse_number(withdrawFix[key])\n        return {\n            'info': response,\n            'withdraw': withdrawFees,\n            'deposit': {},\n        }```\n\nUpdated code:\n```python\ndef fetch_transaction_fees(self, codes=None, params={}):\n        \n        \n        \n        self.load_markets()\n        response = self.privateWalletGetWithdrawStatus(params)\n        #\n        #    {\n        #        \"currency\": \"MTN\",\n        #        \"name\": \"Medicalchain\",\n        #        \"name_cn\": \"Medicalchain\",\n        #        \"deposit\": \"0\",\n        #        \"withdraw_percent\": \"0%\",\n        #        \"withdraw_fix\": \"900\",\n        #        \"withdraw_day_limit\": \"500000\",\n        #        \"withdraw_day_limit_remain\": \"500000\",\n        #        \"withdraw_amount_mini\": \"900.1\",\n        #        \"withdraw_eachtime_limit\": \"90000000000\",\n        #        \"withdraw_fix_on_chains\": {\n        #            \"ETH\": \"900\"\n        #        }\n        #    }\n        #\n        result = {}\n        withdrawFees = {}\n        for i in range(0, len(response)):\n            withdrawFees = {}\n            entry = response[i]\n            currencyId = self.safe_string(entry, 'currency')\n            code = self.safe_currency_code(currencyId)\n            if (codes is not None) and not self.in_array(code, codes):\n                continue\n            withdrawFixOnChains = self.safe_value(entry, 'withdraw_fix_on_chains')\n            if withdrawFixOnChains is None:\n                withdrawFees = self.safe_number(entry, 'withdraw_fix')\n            else:\n                chainKeys = list(withdrawFixOnChains.keys())\n                for i in range(0, len(chainKeys)):\n                    chainKey = chainKeys[i]\n                    withdrawFees[chainKey] = self.parse_number(withdrawFixOnChains[chainKey])\n            result[code] = {\n                'withdraw': withdrawFees,\n                'deposit': None,\n                'info': entry,\n            }\n        return result\n```\n\nOriginal documentation:\n```python\nfetch transaction fees\n        :param [str]|None codes: not used by gate fetchTransactionFees()\n        :param dict params: extra parameters specific to the gate api endpoint\n        :returns dict: a list of `fee structures <https://docs.ccxt.com/en/latest/manual.html#fee-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch transaction fees\n        see https://www.gate.io/docs/developers/apiv4/en/#retrieve-withdrawal-status\n        :param [str]|None codes: not used by gate fetchTransactionFees()\n        :param dict params: extra parameters specific to the gate api endpoint\n        :returns dict: a list of `fee structures <https://docs.ccxt.com/en/latest/manual.html#fee-structure>`", "file": "python_ccxt_gate.py", "function": "gate.fetch_transaction_fees", "project": "ccxt", "change_stats": {"code_changes": 34, "doc_changes": 3, "total_changes": 37}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convert_tokens_to_string(self, tokens: List[str]) -> str:\n        \n        return self.spm_target.DecodePieces(tokens)```\n\nUpdated code:\n```python\ndef convert_tokens_to_string(self, tokens: List[str]) -> str:\n         \n        if self._decode_use_source_tokenizer:\n            return self.spm_source.DecodePieces(tokens)\n        else:\n            return self.spm_target.DecodePieces(tokens)\n```\n\nOriginal documentation:\n```python\nUses target language sentencepiece model\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Uses source spm if _decode_use_source_tokenizer is True, and target spm otherwise", "file": "src_transformers_models_marian_tokenization_marian.py", "function": "MarianTokenizer.convert_tokens_to_string", "project": "transformers", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef send_message(self, message=\"\", **kwargs):\n        \n        \n\n        _LOGGER.debug(\"Sending signal message\")\n\n        data = kwargs.get(ATTR_DATA)\n\n        filenames = None\n        if data is not None:\n            if ATTR_FILENAMES in data:\n                filenames = data[ATTR_FILENAMES]\n            if ATTR_FILENAME in data:\n                _LOGGER.warning(\n                    \"The 'attachment' option is deprecated, please replace it with 'attachments'. This option will become invalid in version 0.108\"\n                )\n                if filenames is None:\n                    filenames = [data[ATTR_FILENAME]]\n                else:\n                    filenames.append(data[ATTR_FILENAME])\n\n        try:\n            self._signal_cli_rest_api.send_message(message, self._recp_nrs, filenames)\n        except SignalCliRestApiError as ex:\n            _LOGGER.error(\"%s\", ex)\n            raise ex```\n\nUpdated code:\n```python\ndef send_message(self, message: str = \"\", **kwargs: Any) -> None:\n        \n\n        _LOGGER.debug(\"Sending signal message\")\n\n        data = kwargs.get(ATTR_DATA)\n\n        try:\n            data = DATA_SCHEMA(data)\n        except vol.Invalid as ex:\n            _LOGGER.error(\"Invalid message data: %s\", ex)\n            raise ex\n\n        filenames = self.get_filenames(data)\n        attachments_as_bytes = self.get_attachments_as_bytes(\n            data, CONF_MAX_ALLOWED_DOWNLOAD_SIZE_BYTES, self._hass\n        )\n\n        try:\n            self._signal_cli_rest_api.send_message(\n                message, self._recp_nrs, filenames, attachments_as_bytes\n            )\n        except SignalCliRestApiError as ex:\n            _LOGGER.error(\"%s\", ex)\n            raise ex\n```\n\nOriginal documentation:\n```python\nSend a message to a one or more recipients.\n\n        Additionally a file can be attached.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Send a message to a one or more recipients. Additionally a file can be attached.", "file": "homeassistant_components_signal_messenger_notify.py", "function": "SignalNotificationService.send_message", "project": "core", "change_stats": {"code_changes": 31, "doc_changes": 6, "total_changes": 37}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_period_range_edges(first, last, offset, closed=\"left\", base=0):\n    \n    \n    \n    if not all(isinstance(obj, Period) for obj in [first, last]):\n        raise TypeError(\"'first' and 'last' must be instances of type Period\")\n\n    # GH 23882\n    first = first.to_timestamp()\n    last = last.to_timestamp()\n    adjust_first = not offset.is_on_offset(first)\n    adjust_last = offset.is_on_offset(last)\n\n    first, last = _get_timestamp_range_edges(\n        first, last, offset, closed=closed, base=base\n    )\n\n    first = (first + int(adjust_first) * offset).to_period(offset)\n    last = (last - int(adjust_last) * offset).to_period(offset)\n    return first, last```\n\nUpdated code:\n```python\ndef _get_period_range_edges(\n    first, last, freq, closed=\"left\", origin=\"start_day\", offset=None\n):\n    \n    \n    \n    if not all(isinstance(obj, Period) for obj in [first, last]):\n        raise TypeError(\"'first' and 'last' must be instances of type Period\")\n\n    # GH 23882\n    first = first.to_timestamp()\n    last = last.to_timestamp()\n    adjust_first = not freq.is_on_offset(first)\n    adjust_last = freq.is_on_offset(last)\n\n    first, last = _get_timestamp_range_edges(\n        first, last, freq, closed=closed, origin=origin, offset=offset,\n    )\n\n    first = (first + int(adjust_first) * freq).to_period(freq)\n    last = (last - int(adjust_last) * freq).to_period(freq)\n    return first, last\n```\n\nOriginal documentation:\n```python\nAdjust the provided `first` and `last` Periods to the respective Period of\n    the given offset that encompasses them.\n\n    Parameters\n    ----------\n    first : pd.Period\n        The beginning Period of the range to be adjusted.\n    last : pd.Period\n        The ending Period of the range to be adjusted.\n    offset : pd.DateOffset\n        The dateoffset to which the Periods will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    base : int, default 0\n        The \"origin\" of the adjusted Periods.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Period objects.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adjust the provided `first` and `last` Periods to the respective Period of\n    the given offset that encompasses them.\n\n    Parameters\n    ----------\n    first : pd.Period\n        The beginning Period of the range to be adjusted.\n    last : pd.Period\n        The ending Period of the range to be adjusted.\n    freq : pd.DateOffset\n        The freq to which the Periods will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    origin : {'epoch', 'start', 'start_day'}, Timestamp, default 'start_day'\n        The timestamp on which to adjust the grouping. The timezone of origin must\n        match the timezone of the index.\n\n        If a timestamp is not used, these values are also supported:\n\n        - 'epoch': `origin` is 1970-01-01\n        - 'start': `origin` is the first value of the timeseries\n        - 'start_day': `origin` is the first day at midnight of the timeseries\n    offset : pd.Timedelta, default is None\n        An offset timedelta added to the origin.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Period objects.", "file": "pandas_core_resample.py", "function": "_get_period_range_edges", "project": "pandas", "change_stats": {"code_changes": 16, "doc_changes": 19, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef img_as_float(image):\n    \n\n    \n    return convert(image, np.float64)```\n\nUpdated code:\n```python\ndef img_as_float(image, force_copy=False):\n    \n\n    \n    return convert(image, np.float64, force_copy)\n```\n\nOriginal documentation:\n```python\nConvert an image to double-precision floating point format.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n\n    Returns\n    -------\n    out : ndarray of float64\n        Output image.\n\n    Notes\n    -----\n    The range of a floating point image is [0, 1].\n    Negative input values will be shifted to the positive domain.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert an image to double-precision floating point format.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    force_copy : bool\n        Force a copy of the data, irrespective of its current dtype.\n\n    Returns\n    -------\n    out : ndarray of float64\n        Output image.\n\n    Notes\n    -----\n    The range of a floating point image is [0, 1].\n    Negative input values will be shifted to the positive domain.", "file": "skimage_util_dtype.py", "function": "img_as_float", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, xy, width, height, angle=0.0, theta1=0.0, theta2=360.0, **kwargs):\n        \n        \n        \n        fill = kwargs.pop('fill')\n        if fill:\n            raise ValueError(\"Arc objects can not be filled\")\n        kwargs['fill'] = False\n\n        Ellipse.__init__(self, xy, width, height, angle, **kwargs)\n\n        self._theta1 = theta1\n        self._theta2 = theta2```\n\nUpdated code:\n```python\ndef __init__(self, xy, width, height, angle=0.0, theta1=0.0, theta2=360.0, **kwargs):\n        \n        \n        \n        fill = kwargs.pop('fill')\n        if fill:\n            raise ValueError(\"Arc objects can not be filled\")\n        kwargs['fill'] = False\n\n        Ellipse.__init__(self, xy, width, height, angle, **kwargs)\n\n        self.theta1 = theta1\n        self.theta2 = theta2\n```\n\nOriginal documentation:\n```python\nxy - center of ellipse\n        width - length of horizontal axis\n        height - length of vertical axis\n        angle - rotation in degrees (anti-clockwise)\n        theta1 - starting angle of the arc in degrees\n        theta2 - ending angle of the arc in degrees\n\n        If theta1 and theta2 are not provided, the arc will form a\n        complete ellipse.\n\n        Valid kwargs are:\n        %(Patch)s\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "The following args are supported:\n\n        *xy*\n          center of ellipse\n\n        *width*\n          length of horizontal axis\n\n        *height*\n          length of vertical axis\n\n        *angle*\n          rotation in degrees (anti-clockwise)\n\n        *theta1*\n          starting angle of the arc in degrees\n\n        *theta2*\n          ending angle of the arc in degrees\n\n        If *theta1* and *theta2* are not provided, the arc will form a\n        complete ellipse.\n\n        Valid kwargs are:\n\n        %(Patch)s", "file": "lib_matplotlib_patches.py", "function": "Arc.__init__", "project": "matplotlib", "change_stats": {"code_changes": 6, "doc_changes": 30, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _build_mwf_output_waveform(self, output_dict):\n         \n        \n        import norbert  # pylint: disable=import-error\n        x = self._features[f'{self._mix_name}_stft']\n        v = tf.stack(\n            [\n                pad_and_reshape(\n                    output_dict[f'{instrument}_spectrogram'],\n                    self._frame_length,\n                    self._F)[:tf.shape(x)[0], ...]\n                for instrument in self._instruments\n            ],\n            axis=3)\n        input_args = [v, x]\n        stft_function = tf.py_function(\n            lambda v, x: norbert.wiener(v.numpy(), x.numpy()),\n            input_args,\n            tf.complex64),\n        return {\n            instrument: self._inverse_stft(stft_function[0][:, :, :, k])\n            for k, instrument in enumerate(self._instruments)\n        }```\n\nUpdated code:\n```python\ndef _build_mwf_output_waveform(self):\n         \n        \n        import norbert  # pylint: disable=import-error\n        output_dict = self.model_outputs\n        x = self.stft_feature\n        v = tf.stack(\n            [\n                pad_and_reshape(\n                    output_dict[f'{instrument}_spectrogram'],\n                    self._frame_length,\n                    self._F)[:tf.shape(x)[0], ...]\n                for instrument in self._instruments\n            ],\n            axis=3)\n        input_args = [v, x]\n        stft_function = tf.py_function(\n            lambda v, x: norbert.wiener(v.numpy(), x.numpy()),\n            input_args,\n            tf.complex64),\n        return {\n            instrument: self._inverse_stft(stft_function[0][:, :, :, k])\n            for k, instrument in enumerate(self._instruments)\n        }\n```\n\nOriginal documentation:\n```python\nPerform separation with multichannel Wiener Filtering using Norbert.\n        Note: multichannel Wiener Filtering is not coded in Tensorflow and thus\n        may be quite slow.\n\n        :param output_dict: dictionary of estimated spectrogram (key: instrument\n            name, value: estimated spectrogram of the instrument)\n        :returns: dictionary of separated waveforms (key: instrument name,\n            value: estimated waveform of the instrument)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform separation with multichannel Wiener Filtering using Norbert.\n        Note: multichannel Wiener Filtering is not coded in Tensorflow and thus\n        may be quite slow.\n\n        :returns: dictionary of separated waveforms (key: instrument name,\n            value: estimated waveform of the instrument)", "file": "spleeter_model___init__.py", "function": "EstimatorSpecBuilder._build_mwf_output_waveform", "project": "spleeter", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
