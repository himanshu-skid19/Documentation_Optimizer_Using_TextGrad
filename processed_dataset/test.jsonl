{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef filter(self, value):\n        \n        self._filter = value\n        self._filter_re = None\n\n        if self.filter is not None:\n            logger.info(\"Set filter to {0}\".format(self.filter))\n            # Compute the regular expression\n            try:\n                self._filter_re = re.compile(self.filter)\n                logger.debug(\"Filter regex compilation OK: {0}\".format(self.filter))\n            except Exception as e:\n                logger.error(\"Cannot compile filter regex: {0} ({1})\".format(self.filter, e))```\n\nUpdated code:\n```python\ndef filter(self, value):\n        \n        \n        self._filter_input = value\n        if value is None:\n            self._filter = None\n            self._filter_key = None\n        else:\n            new_filter = value.split(':')\n            if len(new_filter) == 1:\n                self._filter = new_filter[0]\n                self._filter_key = None\n            else:\n                self._filter = new_filter[1]\n                self._filter_key = new_filter[0]\n\n        self._filter_re = None\n        if self.filter is not None:\n            logger.info(\"Set filter to {0} on key {1}\".format(self.filter, self.filter_key))\n            # Compute the regular expression\n            try:\n                self._filter_re = re.compile(self.filter)\n                logger.debug(\"Filter regex compilation OK: {0}\".format(self.filter))\n            except Exception as e:\n                logger.error(\"Cannot compile filter regex: {0} ({1})\".format(self.filter, e))\n                self._filter = None\n                self._filter_re = None\n                self._filter_key = None\n```\n\nOriginal documentation:\n```python\nSet the filter (as a sting) and compute the regular expression\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the filter (as a sting) and compute the regular expression\n        A filter could be one of the following:\n        - python > Process name of cmd start with python\n        - .*python.* > Process name of cmd contain python\n        - username:nicolargo > Process of nicolargo user", "file": "glances_filter.py", "function": "GlancesFilter.filter", "project": "glances", "change_stats": {"code_changes": 23, "doc_changes": 6, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_multi(choice, end=None):\n      \n    end = end or str(len(g.model))\n    pattern = r'(?<![-\\d])(\\d+-\\d+|-\\d+|\\d+-|\\d+)(?![-\\d])'\n    items = re.findall(pattern, choice)\n    alltracks = []\n\n    for x in items:\n\n        if x.startswith(\"-\"):\n            x = \"1\" + x\n\n        elif x.endswith(\"-\"):\n            x = x + str(end)\n\n        if \"-\" in x:\n            nrange = x.split(\"-\")\n            startend = map(int, nrange)\n            alltracks += _bi_range(*startend)\n\n        else:\n            alltracks.append(int(x))\n\n    return alltracks```\n\nUpdated code:\n```python\ndef parse_multi(choice, end=None):\n    \n    \n\n    \n    end = end or str(len(g.model))\n    pattern = r'(?<![-\\d\\[\\]])(\\d+-\\d+|-\\d+|\\d+-|\\d+)(?:\\[(\\d+)\\])?(?![-\\d\\[\\]])'\n    items = re.findall(pattern, choice)\n    alltracks = []\n\n    for x, nreps in items:\n        # nreps is in the inclusive range [1,100]\n        nreps = min(int(nreps), 100) if nreps else 1\n        for _ in range(nreps):\n\n            if x.startswith(\"-\"):\n                x = \"1\" + x\n\n            elif x.endswith(\"-\"):\n                x = x + str(end)\n\n            if \"-\" in x:\n                nrange = x.split(\"-\")\n                startend = map(int, nrange)\n                alltracks += _bi_range(*startend)\n\n            else:\n                alltracks.append(int(x))\n\n    return alltracks\n```\n\nOriginal documentation:\n```python\nHandle ranges like 5-9, 9-5, 5- and -5. Return list of ints.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Handle ranges like 5-9, 9-5, 5- and -5 with optional repetitions number [n]\n\n    eg. 2-4[2] is the same as 2 3 4 2 3 4 and 3[4] is 3 3 3 3\n\n    Return list of ints.", "file": "mps_youtube_util.py", "function": "parse_multi", "project": "yewtube", "change_stats": {"code_changes": 34, "doc_changes": 8, "total_changes": 42}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_compatible_with(\n        self, other: DTScalarOrNaT, setitem: bool = False\n    ) -> None:\n        \n        \n        \n        raise AbstractMethodError(self)```\n\nUpdated code:\n```python\ndef _check_compatible_with(self, other: DTScalarOrNaT) -> None:\n        \n        \n        \n        raise AbstractMethodError(self)\n```\n\nOriginal documentation:\n```python\nVerify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n        setitem : bool, default False\n            For __setitem__ we may have stricter compatibility restrictions than\n            for comparisons.\n\n        Raises\n        ------\n        Exception\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n\n        Raises\n        ------\n        Exception", "file": "pandas_core_arrays_datetimelike.py", "function": "DatetimeLikeArrayMixin._check_compatible_with", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_dif_from_id(project, meta, fileobj=None, file=None):\n    \n    \n    if meta.file_format == \"proguard\":\n        object_name = \"proguard-mapping\"\n    elif meta.file_format in (\"macho\", \"elf\", \"pdb\", \"pe\", \"wasm\", \"sourcebundle\"):\n        object_name = meta.name\n    elif meta.file_format == \"breakpad\":\n        object_name = meta.name[:-4] if meta.name.endswith(\".sym\") else meta.name\n    else:\n        raise TypeError(f\"unknown dif type {meta.file_format!r}\")\n\n    if file is not None:\n        checksum = file.checksum\n    elif fileobj is not None:\n        h = hashlib.sha1()\n        while True:\n            chunk = fileobj.read(16384)\n            if not chunk:\n                break\n            h.update(chunk)\n        checksum = h.hexdigest()\n        fileobj.seek(0, 0)\n    else:\n        raise RuntimeError(\"missing file object\")\n\n    dif = (\n        ProjectDebugFile.objects.select_related(\"file\")\n        .filter(project=project, debug_id=meta.debug_id, checksum=checksum, data__isnull=False)\n        .order_by(\"-id\")\n        .first()\n    )\n\n    if dif is not None:\n        return dif, False\n\n    if file is None:\n        file = File.objects.create(\n            name=meta.debug_id,\n            type=\"project.dif\",\n            headers={\"Content-Type\": DIF_MIMETYPES[meta.file_format]},\n        )\n        file.putfile(fileobj)\n    else:\n        file.type = \"project.dif\"\n        file.headers[\"Content-Type\"] = DIF_MIMETYPES[meta.file_format]\n        file.save()\n\n    dif = ProjectDebugFile.objects.create(\n        file=file,\n        checksum=file.checksum,\n        debug_id=meta.debug_id,\n        code_id=meta.code_id,\n        cpu_name=meta.arch,\n        object_name=object_name,\n        project=project,\n        data=meta.data,\n    )\n\n    # The DIF we've just created might actually be removed here again. But since\n    # this can happen at any time in near or distant future, we don't care and\n    # assume a successful upload. The DIF will be reported to the uploader and\n    # reprocessing can start.\n    clean_redundant_difs(project, meta.debug_id)\n\n    resolve_processing_issue(project=project, scope=\"native\", object=\"dsym:%s\" % meta.debug_id)\n\n    return dif, True```\n\nUpdated code:\n```python\ndef create_dif_from_id(project, meta, fileobj=None, file=None):\n    \n    \n    if meta.file_format == \"proguard\":\n        object_name = \"proguard-mapping\"\n    elif meta.file_format in (\n        \"macho\",\n        \"elf\",\n        \"pdb\",\n        \"pe\",\n        \"wasm\",\n        \"sourcebundle\",\n        \"bcsymbolmap\",\n        \"plist\",\n    ):\n        object_name = meta.name\n    elif meta.file_format == \"breakpad\":\n        object_name = meta.name[:-4] if meta.name.endswith(\".sym\") else meta.name\n    else:\n        raise TypeError(f\"unknown dif type {meta.file_format!r}\")\n\n    if file is not None:\n        checksum = file.checksum\n    elif fileobj is not None:\n        h = hashlib.sha1()\n        while True:\n            chunk = fileobj.read(16384)\n            if not chunk:\n                break\n            h.update(chunk)\n        checksum = h.hexdigest()\n        fileobj.seek(0, 0)\n    else:\n        raise RuntimeError(\"missing file object\")\n\n    dif = (\n        ProjectDebugFile.objects.select_related(\"file\")\n        .filter(project=project, debug_id=meta.debug_id, checksum=checksum, data__isnull=False)\n        .order_by(\"-id\")\n        .first()\n    )\n\n    if dif is not None:\n        return dif, False\n\n    if file is None:\n        file = File.objects.create(\n            name=meta.debug_id,\n            type=\"project.dif\",\n            headers={\"Content-Type\": DIF_MIMETYPES[meta.file_format]},\n        )\n        file.putfile(fileobj)\n    else:\n        file.type = \"project.dif\"\n        file.headers[\"Content-Type\"] = DIF_MIMETYPES[meta.file_format]\n        file.save()\n\n    dif = ProjectDebugFile.objects.create(\n        file=file,\n        checksum=file.checksum,\n        debug_id=meta.debug_id,\n        code_id=meta.code_id,\n        cpu_name=meta.arch,\n        object_name=object_name,\n        project=project,\n        data=meta.data,\n    )\n\n    # The DIF we've just created might actually be removed here again. But since\n    # this can happen at any time in near or distant future, we don't care and\n    # assume a successful upload. The DIF will be reported to the uploader and\n    # reprocessing can start.\n    clean_redundant_difs(project, meta.debug_id)\n\n    resolve_processing_issue(project=project, scope=\"native\", object=\"dsym:%s\" % meta.debug_id)\n\n    return dif, True\n```\n\nOriginal documentation:\n```python\nThis creates a mach dsym file or proguard mapping from the given\n    debug id and open file object to a debug file.  This will not verify the\n    debug id (intentionally so).  Use `detect_dif_from_path` to do that.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates the :class:`ProjectDebugFile` entry for the provided DIF.\n\n    This creates the :class:`ProjectDebugFile` entry for the DIF provided in `meta` (a\n    :class:`DifMeta` object).  If the correct entry already exists this simply returns the\n    existing entry.\n\n    It intentionally does not validate the file, only will ensure a :class:`File` entry\n    exists and set its `ContentType` according to the provided :class:DifMeta`.\n\n    Returns a tuple of `(dif, created)` where `dif` is the `ProjectDebugFile` instance and\n    `created` is a bool.", "file": "src_sentry_models_debugfile.py", "function": "create_dif_from_id", "project": "sentry", "change_stats": {"code_changes": 13, "doc_changes": 16, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef canonicalize_version(version):\n    \n    \n    \n\n    try:\n        version = Version(version)\n    except InvalidVersion:\n        # Legacy versions cannot be normalized\n        return version\n\n    parts = []\n\n    # Epoch\n    if version.epoch != 0:\n        parts.append(\"{0}!\".format(version.epoch))\n\n    # Release segment\n    # NB: This strips trailing '.0's to normalize\n    parts.append(re.sub(r\"(\\.0)+$\", \"\", \".\".join(str(x) for x in version.release)))\n\n    # Pre-release\n    if version.pre is not None:\n        parts.append(\"\".join(str(x) for x in version.pre))\n\n    # Post-release\n    if version.post is not None:\n        parts.append(\".post{0}\".format(version.post))\n\n    # Development release\n    if version.dev is not None:\n        parts.append(\".dev{0}\".format(version.dev))\n\n    # Local version segment\n    if version.local is not None:\n        parts.append(\"+{0}\".format(version.local))\n\n    return \"\".join(parts)```\n\nUpdated code:\n```python\ndef canonicalize_version(_version):\n    # type: (str) -> Union[Version, str]\n    \n    \n    \n\n    try:\n        version = Version(_version)\n    except InvalidVersion:\n        # Legacy versions cannot be normalized\n        return _version\n\n    parts = []\n\n    # Epoch\n    if version.epoch != 0:\n        parts.append(\"{0}!\".format(version.epoch))\n\n    # Release segment\n    # NB: This strips trailing '.0's to normalize\n    parts.append(re.sub(r\"(\\.0)+$\", \"\", \".\".join(str(x) for x in version.release)))\n\n    # Pre-release\n    if version.pre is not None:\n        parts.append(\"\".join(str(x) for x in version.pre))\n\n    # Post-release\n    if version.post is not None:\n        parts.append(\".post{0}\".format(version.post))\n\n    # Development release\n    if version.dev is not None:\n        parts.append(\".dev{0}\".format(version.dev))\n\n    # Local version segment\n    if version.local is not None:\n        parts.append(\"+{0}\".format(version.local))\n\n    return \"\".join(parts)\n```\n\nOriginal documentation:\n```python\nThis is very similar to Version.__str__, but has one subtle differences\n    with the way it handles the release segment.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This is very similar to Version.__str__, but has one subtle difference\n    with the way it handles the release segment.", "file": "pipenv_vendor_packaging_utils.py", "function": "canonicalize_version", "project": "pipenv", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _move_module_if_needed(self, module) -> None:\n        \n        \n        \n        # Move module to device specified. Note that this is done prior to\n        # setting compute_device to ensure that they align.\n        if self.device_id is not None:\n            param = None\n            try:\n                # Get the next unflat param\n                param_gen = module.parameters()\n                while True:\n                    param = next(param_gen)\n                    if not isinstance(param, FlatParameter):\n                        break\n\n                if param.device == torch.device(\"cpu\"):\n                    module = module.to(self.device_id)\n            except StopIteration:\n                # this FSDP instance manages no parameters.\n                pass\n\n            # For GPU modules, module device should match device_id.\n            if param is not None and param.device != self.device_id:\n                raise RuntimeError(\n                    f\"Module on rank {self.rank} is given device_id argument \"\n                    f\"{self.device_id}, but is on {param.device}. \"\n                    \" Either move module before FSDP init or omit device_id argument.\"\n                )\n        else:\n            # device_id argument is not specified\n            # If module is on CPU, log a warning asking user to use `device_id` for faster\n            # GPU init.\n            try:\n                # Get the next unflat param\n                param_gen = module.parameters()\n                while True:\n                    param = next(param_gen)\n                    if not isinstance(param, FlatParameter):\n                        break\n\n                if param.device == torch.device(\"cpu\"):\n                    warnings.warn(\n                        \"Module is put on CPU and will thus have flattening and sharding\"\n                        \" run on CPU, which is less efficient than on GPU. We recommend passing in \"\n                        \"`device_id` argument which will enable FSDP to put module on GPU device,\"\n                        \" module must also be on GPU device to work with `sync_module_states=True` flag\"\n                        \" which requires GPU communication.\"\n                    )\n            except StopIteration:\n                # this FSDP instance manages no parameters\n                pass```\n\nUpdated code:\n```python\ndef _move_module_if_needed(self, module) -> None:\n        \n        \n        \n        # Move module to device specified. Note that this is done prior to\n        # setting compute_device to ensure that they align.\n        if self.device_id is not None:\n            param = None\n            try:\n                # Get the next unflat param\n                param_gen = module.parameters()\n                while True:\n                    param = next(param_gen)\n                    if not isinstance(param, FlatParameter):\n                        break\n\n                if param.device == torch.device(\"cpu\"):\n                    module = module.to(self.device_id)\n            except StopIteration:\n                # this FSDP instance manages no parameters.\n                pass\n\n            # For GPU modules, module device should match device_id.\n            if (\n                param is not None\n                and not isinstance(param, FlatParameter)\n                and param.device != self.device_id\n            ):\n                raise RuntimeError(\n                    f\"Module on rank {self.rank} is given device_id argument \"\n                    f\"{self.device_id}, but is on {param.device}. \"\n                    \" Either move module before FSDP init or omit device_id argument.\"\n                )\n        else:\n            # device_id argument is not specified\n            # If module is on CPU, log a warning asking user to use `device_id` for faster\n            # GPU init.\n            try:\n                # Get the next unflat param\n                param_gen = module.parameters()\n                while True:\n                    param = next(param_gen)\n                    if not isinstance(param, FlatParameter):\n                        break\n\n                if param.device == torch.device(\"cpu\"):\n                    warnings.warn(\n                        \"Module is put on CPU and will thus have flattening and sharding\"\n                        \" run on CPU, which is less efficient than on GPU. We recommend passing in \"\n                        \"`device_id` argument which will enable FSDP to put module on GPU device,\"\n                        \" module must also be on GPU device to work with `sync_module_states=True` flag\"\n                        \" which requires GPU communication.\"\n                    )\n            except StopIteration:\n                # this FSDP instance manages no parameters\n                pass\n```\n\nOriginal documentation:\n```python\nMoves module appropriately depending on device_id and\n        whether module is on CPU. Returns a ``bool`` indicating\n        whether the module needs to be moved back to CPU before\n        returning to user.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Moves module if module is on CPU and device_id is specified.\n        If device_id is not specified and module is on CPU, we log a\n        warning to user mentioning to use ``device_id`` argument to speed\n        up initialization performance.", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel._move_module_if_needed", "project": "pytorch", "change_stats": {"code_changes": 8, "doc_changes": 10, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unit_of_measurement(self) -> str:\n        \n        return TEMP_CELSIUS```\n\nUpdated code:\n```python\ndef unit_of_measurement(self) -> str:\n        \n        return TEMP_FAHRENHEIT\n```\n\nOriginal documentation:\n```python\nReturn gallons as the unit measurement for water.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return fahrenheit as the unit measurement for temperature.", "file": "homeassistant_components_flo_sensor.py", "function": "FloTemperatureSensor.unit_of_measurement", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _batched_insert(self, objs, fields, batch_size):\n        \n        \n        \n        if not objs:\n            return\n        ops = connections[self.db].ops\n        batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))\n        inserted_ids = []\n        for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n            if connections[self.db].features.can_return_ids_from_bulk_insert:\n                inserted_id = self._insert(item, fields=fields, using=self.db, return_id=True)\n                if isinstance(inserted_id, list):\n                    inserted_ids.extend(inserted_id)\n                else:\n                    inserted_ids.append(inserted_id)\n            else:\n                self._insert(item, fields=fields, using=self.db)\n        return inserted_ids```\n\nUpdated code:\n```python\ndef _batched_insert(self, objs, fields, batch_size):\n        \n        \n        \n        ops = connections[self.db].ops\n        batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))\n        inserted_ids = []\n        for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n            if connections[self.db].features.can_return_ids_from_bulk_insert:\n                inserted_id = self._insert(item, fields=fields, using=self.db, return_id=True)\n                if isinstance(inserted_id, list):\n                    inserted_ids.extend(inserted_id)\n                else:\n                    inserted_ids.append(inserted_id)\n            else:\n                self._insert(item, fields=fields, using=self.db)\n        return inserted_ids\n```\n\nOriginal documentation:\n```python\nA helper method for bulk_create() to insert the bulk one batch at a\n        time. Insert recursively a batch from the front of the bulk and then\n        _batched_insert() the remaining objects again.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Helper method for bulk_create() to insert objs one batch at a time.", "file": "django_db_models_query.py", "function": "QuerySet._batched_insert", "project": "django", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_m2m_attr(self, related, attr):\n        \"\"\n        cache_attr = '_m2m_%s_cache' % attr\n        if hasattr(self, cache_attr):\n            return getattr(self, cache_attr)\n        if self.rel.through_fields is not None:\n            link_field_name = self.rel.through_fields[0]\n        else:\n            link_field_name = None\n        for f in self.rel.through._meta.fields:\n            if (f.is_relation and f.rel.to == related.related_model and\n                    (link_field_name is None or link_field_name == f.name)):\n                setattr(self, cache_attr, getattr(f, attr))\n                return getattr(self, cache_attr)```\n\nUpdated code:\n```python\ndef _get_m2m_attr(self, related, attr):\n        \n        \n        \n        cache_attr = '_m2m_%s_cache' % attr\n        if hasattr(self, cache_attr):\n            return getattr(self, cache_attr)\n        if self.rel.through_fields is not None:\n            link_field_name = self.rel.through_fields[0]\n        else:\n            link_field_name = None\n        for f in self.rel.through._meta.fields:\n            if (f.is_relation and f.rel.to == related.related_model and\n                    (link_field_name is None or link_field_name == f.name)):\n                setattr(self, cache_attr, getattr(f, attr))\n                return getattr(self, cache_attr)\n```\n\nOriginal documentation:\n```python\nFunction that can be curried to provide the source accessor or DB column name for the m2m table\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Function that can be curried to provide the source accessor or DB\n        column name for the m2m table.", "file": "django_db_models_fields_related.py", "function": "ManyToManyField._get_m2m_attr", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _strip_prompts(prompt_re, continuation_re=None):\n    \n    \n    if continuation_re is None:\n        continuation_re = prompt_re\n    line = ''\n    while True:\n        line = (yield line)\n        \n        # First line of cell\n        if line is None:\n            continue\n        out, n1 = prompt_re.subn('', line, count=1)\n        line = (yield out)\n        \n        # Second line of cell, because people often copy from just after the\n        # first prompt, so we might not see it in the first line.\n        if line is None:\n            continue\n        # check for first prompt if not found on first line, continuation otherwise\n        if n1:\n            pat = continuation_re\n        else:\n            pat = prompt_re\n        out, n2 = pat.subn('', line, count=1)\n        line = (yield out)\n        \n        if n1 or n2:\n            # Found the input prompt in the first two lines - check for it in\n            # the rest of the cell as well.\n            while line is not None:\n                line = (yield continuation_re.sub('', line, count=1))\n        \n        else:\n            # Prompts not in input - wait for reset\n            while line is not None:\n                line = (yield line)```\n\nUpdated code:\n```python\ndef _strip_prompts(prompt_re, initial_re=None):\n    \n    \n    if initial_re is None:\n        initial_re = prompt_re\n    line = ''\n    while True:\n        line = (yield line)\n        \n        # First line of cell\n        if line is None:\n            continue\n        out, n1 = initial_re.subn('', line, count=1)\n        line = (yield out)\n        \n        if line is None:\n            continue\n        # check for any prompt on the second line of the cell,\n        # because people often copy from just after the first prompt,\n        # so we might not see it in the first line.\n        out, n2 = prompt_re.subn('', line, count=1)\n        line = (yield out)\n        \n        if n1 or n2:\n            # Found a prompt in the first two lines - check for it in\n            # the rest of the cell as well.\n            while line is not None:\n                line = (yield prompt_re.sub('', line, count=1))\n        \n        else:\n            # Prompts not in input - wait for reset\n            while line is not None:\n                line = (yield line)\n```\n\nOriginal documentation:\n```python\nRemove matching input prompts from a block of input.\n    \n    prompt_re is stripped only once, on either the first or second line.\n    If prompt_re is found on one of the first two lines,\n    continuation_re is stripped from lines thereafter.\n    \n    If continuation_re is unspecified, prompt_re will be used for both.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Remove matching input prompts from a block of input.\n    \n    Parameters\n    ----------\n    prompt_re : regular expression\n        A regular expression matching any input prompt (including continuation)\n    initial_re : regular expression, optional\n        A regular expression matching only the initial prompt, but not continuation.\n        If no initial expression is given, prompt_re will be used everywhere.\n        Used mainly for plain Python prompts, where the continuation prompt\n        ``...`` is a valid Python expression in Python 3, so shouldn't be stripped.\n    \n    If initial_re and prompt_re differ,\n    only initial_re will be tested against the first line.\n    If any prompt is found on the first two lines,\n    prompts will be stripped from the rest of the block.", "file": "IPython_core_inputtransformer.py", "function": "_strip_prompts", "project": "ipython", "change_stats": {"code_changes": 26, "doc_changes": 19, "total_changes": 45}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef duplicated(self, keep=\"first\") -> Series:  # type: ignore[override]\n        \n        \n        \n        res = base.IndexOpsMixin.duplicated(self, keep=keep)\n        result = self._constructor(res, index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")```\n\nUpdated code:\n```python\ndef duplicated(self, keep=\"first\") -> Series:\n        \n        \n        \n        res = self._duplicated(keep=keep)\n        result = self._constructor(res, index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n```\n\nOriginal documentation:\n```python\nIndicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series[bool]\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool", "file": "pandas_core_series.py", "function": "Series.duplicated", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef show_topic(self, topicid, topn=10):\n        \n        \n\n        \n        return [(self.id2word[id], value) for id, value in self.get_topic_terms(topicid, topn)]```\n\nUpdated code:\n```python\ndef show_topic(self, topicid, topn=10):\n        \n        \n\n        \n        topic = self.state.get_lambda()[topicid]\n        topic = topic / topic.sum()  # normalize to probability distribution\n        bestn = matutils.argsort(topic, topn, reverse=True)\n        beststr = [(topic[id], self.id2word[id]) for id in bestn]\n        return beststr\n```\n\nOriginal documentation:\n```python\nReturn a list of `(word, probability)` 2-tuples for the most probable\n        words in topic `topicid`.\n\n        Only return 2-tuples for the topn most probable words (ignore the rest).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a list of `(words_probability, word)` 2-tuples for the most probable\n        words in topic `topicid`.\n\n        Only return 2-tuples for the topn most probable words (ignore the rest).", "file": "gensim_models_ldamodel.py", "function": "LdaModel.show_topic", "project": "gensim", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef apply_weights(self,\n                      weights: Dict[str, torch.Tensor],\n                      x: torch.Tensor,\n                      bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef apply_weights(self,\n                      layer: torch.nn.Module,\n                      x: torch.Tensor,\n                      bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \n        raise NotImplementedError\n```\n\nOriginal documentation:\n```python\nApply the weights to the input tensor.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Apply the weights in layer to the input tensor.\n\n        Expects create_weights to have been called before on the layer.", "file": "vllm_model_executor_layers_linear.py", "function": "LinearMethodBase.apply_weights", "project": "vllm", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _ash_beacon_attack(self):\n        \n        \n        \n        logger.info('Ash beacon attack')\n        confirm_timer = Timer(1, count=2).start()\n        ash_combat = AshCombat(self.config, self.device)\n\n        while 1:\n            self.device.screenshot()\n            percent = color_bar_percentage(\n                self.device.image, BEACON_HP_PERCENTAGE.area, prev_color=(181, 56, 57), threshold=15)\n            logger.attr('Ash_beacon_hp', f'{int(percent * 100)}%')\n\n            # End\n            if self.appear(BEACON_EMPTY, offset=(20, 20)):\n                if confirm_timer.reached():\n                    logger.info('Ash beacon attack finished')\n                    break\n            else:\n                confirm_timer.reset()\n\n            if self.appear_then_click(BEACON_REWARD, interval=2):\n                continue\n            if self.appear(GET_ITEMS_1, interval=2):\n                self.device.click(BEACON_REWARD)\n                continue\n            if self.appear(ASH_START, offset=(30, 30)):\n                self.ui_click(ASH_START, check_button=BATTLE_PREPARATION, offset=(30, 30), skip_first_screenshot=True)\n                ash_combat.combat(expected_end=self.is_in_ash)\n                continue```\n\nUpdated code:\n```python\ndef _ash_beacon_attack(self):\n        \n        \n        \n        logger.info('Ash beacon attack')\n        confirm_timer = Timer(1, count=2).start()\n        ash_combat = AshCombat(self.config, self.device)\n\n        while 1:\n            self.device.screenshot()\n            percent = color_bar_percentage(\n                self.device.image, BEACON_HP_PERCENTAGE.area, prev_color=(181, 56, 57), threshold=15)\n            logger.attr('Ash_beacon_hp', f'{int(percent * 100)}%')\n\n            # End\n            if self.appear(BEACON_EMPTY, offset=(20, 20)):\n                if confirm_timer.reached():\n                    logger.info('Ash beacon attack finished')\n                    return True\n            elif self.appear(BEACON_ENTER, offset=(20, 20)):\n                # If previous beacon is not completed, the previous beacon is attacked in this round.\n                # Then found a new beacon, after attack.\n                if confirm_timer.reached():\n                    logger.info('Ash beacon attack finished, but found another beacon')\n                    return False\n            else:\n                confirm_timer.reset()\n\n            if self.appear_then_click(BEACON_REWARD, interval=2):\n                continue\n            if self.appear(GET_ITEMS_1, interval=2):\n                self.device.click(BEACON_REWARD)\n                continue\n            if self.appear(ASH_START, offset=(30, 30)):\n                self.ui_click(ASH_START, check_button=BATTLE_PREPARATION, offset=(30, 30), skip_first_screenshot=True)\n                ash_combat.combat(expected_end=self.is_in_ash)\n                continue\n```\n\nOriginal documentation:\n```python\nAttack ash beacon until it's killed.\n\n        Pages:\n            in: is_in_ash\n            out: is_in_ash\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Attack ash beacon until it's killed.\n\n        Returns:\n            bool: If all beacon finished\n\n        Pages:\n            in: is_in_ash\n            out: is_in_ash", "file": "module_os_ash_ash.py", "function": "OSAsh._ash_beacon_attack", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 10, "doc_changes": 5, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_fleet_bar(self, image):\n        \n        \n        \n        result = []\n        for index, y in enumerate(range(0, image.size[1], self.FLEET_BAR_SHAPE_Y + self.FLEET_BAR_MARGIN_Y)):\n            area = (0, y, image.size[0], y + self.FLEET_BAR_SHAPE_Y)\n            stat = ImageStat.Stat(image.crop(area))\n            if np.std(stat.mean, ddof=1) > self.FLEET_BAR_ACTIVE_STD:\n                result.append(4 - index)\n\n        logger.info('Current selected: %s' % str(result))\n        return result```\n\nUpdated code:\n```python\ndef parse_fleet_bar(self, image):\n        \n        \n        \n        width, height = image_size(image)\n        result = []\n        for index, y in enumerate(range(0, height, self.FLEET_BAR_SHAPE_Y + self.FLEET_BAR_MARGIN_Y)):\n            area = (0, y, width, y + self.FLEET_BAR_SHAPE_Y)\n            mean = get_color(image, area)\n            if np.std(mean, ddof=1) > self.FLEET_BAR_ACTIVE_STD:\n                result.append(4 - index)\n\n        logger.info('Current selected: %s' % str(result))\n        return result\n```\n\nOriginal documentation:\n```python\nArgs:\n            image (PIL.Image.Image): Image of dropdown menu.\n\n        Returns:\n            list: List of int. Currently selected fleet ranges from 1 to 4.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            image (np.ndarray): Image of dropdown menu.\n\n        Returns:\n            list: List of int. Currently selected fleet ranges from 1 to 4.", "file": "module_os_map_fleet_selector.py", "function": "FleetSelector.parse_fleet_bar", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run_tests(self, test_labels, extra_tests=None, **kwargs):\n        \n        \n        \n        self.setup_test_environment()\n        suite = self.build_suite(test_labels, extra_tests)\n        databases = self.get_databases(suite)\n        serialized_aliases = set(\n            alias\n            for alias, serialize in databases.items() if serialize\n        )\n        with self.time_keeper.timed('Total database setup'):\n            old_config = self.setup_databases(\n                aliases=databases,\n                serialized_aliases=serialized_aliases,\n            )\n        run_failed = False\n        try:\n            self.run_checks(databases)\n            result = self.run_suite(suite)\n        except Exception:\n            run_failed = True\n            raise\n        finally:\n            try:\n                with self.time_keeper.timed('Total database teardown'):\n                    self.teardown_databases(old_config)\n                self.teardown_test_environment()\n            except Exception:\n                # Silence teardown exceptions if an exception was raised during\n                # runs to avoid shadowing it.\n                if not run_failed:\n                    raise\n        self.time_keeper.print_results()\n        return self.suite_result(suite, result)```\n\nUpdated code:\n```python\ndef run_tests(self, test_labels, extra_tests=None, **kwargs):\n        \n        \n        \n        if extra_tests is not None:\n            warnings.warn(\n                'The extra_tests argument is deprecated.',\n                RemovedInDjango50Warning,\n                stacklevel=2,\n            )\n        self.setup_test_environment()\n        suite = self.build_suite(test_labels, extra_tests)\n        databases = self.get_databases(suite)\n        serialized_aliases = set(\n            alias\n            for alias, serialize in databases.items() if serialize\n        )\n        with self.time_keeper.timed('Total database setup'):\n            old_config = self.setup_databases(\n                aliases=databases,\n                serialized_aliases=serialized_aliases,\n            )\n        run_failed = False\n        try:\n            self.run_checks(databases)\n            result = self.run_suite(suite)\n        except Exception:\n            run_failed = True\n            raise\n        finally:\n            try:\n                with self.time_keeper.timed('Total database teardown'):\n                    self.teardown_databases(old_config)\n                self.teardown_test_environment()\n            except Exception:\n                # Silence teardown exceptions if an exception was raised during\n                # runs to avoid shadowing it.\n                if not run_failed:\n                    raise\n        self.time_keeper.print_results()\n        return self.suite_result(suite, result)\n```\n\nOriginal documentation:\n```python\nRun the unit tests for all the test labels in the provided list.\n\n        Test labels should be dotted Python paths to test modules, test\n        classes, or test methods.\n\n        A list of 'extra' tests may also be provided; these tests\n        will be added to the test suite.\n\n        Return the number of tests that failed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run the unit tests for all the test labels in the provided list.\n\n        Test labels should be dotted Python paths to test modules, test\n        classes, or test methods.\n\n        Return the number of tests that failed.", "file": "django_test_runner.py", "function": "DiscoverRunner.run_tests", "project": "django", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef amap(fn,*args):\n    \n    return array(map(fn,*args))```\n\nUpdated code:\n```python\ndef amap(fn,*args):\n    \n    \n    return npy.array(map(fn,*args))\n```\n\nOriginal documentation:\n```python\namap(function, sequence[, sequence, ...]) -> array.\n\n    Works like map(), but it returns an array.  This is just a convenient\n    shorthand for Numeric.array(map(...))\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "amap(function, sequence[, sequence, ...]) -> array.\n\n    Works like map(), but it returns an array.  This is just a convenient\n    shorthand for numpy.array(map(...))", "file": "lib_matplotlib_mlab.py", "function": "amap", "project": "matplotlib", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_config_value(config_key_as_arr, is_initial_config=False, should_decrypt=True):\n        \n        \n        \n        config_key = functools.reduce(lambda x, y: x + '.' + y, config_key_as_arr)\n        config = mongo.db.config.find_one({'name': 'initial' if is_initial_config else 'newconfig'}, {config_key: 1})\n        for config_key_part in config_key_as_arr:\n            if config_key_part not in config:\n                return False\n            config = config[config_key_part]\n        if should_decrypt:\n            if config_key_as_arr in ENCRYPTED_CONFIG_ARRAYS:\n                config = [encryptor.dec(x) for x in config]\n            elif config_key_as_arr in ENCRYPTED_CONFIG_STRINGS:\n                config = encryptor.dec(config)\n        return config```\n\nUpdated code:\n```python\ndef get_config_value(config_key_as_arr, is_initial_config=False, should_decrypt=True):\n        \n        \n        \n        config_key = functools.reduce(lambda x, y: x + '.' + y, config_key_as_arr)\n        config = mongo.db.config.find_one({'name': 'initial' if is_initial_config else 'newconfig'}, {config_key: 1})\n        for config_key_part in config_key_as_arr:\n            config = config[config_key_part]\n        if should_decrypt:\n            if config_key_as_arr in ENCRYPTED_CONFIG_ARRAYS:\n                config = [encryptor.dec(x) for x in config]\n            elif config_key_as_arr in ENCRYPTED_CONFIG_STRINGS:\n                config = encryptor.dec(config)\n        return config\n```\n\nOriginal documentation:\n```python\nGet a specific config value.\n        :param config_key_as_arr: The config key as an array. e.g. ['basic', 'credentials', 'exploit_password_list'].\n        :param is_initial_config: If True, returns the value of the initial config instead of the current config.\n        :param should_decrypt: If True, the value of the config key will be decrypted\n                               (if it's in the list of encrypted config values).\n        :return: The value of the requested config key or False, if such key doesn't exist.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get a specific config value.\n        :param config_key_as_arr: The config key as an array. e.g. ['basic', 'credentials', 'exploit_password_list'].\n        :param is_initial_config: If True, returns the value of the initial config instead of the current config.\n        :param should_decrypt: If True, the value of the config key will be decrypted\n                               (if it's in the list of encrypted config values).\n        :return: The value of the requested config key.", "file": "monkey_monkey_island_cc_services_config.py", "function": "ConfigService.get_config_value", "project": "monkey", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _set_value(self, label, value, takeable: bool = False):\n        \n        \n        \n        try:\n            if takeable:\n                self._values[label] = value\n            else:\n                loc = self.index.get_loc(label)\n                validate_numeric_casting(self.dtype, value)\n                self._values[loc] = value\n        except KeyError:\n\n            # set using a non-recursive method\n            self.loc[label] = value\n\n        return self```\n\nUpdated code:\n```python\ndef _set_value(self, label, value, takeable: bool = False):\n        \n        \n        \n        try:\n            if takeable:\n                self._values[label] = value\n            else:\n                loc = self.index.get_loc(label)\n                validate_numeric_casting(self.dtype, value)\n                self._values[loc] = value\n        except KeyError:\n\n            # set using a non-recursive method\n            self.loc[label] = value\n```\n\nOriginal documentation:\n```python\nQuickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        Series\n            If label is contained, will be reference to calling Series,\n            otherwise a new object.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False", "file": "pandas_core_series.py", "function": "Series._set_value", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _find_position_of_minute(self, minute_dt, adjust_half_day_minutes):\n        \n        \n        \n        return find_position_of_minute(\n            self._market_open_values,\n            self._market_close_values,\n            minute_dt.value / NANOS_IN_MINUTE,\n            US_EQUITIES_MINUTES_PER_DAY,\n            adjust_half_day_minutes\n        )```\n\nUpdated code:\n```python\ndef _find_position_of_minute(self, minute_dt):\n        \n        \n        \n        return find_position_of_minute(\n            self._market_open_values,\n            self._market_close_values,\n            minute_dt.value / NANOS_IN_MINUTE,\n            US_EQUITIES_MINUTES_PER_DAY,\n        )\n```\n\nOriginal documentation:\n```python\nInternal method that returns the position of the given minute in the\n        list of every trading minute since market open of the first trading\n        day. Adjusts non market minutes to the last close.\n\n        ex. this method would return 1 for 2002-01-02 9:32 AM Eastern, if\n        2002-01-02 is the first trading day of the dataset.\n\n        Parameters\n        ----------\n        minute_dt: pd.Timestamp\n            The minute whose position should be calculated.\n\n        adjust_half_day_minutes: boolean\n            Whether or not we want to adjust minutes to early close on half\n            days.\n\n        Returns\n        -------\n        int: The position of the given minute in the list of all trading\n        minutes since market open on the first trading day.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Internal method that returns the position of the given minute in the\n        list of every trading minute since market open of the first trading\n        day. Adjusts non market minutes to the last close.\n\n        ex. this method would return 1 for 2002-01-02 9:32 AM Eastern, if\n        2002-01-02 is the first trading day of the dataset.\n\n        Parameters\n        ----------\n        minute_dt: pd.Timestamp\n            The minute whose position should be calculated.\n\n        Returns\n        -------\n        int: The position of the given minute in the list of all trading\n        minutes since market open on the first trading day.", "file": "zipline_data_minute_bars.py", "function": "BcolzMinuteBarReader._find_position_of_minute", "project": "zipline", "change_stats": {"code_changes": 5, "doc_changes": 6, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_npz(file, obj, path='', strict=True):\n    \n\n    \n    with numpy.load(file) as f:\n        d = NpzDeserializer(f, path=path, strict=strict)\n        d.load(obj)```\n\nUpdated code:\n```python\ndef load_npz(file, obj, path='', strict=True, ignore_names=None):\n    \n\n    \n    with numpy.load(file) as f:\n        d = NpzDeserializer(\n            f, path=path, strict=strict, ignore_names=ignore_names)\n        d.load(obj)\n```\n\nOriginal documentation:\n```python\nLoads an object from the file in NPZ format.\n\n    This is a short-cut function to load from an `.npz` file that contains only\n    one object.\n\n    Args:\n        file (str or file-like): File to be loaded.\n        obj: Object to be deserialized. It must support serialization protocol.\n        path (str): The path in the hierarchy of the serialized data under\n            which the data is to be loaded. The default behavior (blank) will\n            load all data under the root path.\n        strict (bool): If ``True``, the deserializer raises an error when an\n            expected value is not found in the given NPZ file. Otherwise,\n            it ignores the value and skip deserialization.\n\n    .. seealso::\n        :func:`chainer.serializers.save_npz`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads an object from the file in NPZ format.\n\n    This is a short-cut function to load from an `.npz` file that contains only\n    one object.\n\n    Args:\n        file (str or file-like): File to be loaded.\n        obj: Object to be deserialized. It must support serialization protocol.\n        path (str): The path in the hierarchy of the serialized data under\n            which the data is to be loaded. The default behavior (blank) will\n            load all data under the root path.\n        strict (bool): If ``True``, the deserializer raises an error when an\n            expected value is not found in the given NPZ file. Otherwise,\n            it ignores the value and skip deserialization.\n        ignore_names (string, callable or list of them):\n            If callable, it is a function that takes a name of a parameter\n            and a persistent and returns ``True`` when it needs to be skipped.\n            If string, this is a name of a parameter or persistent that are\n            going to be skipped.\n            This can also be a list of callables and strings that behave as\n            described above.\n\n    .. seealso::\n        :func:`chainer.serializers.save_npz`", "file": "chainer_serializers_npz.py", "function": "load_npz", "project": "chainer", "change_stats": {"code_changes": 7, "doc_changes": 9, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cancel_order(self, id: str, symbol: Str = None, params={}):\n        \n        \n        \n        self.check_required_symbol('cancelOrder', symbol)\n        self.load_markets()\n        market = self.market(symbol)\n        origClientOrderId = self.safe_value(params, 'origClientOrderId')\n        request = {\n            'symbol': market['id'],\n            # 'orderId': int(id),\n            # 'origClientOrderId': id,\n        }\n        if origClientOrderId is None:\n            request['orderId'] = id\n        else:\n            request['origClientOrderId'] = origClientOrderId\n        response = self.privateDeleteV2Order(self.extend(request, params))\n        #\n        #     {\n        #         \"symbol\": \"DOGE/USD\",\n        #         \"orderId\": \"00000000-0000-0003-0000-000006db764c\",\n        #         \"price\": \"0.13\",\n        #         \"origQty\": \"30.0\",\n        #         \"executedQty\": \"0.0\",  # positive for BUY, negative for SELL\n        #         \"status\": \"CANCELED\",\n        #         \"timeInForce\": \"GTC\",\n        #         \"type\": \"LIMIT\",\n        #         \"side\": \"BUY\",\n        #     }\n        #\n        return self.parse_order(response, market)```\n\nUpdated code:\n```python\ndef cancel_order(self, id: str, symbol: Str = None, params={}):\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')\n        self.load_markets()\n        market = self.market(symbol)\n        origClientOrderId = self.safe_value(params, 'origClientOrderId')\n        request = {\n            'symbol': market['id'],\n            # 'orderId': int(id),\n            # 'origClientOrderId': id,\n        }\n        if origClientOrderId is None:\n            request['orderId'] = id\n        else:\n            request['origClientOrderId'] = origClientOrderId\n        response = self.privateDeleteV2Order(self.extend(request, params))\n        #\n        #     {\n        #         \"symbol\": \"DOGE/USD\",\n        #         \"orderId\": \"00000000-0000-0003-0000-000006db764c\",\n        #         \"price\": \"0.13\",\n        #         \"origQty\": \"30.0\",\n        #         \"executedQty\": \"0.0\",  # positive for BUY, negative for SELL\n        #         \"status\": \"CANCELED\",\n        #         \"timeInForce\": \"GTC\",\n        #         \"type\": \"LIMIT\",\n        #         \"side\": \"BUY\",\n        #     }\n        #\n        return self.parse_order(response, market)\n```\n\nOriginal documentation:\n```python\ncancels an open order\n        :param str id: order id\n        :param str symbol: unified symbol of the market the order was made in\n        :param dict [params]: extra parameters specific to the currencycom api endpoint\n        :returns dict: An `order structure <https://github.com/ccxt/ccxt/wiki/Manual#order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "cancels an open order\n        :param str id: order id\n        :param str symbol: unified symbol of the market the order was made in\n        :param dict [params]: extra parameters specific to the currencycom api endpoint\n        :returns dict: An `order structure <https://docs.ccxt.com/#/?id=order-structure>`", "file": "python_ccxt_currencycom.py", "function": "currencycom.cancel_order", "project": "ccxt", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_real_amount(self, trade: Trade, order: Dict, order_amount: float = None) -> float:\n        \n        \n        \n        if order_amount is None:\n            order_amount = order['amount']\n        # Only run for closed orders\n        if trade.fee_open == 0 or order['status'] == 'open':\n            return order_amount\n\n        trade_base_currency = self.exchange.get_pair_base_currency(trade.pair)\n        # use fee from order-dict if possible\n        if ('fee' in order and order['fee'] is not None and\n                (order['fee'].keys() >= {'currency', 'cost'})):\n            if (order['fee']['currency'] is not None and\n                    order['fee']['cost'] is not None and\n                    trade_base_currency == order['fee']['currency']):\n                new_amount = order_amount - order['fee']['cost']\n                logger.info(\"Applying fee on amount for %s (from %s to %s) from Order\",\n                            trade, order['amount'], new_amount)\n                return new_amount\n\n        # Fallback to Trades\n        trades = self.exchange.get_trades_for_order(trade.open_order_id, trade.pair,\n                                                    trade.open_date)\n\n        if len(trades) == 0:\n            logger.info(\"Applying fee on amount for %s failed: myTrade-Dict empty found\", trade)\n            return order_amount\n        amount = 0\n        fee_abs = 0\n        for exectrade in trades:\n            amount += exectrade['amount']\n            if (\"fee\" in exectrade and exectrade['fee'] is not None and\n                    (exectrade['fee'].keys() >= {'currency', 'cost'})):\n                # only applies if fee is in quote currency!\n                if (exectrade['fee']['currency'] is not None and\n                        exectrade['fee']['cost'] is not None and\n                        trade_base_currency == exectrade['fee']['currency']):\n                    fee_abs += exectrade['fee']['cost']\n\n        if not isclose(amount, order_amount, abs_tol=constants.MATH_CLOSE_PREC):\n            logger.warning(f\"Amount {amount} does not match amount {trade.amount}\")\n            raise DependencyException(\"Half bought? Amounts don't match\")\n        real_amount = amount - fee_abs\n        if fee_abs != 0:\n            logger.info(f\"Applying fee on amount for {trade} \"\n                        f\"(from {order_amount} to {real_amount}) from Trades\")\n        return real_amount```\n\nUpdated code:\n```python\ndef get_real_amount(self, trade: Trade, order: Dict, order_amount: float = None) -> float:\n        \n        \n        \n        # Init variables\n        if order_amount is None:\n            order_amount = order['amount']\n        # Only run for closed orders\n        if trade.fee_updated(order.get('side', '')) or order['status'] == 'open':\n            return order_amount\n\n        trade_base_currency = self.exchange.get_pair_base_currency(trade.pair)\n        # use fee from order-dict if possible\n        if self.exchange.order_has_fee(order):\n            fee_cost, fee_currency, fee_rate = self.exchange.extract_cost_curr_rate(order)\n            logger.info(f\"Fee for Trade {trade} [{order.get('side')}]: \"\n                        f\"{fee_cost:.8g} {fee_currency} - rate: {fee_rate}\")\n\n            trade.update_fee(fee_cost, fee_currency, fee_rate, order.get('side', ''))\n            if trade_base_currency == fee_currency:\n                # Apply fee to amount\n                return self.apply_fee_conditional(trade, trade_base_currency,\n                                                  amount=order_amount, fee_abs=fee_cost)\n            return order_amount\n        return self.fee_detection_from_trades(trade, order, order_amount)\n```\n\nOriginal documentation:\n```python\nGet real amount for the trade\n        Necessary for exchanges which charge fees in base currency (e.g. binance)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Detect and update trade fee.\n        Calls trade.update_fee() uppon correct detection.\n        Returns modified amount if the fee was taken from the destination currency.\n        Necessary for exchanges which charge fees in base currency (e.g. binance)\n        :return: identical (or new) amount for the trade", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.get_real_amount", "project": "freqtrade", "change_stats": {"code_changes": 50, "doc_changes": 7, "total_changes": 57}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, folder: Optional[str] = None, format_strs: Optional[List[str]] = None):\n        \n        \n        \n        self.dir = folder\n        self.format_strs = format_strs\n        self.prevlogger = None```\n\nUpdated code:\n```python\ndef __init__(self, folder: Optional[str] = None, format_strings: Optional[List[str]] = None):\n        \n        \n        \n        self.dir = folder\n        self.format_strings = format_strings\n        self.prev_logger = None\n```\n\nOriginal documentation:\n```python\nClass for using context manager while logging\n\n        usage:\n        with ScopedConfigure(folder=None, format_strs=None):\n            {code}\n\n        :param folder: (str) the logging folder\n        :param format_strs: ([str]) the list of output logging format\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Class for using context manager while logging\n\n        usage:\n        with ScopedConfigure(folder=None, format_strings=None):\n            {code}\n\n        :param folder: (str) the logging folder\n        :param format_strings: ([str]) the list of output logging format", "file": "stable_baselines3_common_logger.py", "function": "ScopedConfigure.__init__", "project": "stable-baselines3", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_historic_trades(self, pair: str,\n                            since: Optional[int] = None,\n                            until: Optional[int] = None,\n                            from_id: Optional[str] = None) -> List:\n        \n        \n        \n\n        if self._trades_pagination == 'time':\n            return asyncio.get_event_loop().run_until_complete(\n                self._async_get_trade_history(pair=pair, since=since, until=until))\n        elif self._trades_pagination == 'id':\n            # Use id-based trade-downloader\n            return asyncio.get_event_loop().run_until_complete(\n                self._async_get_trade_history_id(pair=pair, since=since,\n                                                 until=until, from_id=from_id))```\n\nUpdated code:\n```python\ndef get_historic_trades(self, pair: str,\n                            since: Optional[int] = None,\n                            until: Optional[int] = None,\n                            from_id: Optional[str] = None) -> List:\n        \n        \n        \n        if not until:\n            # Current milliseconds\n            until = ccxt.Exchange.milliseconds()\n        if self._trades_pagination == 'time':\n            return asyncio.get_event_loop().run_until_complete(\n                self._async_get_trade_history(pair=pair, since=since, until=until))\n\n        elif self._trades_pagination == 'id':\n            # Use id-based trade-downloader\n            return asyncio.get_event_loop().run_until_complete(\n                self._async_get_trade_history_id(pair=pair, since=since,\n                                                 until=until, from_id=from_id))\n```\n\nOriginal documentation:\n```python\nGets candle history using asyncio and returns the list of candles.\n        Handles all async doing.\n        Async over one pair, assuming we get `_ohlcv_candle_limit` candles per call.\n        :param pair: Pair to download\n        :param ticker_interval: Interval to get\n        :param since_ms: Timestamp in milliseconds to get history from\n        :param from_id: Download data starting with ID (if id is known)\n        :returns List of tickers\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets candle history using asyncio and returns the list of candles.\n        Handles all async doing.\n        Async over one pair, assuming we get `_ohlcv_candle_limit` candles per call.\n        :param pair: Pair to download\n        :param ticker_interval: Interval to get\n        :param since: Timestamp in milliseconds to get history from\n        :param until: Timestamp in milliseconds. Defaults to current timestamp if not defined.\n        :param from_id: Download data starting with ID (if id is known)\n        :returns List of tickers", "file": "freqtrade_exchange_exchange.py", "function": "Exchange.get_historic_trades", "project": "freqtrade", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pm():\n    \n    if hasattr(sys, 'last_exc'):\n        tb = sys.last_exc.__traceback__\n    else:\n        tb = sys.last_traceback\n    post_mortem(tb)```\n\nUpdated code:\n```python\ndef pm():\n    \n    post_mortem(sys.last_exc)\n```\n\nOriginal documentation:\n```python\nEnter post-mortem debugging of the traceback found in sys.last_traceback.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Enter post-mortem debugging of the traceback found in sys.last_exc.", "file": "Lib_pdb.py", "function": "pm", "project": "cpython", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sample(self, mask: Optional[np.ndarray] = None) -> np.ndarray:\n        \n        \n        if mask is not None:\n            assert isinstance(\n                mask, np.ndarray\n            ), f\"The expected type of the mask is np.ndarray, actual type: {type(mask)}\"\n            assert (\n                mask.dtype == np.int8\n            ), f\"The expected dtype of the mask is np.int8, actual dtype: {mask.dtype}\"\n            assert (\n                mask.shape == self.shape\n            ), f\"The expected shape of the mask is {self.shape}, actual shape: {mask.shape}\"\n            assert np.all(\n                np.logical_or(mask == 0, mask == 1)\n            ), f\"All values of a mask should be 0 or 1, actual values: {mask}\"\n\n            return mask * self.np_random.integers(\n                low=0, high=2, size=self.n, dtype=self.dtype\n            )\n\n        return self.np_random.integers(low=0, high=2, size=self.n, dtype=self.dtype)```\n\nUpdated code:\n```python\ndef sample(self, mask: Optional[np.ndarray] = None) -> np.ndarray:\n        \n        \n        if mask is not None:\n            assert isinstance(\n                mask, np.ndarray\n            ), f\"The expected type of the mask is np.ndarray, actual type: {type(mask)}\"\n            assert (\n                mask.dtype == np.int8\n            ), f\"The expected dtype of the mask is np.int8, actual dtype: {mask.dtype}\"\n            assert (\n                mask.shape == self.shape\n            ), f\"The expected shape of the mask is {self.shape}, actual shape: {mask.shape}\"\n            assert np.all(\n                (mask == 0) | (mask == 1) | (mask == 2)\n            ), f\"All values of a mask should be 0, 1 or 2, actual values: {mask}\"\n\n            return np.where(\n                mask == 2,\n                self.np_random.integers(low=0, high=2, size=self.n, dtype=self.dtype),\n                mask.astype(self.dtype),\n            )\n\n        return self.np_random.integers(low=0, high=2, size=self.n, dtype=self.dtype)\n```\n\nOriginal documentation:\n```python\nGenerates a single random sample from this space.\n\n        A sample is drawn by independent, fair coin tosses (one toss per binary variable of the space).\n\n        Args:\n            mask: An optional np.ndarray to mask samples with expected shape of ``space.shape``.\n                Where mask == 0 then the samples will be 0.\n\n        Returns:\n            Sampled values from space\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates a single random sample from this space.\n\n        A sample is drawn by independent, fair coin tosses (one toss per binary variable of the space).\n\n        Args:\n            mask: An optional np.ndarray to mask samples with expected shape of ``space.shape``.\n                For mask == 0 then the samples will be 0 and mask == 1 then random samples will be generated.\n                The expected mask shape is the space shape and mask dtype is `np.int8`.\n\n        Returns:\n            Sampled values from space", "file": "gym_spaces_multi_binary.py", "function": "MultiBinary.sample", "project": "gym", "change_stats": {"code_changes": 12, "doc_changes": 5, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_input(\n        self, X, in_fit, check_positive=False, check_shape=False, check_method=False\n    ):\n        \n        \n        X = self._validate_data(\n            X,\n            ensure_2d=True,\n            dtype=FLOAT_DTYPES,\n            copy=self.copy,\n            force_all_finite=\"allow-nan\",\n            reset=in_fit,\n        )\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n            if check_positive and self.method == \"box-cox\" and np.nanmin(X) <= 0:\n                raise ValueError(\n                    \"The Box-Cox transformation can only be \"\n                    \"applied to strictly positive data\"\n                )\n\n        if check_shape and not X.shape[1] == len(self.lambdas_):\n            raise ValueError(\n                \"Input data has a different number of features \"\n                \"than fitting data. Should have {n}, data has {m}\".format(\n                    n=len(self.lambdas_), m=X.shape[1]\n                )\n            )\n\n        valid_methods = (\"box-cox\", \"yeo-johnson\")\n        if check_method and self.method not in valid_methods:\n            raise ValueError(\n                \"'method' must be one of {}, got {} instead.\".format(\n                    valid_methods, self.method\n                )\n            )\n\n        return X```\n\nUpdated code:\n```python\ndef _check_input(self, X, in_fit, check_positive=False, check_shape=False):\n        \n        \n        X = self._validate_data(\n            X,\n            ensure_2d=True,\n            dtype=FLOAT_DTYPES,\n            copy=self.copy,\n            force_all_finite=\"allow-nan\",\n            reset=in_fit,\n        )\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n            if check_positive and self.method == \"box-cox\" and np.nanmin(X) <= 0:\n                raise ValueError(\n                    \"The Box-Cox transformation can only be \"\n                    \"applied to strictly positive data\"\n                )\n\n        if check_shape and not X.shape[1] == len(self.lambdas_):\n            raise ValueError(\n                \"Input data has a different number of features \"\n                \"than fitting data. Should have {n}, data has {m}\".format(\n                    n=len(self.lambdas_), m=X.shape[1]\n                )\n            )\n\n        return X\n```\n\nOriginal documentation:\n```python\nValidate the input before fit and transform.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n\n        in_fit : bool\n            Whether or not `_check_input` is called from `fit` or other\n            methods, e.g. `predict`, `transform`, etc.\n\n        check_positive : bool, default=False\n            If True, check that all data is positive and non-zero (only if\n            ``self.method=='box-cox'``).\n\n        check_shape : bool, default=False\n            If True, check that n_features matches the length of self.lambdas_\n\n        check_method : bool, default=False\n            If True, check that the transformation method is valid.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Validate the input before fit and transform.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n\n        in_fit : bool\n            Whether or not `_check_input` is called from `fit` or other\n            methods, e.g. `predict`, `transform`, etc.\n\n        check_positive : bool, default=False\n            If True, check that all data is positive and non-zero (only if\n            ``self.method=='box-cox'``).\n\n        check_shape : bool, default=False\n            If True, check that n_features matches the length of self.lambdas_", "file": "sklearn_preprocessing__data.py", "function": "PowerTransformer._check_input", "project": "scikit-learn", "change_stats": {"code_changes": 14, "doc_changes": 5, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_deterministic():\n    \n    \n    return _C._get_deterministic()```\n\nUpdated code:\n```python\ndef is_deterministic():\n    \n    \n    warnings.warn((\n        \"torch.is_deterministic is deprecated and will be removed in a future \"\n        \"release. Please use torch.are_deterministic_algorithms_enabled instead\"))\n    return are_deterministic_algorithms_enabled()\n```\n\nOriginal documentation:\n```python\nr\"\"\"Returns True if the global deterministic flag is turned on. Refer to\n    :func:`torch.set_deterministic` documentation for more details.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"This function is deprecated and will be removed in a future release.\n    Please use :func:`torch.are_deterministic_algorithms_enabled` instead.", "file": "torch___init__.py", "function": "is_deterministic", "project": "pytorch", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef analyze_all_pipes(pipeline, warn=True):\n    \n    \n    problems = {}\n    for i, (name, pipe) in enumerate(pipeline):\n        problems[name] = analyze_pipes(pipeline, name, pipe, i, warn=warn)\n    return problems```\n\nUpdated code:\n```python\ndef analyze_all_pipes(nlp: \"Language\", warn: bool = True) -> Dict[str, List[str]]:\n    \n    \n    problems = {}\n    for i, name in enumerate(nlp.pipe_names):\n        problems[name] = analyze_pipes(nlp, name, i, warn=warn)\n    return problems\n```\n\nOriginal documentation:\n```python\nAnalyze all pipes in the pipeline in order.\n\n    pipeline (list): A list of (name, pipe) tuples e.g. nlp.pipeline.\n    warn (bool): Show user warning if problem is found.\n    RETURNS (dict): The problems found, keyed by component name.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Analyze all pipes in the pipeline in order.\n\n    nlp (Language): The current nlp object.\n    warn (bool): Show user warning if problem is found.\n    RETURNS (Dict[str, List[str]]): The problems found, keyed by component name.", "file": "spacy_pipe_analysis.py", "function": "analyze_all_pipes", "project": "spaCy", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef dict_learning(\n    X,\n    n_components,\n    *,\n    alpha,\n    max_iter=100,\n    tol=1e-8,\n    method=\"lars\",\n    n_jobs=None,\n    dict_init=None,\n    code_init=None,\n    callback=None,\n    verbose=False,\n    random_state=None,\n    return_n_iter=False,\n    positive_dict=False,\n    positive_code=False,\n    method_max_iter=1000,\n):\n    \n    \n    if method not in (\"lars\", \"cd\"):\n        raise ValueError(\"Coding method %r not supported as a fit algorithm.\" % method)\n\n    _check_positive_coding(method, positive_code)\n\n    method = \"lasso_\" + method\n\n    t0 = time.time()\n    # Avoid integer division problems\n    alpha = float(alpha)\n    random_state = check_random_state(random_state)\n\n    # Init the code and the dictionary with SVD of Y\n    if code_init is not None and dict_init is not None:\n        code = np.array(code_init, order=\"F\")\n        # Don't copy V, it will happen below\n        dictionary = dict_init\n    else:\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\n        # flip the initial code's sign to enforce deterministic output\n        code, dictionary = svd_flip(code, dictionary)\n        dictionary = S[:, np.newaxis] * dictionary\n    r = len(dictionary)\n    if n_components <= r:  # True even if n_components=None\n        code = code[:, :n_components]\n        dictionary = dictionary[:n_components, :]\n    else:\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\n        dictionary = np.r_[\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\n        ]\n\n    # Fortran-order dict better suited for the sparse coding which is the\n    # bottleneck of this algorithm.\n    dictionary = np.asfortranarray(dictionary)\n\n    errors = []\n    current_cost = np.nan\n\n    if verbose == 1:\n        print(\"[dict_learning]\", end=\" \")\n\n    # If max_iter is 0, number of iterations returned should be zero\n    ii = -1\n\n    for ii in range(max_iter):\n        dt = time.time() - t0\n        if verbose == 1:\n            sys.stdout.write(\".\")\n            sys.stdout.flush()\n        elif verbose:\n            print(\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\n                % (ii, dt, dt / 60, current_cost)\n            )\n\n        # Update code\n        code = sparse_encode(\n            X,\n            dictionary,\n            algorithm=method,\n            alpha=alpha,\n            init=code,\n            n_jobs=n_jobs,\n            positive=positive_code,\n            max_iter=method_max_iter,\n            verbose=verbose,\n        )\n\n        # Update dictionary in place\n        _update_dict(\n            dictionary,\n            X,\n            code,\n            verbose=verbose,\n            random_state=random_state,\n            positive=positive_dict,\n        )\n\n        # Cost function\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\n            np.abs(code)\n        )\n        errors.append(current_cost)\n\n        if ii > 0:\n            dE = errors[-2] - errors[-1]\n            # assert(dE >= -tol * errors[-1])\n            if dE < tol * errors[-1]:\n                if verbose == 1:\n                    # A line return\n                    print(\"\")\n                elif verbose:\n                    print(\"--- Convergence reached after %d iterations\" % ii)\n                break\n        if ii % 5 == 0 and callback is not None:\n            callback(locals())\n\n    if return_n_iter:\n        return code, dictionary, errors, ii + 1\n    else:\n        return code, dictionary, errors```\n\nUpdated code:\n```python\ndef dict_learning(\n    X,\n    n_components,\n    *,\n    alpha,\n    max_iter=100,\n    tol=1e-8,\n    method=\"lars\",\n    n_jobs=None,\n    dict_init=None,\n    code_init=None,\n    callback=None,\n    verbose=False,\n    random_state=None,\n    return_n_iter=False,\n    positive_dict=False,\n    positive_code=False,\n    method_max_iter=1000,\n):\n    \n    \n    estimator = DictionaryLearning(\n        n_components=n_components,\n        alpha=alpha,\n        max_iter=max_iter,\n        tol=tol,\n        fit_algorithm=method,\n        n_jobs=n_jobs,\n        dict_init=dict_init,\n        callback=callback,\n        code_init=code_init,\n        verbose=verbose,\n        random_state=random_state,\n        positive_code=positive_code,\n        positive_dict=positive_dict,\n        transform_max_iter=method_max_iter,\n    )\n    code = estimator.fit_transform(X)\n    if return_n_iter:\n        return (\n            code,\n            estimator.components_,\n            estimator.error_,\n            estimator.n_iter_,\n        )\n    return code, estimator.components_, estimator.error_\n```\n\nOriginal documentation:\n```python\nSolve a dictionary learning matrix factorization problem.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                     (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\n    which is the sum of the absolute values of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int\n        Number of dictionary atoms to extract.\n\n    alpha : int\n        Sparsity controlling parameter.\n\n    max_iter : int, default=100\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        The method used:\n\n        * `'lars'`: uses the least angle regression method to solve the lasso\n           problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value for the dictionary for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the sparse code for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for randomly initializing the dictionary. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse code factor in the matrix factorization.\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The dictionary factor in the matrix factorization.\n\n    errors : array\n        Vector of errors at each iteration.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    See Also\n    --------\n    dict_learning_online : Solve a dictionary learning matrix factorization\n        problem online.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate version\n        of the dictionary learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Solve a dictionary learning matrix factorization problem.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                     (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\n    which is the sum of the absolute values of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int\n        Number of dictionary atoms to extract.\n\n    alpha : int or float\n        Sparsity controlling parameter.\n\n    max_iter : int, default=100\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        The method used:\n\n        * `'lars'`: uses the least angle regression method to solve the lasso\n           problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value for the dictionary for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the sparse code for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for randomly initializing the dictionary. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse code factor in the matrix factorization.\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The dictionary factor in the matrix factorization.\n\n    errors : array\n        Vector of errors at each iteration.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    See Also\n    --------\n    dict_learning_online : Solve a dictionary learning matrix factorization\n        problem online.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate version\n        of the dictionary learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.", "file": "sklearn_decomposition__dict_learning.py", "function": "dict_learning", "project": "scikit-learn", "change_stats": {"code_changes": 127, "doc_changes": 6, "total_changes": 133}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _enhance_handler(self):\n        \n        \n        \n        total = self.enhance_ships()\n\n        self.dock_quit()\n        self.config.DOCK_FULL_TRIGGERED = True\n\n        return total```\n\nUpdated code:\n```python\ndef _enhance_handler(self):\n        \n        \n        \n        total = self.enhance_ships()\n        _, remain, _ = OCR_DOCK_AMOUNT.ocr(self.device.image)\n\n        self.dock_quit()\n        self.config.DOCK_FULL_TRIGGERED = True\n\n        return total, remain\n```\n\nOriginal documentation:\n```python\nPages:\n            in: RETIRE_APPEAR\n            out:\n\n        Returns:\n            int: enhance turn count\n\n        Pages:\n            in: DOCK_CHECK\n            out: the page before retirement popup\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Pages:\n            in: RETIRE_APPEAR\n            out:\n\n        Returns:\n            tuple(int, int): (enhance turn count, remaining dock amount)\n\n        Pages:\n            in: DOCK_CHECK\n            out: the page before retirement popup", "file": "module_retire_enhancement.py", "function": "Enhancement._enhance_handler", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fixnewlines(self):\n        \"\"\n        if (self.text.get(\"end-2c\") != '\\n'\n            and not hasattr(self.editwin, \"interp\")):  # Not shell.\n            self.text.insert(\"end-1c\", \"\\n\")\n        text = self.text.get(\"1.0\", \"end-1c\")\n        if self.eol_convention != \"\\n\":\n            text = text.replace(\"\\n\", self.eol_convention)\n        return text```\n\nUpdated code:\n```python\ndef fixnewlines(self):\n        \n        \n\n        if hasattr(self.editwin, \"interp\"):  # Saving shell.\n            text = self.editwin.get_prompt_text('1.0', self.text.index('end-1c'))\n        else:\n            if self.text.get(\"end-2c\") != '\\n':\n                self.text.insert(\"end-1c\", \"\\n\")  # Changes 'end-1c' value.\n            text = self.text.get('1.0', \"end-1c\")\n        if self.eol_convention != \"\\n\":\n            text = text.replace(\"\\n\", self.eol_convention)\n        return text\n```\n\nOriginal documentation:\n```python\nReturn text with final \\n if needed and os eols.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return text with os eols.\n\n        Add prompts if shell else final \\n if missing.", "file": "Lib_idlelib_iomenu.py", "function": "IOBinding.fixnewlines", "project": "cpython", "change_stats": {"code_changes": 16, "doc_changes": 6, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, params, modifier_rank=None, fwd_module=None, enabled=True):\n        \n        \n\n        self.enabled = enabled\n        if not enabled:\n            return\n\n        if not (isinstance(params, list) or isinstance(params, tuple)):\n            params = [params]\n\n        # enable if at least one is zero-param, otherwise a noop\n        if not any(is_zero_param(p) for p in params):\n            self.enabled = False\n            return\n\n        self.params = [p for p in params if hasattr(p, \"ds_id\")]\n        self.src_rank = None\n        if modifier_rank is not None:\n            if self.params[0].ds_process_group == torch.distributed.group.WORLD:\n                self.src_rank = modifier_rank\n            else:\n                # A group was specified; convert DP rank to global rank\n                self.src_rank = _get_global_rank(self.params[0].ds_process_group,\n                                                 modifier_rank)\n        self.fwd_module = fwd_module\n        if self.fwd_module is not None:\n            # is a no-op if already registered\n            for p in self.params:\n                register_external_parameter(self.fwd_module, p)```\n\nUpdated code:\n```python\ndef __init__(self, params, modifier_rank=None, fwd_module=None, enabled=True):\n        \n        \n\n        self.enabled = enabled\n        if not enabled:\n            return\n\n        if not (isinstance(params, list) or isinstance(params, tuple)):\n            params = [params]\n\n        # enable if at least one is zero-param, otherwise a noop\n        if not any(is_zero_param(p) for p in params):\n            self.enabled = False\n            return\n\n        self.params = [p for p in params if hasattr(p, \"ds_id\")]\n        self.src_rank = None\n        if modifier_rank is not None:\n            if self.params[0].ds_process_group == dist.get_world_group():\n                self.src_rank = modifier_rank\n            else:\n                # A group was specified; convert DP rank to global rank\n                self.src_rank = dist.get_global_rank(self.params[0].ds_process_group,\n                                                     modifier_rank)\n        self.fwd_module = fwd_module\n        if self.fwd_module is not None:\n            # is a no-op if already registered\n            for p in self.params:\n                register_external_parameter(self.fwd_module, p)\n```\n\nOriginal documentation:\n```python\nA context that collects parameters that were partitioned via a\n        :class:`deepspeed.zero.Init` context. The parameters are partitioned\n        again upon exit.\n\n        Args:\n            params (``torch.nn.Parameter``): A single parameter, a list, or a tuple of parameters to collect.\n                It's assumed that all parameters are zero params.\n            modifier_rank (int, optional): If specified, this rank's parameter will be\n                broadcasted on exit from the context. This argument is required if ``params`` are\n                modified, so that all processes have a consistent view of the data. Defaults\n                to ``None``.\n            fwd_module (``torch.nn.Module``, optional): If specified, ``params`` will be\n                registered as external parameters of ``fwd_module``. See :meth:`deepspeed.zero.register_external_parameter`.\n            enabled (bool, optional): If ``False``, this context is a no-op. Defaults to ``True``.\n\n        Important: Make sure to use ``modifier_rank`` that is not ``None`` (e.g., ``modifier_rank=0``)\n        if you need the GPU memory allocated by gather to be released upon exit from the context manager.\n\n        Examples\n        ========\n\n        #. Allocate a partitioned module, initialize its weight on rank 0, and update all\n           processes.\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init():\n                    linear = torch.nn.Linear(1000,1000)\n\n                with deepspeed.zero.GatheredParameters(linear.weight,\n                                                       modifier_rank=0):\n                    if torch.distributed.get_rank() == 0:\n                        linear.weight.zero_()\n\n                with deepspeed.zero.GatheredParameters(linear.weight,\n                                                       modifier_rank=0):\n                    if torch.distributed.get_rank() == 0:\n                        linear.weight.zero_()\n\n        #. Collect a partitioned weight to pass to another module during\n           training. The parameter will be registered as an external parameter\n           and made available during the backward pass.\n\n            .. code-block:: python\n                :emphasize-lines: 6\n\n                def forward(self, input):\n                    x = self.layer1(input)\n\n                    # self.layer1.weight is required by self.layer2.forward\n                    with deepspeed.zero.GatheredParameters(self.layer1.weight,\n                                                           fwd_module=self):\n                        y = self.layer2(x, self.layer1.weight)\n                    return y\n\n\n        #. Pretrained model loading\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init():\n                    model = MyModel()\n\n                state_dict = torch.load(model_path, map_location=\"cpu\")\n\n                def load(module: nn.Module, prefix=\"\"):\n                    # because zero3 puts placeholders in model params, this context\n                    # manager gathers (unpartitions) the params of the current layer, then loads from\n                    # the state dict and then re-partitions them again\n                    with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                        if torch.distributed.get_rank() == 0:\n                            module._load_from_state_dict(state_dict, prefix)\n\n                    for name, child in module._modules.items():\n                        if child is not None:\n                            load(child, prefix + name + \".\")\n\n                load(model, prefix=\"\")\n\n        If this approach is not used, then the full model will first be copied to each GPU. For models\n        bigger than the memory of a single GPU, this method is required.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A context that collects parameters that were partitioned via a\n        :class:`deepspeed.zero.Init` context. The parameters are partitioned\n        again upon exit.\n\n        Args:\n            params (``torch.nn.Parameter``): A single parameter, a list, or a tuple of parameters to collect.\n                It's assumed that all parameters are zero params.\n            modifier_rank (int, optional): If specified, this rank's parameter will be\n                broadcasted on exit from the context. This argument is required if ``params`` are\n                modified, so that all processes have a consistent view of the data. Defaults\n                to ``None``.\n            fwd_module (``torch.nn.Module``, optional): If specified, ``params`` will be\n                registered as external parameters of ``fwd_module``. See :meth:`deepspeed.zero.register_external_parameter`.\n            enabled (bool, optional): If ``False``, this context is a no-op. Defaults to ``True``.\n\n        Important: Make sure to use ``modifier_rank`` that is not ``None`` (e.g., ``modifier_rank=0``)\n        if you need the GPU memory allocated by gather to be released upon exit from the context manager.\n\n        Examples\n        ========\n\n        #. Allocate a partitioned module, initialize its weight on rank 0, and update all\n           processes.\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init():\n                    linear = torch.nn.Linear(1000,1000)\n\n                with deepspeed.zero.GatheredParameters(linear.weight,\n                                                       modifier_rank=0):\n                    if deepspeed.comm.get_rank() == 0:\n                        linear.weight.zero_()\n\n                with deepspeed.zero.GatheredParameters(linear.weight,\n                                                       modifier_rank=0):\n                    if deepspeed.comm.get_rank() == 0:\n                        linear.weight.zero_()\n\n        #. Collect a partitioned weight to pass to another module during\n           training. The parameter will be registered as an external parameter\n           and made available during the backward pass.\n\n            .. code-block:: python\n                :emphasize-lines: 6\n\n                def forward(self, input):\n                    x = self.layer1(input)\n\n                    # self.layer1.weight is required by self.layer2.forward\n                    with deepspeed.zero.GatheredParameters(self.layer1.weight,\n                                                           fwd_module=self):\n                        y = self.layer2(x, self.layer1.weight)\n                    return y\n\n\n        #. Pretrained model loading\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init():\n                    model = MyModel()\n\n                state_dict = torch.load(model_path, map_location=\"cpu\")\n\n                def load(module: nn.Module, prefix=\"\"):\n                    # because zero3 puts placeholders in model params, this context\n                    # manager gathers (unpartitions) the params of the current layer, then loads from\n                    # the state dict and then re-partitions them again\n                    with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                        if deepspeed.comm.get_rank() == 0:\n                            module._load_from_state_dict(state_dict, prefix)\n\n                    for name, child in module._modules.items():\n                        if child is not None:\n                            load(child, prefix + name + \".\")\n\n                load(model, prefix=\"\")\n\n        If this approach is not used, then the full model will first be copied to each GPU. For models\n        bigger than the memory of a single GPU, this method is required.", "file": "deepspeed_runtime_zero_partition_parameters.py", "function": "GatheredParameters.__init__", "project": "DeepSpeed", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lab2xyz(lab):\n    \n\n    \n\n    arr = _prepare_colorarray(lab).copy()\n\n    L, a, b = arr[:, :, 0], arr[:, :, 1], arr[:, :, 2]\n    y = (L + 16.) / 116.\n    x = (a / 500.) + y\n    z = y - (b / 200.)\n\n    out = np.dstack([x, y, z])\n\n    mask = out > 0.2068966\n    out[mask] = np.power(out[mask], 3.)\n    out[~mask] = (out[~mask] - 16.0 / 116.) / 7.787\n\n    # rescale Observer= 2 deg, Illuminant= D65\n    out *= lab_ref_white\n    return out```\n\nUpdated code:\n```python\ndef lab2xyz(lab, illuminant=\"D65\", observer=2):\n    \n\n    \n\n    arr = _prepare_colorarray(lab).copy()\n\n    L, a, b = arr[:, :, 0], arr[:, :, 1], arr[:, :, 2]\n    y = (L + 16.) / 116.\n    x = (a / 500.) + y\n    z = y - (b / 200.)\n\n    out = np.dstack([x, y, z])\n\n    mask = out > 0.2068966\n    out[mask] = np.power(out[mask], 3.)\n    out[~mask] = (out[~mask] - 16.0 / 116.) / 7.787\n\n    # rescale to the reference white (illuminant)\n    xyz_ref_white = get_xyz_coords(illuminant, observer)\n    out *= xyz_ref_white\n    return out\n```\n\nOriginal documentation:\n```python\nCIE-LAB to XYZcolor space conversion.\n\n    Parameters\n    ----------\n    lab : array_like\n        The image in lab format, in a 3-D array of shape ``(.., .., 3)``.\n\n    Returns\n    -------\n    out : ndarray\n        The image in XYZ format, in a 3-D array of shape ``(.., .., 3)``.\n\n    Raises\n    ------\n    ValueError\n        If `lab` is not a 3-D array of shape ``(.., .., 3)``.\n\n    Notes\n    -----\n    Observer = 2A, Illuminant = D65\n    CIE XYZ tristimulus values x_ref = 95.047, y_ref = 100., z_ref = 108.883\n\n    References\n    ----------\n    .. [1] http://www.easyrgb.com/index.php?X=MATH&H=07#text7\n    .. [2] http://en.wikipedia.org/wiki/Lab_color_space\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "CIE-LAB to XYZcolor space conversion.\n\n    Parameters\n    ----------\n    lab : array_like\n        The image in lab format, in a 3-D array of shape ``(.., .., 3)``.\n    illuminant : {'A', 'D50', 'D55', 'D65', 'D75', 'E'}, optional\n        The name of the illuminant (the function is NOT case sensitive).\n    observer : int, optional\n        The aperture angle of the observer.\n\n    Returns\n    -------\n    out : ndarray\n        The image in XYZ format, in a 3-D array of shape ``(.., .., 3)``.\n\n    Raises\n    ------\n    ValueError\n        If `lab` is not a 3-D array of shape ``(.., .., 3)``.\n    ValueError\n        If either the illuminant or the observer angle are not supported or\n        unknown.\n\n\n    Notes\n    -----\n    By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref\n    = 95.047, y_ref = 100., z_ref = 108.883. See function 'get_xyz_coords' for\n    a list of supported illuminants.\n\n    References\n    ----------\n    .. [1] http://www.easyrgb.com/index.php?X=MATH&H=07#text7\n    .. [2] http://en.wikipedia.org/wiki/Lab_color_space", "file": "skimage_color_colorconv.py", "function": "lab2xyz", "project": "scikit-image", "change_stats": {"code_changes": 9, "doc_changes": 15, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fit(self, X, y=None, sample_weight=None):\n        \n        \n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        n_samples, n_features = X.shape\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self.tol > 0.0:\n            tol = _tolerance(X, self.tol)\n\n            # using tol-based early stopping needs the allocation of a\n            # dedicated before which can be expensive for high dim data:\n            # hence we allocate it outside of the main loop\n            old_center_buffer = np.zeros(n_features, dtype=X.dtype)\n        else:\n            tol = 0.0\n            # no need for the center buffer if tol-based early stopping is\n            # disabled\n            old_center_buffer = np.zeros(0, dtype=X.dtype)\n\n        distances = np.zeros(self.batch_size, dtype=X.dtype)\n        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n        n_iter = int(self.max_iter * n_batches)\n\n        self._check_mkl_vcomp(X, self.batch_size)\n\n        validation_indices = random_state.randint(0, n_samples,\n                                                  self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n        x_squared_norms_valid = x_squared_norms[validation_indices]\n\n        # perform several inits with random sub-sets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(\"Init %d/%d with method: %s\"\n                      % (init_idx + 1, self._n_init, init))\n            weight_sums = np.zeros(self.n_clusters, dtype=sample_weight.dtype)\n\n            # TODO: once the `k_means` function works with sparse input we\n            # should refactor the following init to use it instead.\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans\n            cluster_centers = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size)\n\n            # Compute the label assignment on the init dataset\n            _mini_batch_step(\n                X_valid, sample_weight_valid,\n                x_squared_norms[validation_indices], cluster_centers,\n                weight_sums, old_center_buffer, False, distances=None,\n                verbose=self.verbose)\n\n            # Keep only the best cluster centers across independent inits on\n            # the common validation set\n            _, inertia = _labels_inertia(X_valid, sample_weight_valid,\n                                         x_squared_norms_valid,\n                                         cluster_centers)\n            if self.verbose:\n                print(\"Inertia for init %d/%d: %f\"\n                      % (init_idx + 1, self._n_init, inertia))\n            if best_inertia is None or inertia < best_inertia:\n                self.cluster_centers_ = cluster_centers\n                self._counts = weight_sums\n                best_inertia = inertia\n\n        # Empty context to be used inplace by the convergence check routine\n        convergence_context = {}\n\n        # Perform the iterative optimization until the final convergence\n        # criterion\n        for iteration_idx in range(n_iter):\n            # Sample a minibatch from the full dataset\n            minibatch_indices = random_state.randint(\n                0, n_samples, self.batch_size)\n\n            # Perform the actual update step on the minibatch data\n            batch_inertia, centers_squared_diff = _mini_batch_step(\n                X[minibatch_indices], sample_weight[minibatch_indices],\n                x_squared_norms[minibatch_indices],\n                self.cluster_centers_, self._counts,\n                old_center_buffer, tol > 0.0, distances=distances,\n                # Here we randomly choose whether to perform\n                # random reassignment: the choice is done as a function\n                # of the iteration index, and the minimum number of\n                # counts, in order to force this reassignment to happen\n                # every once in a while\n                random_reassign=((iteration_idx + 1)\n                                 % (10 + int(self._counts.min())) == 0),\n                random_state=random_state,\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose)\n\n            # Monitor convergence and do early stopping if necessary\n            if _mini_batch_convergence(\n                    self, iteration_idx, n_iter, tol, n_samples,\n                    centers_squared_diff, batch_inertia, convergence_context,\n                    verbose=self.verbose):\n                break\n\n        self.n_iter_ = iteration_idx + 1\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = \\\n                    self._labels_inertia_minibatch(X, sample_weight)\n\n        return self```\n\nUpdated code:\n```python\ndef fit(self, X, y=None, sample_weight=None):\n        \n        \n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        n_samples, n_features = X.shape\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        self._check_mkl_vcomp(X, self._batch_size)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        # Validation set for the init\n        validation_indices = random_state.randint(0, n_samples,\n                                                  self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n        x_squared_norms_valid = x_squared_norms[validation_indices]\n\n        # perform several inits with random subsets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(f\"Init {init_idx + 1}/{self._n_init} with method {init}\")\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans.\n            cluster_centers = self._init_centroids(\n                X, x_squared_norms=x_squared_norms, init=init,\n                random_state=random_state, init_size=self._init_size)\n\n            # Compute inertia on a validation set.\n            _, inertia = _labels_inertia_threadpool_limit(\n                X_valid, sample_weight_valid, x_squared_norms_valid,\n                cluster_centers, n_threads=self._n_threads)\n\n            if self.verbose:\n                print(f\"Inertia for init {init_idx + 1}/{self._n_init}: \"\n                      f\"{inertia}\")\n            if best_inertia is None or inertia < best_inertia:\n                init_centers = cluster_centers\n                best_inertia = inertia\n\n        centers = init_centers\n        centers_new = np.empty_like(centers)\n\n        # Initialize counts\n        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n        # Attributes to monitor the convergence\n        self._ewa_inertia = None\n        self._ewa_inertia_min = None\n        self._no_improvement = 0\n\n        # Initialize number of samples seen since last reassignment\n        self._n_since_last_reassign = 0\n\n        n_steps = (self.max_iter * n_samples) // self._batch_size\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            # Perform the iterative optimization until convergence\n            for i in range(n_steps):\n                # Sample a minibatch from the full dataset\n                minibatch_indices = random_state.randint(0, n_samples,\n                                                         self._batch_size)\n\n                # Perform the actual update step on the minibatch data\n                batch_inertia = _mini_batch_step(\n                    X=X[minibatch_indices],\n                    x_squared_norms=x_squared_norms[minibatch_indices],\n                    sample_weight=sample_weight[minibatch_indices],\n                    centers=centers,\n                    centers_new=centers_new,\n                    weight_sums=self._counts,\n                    random_state=random_state,\n                    random_reassign=self._random_reassign(),\n                    reassignment_ratio=self.reassignment_ratio,\n                    verbose=self.verbose,\n                    n_threads=self._n_threads)\n\n                if self._tol > 0.0:\n                    centers_squared_diff = np.sum((centers_new - centers)**2)\n                else:\n                    centers_squared_diff = 0\n\n                centers, centers_new = centers_new, centers\n\n                # Monitor convergence and do early stopping if necessary\n                if self._mini_batch_convergence(\n                        i, n_steps, n_samples, centers_squared_diff,\n                        batch_inertia):\n                    break\n\n        self.cluster_centers_ = centers\n\n        self.n_steps_ = i + 1\n        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X, sample_weight, x_squared_norms, self.cluster_centers_,\n                n_threads=self._n_threads)\n        else:\n            self.inertia_ = self._ewa_inertia * n_samples\n\n        return self\n```\n\nOriginal documentation:\n```python\nCompute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self", "file": "sklearn_cluster__kmeans.py", "function": "MiniBatchKMeans.fit", "project": "scikit-learn", "change_stats": {"code_changes": 154, "doc_changes": 6, "total_changes": 160}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _play(self, message):\n         \n        \n        LOG.info('SimpleAudioService._play')\n\n        # Stop any existing audio playback\n        self._stop_running_process()\n\n        repeat = message.data.get('repeat', False)\n        self._is_playing = True\n        self._paused = False\n        if isinstance(self.tracks[self.index], list):\n            track = self.tracks[self.index][0]\n            mime = self.tracks[self.index][1]\n            mime = mime.split('/')\n        else:  # Assume string\n            track = self.tracks[self.index]\n            mime = find_mime(track)\n        LOG.debug('Mime info: {}'.format(mime))\n\n        # Indicate to audio service which track is being played\n        if self._track_start_callback:\n            self._track_start_callback(track)\n\n        # Replace file:// uri's with normal paths\n        track = track.replace('file://', '')\n        try:\n            if 'mpeg' in mime[1]:\n                self.process = play_mp3(track)\n            elif 'ogg' in mime[1]:\n                self.process = play_ogg(track)\n            elif 'wav' in mime[1]:\n                self.process = play_wav(track)\n            else:\n                # If no mime info could be determined guess mp3\n                self.process = play_mp3(track)\n        except FileNotFoundError as e:\n            LOG.error('Couldn\\'t play audio, {}'.format(repr(e)))\n            self.process = None\n        except Exception as e:\n            LOG.exception(repr(e))\n            self.process = None\n\n        # Wait for completion or stop request\n        while (self._is_process_running() and not self._stop_signal):\n            sleep(0.25)\n\n        if self._stop_signal:\n            self._stop_running_process()\n            self._is_playing = False\n            self._paused = False\n            return\n        else:\n            self.process = None\n\n        # if there are more tracks available play next\n        self.index += 1\n        if self.index < len(self.tracks) or repeat:\n            if self.index >= len(self.tracks):\n                self.index = 0\n            self.bus.emit(Message('SimpleAudioServicePlay',\n                                  {'repeat': repeat}))\n        else:\n            self._track_start_callback(None)\n            self._is_playing = False\n            self._paused = False```\n\nUpdated code:\n```python\ndef _play(self, message):\n        \n        \n        LOG.info('SimpleAudioService._play')\n\n        # Stop any existing audio playback\n        self._stop_running_process()\n\n        repeat = message.data.get('repeat', False)\n        self._is_playing = True\n        self._paused = False\n        with self.track_lock:\n            if len(self.tracks) > self.index:\n                track, mime = self._get_track(self.tracks[self.index])\n            else:\n                return\n\n        LOG.debug('Mime info: {}'.format(mime))\n\n        # Indicate to audio service which track is being played\n        if self._track_start_callback:\n            self._track_start_callback(track)\n\n        # Replace file:// uri's with normal paths\n        track = track.replace('file://', '')\n        try:\n            if 'mpeg' in mime[1]:\n                self.process = play_mp3(track)\n            elif 'ogg' in mime[1]:\n                self.process = play_ogg(track)\n            elif 'wav' in mime[1]:\n                self.process = play_wav(track)\n            else:\n                # If no mime info could be determined guess mp3\n                self.process = play_mp3(track)\n        except FileNotFoundError as e:\n            LOG.error('Couldn\\'t play audio, {}'.format(repr(e)))\n            self.process = None\n        except Exception as e:\n            LOG.exception(repr(e))\n            self.process = None\n\n        # Wait for completion or stop request\n        while (self._is_process_running() and not self._stop_signal):\n            sleep(0.25)\n\n        if self._stop_signal:\n            self._stop_running_process()\n            self._is_playing = False\n            self._paused = False\n            return\n        else:\n            self.process = None\n\n        # if there are more tracks available play next\n        self.index += 1\n        with self.track_lock:\n            if self.index < len(self.tracks) or repeat:\n                if self.index >= len(self.tracks):\n                    self.index = 0\n                self.bus.emit(Message('SimpleAudioServicePlay',\n                                      {'repeat': repeat}))\n            else:\n                self._track_start_callback(None)\n                self._is_playing = False\n                self._paused = False\n```\n\nOriginal documentation:\n```python\nImplementation specific async method to handle playback.\n            This allows mpg123 service to use the \"next method as well\n            as basic play/stop.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Implementation specific async method to handle playback.\n\n        This allows mpg123 service to use the next method as well\n        as basic play/stop.", "file": "mycroft_audio_services_simple___init__.py", "function": "SimpleAudioService._play", "project": "mycroft-core", "change_stats": {"code_changes": 36, "doc_changes": 7, "total_changes": 43}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef changed(self):\n        \n        \n        \n        for observer in self.observers:\n            observer.notify(self)\n        for key in self.update_dict:\n            self.update_dict[key] = True```\n\nUpdated code:\n```python\ndef changed(self):\n        \n        \n        \n        self.callbacksSM.process('changed', self)\n\n        for key in self.update_dict:\n            self.update_dict[key] = True\n```\n\nOriginal documentation:\n```python\nCall this whenever the mappable is changed so observers can\n        update state\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Call this whenever the mappable is changed to notify all the\n        callbackSM listeners to the 'changed' signal", "file": "lib_matplotlib_cm.py", "function": "ScalarMappable.changed", "project": "matplotlib", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef complete(self, text, line=None, cursor_pos=None):\n        \n        \n\n        # Inject names into __builtin__ so we can complete on the added names.\n        with self.builtin_trap:\n            return self.Completer.complete(text,line_buffer=text)```\n\nUpdated code:\n```python\ndef complete(self, text, line=None, cursor_pos=None):\n        \n        \n\n        # Inject names into __builtin__ so we can complete on the added names.\n        with self.builtin_trap:\n            return self.Completer.complete(text, line, cursor_pos)\n```\n\nOriginal documentation:\n```python\nReturn a sorted list of all possible completions on text.\n\n        Parameters\n        ----------\n\n           text : string\n             A string of text to be completed on.\n\n           line : string, optional\n             The complete line that text is part of.\n\n           cursor_pos : int, optional\n             The position of the cursor on the input line.\n\n        The optional arguments allow the completion to take more context into\n        account, and are part of the low-level completion API.\n        \n        This is a wrapper around the completion mechanism, similar to what\n        readline does at the command line when the TAB key is hit.  By\n        exposing it as a method, it can be used by other non-readline\n        environments (such as GUIs) for text completion.\n\n        Simple usage example:\n\n        In [7]: x = 'hello'\n\n        In [8]: x\n        Out[8]: 'hello'\n\n        In [9]: print x\n        hello\n\n        In [10]: _ip.complete('x.l')\n        Out[10]: ['x.ljust', 'x.lower', 'x.lstrip']\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the completed text and a list of completions.\n\n        Parameters\n        ----------\n\n           text : string\n             A string of text to be completed on.  It can be given as empty and\n             instead a line/position pair are given.  In this case, the\n             completer itself will split the line like readline does.\n\n           line : string, optional\n             The complete line that text is part of.\n\n           cursor_pos : int, optional\n             The position of the cursor on the input line.\n\n        Returns\n        -------\n          text : string\n            The actual text that was completed.\n\n          matches : list\n            A sorted list with all possible completions.\n\n        The optional arguments allow the completion to take more context into\n        account, and are part of the low-level completion API.\n        \n        This is a wrapper around the completion mechanism, similar to what\n        readline does at the command line when the TAB key is hit.  By\n        exposing it as a method, it can be used by other non-readline\n        environments (such as GUIs) for text completion.\n\n        Simple usage example:\n\n        In [1]: x = 'hello'\n\n        In [2]: _ip.complete('x.l')\n        Out[2]: ('x.l', ['x.ljust', 'x.lower', 'x.lstrip'])", "file": "IPython_core_interactiveshell.py", "function": "InteractiveShell.complete", "project": "ipython", "change_stats": {"code_changes": 4, "doc_changes": 28, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef writelines(self, list_of_data):\n        \n        \n        for data in list_of_data:\n            self.write(data)```\n\nUpdated code:\n```python\ndef writelines(self, list_of_data):\n        \n        \n        if not PY34:\n            # In Python 3.3, bytes.join() doesn't handle memoryview.\n            list_of_data = (\n                bytes(data) if isinstance(data, memoryview) else data\n                for data in list_of_data)\n        self.write(b''.join(list_of_data))\n```\n\nOriginal documentation:\n```python\nWrite a list (or any iterable) of data bytes to the transport.\n\n        The default implementation just calls write() for each item in\n        the list/iterable.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write a list (or any iterable) of data bytes to the transport.\n\n        The default implementation concatenates the arguments and\n        calls write() on the result.", "file": "Lib_asyncio_transports.py", "function": "WriteTransport.writelines", "project": "cpython", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_alignments(self, arguments: Namespace) -> Alignments | None:\n         \n        \n        if arguments.alignments:\n            logger.debug(\"Alignments location provided: %s\", arguments.alignments)\n            return Alignments(os.path.dirname(arguments.alignments),\n                              filename=os.path.basename(arguments.alignments))\n        if self._input_is_faces and arguments.processing == \"output\":\n            logger.debug(\"No alignments file provided for faces. Using PNG Header for output\")\n            return None\n        if self._input_is_faces:\n            logger.warning(\"Faces input selected without an alignments file. Masks wil only \"\n                           \"be updated in the faces' PNG Header\")\n            return None\n\n        folder = arguments.input\n        if self._loader.is_video:\n            logger.debug(\"Alignments from Video File: '%s'\", folder)\n            folder, filename = os.path.split(folder)\n            filename = f\"{os.path.splitext(filename)[0]}_alignments.fsa\"\n        else:\n            logger.debug(\"Alignments from Input Folder: '%s'\", folder)\n            filename = \"alignments\"\n\n        return Alignments(folder, filename=filename)```\n\nUpdated code:\n```python\ndef _get_alignments(self, alignments: str | None, input_location: str) -> Alignments | None:\n         \n        \n        if alignments:\n            logger.debug(\"Alignments location provided: %s\", alignments)\n            return Alignments(os.path.dirname(alignments),\n                              filename=os.path.basename(alignments))\n        if self._input_is_faces and self._update_type == \"output\":\n            logger.debug(\"No alignments file provided for faces. Using PNG Header for output\")\n            return None\n        if self._input_is_faces:\n            logger.warning(\"Faces input selected without an alignments file. Masks wil only \"\n                           \"be updated in the faces' PNG Header\")\n            return None\n\n        folder = input_location\n        if self._loader.is_video:\n            logger.debug(\"Alignments from Video File: '%s'\", folder)\n            folder, filename = os.path.split(folder)\n            filename = f\"{os.path.splitext(filename)[0]}_alignments.fsa\"\n        else:\n            logger.debug(\"Alignments from Input Folder: '%s'\", folder)\n            filename = \"alignments\"\n\n        retval = Alignments(folder, filename=filename)\n        self._loader.add_alignments(retval)\n        return retval\n```\n\nOriginal documentation:\n```python\nObtain the alignments from either the given alignments location or the default\n        location.\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n\n        Returns\n        -------\n        ``None`` or :class:`lib.align.alignments.Alignments`:\n            If output is requested, returns a :class:`lib.image.ImagesSaver` otherwise\n            returns ``None``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Obtain the alignments from either the given alignments location or the default\n        location.\n\n        Parameters\n        ----------\n        alignments: str | None\n            Full path to the alignemnts file if provided or ``None`` if not\n        input_location: str\n            Full path to the source files to be used by the mask tool\n\n        Returns\n        -------\n        ``None`` or :class:`lib.align.alignments.Alignments`:\n            If output is requested, returns a :class:`lib.image.ImagesSaver` otherwise\n            returns ``None``", "file": "tools_mask_mask.py", "function": "_Mask._get_alignments", "project": "faceswap", "change_stats": {"code_changes": 20, "doc_changes": 8, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef new_order(self, csr_pem):\n        \n\n        \n        if self.acme_version == 1:\n            client_v1 = cast(Client, self.client)\n            csr = OpenSSL.crypto.load_certificate_request(OpenSSL.crypto.FILETYPE_PEM, csr_pem)\n            # pylint: disable=protected-access\n            dnsNames = crypto_util._pyopenssl_cert_or_req_all_names(csr)\n            authorizations = []\n            for domain in dnsNames:\n                authorizations.append(client_v1.request_domain_challenges(domain))\n            return messages.OrderResource(authorizations=authorizations, csr_pem=csr_pem)\n        return cast(ClientV2, self.client).new_order(csr_pem)```\n\nUpdated code:\n```python\ndef new_order(self, csr_pem: bytes) -> messages.OrderResource:\n        \n\n        \n        if self.acme_version == 1:\n            client_v1 = cast(Client, self.client)\n            csr = OpenSSL.crypto.load_certificate_request(OpenSSL.crypto.FILETYPE_PEM, csr_pem)\n            # pylint: disable=protected-access\n            dnsNames = crypto_util._pyopenssl_cert_or_req_all_names(csr)\n            authorizations = []\n            for domain in dnsNames:\n                authorizations.append(client_v1.request_domain_challenges(domain))\n            return messages.OrderResource(authorizations=authorizations, csr_pem=csr_pem)\n        return cast(ClientV2, self.client).new_order(csr_pem)\n```\n\nOriginal documentation:\n```python\nRequest a new Order object from the server.\n\n        If using ACMEv1, returns a dummy OrderResource with only\n        the authorizations field filled in.\n\n        :param str csr_pem: A CSR in PEM format.\n\n        :returns: The newly created order.\n        :rtype: OrderResource\n\n        :raises errors.WildcardUnsupportedError: if a wildcard domain is\n            requested but unsupported by the ACME version\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Request a new Order object from the server.\n\n        If using ACMEv1, returns a dummy OrderResource with only\n        the authorizations field filled in.\n\n        :param bytes csr_pem: A CSR in PEM format.\n\n        :returns: The newly created order.\n        :rtype: OrderResource\n\n        :raises errors.WildcardUnsupportedError: if a wildcard domain is\n            requested but unsupported by the ACME version", "file": "acme_acme_client.py", "function": "BackwardsCompatibleClientV2.new_order", "project": "certbot", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef finish(self, partial_movie_files=None):\n        \n        \n        \n        if config[\"write_to_movie\"]:\n            if hasattr(self, \"writing_process\"):\n                self.writing_process.terminate()\n            self.combine_movie_files(partial_movie_files=partial_movie_files)\n            if config[\"flush_cache\"]:\n                self.flush_cache_directory()\n            else:\n                self.clean_cache()```\n\nUpdated code:\n```python\ndef finish(self, partial_movie_files=None):\n        \n        \n        \n        if write_to_movie():\n            if hasattr(self, \"writing_process\"):\n                self.writing_process.terminate()\n            self.combine_movie_files(partial_movie_files=partial_movie_files)\n            if config[\"flush_cache\"]:\n                self.flush_cache_directory()\n            else:\n                self.clean_cache()\n        elif is_png_format():\n            target_dir, _ = os.path.splitext(self.image_file_path)\n            logger.info(\"\\n%i images ready at %s\\n\", self.frame_count, target_dir)\n```\n\nOriginal documentation:\n```python\nFinishes writing to the FFMPEG buffer.\n        Combines the partial movie files into the\n        whole scene.\n        If save_last_frame is True, saves the last\n        frame in the default image directory.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Finishes writing to the FFMPEG buffer or writing images\n        to output directory.\n        Combines the partial movie files into the\n        whole scene.\n        If save_last_frame is True, saves the last\n        frame in the default image directory.", "file": "manim_scene_scene_file_writer.py", "function": "SceneFileWriter.finish", "project": "manim", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        max_batch_size: int,\n        timeout_s: float,\n        handle_batch_func: Optional[Callable] = None,\n    ) -> None:\n        \n        \n        self.queue: asyncio.Queue[_SingleRequest] = asyncio.Queue()\n        self.max_batch_size = max_batch_size\n        self.timeout_s = timeout_s\n        self.queue_put_event = asyncio.Event()\n\n        self._handle_batch_task = None\n        if handle_batch_func is not None:\n            self._handle_batch_task = get_or_create_event_loop().create_task(\n                self._process_batches(handle_batch_func)\n            )```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        max_batch_size: int,\n        batch_wait_timeout_s: float,\n        handle_batch_func: Optional[Callable] = None,\n    ) -> None:\n        \n        \n        self.queue: asyncio.Queue[_SingleRequest] = asyncio.Queue()\n        self.max_batch_size = max_batch_size\n        self.batch_wait_timeout_s = batch_wait_timeout_s\n        self.queue_put_event = asyncio.Event()\n\n        self._handle_batch_task = None\n        if handle_batch_func is not None:\n            self._handle_batch_task = get_or_create_event_loop().create_task(\n                self._process_batches(handle_batch_func)\n            )\n```\n\nOriginal documentation:\n```python\nAsync queue that accepts individual items and returns batches.\n\n        Respects max_batch_size and timeout_s; a batch will be returned when\n        max_batch_size elements are available or the timeout has passed since\n        the previous get.\n\n        If handle_batch_func is passed in, a background coroutine will run to\n        poll from the queue and call handle_batch_func on the results.\n\n        Arguments:\n            max_batch_size: max number of elements to return in a batch.\n            timeout_s: time to wait before returning an incomplete\n                batch.\n            handle_batch_func(Optional[Callable]): callback to run in the\n                background to handle batches if provided.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Async queue that accepts individual items and returns batches.\n\n        Respects max_batch_size and timeout_s; a batch will be returned when\n        max_batch_size elements are available or the timeout has passed since\n        the previous get.\n\n        If handle_batch_func is passed in, a background coroutine will run to\n        poll from the queue and call handle_batch_func on the results.\n\n        Cannot be pickled.\n\n        Arguments:\n            max_batch_size: max number of elements to return in a batch.\n            timeout_s: time to wait before returning an incomplete\n                batch.\n            handle_batch_func(Optional[Callable]): callback to run in the\n                background to handle batches if provided.", "file": "python_ray_serve_batching.py", "function": "_BatchQueue.__init__", "project": "ray", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef overlay(self, seg, position=0, loop=False, times=None):\n        \n        \n        \n\n        if loop:\n            # match loop=True's behavior with new times (count) mechinism.\n            times = -1\n        elif times is None:\n            # no times specified, just once through\n            times = 1\n        elif times == 0:\n            # it's a no-op, make a copy since we never mutate\n            return self._spawn(self._data)\n\n        output = StringIO()\n\n        seg1, seg2 = AudioSegment._sync(self, seg)\n        sample_width = seg1.sample_width\n        spawn = seg1._spawn\n\n        output.write(seg1[:position]._data)\n\n        # drop down to the raw data\n        seg1 = seg1[position:]._data\n        seg2 = seg2._data\n        pos = 0\n        seg1_len = len(seg1)\n        seg2_len = len(seg2)\n        while times:\n            remaining = max(0, seg1_len - pos)\n            if seg2_len >= remaining:\n                seg2 = seg2[:remaining]\n                seg2_len = remaining\n                # we've hit the end, we're done looping (if we were) and this\n                # is our last go-around\n                times = 1\n\n            output.write(audioop.add(seg1[pos:pos + seg2_len], seg2,\n                                     sample_width))\n            pos += seg2_len\n\n            # dec times to break our while loop (eventually)\n            times -= 1\n\n        output.write(seg1[pos:])\n\n        return spawn(data=output)```\n\nUpdated code:\n```python\ndef overlay(self, seg, position=0, loop=False, times=None, gain_during_overlay=None):\n        \n        \n        \n\n        if loop:\n            # match loop=True's behavior with new times (count) mechinism.\n            times = -1\n        elif times is None:\n            # no times specified, just once through\n            times = 1\n        elif times == 0:\n            # it's a no-op, make a copy since we never mutate\n            return self._spawn(self._data)\n\n        output = StringIO()\n\n        seg1, seg2 = AudioSegment._sync(self, seg)\n        sample_width = seg1.sample_width\n        spawn = seg1._spawn\n\n        output.write(seg1[:position]._data)\n\n        # drop down to the raw data\n        seg1 = seg1[position:]._data\n        seg2 = seg2._data\n        pos = 0\n        seg1_len = len(seg1)\n        seg2_len = len(seg2)\n        while times:\n            remaining = max(0, seg1_len - pos)\n            if seg2_len >= remaining:\n                seg2 = seg2[:remaining]\n                seg2_len = remaining\n                # we've hit the end, we're done looping (if we were) and this\n                # is our last go-around\n                times = 1\n\n            if gain_during_overlay:\n                seg1_overlaid = seg1[pos:pos + seg2_len]\n                seg1_adjusted_gain = audioop.mul(seg1_overlaid, self.sample_width,\n                                               db_to_float(float(gain_during_overlay)))\n                output.write(audioop.add(seg1_adjusted_gain, seg2, sample_width))\n            else:\n                output.write(audioop.add(seg1[pos:pos + seg2_len], seg2,\n                                     sample_width))\n            pos += seg2_len\n\n            # dec times to break our while loop (eventually)\n            times -= 1\n\n        output.write(seg1[pos:])\n\n        return spawn(data=output)\n```\n\nOriginal documentation:\n```python\nOverlay the provided segment on to this segment starting at the\n        specificed position and using the specfied looping beahvior.\n\n        seg (AudioSegment):\n            The audio segment to overlay on to this one.\n\n        position (optional int):\n            The position to start overlaying the provided segment in to this\n            one.\n\n        loop (optional bool):\n            Loop seg as many times as necessary to match this segment's length.\n            Overrides loops param.\n\n        times (optional int):\n            Loop seg the specified number of times or until it matches this\n            segment's length. 1 means once, 2 means twice, ... 0 would make the\n            call a no-op\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Overlay the provided segment on to this segment starting at the\n        specificed position and using the specfied looping beahvior.\n\n        seg (AudioSegment):\n            The audio segment to overlay on to this one.\n\n        position (optional int):\n            The position to start overlaying the provided segment in to this\n            one.\n\n        loop (optional bool):\n            Loop seg as many times as necessary to match this segment's length.\n            Overrides loops param.\n\n        times (optional int):\n            Loop seg the specified number of times or until it matches this\n            segment's length. 1 means once, 2 means twice, ... 0 would make the\n            call a no-op\n        gain_during_overlay (optional int):\n            Changes this segment's volume by the specified amount during the\n            duration of time that seg is overlaid on top of it. When negative,\n            this has the effect of 'ducking' the audio under the overlay.", "file": "pydub_audio_segment.py", "function": "AudioSegment.overlay", "project": "pydub", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef launch(\n    main_func,\n    num_gpus_per_machine,\n    num_machines=1,\n    machine_rank=0,\n    dist_url=None,\n    args=(),\n    timeout=DEFAULT_TIMEOUT,\n):\n    \n    \n    \n    world_size = num_machines * num_gpus_per_machine\n    if world_size > 1:\n        # https://github.com/pytorch/pytorch/pull/14391\n        # TODO prctl in spawned processes\n\n        if dist_url == \"auto\":\n            assert num_machines == 1, \"dist_url=auto not supported in multi-machine jobs.\"\n            port = _find_free_port()\n            dist_url = f\"tcp://127.0.0.1:{port}\"\n        if num_machines > 1 and dist_url.startswith(\"file://\"):\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"file:// is not a reliable init_method in multi-machine jobs. Prefer tcp://\"\n            )\n\n        mp.spawn(\n            _distributed_worker,\n            nprocs=num_gpus_per_machine,\n            args=(\n                main_func,\n                world_size,\n                num_gpus_per_machine,\n                machine_rank,\n                dist_url,\n                args,\n                timeout,\n            ),\n            daemon=False,\n        )\n    else:\n        main_func(*args)```\n\nUpdated code:\n```python\ndef launch(\n    main_func,\n    # Should be num_processes_per_machine, but kept for compatibility.\n    num_gpus_per_machine,\n    num_machines=1,\n    machine_rank=0,\n    dist_url=None,\n    args=(),\n    timeout=DEFAULT_TIMEOUT,\n):\n    \n    \n    \n    world_size = num_machines * num_gpus_per_machine\n    if world_size > 1:\n        # https://github.com/pytorch/pytorch/pull/14391\n        # TODO prctl in spawned processes\n\n        if dist_url == \"auto\":\n            assert num_machines == 1, \"dist_url=auto not supported in multi-machine jobs.\"\n            port = _find_free_port()\n            dist_url = f\"tcp://127.0.0.1:{port}\"\n        if num_machines > 1 and dist_url.startswith(\"file://\"):\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"file:// is not a reliable init_method in multi-machine jobs. Prefer tcp://\"\n            )\n\n        mp.start_processes(\n            _distributed_worker,\n            nprocs=num_gpus_per_machine,\n            args=(\n                main_func,\n                world_size,\n                num_gpus_per_machine,\n                machine_rank,\n                dist_url,\n                args,\n                timeout,\n            ),\n            daemon=False,\n        )\n    else:\n        main_func(*args)\n```\n\nOriginal documentation:\n```python\nLaunch multi-gpu or distributed training.\n    This function must be called on all machines involved in the training.\n    It will spawn child processes (defined by ``num_gpus_per_machine``) on each machine.\n\n    Args:\n        main_func: a function that will be called by `main_func(*args)`\n        num_gpus_per_machine (int): number of GPUs per machine\n        num_machines (int): the total number of machines\n        machine_rank (int): the rank of this machine\n        dist_url (str): url to connect to for distributed jobs, including protocol\n                       e.g. \"tcp://127.0.0.1:8686\".\n                       Can be set to \"auto\" to automatically select a free port on localhost\n        timeout (timedelta): timeout of the distributed workers\n        args (tuple): arguments passed to main_func\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Launch multi-process or distributed training.\n    This function must be called on all machines involved in the training.\n    It will spawn child processes (defined by ``num_gpus_per_machine``) on each machine.\n\n    Args:\n        main_func: a function that will be called by `main_func(*args)`\n        num_gpus_per_machine (int): number of processes per machine. When\n            using GPUs, this should be the number of GPUs.\n        num_machines (int): the total number of machines\n        machine_rank (int): the rank of this machine\n        dist_url (str): url to connect to for distributed jobs, including protocol\n                       e.g. \"tcp://127.0.0.1:8686\".\n                       Can be set to \"auto\" to automatically select a free port on localhost\n        timeout (timedelta): timeout of the distributed workers\n        args (tuple): arguments passed to main_func", "file": "detectron2_engine_launch.py", "function": "launch", "project": "detectron2", "change_stats": {"code_changes": 5, "doc_changes": 7, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_python_install(self, index):\n        \n        python_install = {}\n        key = 'python.install.{}'.format(index)\n        raw_install = self.raw_config['python']['install'][str(index)]\n        with self.catch_validation_error(key):\n            validate_dict(raw_install)\n\n        if 'requirements' in raw_install:\n            requirements_key = key + '.requirements'\n            with self.catch_validation_error(requirements_key):\n                requirements = validate_file(\n                    self.pop_config(requirements_key),\n                    self.base_path\n                )\n                python_install['requirements'] = requirements\n        elif 'path' in raw_install:\n            path_key = key + '.path'\n            with self.catch_validation_error(path_key):\n                path = validate_directory(\n                    self.pop_config(path_key),\n                    self.base_path\n                )\n                python_install['path'] = path\n\n            method_key = key + '.method'\n            with self.catch_validation_error(method_key):\n                method = validate_choice(\n                    self.pop_config(method_key, PIP),\n                    self.valid_install_method\n                )\n                python_install['method'] = method\n\n            extrareq_key = key + '.extra_requirements'\n            with self.catch_validation_error(extrareq_key):\n                extra_requirements = validate_list(\n                    self.pop_config(extrareq_key, [])\n                )\n                if extra_requirements and python_install['method'] != PIP:\n                    self.error(\n                        extrareq_key,\n                        'You need to install your project with pip '\n                        'to use extra_requirements',\n                        code=PYTHON_INVALID,\n                    )\n                python_install['extra_requirements'] = extra_requirements\n        else:\n            self.error(\n                key,\n                '\"path\" or \"requirements\" key is required',\n                code=CONFIG_REQUIRED,\n            )\n        return python_install```\n\nUpdated code:\n```python\ndef validate_python_install(self, index):\n        \n        python_install = {}\n        key = 'python.install.{}'.format(index)\n        raw_install = self.raw_config['python']['install'][str(index)]\n        with self.catch_validation_error(key):\n            validate_dict(raw_install)\n\n        if 'requirements' in raw_install:\n            requirements_key = key + '.requirements'\n            with self.catch_validation_error(requirements_key):\n                requirements = validate_file(\n                    self.pop_config(requirements_key),\n                    self.base_path\n                )\n                python_install['requirements'] = requirements\n        elif 'path' in raw_install:\n            path_key = key + '.path'\n            with self.catch_validation_error(path_key):\n                path = validate_directory(\n                    self.pop_config(path_key),\n                    self.base_path\n                )\n                python_install['path'] = path\n\n            method_key = key + '.method'\n            with self.catch_validation_error(method_key):\n                method = validate_choice(\n                    self.pop_config(method_key, PIP),\n                    self.valid_install_method\n                )\n                python_install['method'] = method\n\n            extra_req_key = key + '.extra_requirements'\n            with self.catch_validation_error(extra_req_key):\n                extra_requirements = validate_list(\n                    self.pop_config(extra_req_key, [])\n                )\n                if extra_requirements and python_install['method'] != PIP:\n                    self.error(\n                        extra_req_key,\n                        'You need to install your project with pip '\n                        'to use extra_requirements',\n                        code=PYTHON_INVALID,\n                    )\n                python_install['extra_requirements'] = extra_requirements\n        else:\n            self.error(\n                key,\n                '\"path\" or \"requirements\" key is required',\n                code=CONFIG_REQUIRED,\n            )\n        return python_install\n```\n\nOriginal documentation:\n```python\nValidates the python.install.index key.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Validates the python.install.{index} key.", "file": "readthedocs_config_config.py", "function": "BuildConfigV2.validate_python_install", "project": "readthedocs.org", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update(\n        self, table_name: str, data: pd.DataFrame, columns: List[str] = None\n    ) -> HandlerResponse:\n        \n        \n        \n        return super().update(table_name, data, columns)```\n\nUpdated code:\n```python\ndef update(\n        self,\n        table_name: str,\n        data: pd.DataFrame,\n        key_columns: List[str] = None,\n    ):\n        \n        \n        \n        collection = self._client.get_collection(table_name)\n\n        # drop columns with all None values\n\n        data.dropna(axis=1, inplace=True)\n\n        data = data.to_dict(orient=\"list\")\n\n        collection.update(\n            ids=data[TableField.ID.value],\n            documents=data.get(TableField.CONTENT.value),\n            embeddings=data[TableField.EMBEDDINGS.value],\n            metadatas=data.get(TableField.METADATA.value),\n        )\n```\n\nOriginal documentation:\n```python\nUpdate data in the ChromaDB database.\n        TODO: not implemented yet\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update data in the ChromaDB database.", "file": "mindsdb_integrations_handlers_chromadb_handler_chromadb_handler.py", "function": "ChromaDBHandler.update", "project": "mindsdb", "change_stats": {"code_changes": 24, "doc_changes": 3, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_bytes(obj):\n      \n    if not isinstance(obj, six.string_types):\n        return obj\n    return obj.encode(DEFAULT_ENCODING)```\n\nUpdated code:\n```python\ndef to_bytes(obj, encoding=DEFAULT_ENCODING, errors='strict'):\n     \n    \n    return obj.encode(encoding, errors) if isinstance(obj, six.text_type) else obj\n```\n\nOriginal documentation:\n```python\nConvert a string/bytes object to bytes\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "If ``obj`` is an instance of ``text_type``, return\n    ``obj.encode(encoding, errors)``, otherwise return ``obj``", "file": "localstack_utils_common.py", "function": "to_bytes", "project": "localstack", "change_stats": {"code_changes": 11, "doc_changes": 5, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef correct_pad(input_c: int, kernel_size: int):\n    \n    \n\n    kernel_size = (kernel_size, kernel_size)\n\n    adjust = (1 - input_c % 2, 1 - input_c % 2)\n    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n    return ((correct[0] - adjust[0], correct[0]),\n            (correct[1] - adjust[1], correct[1]))```\n\nUpdated code:\n```python\ndef correct_pad(input_size: Optional[int, Tuple], kernel_size: int):\n    \n    \n\n    if isinstance(input_size, int):\n        input_size = (input_size, input_size)\n\n    kernel_size = (kernel_size, kernel_size)\n\n    adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n    return ((correct[0] - adjust[0], correct[0]),\n            (correct[1] - adjust[1], correct[1]))\n```\n\nOriginal documentation:\n```python\nReturns a tuple for zero-padding for 2D convolution with downsampling.\n\n    Arguments:\n      input_c: Input tensor channel.\n      kernel_size: An integer or tuple/list of 2 integers.\n\n    Returns:\n      A tuple.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a tuple for zero-padding for 2D convolution with downsampling.\n\n    Arguments:\n      input_size: Input tensor size.\n      kernel_size: An integer or tuple/list of 2 integers.\n\n    Returns:\n      A tuple.", "file": "tensorflow_classification_Test6_mobilenet_model_v3.py", "function": "correct_pad", "project": "deep-learning-for-image-processing", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(self, stat_name, criticity, commands, repeat, mustache_dict=None):\n        \n        \n        if (self.get(stat_name) == criticity and not repeat) or \\\n           not self.start_timer.finished():\n            # Action already executed => Exit\n            return False\n\n        logger.debug(\"{} action {} for {} ({}) with stats {}\".format(\n            \"Repeat\" if repeat else \"Run\",\n            commands, stat_name, criticity, mustache_dict))\n\n        # Run all actions in background\n        for cmd in commands:\n            # Replace {{arg}} by the dict one (Thk to {Mustache})\n            if chevron_tag:\n                cmd_full = chevron.render(cmd, mustache_dict)\n            else:\n                cmd_full = cmd\n            # Execute the action\n            logger.info(\"Action triggered for {} ({}): {}\".format(stat_name,\n                                                                  criticity,\n                                                                  cmd_full))\n            try:\n                ret = secure_popen(cmd_full)\n            except OSError as e:\n                logger.error(\"Action error for {} ({}): {}\".format(stat_name,\n                                                                   criticity,\n                                                                   e))\n            else:\n                logger.debug(\"Action result for {} ({}): {}\".format(stat_name,\n                                                                    criticity, \n                                                                    ret))\n\n        self.set(stat_name, criticity)\n\n        return True```\n\nUpdated code:\n```python\ndef run(self, stat_name, criticality, commands, repeat, mustache_dict=None):\n        \n        \n        if (self.get(stat_name) == criticality and not repeat) or \\\n           not self.start_timer.finished():\n            # Action already executed => Exit\n            return False\n\n        logger.debug(\"{} action {} for {} ({}) with stats {}\".format(\n            \"Repeat\" if repeat else \"Run\",\n            commands, stat_name, criticality, mustache_dict))\n\n        # Run all actions in background\n        for cmd in commands:\n            # Replace {{arg}} by the dict one (Thk to {Mustache})\n            if chevron_tag:\n                cmd_full = chevron.render(cmd, mustache_dict)\n            else:\n                cmd_full = cmd\n            # Execute the action\n            logger.info(\"Action triggered for {} ({}): {}\".format(stat_name,\n                                                                  criticality,\n                                                                  cmd_full))\n            try:\n                ret = secure_popen(cmd_full)\n            except OSError as e:\n                logger.error(\"Action error for {} ({}): {}\".format(stat_name,\n                                                                   criticality,\n                                                                   e))\n            else:\n                logger.debug(\"Action result for {} ({}): {}\".format(stat_name,\n                                                                    criticality,\n                                                                    ret))\n\n        self.set(stat_name, criticality)\n\n        return True\n```\n\nOriginal documentation:\n```python\nRun the commands (in background).\n\n        - stats_name: plugin_name (+ header)\n        - criticity: criticity of the trigger\n        - commands: a list of command line with optional {{mustache}}\n        - If True, then repeat the action\n        - mustache_dict: Plugin stats (can be use within {{mustache}})\n\n        Return True if the commands have been ran.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run the commands (in background).\n\n        :param stat_name: plugin_name (+ header)\n        :param criticality: criticality of the trigger\n        :param commands: a list of command line with optional {{mustache}}\n        :param repeat: If True, then repeat the action\n        :param  mustache_dict: Plugin stats (can be use within {{mustache}})\n\n        :return: True if the commands have been ran.", "file": "glances_actions.py", "function": "GlancesActions.run", "project": "glances", "change_stats": {"code_changes": 16, "doc_changes": 14, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef tokenize(self, text, never_split=None):\n        \n        \n        \n        # union() returns a new set by concatenating the two sets.\n        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if token not in never_split:\n                if self.do_lower_case:\n                    token = token.lower()\n                    if self.strip_accents is not False:\n                        token = self._run_strip_accents(token)\n                elif self.strip_accents:\n                    token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token, never_split))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens```\n\nUpdated code:\n```python\ndef tokenize(self, text, never_split=None):\n        \n        \n        \n        # union() returns a new set by concatenating the two sets.\n        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        # prevents treating the same character with different unicode codepoints as different characters\n        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n        split_tokens = []\n        for token in orig_tokens:\n            if token not in never_split:\n                if self.do_lower_case:\n                    token = token.lower()\n                    if self.strip_accents is not False:\n                        token = self._run_strip_accents(token)\n                elif self.strip_accents:\n                    token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token, never_split))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n```\n\nOriginal documentation:\n```python\nBasic Tokenization of a piece of text. Split on \"white spaces\" only, for sub-word tokenization, see\n        WordPieceTokenizer.\n\n        Args:\n            never_split (`List[str]`, *optional*)\n                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n\n        Args:\n            never_split (`List[str]`, *optional*)\n                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n                [`PreTrainedTokenizer.tokenize`]) List of token not to split.", "file": "src_transformers_models_tapas_tokenization_tapas.py", "function": "BasicTokenizer.tokenize", "project": "transformers", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, x_input, mask, cache=None):\n        \n        \n        if isinstance(x_input, tuple):\n            x, pos_emb = x_input[0], x_input[1]\n        else:\n            x, pos_emb = x_input, None\n\n        # whether to use macaron style\n        if self.feed_forward_macaron is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_ff_macaron(x)\n            x = residual + self.ff_scale * self.dropout(self.feed_forward_macaron(x))\n            if not self.normalize_before:\n                x = self.norm_ff_macaron(x)\n\n        # multi-headed self-attention module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_mha(x)\n\n        if cache is None:\n            x_q = x\n        else:\n            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n            x_q = x[:, -1:, :]\n            residual = residual[:, -1:, :]\n            mask = None if mask is None else mask[:, -1:, :]\n\n        if pos_emb is not None:\n            x_att = self.self_attn(x_q, x, x, pos_emb, mask)\n        else:\n            x_att = self.self_attn(x_q, x, x, mask)\n\n        if self.concat_after:\n            x_concat = torch.cat((x, x_att), dim=-1)\n            x = residual + self.concat_linear(x_concat)\n        else:\n            x = residual + self.dropout(x_att)\n        if not self.normalize_before:\n            x = self.norm_mha(x)\n\n        # convolution module\n        if self.conv_module is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_conv(x)\n            x = residual + self.dropout(self.conv_module(x))\n            if not self.normalize_before:\n                x = self.norm_conv(x)\n\n        # feed forward module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_ff(x)\n        x = residual + self.ff_scale * self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm_ff(x)\n\n        if self.conv_module is not None:\n            x = self.norm_final(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        if pos_emb is not None:\n            return (x, pos_emb), mask\n\n        return x, mask```\n\nUpdated code:\n```python\ndef forward(self, x_input, mask, cache=None):\n        \n\n        \n        if isinstance(x_input, tuple):\n            x, pos_emb = x_input[0], x_input[1]\n        else:\n            x, pos_emb = x_input, None\n\n        skip_layer = False\n        # with stochastic depth, residual connection `x + f(x)` becomes\n        # `x <- x + 1 / (1 - p) * f(x)` at training time.\n        stoch_layer_coeff = 1.0\n        if self.training and self.stochastic_depth_rate > 0:\n            skip_layer = torch.rand(1).item() < self.stochastic_depth_rate\n            stoch_layer_coeff = 1. / (1 - self.stochastic_depth_rate)\n\n        if not skip_layer:\n            # whether to use macaron style\n            if self.feed_forward_macaron is not None:\n                residual = x\n                if self.normalize_before:\n                    x = self.norm_ff_macaron(x)\n                x = residual + stoch_layer_coeff * self.ff_scale * self.dropout(self.feed_forward_macaron(x))\n                if not self.normalize_before:\n                    x = self.norm_ff_macaron(x)\n\n            # multi-headed self-attention module\n            residual = x\n            if self.normalize_before:\n                x = self.norm_mha(x)\n\n            if cache is None:\n                x_q = x\n            else:\n                assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n                x_q = x[:, -1:, :]\n                residual = residual[:, -1:, :]\n                mask = None if mask is None else mask[:, -1:, :]\n\n            if pos_emb is not None:\n                x_att = self.self_attn(x_q, x, x, pos_emb, mask)\n            else:\n                x_att = self.self_attn(x_q, x, x, mask)\n\n            if self.concat_after:\n                x_concat = torch.cat((x, x_att), dim=-1)\n                x = residual + stoch_layer_coeff * self.concat_linear(x_concat)\n            else:\n                x = residual + stoch_layer_coeff * self.dropout(x_att)\n            if not self.normalize_before:\n                x = self.norm_mha(x)\n\n            # convolution module\n            if self.conv_module is not None:\n                residual = x\n                if self.normalize_before:\n                    x = self.norm_conv(x)\n                x = residual + stoch_layer_coeff * self.dropout(self.conv_module(x))\n                if not self.normalize_before:\n                    x = self.norm_conv(x)\n\n            # feed forward module\n            residual = x\n            if self.normalize_before:\n                x = self.norm_ff(x)\n            x = residual + stoch_layer_coeff * self.ff_scale * self.dropout(self.feed_forward(x))\n            if not self.normalize_before:\n                x = self.norm_ff(x)\n\n            if self.conv_module is not None:\n                x = self.norm_final(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        if pos_emb is not None:\n            return (x, pos_emb), mask\n\n        return x, mask\n```\n\nOriginal documentation:\n```python\nCompute encoded features.\n\n        :param torch.Tensor x_input: encoded source features, w/o pos_emb\n        tuple((batch, max_time_in, size), (1, max_time_in, size))\n        or (batch, max_time_in, size)\n        :param torch.Tensor mask: mask for x (batch, max_time_in)\n        :param torch.Tensor cache: cache for x (batch, max_time_in - 1, size)\n        :rtype: Tuple[torch.Tensor, torch.Tensor]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute encoded features.\n\n        Args:\n            x_input (Union[Tuple, torch.Tensor]): Input tensor w/ or w/o pos emb.\n                - w/ pos emb: Tuple of tensors [(#batch, time, size), (1, time, size)].\n                - w/o pos emb: Tensor (#batch, time, size).\n            mask (torch.Tensor): Mask tensor for the input (#batch, time).\n            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time, size).\n            torch.Tensor: Mask tensor (#batch, time).", "file": "espnet_nets_pytorch_backend_conformer_encoder_layer.py", "function": "EncoderLayer.forward", "project": "espnet", "change_stats": {"code_changes": 102, "doc_changes": 18, "total_changes": 120}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_index(self, columns=None, optlevel=None, kind: Optional[str] = None):\n        \n        \n\n        \n\n        if not self.infer_axes():\n            return\n        if columns is False:\n            return\n\n        # index all indexables and data_columns\n        if columns is None or columns is True:\n            columns = [a.cname for a in self.axes if a.is_data_indexable]\n        if not isinstance(columns, (tuple, list)):\n            columns = [columns]\n\n        kw = dict()\n        if optlevel is not None:\n            kw[\"optlevel\"] = optlevel\n        if kind is not None:\n            kw[\"kind\"] = kind\n\n        table = self.table\n        for c in columns:\n            v = getattr(table.cols, c, None)\n            if v is not None:\n\n                # remove the index if the kind/optlevel have changed\n                if v.is_indexed:\n                    index = v.index\n                    cur_optlevel = index.optlevel\n                    cur_kind = index.kind\n\n                    if kind is not None and cur_kind != kind:\n                        v.remove_index()\n                    else:\n                        kw[\"kind\"] = cur_kind\n\n                    if optlevel is not None and cur_optlevel != optlevel:\n                        v.remove_index()\n                    else:\n                        kw[\"optlevel\"] = cur_optlevel\n\n                # create the index\n                if not v.is_indexed:\n                    if v.type.startswith(\"complex\"):\n                        raise TypeError(\n                            \"Columns containing complex values can be stored \"\n                            \"but cannot\"\n                            \" be indexed when using table format. Either use \"\n                            \"fixed format, set index=False, or do not include \"\n                            \"the columns containing complex values to \"\n                            \"data_columns when initializing the table.\"\n                        )\n                    v.create_index(**kw)```\n\nUpdated code:\n```python\ndef create_index(self, columns=None, optlevel=None, kind: Optional[str] = None):\n        \n        \n        \n\n        if not self.infer_axes():\n            return\n        if columns is False:\n            return\n\n        # index all indexables and data_columns\n        if columns is None or columns is True:\n            columns = [a.cname for a in self.axes if a.is_data_indexable]\n        if not isinstance(columns, (tuple, list)):\n            columns = [columns]\n\n        kw = dict()\n        if optlevel is not None:\n            kw[\"optlevel\"] = optlevel\n        if kind is not None:\n            kw[\"kind\"] = kind\n\n        table = self.table\n        for c in columns:\n            v = getattr(table.cols, c, None)\n            if v is not None:\n\n                # remove the index if the kind/optlevel have changed\n                if v.is_indexed:\n                    index = v.index\n                    cur_optlevel = index.optlevel\n                    cur_kind = index.kind\n\n                    if kind is not None and cur_kind != kind:\n                        v.remove_index()\n                    else:\n                        kw[\"kind\"] = cur_kind\n\n                    if optlevel is not None and cur_optlevel != optlevel:\n                        v.remove_index()\n                    else:\n                        kw[\"optlevel\"] = cur_optlevel\n\n                # create the index\n                if not v.is_indexed:\n                    if v.type.startswith(\"complex\"):\n                        raise TypeError(\n                            \"Columns containing complex values can be stored \"\n                            \"but cannot\"\n                            \" be indexed when using table format. Either use \"\n                            \"fixed format, set index=False, or do not include \"\n                            \"the columns containing complex values to \"\n                            \"data_columns when initializing the table.\"\n                        )\n                    v.create_index(**kw)\n```\n\nOriginal documentation:\n```python\nCreate a pytables index on the specified columns\n          note: cannot index Time64Col() or ComplexCol currently;\n          PyTables must be >= 3.0\n\n        Parameters\n        ----------\n        columns : None, bool, or listlike[str]\n            Indicate which columns to create an index on.\n\n            * False : Do not create any indexes.\n            * True : Create indexes on all columns.\n            * None : Create indexes on all columns.\n            * listlike : Create indexes on the given columns.\n\n        optlevel : int or None, default None\n            Optimization level, if None, pytables defaults to 6.\n        kind : str or None, default None\n            Kind of index, if None, pytables defaults to \"medium\"\n\n        Raises\n        ------\n        raises if the node is not a table\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a pytables index on the specified columns.\n\n        Parameters\n        ----------\n        columns : None, bool, or listlike[str]\n            Indicate which columns to create an index on.\n\n            * False : Do not create any indexes.\n            * True : Create indexes on all columns.\n            * None : Create indexes on all columns.\n            * listlike : Create indexes on the given columns.\n\n        optlevel : int or None, default None\n            Optimization level, if None, pytables defaults to 6.\n        kind : str or None, default None\n            Kind of index, if None, pytables defaults to \"medium\".\n\n        Raises\n        ------\n        TypeError if trying to create an index on a complex-type column.\n\n        Notes\n        -----\n        Cannot index Time64Col or ComplexCol.\n        Pytables must be >= 3.0.", "file": "pandas_io_pytables.py", "function": "Table.create_index", "project": "pandas", "change_stats": {"code_changes": 3, "doc_changes": 15, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef filter(\n        self,\n        function: Optional[Callable] = None,\n        with_indices=False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: Optional[int] = 1000,\n        keep_in_memory: bool = False,\n        load_from_cache_file: bool = True,\n        cache_file_name: Optional[str] = None,\n        writer_batch_size: Optional[int] = 1000,\n        fn_kwargs: Optional[dict] = None,\n        num_proc: Optional[int] = None,\n        suffix_template: str = \"_{rank:05d}_of_{num_proc:05d}\",\n        new_fingerprint: Optional[str] = None,\n        desc: Optional[str] = None,\n    ) -> \"Dataset\":\n        \n        \n        if len(self.list_indexes()) > 0:\n            raise DatasetTransformationNotAllowedError(\n                \"Using `.filter` on a dataset with attached indexes is not allowed. You can first run `.drop_index() to remove your index and then re-add it.`\"\n            )\n\n        if function is None:\n            function = lambda x: True  # noqa: E731\n\n        if len(self) == 0:\n            return self\n\n        indices = self.map(\n            function=partial(\n                get_indices_from_mask_function, function, batched, with_indices, input_columns, self._indices\n            ),\n            with_indices=True,\n            features=Features({\"indices\": Value(\"uint64\")}),\n            batched=True,\n            batch_size=batch_size,\n            remove_columns=self.column_names,\n            keep_in_memory=keep_in_memory,\n            load_from_cache_file=load_from_cache_file,\n            cache_file_name=cache_file_name,\n            writer_batch_size=writer_batch_size,\n            fn_kwargs=fn_kwargs,\n            num_proc=num_proc,\n            suffix_template=suffix_template,\n            new_fingerprint=new_fingerprint,\n            input_columns=input_columns,\n            desc=desc,\n        )\n        new_dataset = copy.deepcopy(self)\n        new_dataset._indices = indices.data\n        new_dataset._fingerprint = new_fingerprint\n        return new_dataset```\n\nUpdated code:\n```python\ndef filter(\n        self,\n        function: Optional[Callable] = None,\n        with_indices=False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: Optional[int] = 1000,\n        keep_in_memory: bool = False,\n        load_from_cache_file: Optional[bool] = None,\n        cache_file_name: Optional[str] = None,\n        writer_batch_size: Optional[int] = 1000,\n        fn_kwargs: Optional[dict] = None,\n        num_proc: Optional[int] = None,\n        suffix_template: str = \"_{rank:05d}_of_{num_proc:05d}\",\n        new_fingerprint: Optional[str] = None,\n        desc: Optional[str] = None,\n    ) -> \"Dataset\":\n        \n        \n        if len(self.list_indexes()) > 0:\n            raise DatasetTransformationNotAllowedError(\n                \"Using `.filter` on a dataset with attached indexes is not allowed. You can first run `.drop_index() to remove your index and then re-add it.`\"\n            )\n\n        if function is None:\n            function = lambda x: True  # noqa: E731\n\n        if len(self) == 0:\n            return self\n\n        indices = self.map(\n            function=partial(\n                get_indices_from_mask_function, function, batched, with_indices, input_columns, self._indices\n            ),\n            with_indices=True,\n            features=Features({\"indices\": Value(\"uint64\")}),\n            batched=True,\n            batch_size=batch_size,\n            remove_columns=self.column_names,\n            keep_in_memory=keep_in_memory,\n            load_from_cache_file=load_from_cache_file,\n            cache_file_name=cache_file_name,\n            writer_batch_size=writer_batch_size,\n            fn_kwargs=fn_kwargs,\n            num_proc=num_proc,\n            suffix_template=suffix_template,\n            new_fingerprint=new_fingerprint,\n            input_columns=input_columns,\n            desc=desc,\n        )\n        new_dataset = copy.deepcopy(self)\n        new_dataset._indices = indices.data\n        new_dataset._fingerprint = new_fingerprint\n        return new_dataset\n```\n\nOriginal documentation:\n```python\nApply a filter function to all the elements in the table in batches\n        and update the table so that the dataset only includes examples according to the filter function.\n\n        Args:\n            function (`Callable`): Callable with one of the following signatures:\n\n                - `function(example: Dict[str, Any]) -> bool` if `with_indices=False, batched=False`\n                - `function(example: Dict[str, Any], indices: int) -> bool` if `with_indices=True, batched=False`\n                - `function(example: Dict[str, List]) -> List[bool]` if `with_indices=False, batched=True`\n                - `function(example: Dict[str, List], indices: List[int]) -> List[bool]` if `with_indices=True, batched=True`\n\n                If no function is provided, defaults to an always `True` function: `lambda x: True`.\n            with_indices (`bool`, defaults to `False`):\n                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n            input_columns (`str` or `List[str]`, *optional*):\n                The columns to be passed into `function` as\n                positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n            batched (`bool`, defaults to `False`):\n                Provide batch of examples to `function`.\n            batch_size (`int`, *optional*, defaults to `1000`):\n                Number of examples per batch provided to `function` if\n                `batched = True`. If `batched = False`, one example per batch is passed to `function`.\n                If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n            keep_in_memory (`bool`, defaults to `False`):\n                Keep the dataset in memory instead of writing it to a cache file.\n            load_from_cache_file (`bool`, defaults to `True`):\n                If a cache file storing the current computation from `function`\n                can be identified, use it instead of recomputing.\n            cache_file_name (`str`, *optional*):\n                Provide the name of a path for the cache file. It is used to store the\n                results of the computation instead of the automatically generated cache file name.\n            writer_batch_size (`int`, defaults to `1000`):\n                Number of rows per write operation for the cache file writer.\n                This value is a good trade-off between memory usage during the processing, and processing speed.\n                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n            fn_kwargs (`dict`, *optional*):\n                Keyword arguments to be passed to `function`.\n            num_proc (`int`, *optional*):\n                Number of processes for multiprocessing. By default it doesn't\n                use multiprocessing.\n            suffix_template (`str`):\n                If `cache_file_name` is specified, then this suffix will be added at the end of the base name of each.\n                For example, if `cache_file_name` is `\"processed.arrow\"`, then for `rank = 1` and `num_proc = 4`,\n                the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix (default\n                `_{rank:05d}_of_{num_proc:05d}`).\n            new_fingerprint (`str`, *optional*):\n                The new fingerprint of the dataset after transform.\n                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n            desc (`str`, *optional*, defaults to `None`):\n                Meaningful description to be displayed alongside with the progress bar while filtering examples.\n\n        Example:\n\n        ```py\n        >>> from datasets import load_dataset\n        >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n        >>> ds.filter(lambda x: x[\"label\"] == 1)\n        Dataset({\n            features: ['text', 'label'],\n            num_rows: 533\n        })\n        ```\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Apply a filter function to all the elements in the table in batches\n        and update the table so that the dataset only includes examples according to the filter function.\n\n        Args:\n            function (`Callable`): Callable with one of the following signatures:\n\n                - `function(example: Dict[str, Any]) -> bool` if `with_indices=False, batched=False`\n                - `function(example: Dict[str, Any], indices: int) -> bool` if `with_indices=True, batched=False`\n                - `function(example: Dict[str, List]) -> List[bool]` if `with_indices=False, batched=True`\n                - `function(example: Dict[str, List], indices: List[int]) -> List[bool]` if `with_indices=True, batched=True`\n\n                If no function is provided, defaults to an always `True` function: `lambda x: True`.\n            with_indices (`bool`, defaults to `False`):\n                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n            input_columns (`str` or `List[str]`, *optional*):\n                The columns to be passed into `function` as\n                positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n            batched (`bool`, defaults to `False`):\n                Provide batch of examples to `function`.\n            batch_size (`int`, *optional*, defaults to `1000`):\n                Number of examples per batch provided to `function` if\n                `batched = True`. If `batched = False`, one example per batch is passed to `function`.\n                If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n            keep_in_memory (`bool`, defaults to `False`):\n                Keep the dataset in memory instead of writing it to a cache file.\n            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n                If a cache file storing the current computation from `function`\n                can be identified, use it instead of recomputing.\n            cache_file_name (`str`, *optional*):\n                Provide the name of a path for the cache file. It is used to store the\n                results of the computation instead of the automatically generated cache file name.\n            writer_batch_size (`int`, defaults to `1000`):\n                Number of rows per write operation for the cache file writer.\n                This value is a good trade-off between memory usage during the processing, and processing speed.\n                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n            fn_kwargs (`dict`, *optional*):\n                Keyword arguments to be passed to `function`.\n            num_proc (`int`, *optional*):\n                Number of processes for multiprocessing. By default it doesn't\n                use multiprocessing.\n            suffix_template (`str`):\n                If `cache_file_name` is specified, then this suffix will be added at the end of the base name of each.\n                For example, if `cache_file_name` is `\"processed.arrow\"`, then for `rank = 1` and `num_proc = 4`,\n                the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix (default\n                `_{rank:05d}_of_{num_proc:05d}`).\n            new_fingerprint (`str`, *optional*):\n                The new fingerprint of the dataset after transform.\n                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n            desc (`str`, *optional*, defaults to `None`):\n                Meaningful description to be displayed alongside with the progress bar while filtering examples.\n\n        Example:\n\n        ```py\n        >>> from datasets import load_dataset\n        >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n        >>> ds.filter(lambda x: x[\"label\"] == 1)\n        Dataset({\n            features: ['text', 'label'],\n            num_rows: 533\n        })\n        ```", "file": "src_datasets_arrow_dataset.py", "function": "Dataset.filter", "project": "datasets", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef assert_df(self, dataframe: DataFrame, df_len: int, df_close: float, df_date: datetime):\n          \n        message = \"\"\n        if df_len != len(dataframe):\n            message = \"length\"\n        elif df_close != dataframe[\"close\"].iloc[-1]:\n            message = \"last close price\"\n        elif df_date != dataframe[\"date\"].iloc[-1]:\n            message = \"last date\"\n        if message:\n            if self.disable_dataframe_checks:\n                logger.warning(f\"Dataframe returned from strategy has mismatching {message}.\")\n            else:\n                raise StrategyError(f\"Dataframe returned from strategy has mismatching {message}.\")```\n\nUpdated code:\n```python\ndef assert_df(self, dataframe: DataFrame, df_len: int, df_close: float, df_date: datetime):\n        \n        \n        \n        message = \"\"\n        if df_len != len(dataframe):\n            message = \"length\"\n        elif df_close != dataframe[\"close\"].iloc[-1]:\n            message = \"last close price\"\n        elif df_date != dataframe[\"date\"].iloc[-1]:\n            message = \"last date\"\n        if message:\n            if self.disable_dataframe_checks:\n                logger.warning(f\"Dataframe returned from strategy has mismatching {message}.\")\n            else:\n                raise StrategyError(f\"Dataframe returned from strategy has mismatching {message}.\")\n```\n\nOriginal documentation:\n```python\nmake sure data is unmodified\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure dataframe (length, last candle) was not modified, and has all elements we need.", "file": "freqtrade_strategy_interface.py", "function": "IStrategy.assert_df", "project": "freqtrade", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef new(self, genre, save=False, upload=False, info=''):\n        \n        \n        \n        return DropImage(stat=self, genre=genre, save=save, upload=upload, info=info)```\n\nUpdated code:\n```python\ndef new(self, genre, method='do_not', info=''):\n        \n        \n        \n        save = 'save' in method\n        upload = 'upload' in method\n        return DropImage(stat=self, genre=genre, save=save, upload=upload, info=info)\n```\n\nOriginal documentation:\n```python\nArgs:\n            genre (str):\n            save (bool): If save image to local file system.\n            upload (bool): If upload image to Azur Stats.\n            info (str): Extra info append to filename.\n\n        Returns:\n            DropImage:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            genre (str):\n            method (str): The method about save and upload image.\n            info (str): Extra info append to filename.\n\n        Returns:\n            DropImage:", "file": "module_statistics_azurstats.py", "function": "AzurStats.new", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,\n               filter_size,\n               output_size,\n               num_units,\n               is_training,\n               forget_bias=1.0,\n               activation=tf.tanh,\n               use_batch_norm=False,\n               flatten_state=False,\n               groups=4,\n               clip_state=False,\n               scale_state=False,\n               output_bottleneck=False,\n               pre_bottleneck=False,\n               is_quantized=False,\n               visualize_gates=False):\n    \n    \n    if clip_state and scale_state:\n      raise ValueError('clip_state and scale_state cannot both be enabled.')\n\n    self._filter_size = list(filter_size)\n    self._output_size = list(output_size)\n    self._num_units = num_units\n    self._is_training = is_training\n    self._forget_bias = forget_bias\n    self._activation = activation\n    self._use_batch_norm = use_batch_norm\n    self._viz_gates = visualize_gates\n    self._flatten_state = flatten_state\n    self._param_count = self._num_units\n    self._groups = groups\n    self._scale_state = scale_state\n    self._clip_state = clip_state\n    self._output_bottleneck = output_bottleneck\n    self._pre_bottleneck = pre_bottleneck\n    self._is_quantized = is_quantized\n    for dim in self._output_size:\n      self._param_count *= dim```\n\nUpdated code:\n```python\ndef __init__(self,\n               filter_size,\n               output_size,\n               num_units,\n               is_training,\n               forget_bias=1.0,\n               activation=tf.tanh,\n               use_batch_norm=False,\n               flatten_state=False,\n               groups=4,\n               clip_state=False,\n               scale_state=False,\n               output_bottleneck=False,\n               pre_bottleneck=False,\n               is_quantized=False,\n               visualize_gates=False,\n               conv_op_overrides=None):\n    \n    \n    if clip_state and scale_state:\n      raise ValueError('clip_state and scale_state cannot both be enabled.')\n\n    self._filter_size = list(filter_size)\n    self._output_size = list(output_size)\n    self._num_units = num_units\n    self._is_training = is_training\n    self._forget_bias = forget_bias\n    self._activation = activation\n    self._use_batch_norm = use_batch_norm\n    self._viz_gates = visualize_gates\n    self._flatten_state = flatten_state\n    self._param_count = self._num_units\n    self._groups = groups\n    self._scale_state = scale_state\n    self._clip_state = clip_state\n    self._output_bottleneck = output_bottleneck\n    self._pre_bottleneck = pre_bottleneck\n    self._is_quantized = is_quantized\n    for dim in self._output_size:\n      self._param_count *= dim\n    self._conv_op_overrides = conv_op_overrides\n    if self._conv_op_overrides and len(self._conv_op_overrides) != 2:\n      raise ValueError('Bottleneck and Convolutional layer should be overriden'\n                       'together')\n```\n\nOriginal documentation:\n```python\nInitialize the basic LSTM cell.\n\n    Args:\n      filter_size: collection, conv filter size\n      output_size: collection, the width/height dimensions of the cell/output\n      num_units: int, The number of channels in the LSTM cell.\n      is_training: Whether the LSTM is in training mode.\n      forget_bias: float, The bias added to forget gates (see above).\n      activation: Activation function of the inner states.\n      use_batch_norm: if True, use batch norm after convolution\n      flatten_state: if True, state tensor will be flattened and stored as a 2-d\n        tensor. Use for exporting the model to tfmini\n      groups: Number of groups to split the state into. Must evenly divide\n        num_units.\n      clip_state: if True, clips state between [-6, 6].\n      scale_state: if True, scales state so that all values are under 6 at all\n        times.\n      output_bottleneck: if True, the cell bottleneck will be concatenated to\n        the cell output.\n      pre_bottleneck: if True, cell assumes that bottlenecking was performing\n        before the function was called.\n      is_quantized: if True, the model is in quantize mode, which requires\n        quantization friendly concat and separable_conv2d ops.\n      visualize_gates: if True, add histogram summaries of all gates and outputs\n        to tensorboard\n\n    Raises:\n      ValueError: when both clip_state and scale_state are enabled.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the basic LSTM cell.\n\n    Args:\n      filter_size: collection, conv filter size\n      output_size: collection, the width/height dimensions of the cell/output\n      num_units: int, The number of channels in the LSTM cell.\n      is_training: Whether the LSTM is in training mode.\n      forget_bias: float, The bias added to forget gates (see above).\n      activation: Activation function of the inner states.\n      use_batch_norm: if True, use batch norm after convolution\n      flatten_state: if True, state tensor will be flattened and stored as a 2-d\n        tensor. Use for exporting the model to tfmini\n      groups: Number of groups to split the state into. Must evenly divide\n        num_units.\n      clip_state: if True, clips state between [-6, 6].\n      scale_state: if True, scales state so that all values are under 6 at all\n        times.\n      output_bottleneck: if True, the cell bottleneck will be concatenated to\n        the cell output.\n      pre_bottleneck: if True, cell assumes that bottlenecking was performing\n        before the function was called.\n      is_quantized: if True, the model is in quantize mode, which requires\n        quantization friendly concat and separable_conv2d ops.\n      visualize_gates: if True, add histogram summaries of all gates and outputs\n        to tensorboard\n      conv_op_overrides: A list of convolutional operations that override the\n        'bottleneck' and 'convolution' layers before lstm gates. If None, the\n        original implementation of seperable_conv will be used. The length of\n        the list should be two.\n\n    Raises:\n      ValueError: when both clip_state and scale_state are enabled.", "file": "research_lstm_object_detection_lstm_lstm_cells.py", "function": "GroupedConvLSTMCell.__init__", "project": "models", "change_stats": {"code_changes": 9, "doc_changes": 6, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mock_state_change_event(\n    hass: HomeAssistant, new_state: State, old_state: State | None = None\n) -> None:\n    \n    event_data = {\"entity_id\": new_state.entity_id, \"new_state\": new_state}\n\n    if old_state:\n        event_data[\"old_state\"] = old_state\n\n    hass.bus.fire(EVENT_STATE_CHANGED, event_data, context=new_state.context)```\n\nUpdated code:\n```python\ndef mock_state_change_event(\n    hass: HomeAssistant, new_state: State, old_state: State | None = None\n) -> None:\n    \n    event_data = {\n        \"entity_id\": new_state.entity_id,\n        \"new_state\": new_state,\n        \"old_state\": old_state,\n    }\n    hass.bus.fire(EVENT_STATE_CHANGED, event_data, context=new_state.context)\n```\n\nOriginal documentation:\n```python\nMock state change envent.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Mock state change event.", "file": "tests_common.py", "function": "mock_state_change_event", "project": "core", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fixed_batch_normalization(x, gamma, beta, mean, var, eps=1e-5):\n    \n\n    \n    return BatchNormalizationFunction(eps)(x, gamma, beta, mean, var)```\n\nUpdated code:\n```python\ndef fixed_batch_normalization(x, gamma, beta, fixed_mean, fixed_var, eps=2e-5):\n    \n\n    \n    return BatchNormalizationFunction(fixed_mean, fixed_var, False, 0.0,\n                                      eps)(x, gamma, beta)\n```\n\nOriginal documentation:\n```python\nBatch normalization function with fixed statistics.\n\n    This is a variant of batch normalization, where the mean and variance\n    statistics are given by the caller as variables. This is used on testing\n    mode of the batch normalization layer, where batch statistics cannot be\n    used for prediction consistency.\n\n    Args:\n        x (Variable): The input variable.\n        gamma (Variable): The scaling parameter of normalized data.\n        beta (Variable): The shifting parameter of scaled normalized data.\n        mean (Variable): The shifting parameter of input.\n        var (Variable): The square of scaling parameter of input.\n        eps (float): Epsilon value for numerical stability.\n\n    .. seealso::\n       :func:`functions.batch_normalization`,\n       :class:`links.BatchNormalization`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Batch normalization function with fixed statistics.\n\n    This is a variant of batch normalization, where the mean and variance\n    statistics are given by the caller as fixed parameters. This is\n    used on testing mode of the batch normalization layer, where batch\n    statistics cannot be used for prediction consistency.\n\n    Args:\n        x (Variable): The input variable.\n        gamma (Variable): The scaling parameter of normalized data.\n        beta (Variable): The shifting parameter of scaled normalized data.\n        fixed_mean (array): The shifting parameter of input.\n        fixed_var (array): The square of scaling parameter of input.\n        eps (float): Epsilon value for numerical stability.\n\n    .. seealso::\n       :func:`functions.batch_normalization`,\n       :class:`links.BatchNormalization`", "file": "chainer_", "function": "fixed_batch_normalization", "project": "chainer", "change_stats": {"code_changes": 7, "doc_changes": 12, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convolution_2d(x, W, b=None, stride=1, pad=0, cover_all=False, **kwargs):\n    \n\n      # NOQA\n    argument.check_unexpected_kwargs(\n        kwargs, deterministic=\"deterministic argument is not \"\n        \"supported anymore. \"\n        \"Use chainer.using_config('cudnn_deterministic', value) \"\n        \"context where value is either `True` or `False`.\")\n    dilate, groups = argument.parse_kwargs(kwargs,\n                                           ('dilate', 1), ('groups', 1))\n\n    fnode = Convolution2DFunction(stride, pad, cover_all, dilate=dilate,\n                                  groups=groups)\n    if b is None:\n        args = x, W\n    else:\n        args = x, W, b\n    y, = fnode.apply(args)\n    return y```\n\nUpdated code:\n```python\ndef convolution_2d(x, W, b=None, stride=1, pad=0, cover_all=False, **kwargs):\n    \n\n      # NOQA\n    dilate, groups = argument.parse_kwargs(\n        kwargs, ('dilate', 1), ('groups', 1),\n        deterministic=\"deterministic argument is not supported anymore. \"\n        \"Use chainer.using_config('cudnn_deterministic', value) \"\n        \"context where value is either `True` or `False`.\")\n\n    fnode = Convolution2DFunction(stride, pad, cover_all, dilate=dilate,\n                                  groups=groups)\n    if b is None:\n        args = x, W\n    else:\n        args = x, W, b\n    y, = fnode.apply(args)\n    return y\n```\n\nOriginal documentation:\n```python\nconvolution_2d(x, W, b=None, stride=1, pad=0, cover_all=False, *, dilate=1, groups=1)\n\n    Two-dimensional convolution function.\n\n    This is an implementation of two-dimensional convolution in ConvNets.\n    It takes three variables: the input image ``x``, the filter weight ``W``,\n    and the bias vector ``b``.\n\n    Notation: here is a notation for dimensionalities.\n\n    - :math:`n` is the batch size.\n    - :math:`c_I` and :math:`c_O` are the number of the input and output\n      channels, respectively.\n    - :math:`h_I` and :math:`w_I` are the height and width of the input image,\n      respectively.\n    - :math:`h_K` and :math:`w_K` are the height and width of the filters,\n      respectively.\n    - :math:`h_P` and :math:`w_P` are the height and width of the spatial\n      padding size, respectively.\n\n    Then the ``Convolution2D`` function computes correlations between filters\n    and patches of size :math:`(h_K, w_K)` in ``x``.\n    Note that correlation here is equivalent to the inner product between\n    expanded vectors.\n    Patches are extracted at positions shifted by multiples of ``stride`` from\n    the first position ``(-h_P, -w_P)`` for each spatial axis.\n    The right-most (or bottom-most) patches do not run over the padded spatial\n    size.\n\n    Let :math:`(s_Y, s_X)` be the stride of filter application. Then, the\n    output size :math:`(h_O, w_O)` is determined by the following equations:\n\n    .. math::\n\n       h_O &= (h_I + 2h_P - h_K) / s_Y + 1,\\\\\\\\\n       w_O &= (w_I + 2w_P - w_K) / s_X + 1.\n\n    If ``cover_all`` option is ``True``, the filter will cover the all\n    spatial locations. So, if the last stride of filter does not cover the\n    end of spatial locations, an addtional stride will be applied to the end\n    part of spatial locations. In this case, the output size :math:`(h_O, w_O)`\n    is determined by the following equations:\n\n    .. math::\n\n       h_O &= (h_I + 2h_P - h_K + s_Y - 1) / s_Y + 1,\\\\\\\\\n       w_O &= (w_I + 2w_P - w_K + s_X - 1) / s_X + 1.\n\n    If the bias vector is given, then it is added to all spatial locations of\n    the output of convolution.\n\n    The output of this function can be non-deterministic when it uses cuDNN.\n    If ``chainer.configuration.config.cudnn_deterministic`` is ``True`` and\n    cuDNN version is >= v3, it forces cuDNN to use a deterministic algorithm.\n\n    Convolution links can use a feature of cuDNN called autotuning, which\n    selects the most efficient CNN algorithm for images of fixed-size,\n    can provide a significant performance boost for fixed neural nets.\n    To enable, set `chainer.using_config('autotune', True)`\n\n    When the dilation factor is greater than one, cuDNN is not used unless\n    the version is 6.0 or higher.\n\n    .. warning::\n\n        ``deterministic`` argument is not supported anymore since v2.\n        Instead, use ``chainer.using_config('cudnn_deterministic', value)``\n        (value is either ``True`` or ``False``).\n        See :func:`chainer.using_config`.\n\n    Args:\n        x (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`):\n            Input variable of shape :math:`(n, c_I, h_I, w_I)`.\n        W (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`):\n            Weight variable of shape :math:`(c_O, c_I, h_K, w_K)`.\n        b (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`): Bias variable of length :math:`c_O` (optional).\n        stride (:class:`int` or pair of :class:`int` s):\n            Stride of filter applications. ``stride=s`` and ``stride=(s, s)``\n            are equivalent.\n        pad (:class:`int` or pair of :class:`int` s):\n            Spatial padding width for input arrays.\n            ``pad=p`` and ``pad=(p, p)`` are equivalent.\n        cover_all (:class:`bool`):\n            If ``True``, all spatial locations are convoluted into some output\n            pixels.\n        dilate (:class:`int` or pair of :class:`int` s):\n            Dilation factor of filter applications.\n            ``dilate=d`` and ``dilate=(d, d)`` are equivalent.\n        groups (:class:`int`):\n            The number of groups to use grouped convolution.\n            The default is one, where grouped convolution is not used.\n\n    Returns:\n        ~chainer.Variable:\n            Output variable of shape :math:`(n, c_O, h_O, w_O)`.\n\n    .. seealso:: :class:`~chainer.links.Convolution2D`\n\n    .. admonition:: Example\n\n        >>> n = 10\n        >>> c_i, c_o = 3, 1\n        >>> h_i, w_i = 30, 40\n        >>> h_k, w_k = 10, 10\n        >>> h_p, w_p = 5, 5\n        >>> x = np.random.uniform(0, 1, (n, c_i, h_i, w_i)).astype(np.float32)\n        >>> x.shape\n        (10, 3, 30, 40)\n        >>> W = np.random.uniform(0, 1, (c_o, c_i, h_k, w_k)).\\\nastype(np.float32)\n        >>> W.shape\n        (1, 3, 10, 10)\n        >>> b = np.random.uniform(0, 1, (c_o,)).astype(np.float32)\n        >>> b.shape\n        (1,)\n        >>> s_y, s_x = 5, 7\n        >>> y = F.convolution_2d(x, W, b, stride=(s_y, s_x), pad=(h_p, w_p))\n        >>> y.shape\n        (10, 1, 7, 6)\n        >>> h_o = int((h_i + 2 * h_p - h_k) / s_y + 1)\n        >>> w_o = int((w_i + 2 * w_p - w_k) / s_x + 1)\n        >>> y.shape == (n, c_o, h_o, w_o)\n        True\n        >>> y = F.convolution_2d(x, W, b, stride=(s_y, s_x), pad=(h_p, w_p), \\\ncover_all=True)\n        >>> y.shape == (n, c_o, h_o, w_o + 1)\n        True\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "convolution_2d(x, W, b=None, stride=1, pad=0, cover_all=False, *, dilate=1, groups=1)\n\n    Two-dimensional convolution function.\n\n    This is an implementation of two-dimensional convolution in ConvNets.\n    It takes three variables: the input image ``x``, the filter weight ``W``,\n    and the bias vector ``b``.\n\n    Notation: here is a notation for dimensionalities.\n\n    - :math:`n` is the batch size.\n    - :math:`c_I` and :math:`c_O` are the number of the input and output\n      channels, respectively.\n    - :math:`h_I` and :math:`w_I` are the height and width of the input image,\n      respectively.\n    - :math:`h_K` and :math:`w_K` are the height and width of the filters,\n      respectively.\n    - :math:`h_P` and :math:`w_P` are the height and width of the spatial\n      padding size, respectively.\n\n    Then the ``Convolution2D`` function computes correlations between filters\n    and patches of size :math:`(h_K, w_K)` in ``x``.\n    Note that correlation here is equivalent to the inner product between\n    expanded vectors.\n    Patches are extracted at positions shifted by multiples of ``stride`` from\n    the first position ``(-h_P, -w_P)`` for each spatial axis.\n    The right-most (or bottom-most) patches do not run over the padded spatial\n    size.\n\n    Let :math:`(s_Y, s_X)` be the stride of filter application. Then, the\n    output size :math:`(h_O, w_O)` is determined by the following equations:\n\n    .. math::\n\n       h_O &= (h_I + 2h_P - h_K) / s_Y + 1,\\\\\\\\\n       w_O &= (w_I + 2w_P - w_K) / s_X + 1.\n\n    If ``cover_all`` option is ``True``, the filter will cover the all\n    spatial locations. So, if the last stride of filter does not cover the\n    end of spatial locations, an addtional stride will be applied to the end\n    part of spatial locations. In this case, the output size :math:`(h_O, w_O)`\n    is determined by the following equations:\n\n    .. math::\n\n       h_O &= (h_I + 2h_P - h_K + s_Y - 1) / s_Y + 1,\\\\\\\\\n       w_O &= (w_I + 2w_P - w_K + s_X - 1) / s_X + 1.\n\n    If the bias vector is given, then it is added to all spatial locations of\n    the output of convolution.\n\n    The output of this function can be non-deterministic when it uses cuDNN.\n    If ``chainer.configuration.config.cudnn_deterministic`` is ``True`` and\n    cuDNN version is >= v3, it forces cuDNN to use a deterministic algorithm.\n\n    Convolution links can use a feature of cuDNN called autotuning, which\n    selects the most efficient CNN algorithm for images of fixed-size,\n    can provide a significant performance boost for fixed neural nets.\n    To enable, set `chainer.using_config('autotune', True)`\n\n    When the dilation factor is greater than one, cuDNN is not used unless\n    the version is 6.0 or higher.\n\n    .. warning::\n\n        ``deterministic`` argument is not supported anymore since v2.\n        Instead, use ``chainer.using_config('cudnn_deterministic', value)``\n        (value is either ``True`` or ``False``).\n        See :func:`chainer.using_config`.\n\n    Args:\n        x (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`):\n            Input variable of shape :math:`(n, c_I, h_I, w_I)`.\n        W (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`):\n            Weight variable of shape :math:`(c_O, c_I, h_K, w_K)`.\n        b (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n        :class:`cupy.ndarray`): Bias variable of length :math:`c_O` (optional).\n        stride (:class:`int` or pair of :class:`int` s):\n            Stride of filter applications. ``stride=s`` and ``stride=(s, s)``\n            are equivalent.\n        pad (:class:`int` or pair of :class:`int` s):\n            Spatial padding width for input arrays.\n            ``pad=p`` and ``pad=(p, p)`` are equivalent.\n        cover_all (:class:`bool`):\n            If ``True``, all spatial locations are convoluted into some output\n            pixels.\n        dilate (:class:`int` or pair of :class:`int` s):\n            Dilation factor of filter applications.\n            ``dilate=d`` and ``dilate=(d, d)`` are equivalent.\n        groups (:class:`int`): Number of groups of channels. If the number\n            is greater than 1, input tensor :math:`W` is divided into some\n            blocks by this value. For each tensor blocks, convolution\n            operation will be executed independently. Input channel size\n            :math:`c_I` and output channel size :math:`c_O` must be exactly\n            divisible by this value.\n\n    Returns:\n        ~chainer.Variable:\n            Output variable of shape :math:`(n, c_O, h_O, w_O)`.\n\n    .. seealso:: :class:`~chainer.links.Convolution2D`\n\n    .. admonition:: Example\n\n        >>> n = 10\n        >>> c_i, c_o = 3, 1\n        >>> h_i, w_i = 30, 40\n        >>> h_k, w_k = 10, 10\n        >>> h_p, w_p = 5, 5\n        >>> x = np.random.uniform(0, 1, (n, c_i, h_i, w_i)).astype(np.float32)\n        >>> x.shape\n        (10, 3, 30, 40)\n        >>> W = np.random.uniform(0, 1, (c_o, c_i, h_k, w_k)).\\\nastype(np.float32)\n        >>> W.shape\n        (1, 3, 10, 10)\n        >>> b = np.random.uniform(0, 1, (c_o,)).astype(np.float32)\n        >>> b.shape\n        (1,)\n        >>> s_y, s_x = 5, 7\n        >>> y = F.convolution_2d(x, W, b, stride=(s_y, s_x), pad=(h_p, w_p))\n        >>> y.shape\n        (10, 1, 7, 6)\n        >>> h_o = int((h_i + 2 * h_p - h_k) / s_y + 1)\n        >>> w_o = int((w_i + 2 * w_p - w_k) / s_x + 1)\n        >>> y.shape == (n, c_o, h_o, w_o)\n        True\n        >>> y = F.convolution_2d(x, W, b, stride=(s_y, s_x), pad=(h_p, w_p), \\\ncover_all=True)\n        >>> y.shape == (n, c_o, h_o, w_o + 1)\n        True", "file": "chainer_", "function": "convolution_2d", "project": "chainer", "change_stats": {"code_changes": 10, "doc_changes": 11, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _convert_string_array(data, encoding, itemsize=None):\n    \n    \n    \n\n    # encode if needed\n    if encoding is not None and len(data):\n        data = Series(data.ravel()).str.encode(\n            encoding).values.reshape(data.shape)\n\n    # create the sized dtype\n    if itemsize is None:\n        ensured = _ensure_object(data.ravel())\n        itemsize = libwriters.max_len_string_array(ensured)\n\n    data = np.asarray(data, dtype=\"S%d\" % itemsize)\n    return data```\n\nUpdated code:\n```python\ndef _convert_string_array(data, encoding, errors, itemsize=None):\n    \n    \n    \n\n    # encode if needed\n    if encoding is not None and len(data):\n        data = Series(data.ravel()).str.encode(\n            encoding, errors).values.reshape(data.shape)\n\n    # create the sized dtype\n    if itemsize is None:\n        ensured = _ensure_object(data.ravel())\n        itemsize = libwriters.max_len_string_array(ensured)\n\n    data = np.asarray(data, dtype=\"S%d\" % itemsize)\n    return data\n```\n\nOriginal documentation:\n```python\nwe take a string-like that is object dtype and coerce to a fixed size\n    string type\n\n    Parameters\n    ----------\n    data : a numpy array of object dtype\n    encoding : None or string-encoding\n    itemsize : integer, optional, defaults to the max length of the strings\n\n    Returns\n    -------\n    data in a fixed-length string dtype, encoded to bytes if needed\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "we take a string-like that is object dtype and coerce to a fixed size\n    string type\n\n    Parameters\n    ----------\n    data : a numpy array of object dtype\n    encoding : None or string-encoding\n    errors : handler for encoding errors\n    itemsize : integer, optional, defaults to the max length of the strings\n\n    Returns\n    -------\n    data in a fixed-length string dtype, encoded to bytes if needed", "file": "pandas_io_pytables.py", "function": "_convert_string_array", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef text(self):\n        \n        \n        \n        if self.content is None:\n            return None\n        if self.encoding is None:\n            if self.headers is None:\n                self.encoding = 'utf-8'\n            else:\n                self.encoding = self.headers.get('content-type', '').partition(\"charset=\")[2] or 'utf-8'\n        return unicode(self.content, self.encoding, errors='replace')```\n\nUpdated code:\n```python\ndef text(self):\n        \n        \n        \n        if self.content is None:\n            return None\n        if self.encoding is None:\n            if self.headers is None:\n                self.encoding = 'utf-8'\n            else:\n                self.encoding = self.headers.get('content-type', '').partition(\"charset=\")[2] or 'utf-8'\n        return str(self.content, self.encoding, errors='replace')\n```\n\nOriginal documentation:\n```python\nReturns the text content of the response as a decoded string\n        (unicode on python2)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the text content of the response as a decoded string", "file": "locust_contrib_fasthttp.py", "function": "FastResponse.text", "project": "locust", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef open(self, text, insert_tags=None):\n        \n        \n        SearchDialogBase.open(self, text)\n        try:\n            first = text.index(\"sel.first\")\n        except TclError:\n            first = None\n        try:\n            last = text.index(\"sel.last\")\n        except TclError:\n            last = None\n        first = first or text.index(\"insert\")\n        last = last or first\n        self.show_hit(first, last)\n        self.ok = True\n        self.insert_tags = insert_tags```\n\nUpdated code:\n```python\ndef open(self, text, searchphrase=None, *, insert_tags=None):\n        \n        \n        SearchDialogBase.open(self, text, searchphrase)\n        self.ok = True\n        self.insert_tags = insert_tags\n```\n\nOriginal documentation:\n```python\nMake dialog visible on top of others and ready to use.\n\n        Also, highlight the currently selected text and set the\n        search to include the current selection (self.ok).\n\n        Args:\n            text: Text widget being searched.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Make dialog visible on top of others and ready to use.\n\n        Also, set the search to include the current selection\n        (self.ok).\n\n        Args:\n            text: Text widget being searched.\n            searchphrase: String phrase to search.", "file": "Lib_idlelib_replace.py", "function": "ReplaceDialog.open", "project": "cpython", "change_stats": {"code_changes": 17, "doc_changes": 7, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_coords_conditions(doctype, filters=None):\n    \n    from frappe.desk.reportview import get_filters_cond\n    if not frappe.has_permission(doctype):\n        frappe.throw(frappe._(\"Not Permitted\"), frappe.PermissionError)\n\n    return get_filters_cond(doctype, filters, [], with_match_conditions=True)```\n\nUpdated code:\n```python\ndef get_coords_conditions(doctype, filters=None):\n    ''''''\n    from frappe.desk.reportview import get_filters_cond\n    if not frappe.has_permission(doctype):\n        frappe.throw(frappe._(\"Not Permitted\"), frappe.PermissionError)\n\n    return get_filters_cond(doctype, filters, [], with_match_conditions=True)\n```\n\nOriginal documentation:\n```python\nReturns SQL conditions with user permissions and filters for event queries\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns SQL conditions with user permissions and filters for event queries.", "file": "frappe_geo_utils.py", "function": "get_coords_conditions", "project": "frappe", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,\n               bifpn_num_iterations,\n               bifpn_num_filters,\n               fpn_min_level,\n               fpn_max_level,\n               input_max_level,\n               is_training,\n               conv_hyperparams,\n               freeze_batchnorm,\n               bifpn_node_params=None,\n               name=None):\n    \n    \n    super(KerasBiFpnFeatureMaps, self).__init__(name=name)\n    bifpn_node_config = _create_bifpn_node_config(\n        bifpn_num_iterations, bifpn_num_filters, fpn_min_level, fpn_max_level,\n        input_max_level, bifpn_node_params)\n    bifpn_input_config = _create_bifpn_input_config(\n        fpn_min_level, fpn_max_level, input_max_level)\n    bifpn_output_node_names = _get_bifpn_output_node_names(\n        fpn_min_level, fpn_max_level, bifpn_node_config)\n\n    self.bifpn_node_config = bifpn_node_config\n    self.bifpn_output_node_names = bifpn_output_node_names\n    self.node_input_blocks = []\n    self.node_combine_op = []\n    self.node_post_combine_block = []\n\n    all_node_params = bifpn_input_config\n    all_node_names = [node['name'] for node in all_node_params]\n    for node_config in bifpn_node_config:\n      # Maybe transform and/or resample input feature maps.\n      input_blocks = []\n      for input_name in node_config['inputs']:\n        if input_name not in all_node_names:\n          raise ValueError(\n              'Input feature map ({}) does not exist:'.format(input_name))\n        input_index = all_node_names.index(input_name)\n        input_params = all_node_params[input_index]\n        input_block = node_config['input_op'](\n            name='{}/input_{}/'.format(node_config['name'], input_name),\n            input_scale=input_params['scale'],\n            input_num_channels=input_params.get('num_channels', None),\n            output_scale=node_config['scale'],\n            output_num_channels=node_config['num_channels'],\n            conv_hyperparams=conv_hyperparams,\n            is_training=is_training,\n            freeze_batchnorm=freeze_batchnorm)\n        input_blocks.append((input_index, input_block))\n\n      # Combine input feature maps.\n      combine_op = _create_bifpn_combine_op(\n          num_inputs=len(input_blocks),\n          name=(node_config['name'] + '/combine'),\n          combine_method=node_config['combine_method'])\n\n      # Post-combine layers.\n      post_combine_block = []\n      if node_config['post_combine_op']:\n        post_combine_block.extend(node_config['post_combine_op'](\n            name=node_config['name'] + '/post_combine/',\n            conv_hyperparams=conv_hyperparams,\n            is_training=is_training,\n            freeze_batchnorm=freeze_batchnorm))\n\n      self.node_input_blocks.append(input_blocks)\n      self.node_combine_op.append(combine_op)\n      self.node_post_combine_block.append(post_combine_block)\n      all_node_params.append(node_config)\n      all_node_names.append(node_config['name'])```\n\nUpdated code:\n```python\ndef __init__(self,\n               bifpn_num_iterations,\n               bifpn_num_filters,\n               fpn_min_level,\n               fpn_max_level,\n               input_max_level,\n               is_training,\n               conv_hyperparams,\n               freeze_batchnorm,\n               bifpn_node_params=None,\n               use_native_resize_op=False,\n               name=None):\n    \n    \n    super(KerasBiFpnFeatureMaps, self).__init__(name=name)\n    bifpn_node_config = _create_bifpn_node_config(\n        bifpn_num_iterations,\n        bifpn_num_filters,\n        fpn_min_level,\n        fpn_max_level,\n        input_max_level,\n        bifpn_node_params,\n        use_native_resize_op=use_native_resize_op)\n    bifpn_input_config = _create_bifpn_input_config(fpn_min_level,\n                                                    fpn_max_level,\n                                                    input_max_level)\n    bifpn_output_node_names = _get_bifpn_output_node_names(\n        fpn_min_level, fpn_max_level, bifpn_node_config)\n\n    self.bifpn_node_config = bifpn_node_config\n    self.bifpn_output_node_names = bifpn_output_node_names\n    self.node_input_blocks = []\n    self.node_combine_op = []\n    self.node_post_combine_block = []\n\n    all_node_params = bifpn_input_config\n    all_node_names = [node['name'] for node in all_node_params]\n    for node_config in bifpn_node_config:\n      # Maybe transform and/or resample input feature maps.\n      input_blocks = []\n      for input_name in node_config['inputs']:\n        if input_name not in all_node_names:\n          raise ValueError(\n              'Input feature map ({}) does not exist:'.format(input_name))\n        input_index = all_node_names.index(input_name)\n        input_params = all_node_params[input_index]\n        input_block = node_config['input_op'](\n            name='{}/input_{}/'.format(node_config['name'], input_name),\n            input_scale=input_params['scale'],\n            input_num_channels=input_params.get('num_channels', None),\n            output_scale=node_config['scale'],\n            output_num_channels=node_config['num_channels'],\n            conv_hyperparams=conv_hyperparams,\n            is_training=is_training,\n            freeze_batchnorm=freeze_batchnorm)\n        input_blocks.append((input_index, input_block))\n\n      # Combine input feature maps.\n      combine_op = _create_bifpn_combine_op(\n          num_inputs=len(input_blocks),\n          name=(node_config['name'] + '/combine'),\n          combine_method=node_config['combine_method'])\n\n      # Post-combine layers.\n      post_combine_block = []\n      if node_config['post_combine_op']:\n        post_combine_block.extend(node_config['post_combine_op'](\n            name=node_config['name'] + '/post_combine/',\n            conv_hyperparams=conv_hyperparams,\n            is_training=is_training,\n            freeze_batchnorm=freeze_batchnorm))\n\n      self.node_input_blocks.append(input_blocks)\n      self.node_combine_op.append(combine_op)\n      self.node_post_combine_block.append(post_combine_block)\n      all_node_params.append(node_config)\n      all_node_names.append(node_config['name'])\n```\n\nOriginal documentation:\n```python\nConstructor.\n\n    Args:\n      bifpn_num_iterations: The number of top-down bottom-up iterations.\n      bifpn_num_filters: The number of filters (channels) to be used for all\n        feature maps in this BiFPN.\n      fpn_min_level: The minimum pyramid level (highest feature map resolution)\n        to use in the BiFPN.\n      fpn_max_level: The maximum pyramid level (lowest feature map resolution)\n        to use in the BiFPN.\n      input_max_level: The maximum pyramid level that will be provided as input\n        to the BiFPN. Accordingly, the BiFPN will compute any additional pyramid\n        levels from input_max_level up to the desired fpn_max_level, with each\n        successivel level downsampling by a scale factor of 2 by default.\n      is_training: Indicates whether the feature generator is in training mode.\n      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n        containing hyperparameters for convolution ops.\n      freeze_batchnorm: Bool. Whether to freeze batch norm parameters during\n        training or not. When training with a small batch size (e.g. 1), it is\n        desirable to freeze batch norm update and use pretrained batch norm\n        params.\n      bifpn_node_params: An optional dictionary that may be used to specify\n        default parameters for BiFPN nodes, without the need to provide a custom\n        bifpn_node_config. For example, if '{ combine_method: 'sum' }', then all\n        BiFPN nodes will combine input feature maps by summation, rather than\n        by the default fast attention method.\n      name: A string name scope to assign to the model. If 'None', Keras\n        will auto-generate one from the class name.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Constructor.\n\n    Args:\n      bifpn_num_iterations: The number of top-down bottom-up iterations.\n      bifpn_num_filters: The number of filters (channels) to be used for all\n        feature maps in this BiFPN.\n      fpn_min_level: The minimum pyramid level (highest feature map resolution)\n        to use in the BiFPN.\n      fpn_max_level: The maximum pyramid level (lowest feature map resolution)\n        to use in the BiFPN.\n      input_max_level: The maximum pyramid level that will be provided as input\n        to the BiFPN. Accordingly, the BiFPN will compute any additional pyramid\n        levels from input_max_level up to the desired fpn_max_level, with each\n        successivel level downsampling by a scale factor of 2 by default.\n      is_training: Indicates whether the feature generator is in training mode.\n      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n        containing hyperparameters for convolution ops.\n      freeze_batchnorm: Bool. Whether to freeze batch norm parameters during\n        training or not. When training with a small batch size (e.g. 1), it is\n        desirable to freeze batch norm update and use pretrained batch norm\n        params.\n      bifpn_node_params: An optional dictionary that may be used to specify\n        default parameters for BiFPN nodes, without the need to provide a custom\n        bifpn_node_config. For example, if '{ combine_method: 'sum' }', then all\n        BiFPN nodes will combine input feature maps by summation, rather than\n        by the default fast attention method.\n      use_native_resize_op: If True, will use\n        tf.compat.v1.image.resize_nearest_neighbor for unsampling.\n      name: A string name scope to assign to the model. If 'None', Keras\n        will auto-generate one from the class name.", "file": "research_object_detection_models_bidirectional_feature_pyramid_generators.py", "function": "KerasBiFpnFeatureMaps.__init__", "project": "models", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_radii(self, radii):\n        \n        \n        \n        self.a, self.b = radii\n        self.stale = True```\n\nUpdated code:\n```python\ndef set_radii(self, r):\n        \n        \n        \n        if np.shape(r) == (2,):\n            self.a, self.b = r\n        elif np.shape(r) == ():\n            self.a = self.b = float(r)\n        else:\n            raise ValueError(\"Parameter 'r' must be one or two floats.\")\n\n        self._path = None\n        self.stale = True\n```\n\nOriginal documentation:\n```python\nSet the both the semi-major (*a*) and -minor radii (*b*) of the\n        annulus.\n\n        Parameters\n        ----------\n        radii : (float, float)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the both the semi-major (*a*) and -minor radii (*b*) of the\n        annulus.\n\n        Parameters\n        ----------\n        r : (float, float)", "file": "lib_matplotlib_patches.py", "function": "Annulus.set_radii", "project": "matplotlib", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_scale(self, value, **kwargs):\n        \n        \n        \n        warnings.warn(\"This function has been made private and moved\"\n                        \"to `_set_scale`. This wrapper function will be \"\n                        \"removed in 1.4\", mplDeprecation)\n        self._set_scale(value, **kwargs)```\n\nUpdated code:\n```python\ndef set_scale(self, value, **kwargs):\n        \n        \n        \n        self._set_scale(value, **kwargs)\n```\n\nOriginal documentation:\n```python\nDeprecated 1.3.\n\n        This should be a private function (moved to _set_scale)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This should be a private function (moved to _set_scale)", "file": "lib_matplotlib_axis.py", "function": "Axis.set_scale", "project": "matplotlib", "change_stats": {"code_changes": 5, "doc_changes": 6, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_numpyndarrayInput_l1(self):\n        \n        ndarray_matrix = numpy.array([[1, 0, 2],\n                                      [0, 0, 3],\n                                      [4, 5, 6]])\n        normalized = self.model_l1.normalize(ndarray_matrix)\n\n        # Check if output is of same type\n        self.assertTrue(isinstance(normalized, numpy.ndarray))\n\n        # Check if output is correct\n        expected = numpy.array([[0.04761905, 0., 0.0952381],\n                                [0., 0., 0.14285714],\n                                [0.19047619, 0.23809524, 0.28571429]])\n        self.assertTrue(numpy.allclose(normalized, expected))\n\n        # Test if error is raised on unsupported input type\n        self.assertRaises(ValueError, lambda model, doc: model.normalize(doc), self.model_l1, [1, 2, 3])```\n\nUpdated code:\n```python\ndef test_numpyndarrayInput_l1(self):\n        \n        ndarray_matrix = np.array([[1, 0, 2],\n                                      [0, 0, 3],\n                                      [4, 5, 6]])\n        normalized = self.model_l1.normalize(ndarray_matrix)\n\n        # Check if output is of same type\n        self.assertTrue(isinstance(normalized, np.ndarray))\n\n        # Check if output is correct\n        expected = np.array([[0.04761905, 0., 0.0952381],\n                                [0., 0., 0.14285714],\n                                [0.19047619, 0.23809524, 0.28571429]])\n        self.assertTrue(np.allclose(normalized, expected))\n\n        # Test if error is raised on unsupported input type\n        self.assertRaises(ValueError, lambda model, doc: model.normalize(doc), self.model_l1, [1, 2, 3])\n```\n\nOriginal documentation:\n```python\nTest for numpy ndarray input for l1 transformation\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test for np ndarray input for l1 transformation", "file": "gensim_test_test_normmodel.py", "function": "TestNormModel.test_numpyndarrayInput_l1", "project": "gensim", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef l1_loss(pred, target):\n    \n    \n    if target.numel() == 0:\n        return pred.sum() * 0\n\n    assert pred.size() == target.size()\n    loss = torch.abs(pred - target)\n    return loss```\n\nUpdated code:\n```python\ndef l1_loss(pred: Tensor, target: Tensor) -> Tensor:\n    \n    \n    if target.numel() == 0:\n        return pred.sum() * 0\n\n    assert pred.size() == target.size()\n    loss = torch.abs(pred - target)\n    return loss\n```\n\nOriginal documentation:\n```python\nL1 loss.\n\n    Args:\n        pred (torch.Tensor): The prediction.\n        target (torch.Tensor): The learning target of the prediction.\n\n    Returns:\n        torch.Tensor: Calculated loss\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "L1 loss.\n\n    Args:\n        pred (Tensor): The prediction.\n        target (Tensor): The learning target of the prediction.\n\n    Returns:\n        Tensor: Calculated loss", "file": "mmdet_models_losses_smooth_l1_loss.py", "function": "l1_loss", "project": "mmdetection", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef prefixQuery(self, string, prefix=None, where=None, clause=None):\n        \n        \n        \n\n        if conf.direct:\n            return self.payloadDirect(string)\n\n        query = None\n\n        if where is None and kb.technique and kb.technique in kb.injection.data:\n            where = kb.injection.data[kb.technique].where\n\n        # If we are replacing (<where>) the parameter original value with\n        # our payload do not prepend with the prefix\n        if where == PAYLOAD.WHERE.REPLACE:\n            query = \"\"\n\n        # If the technique is stacked queries (<stype>) do not put a space\n        # after the prefix or it is in GROUP BY / ORDER BY (<clause>)\n        elif kb.technique == PAYLOAD.TECHNIQUE.STACKED:\n            query = kb.injection.prefix\n        elif kb.injection.clause == [2, 3] or kb.injection.clause == [ 2 ] or kb.injection.clause == [ 3 ]:\n            query = kb.injection.prefix\n        elif clause == [2, 3] or clause == [ 2 ] or clause == [ 3 ]:\n            query = prefix\n\n        # In any other case prepend with the full prefix\n        else:\n            query = kb.injection.prefix or prefix or \"\"\n\n            if not (string and string[0] == \";\"):\n                query += \" \"\n\n        query = \"%s%s\" % (query, string)\n        query = self.cleanupPayload(query)\n\n        return query```\n\nUpdated code:\n```python\ndef prefixQuery(self, expression, prefix=None, where=None, clause=None):\n        \n        \n        \n\n        if conf.direct:\n            return self.payloadDirect(expression)\n\n        expression = unescaper.unescape(expression)\n        query = None\n\n        if where is None and kb.technique and kb.technique in kb.injection.data:\n            where = kb.injection.data[kb.technique].where\n\n        # If we are replacing (<where>) the parameter original value with\n        # our payload do not prepend with the prefix\n        if where == PAYLOAD.WHERE.REPLACE:\n            query = \"\"\n\n        # If the technique is stacked queries (<stype>) do not put a space\n        # after the prefix or it is in GROUP BY / ORDER BY (<clause>)\n        elif kb.technique == PAYLOAD.TECHNIQUE.STACKED:\n            query = kb.injection.prefix\n        elif kb.injection.clause == [2, 3] or kb.injection.clause == [ 2 ] or kb.injection.clause == [ 3 ]:\n            query = kb.injection.prefix\n        elif clause == [2, 3] or clause == [ 2 ] or clause == [ 3 ]:\n            query = prefix\n\n        # In any other case prepend with the full prefix\n        else:\n            query = kb.injection.prefix or prefix or \"\"\n\n            if not (expression and expression[0] == \";\"):\n                query += \" \"\n\n        query = \"%s%s\" % (query, expression)\n        query = self.cleanupPayload(query)\n\n        return query\n```\n\nOriginal documentation:\n```python\nThis method defines how the input string has to be escaped\n        to perform the injection depending on the injection type\n        identified as valid\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method defines how the input expression has to be escaped\n        to perform the injection depending on the injection type\n        identified as valid", "file": "lib_core_agent.py", "function": "Agent.prefixQuery", "project": "sqlmap", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save(self):\n        \n        \n        \n        self.save_base()```\n\nUpdated code:\n```python\ndef save(self, force_insert=False, force_update=False):\n        \n        \n        \n        if force_insert and force_update:\n            raise ValueError(\"Cannot force both insert and updating in \"\n                    \"model saving.\")\n        self.save_base(force_insert=force_insert, force_update=force_update)\n```\n\nOriginal documentation:\n```python\nSaves the current instance. Override this in a subclass if you want to\n        control the saving process.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Saves the current instance. Override this in a subclass if you want to\n        control the saving process.\n\n        The 'force_insert' and 'force_update' parameters can be used to insist\n        that the \"save\" must be an SQL insert or update (or equivalent for\n        non-SQL backends), respectively. Normally, they should not be set.", "file": "django_db_models_base.py", "function": "Model.save", "project": "django", "change_stats": {"code_changes": 9, "doc_changes": 6, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef call(self, inputs, *args, **kwargs):\n        \n        \n        if self.data_format == 'channels_last':\n            pooled = K.min(inputs, axis=[1, 2])\n        else:\n            pooled = K.min(inputs, axis=[2, 3])\n        return pooled```\n\nUpdated code:\n```python\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \n        \n        if self.data_format == 'channels_last':\n            pooled = K.min(inputs, axis=[1, 2])\n        else:\n            pooled = K.min(inputs, axis=[2, 3])\n        return pooled\n```\n\nOriginal documentation:\n```python\nThis is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: tensor\n            Input tensor, or list/tuple of input tensors\n        args: tuple\n            Additional standard keras Layer arguments\n        kwargs: dict\n            Additional standard keras Layer keyword arguments\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors", "file": "lib_model_layers.py", "function": "GlobalMinPooling2D.call", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_forward_relationships(opts):\n    \n    \n    \n    forward_relations = OrderedDict()\n    for field in [field for field in opts.fields if field.serialize and field.remote_field]:\n        forward_relations[field.name] = RelationInfo(\n            model_field=field,\n            related_model=field.remote_field.model,\n            to_many=False,\n            to_field=_get_to_field(field),\n            has_through_model=False,\n            reverse=False\n        )\n\n    # Deal with forward many-to-many relationships.\n    for field in [field for field in opts.many_to_many if field.serialize]:\n        forward_relations[field.name] = RelationInfo(\n            model_field=field,\n            related_model=field.remote_field.model,\n            to_many=True,\n            # manytomany do not have to_fields\n            to_field=None,\n            has_through_model=(\n                not field.remote_field.through._meta.auto_created\n            ),\n            reverse=False\n        )\n\n    return forward_relations```\n\nUpdated code:\n```python\ndef _get_forward_relationships(opts):\n    \n    \n    \n    forward_relations = {}\n    for field in [field for field in opts.fields if field.serialize and field.remote_field]:\n        forward_relations[field.name] = RelationInfo(\n            model_field=field,\n            related_model=field.remote_field.model,\n            to_many=False,\n            to_field=_get_to_field(field),\n            has_through_model=False,\n            reverse=False\n        )\n\n    # Deal with forward many-to-many relationships.\n    for field in [field for field in opts.many_to_many if field.serialize]:\n        forward_relations[field.name] = RelationInfo(\n            model_field=field,\n            related_model=field.remote_field.model,\n            to_many=True,\n            # manytomany do not have to_fields\n            to_field=None,\n            has_through_model=(\n                not field.remote_field.through._meta.auto_created\n            ),\n            reverse=False\n        )\n\n    return forward_relations\n```\n\nOriginal documentation:\n```python\nReturns an `OrderedDict` of field names to `RelationInfo`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a dict of field names to `RelationInfo`.", "file": "rest_framework_utils_model_meta.py", "function": "_get_forward_relationships", "project": "django-rest-framework", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_gpt_cond_latents(self, audio, sr, length: int = 3):\n        \n        \n\n        audio_22k = torchaudio.functional.resample(audio, sr, 22050)\n        audio_22k = audio_22k[:, : 22050 * length]\n        mel = wav_to_mel_cloning(audio_22k, mel_norms=self.mel_stats.cpu())\n        cond_latent = self.gpt.get_style_emb(mel.to(self.device))\n        return cond_latent.transpose(1, 2)```\n\nUpdated code:\n```python\ndef get_gpt_cond_latents(self, audio, sr, length: int = 3):\n        \n        \n        if sr != 22050:\n            audio = torchaudio.functional.resample(audio, sr, 22050)\n        audio = audio[:, : 22050 * length]\n        if self.args.gpt_use_perceiver_resampler:\n            n_fft = 2048\n            hop_length = 256\n            win_length = 1024\n        else:\n            n_fft = 4096\n            hop_length = 1024\n            win_length = 4096\n        mel = wav_to_mel_cloning(\n            audio,\n            mel_norms=self.mel_stats.cpu(),\n            n_fft=n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            power=2,\n            normalized=False,\n            sample_rate=22050,\n            f_min=0,\n            f_max=8000,\n            n_mels=80,\n        )\n        cond_latent = self.gpt.get_style_emb(mel.to(self.device))\n        return cond_latent.transpose(1, 2)\n```\n\nOriginal documentation:\n```python\nCompute the conditioning latents for the GPT model from the given audio.\n\n        Args:\n            audio_path (str): Path to the audio file.\n            length (int): Length of the audio in seconds. Defaults to 3.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute the conditioning latents for the GPT model from the given audio.\n\n        Args:\n            audio_path (str): Path to the audio file.\n            sr (int): Sample rate of the audio.\n            length (int): Length of the audio in seconds. Defaults to 3.", "file": "TTS_tts_models_xtts.py", "function": "Xtts.get_gpt_cond_latents", "project": "TTS", "change_stats": {"code_changes": 30, "doc_changes": 3, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _adjust_binner_for_upsample(self, binner):\n          \n        if self.closed == 'right':\n            binner = binner[1:]\n        else:\n            binner = binner[:-1]\n        return binner```\n\nUpdated code:\n```python\ndef _adjust_binner_for_upsample(self, binner):\n        \n        \n        \n        if self.closed == 'right':\n            binner = binner[1:]\n        else:\n            binner = binner[:-1]\n        return binner\n```\n\nOriginal documentation:\n```python\nadjust our binner when upsampling\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adjust our binner when upsampling.\n        The range of a new index should not be outside specified range", "file": "pandas_core_resample.py", "function": "DatetimeIndexResampler._adjust_binner_for_upsample", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef constant_time_compare(val1, val2):\n    \n    \n    \n    if not (isinstance(val1, bytes) and isinstance(val2, bytes)):\n        raise TypeError(\"constant_time_compare only supports bytes\")\n    if len(val1) != len(val2):\n        return False\n    result = 0\n    if six.PY3:\n        for x, y in zip(val1, val2):\n            result |= x ^ y\n    else:\n        for x, y in zip(val1, val2):\n            result |= ord(x) ^ ord(y)\n    return result == 0```\n\nUpdated code:\n```python\ndef constant_time_compare(val1, val2):\n    \n    \n    \n    if len(val1) != len(val2):\n        return False\n    result = 0\n    if six.PY3 and isinstance(val1, bytes) and isinstance(val2, bytes):\n        for x, y in zip(val1, val2):\n            result |= x ^ y\n    else:\n        for x, y in zip(val1, val2):\n            result |= ord(x) ^ ord(y)\n    return result == 0\n```\n\nOriginal documentation:\n```python\nReturns True if the two bytestrings are equal, False otherwise.\n\n    The time taken is independent of the number of characters that match.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns True if the two strings are equal, False otherwise.\n\n    The time taken is independent of the number of characters that match.", "file": "django_utils_crypto.py", "function": "constant_time_compare", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(\n        self,\n        sequence: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n        mask: torch.Tensor,\n        cache: torch.Tensor = None,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor]:\n        \n\n        \n        if isinstance(sequence, tuple):\n            sequence, pos_embed = sequence[0], sequence[1]\n        else:\n            sequence, pos_embed = sequence, None\n\n        sequence = sequence.transpose(1, 2)\n        sequence = self.conv(sequence)\n\n        if self.batch_norm:\n            sequence = self.bn(sequence)\n\n        sequence = self.dropout(sequence)\n\n        if self.relu:\n            sequence = self.relu_func(sequence)\n\n        sequence = sequence.transpose(1, 2)\n\n        if mask is not None:\n            mask = self.create_new_mask(mask)\n\n        if pos_embed is not None:\n            sequence = (sequence, self.create_new_pos_embed(pos_embed))\n\n        return sequence, mask```\n\nUpdated code:\n```python\ndef forward(\n        self,\n        x: torch.Tensor,\n        pos_enc: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        chunk_mask: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n\n        \n        x = x.transpose(1, 2)\n\n        x = self.conv(x)\n\n        if self.batch_norm:\n            x = self.bn(x)\n\n        x = self.dropout(x)\n\n        if self.relu:\n            x = self.relu_func(x)\n\n        x = x.transpose(1, 2)\n\n        if self.causal:\n            return x, pos_enc, mask\n\n        return x\n```\n\nOriginal documentation:\n```python\nEncode input sequences.\n\n        Args:\n            sequence: Conv1d input sequences.\n                      (B, T, D_in) or\n                      ((B, T, D_in),  (B, 2 * (T - 1), D_att))\n            mask: Mask of input sequences. (B, 1, T)\n\n        Returns:\n            sequence: Conv1d output sequences.\n                      (B, sub(T), D_out) or\n                      ((B, sub(T), D_out),  (B, 2 * (sub(T) - 1), D_att))\n            mask: Mask of output sequences. (B, 1, sub(T))\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Encode input sequences.\n\n        Args:\n            sequence: Conv1d input sequences. (B, T, D_in)\n            pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_att)\n\n        Returns:\n            x: Conv1d output sequences. (B, sub(T), D_out)", "file": "espnet2_asr_transducer_encoder_blocks_conv1d.py", "function": "Conv1d.forward", "project": "espnet", "change_stats": {"code_changes": 36, "doc_changes": 13, "total_changes": 49}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_args(self,\n                 imw,\n                 imh,\n                 colormap=cv2.COLORMAP_JET,\n                 heatmap_alpha=0.5,\n                 view_img=False,\n                 count_reg_pts=None,\n                 count_txt_thickness=2,\n                 count_txt_color=(0, 0, 0),\n                 count_color=(255, 255, 255),\n                 count_reg_color=(255, 0, 255),\n                 region_thickness=5,\n                 line_dist_thresh=15,\n                 decay_factor=0.99,\n                 shape='circle'):\n        \n        \n        \n        self.imw = imw\n        self.imh = imh\n        self.heatmap_alpha = heatmap_alpha\n        self.view_img = view_img\n        self.colormap = colormap\n\n        # Region and line selection\n        if count_reg_pts is not None:\n\n            if len(count_reg_pts) == 2:\n                print('Line Counter Initiated.')\n                self.count_reg_pts = count_reg_pts\n                self.counting_region = LineString(count_reg_pts)\n\n            elif len(count_reg_pts) == 4:\n                print('Region Counter Initiated.')\n                self.count_reg_pts = count_reg_pts\n                self.counting_region = Polygon(self.count_reg_pts)\n\n            else:\n                print('Region or line points Invalid, 2 or 4 points supported')\n                print('Using Line Counter Now')\n                self.counting_region = Polygon([(20, 400), (1260, 400)])  # dummy points\n\n        # Heatmap new frame\n        self.heatmap = np.zeros((int(self.imh), int(self.imw)), dtype=np.float32)\n\n        self.count_txt_thickness = count_txt_thickness\n        self.count_txt_color = count_txt_color\n        self.count_color = count_color\n        self.region_color = count_reg_color\n        self.region_thickness = region_thickness\n        self.decay_factor = decay_factor\n        self.line_dist_thresh = line_dist_thresh\n        self.shape = shape\n\n        # shape of heatmap, if not selected\n        if self.shape not in ['circle', 'rect']:\n            print(\"Unknown shape value provided, 'circle' & 'rect' supported\")\n            print('Using Circular shape now')\n            self.shape = 'circle'```\n\nUpdated code:\n```python\ndef set_args(self,\n                 imw,\n                 imh,\n                 colormap=cv2.COLORMAP_JET,\n                 heatmap_alpha=0.5,\n                 view_img=False,\n                 view_in_counts=True,\n                 view_out_counts=True,\n                 count_reg_pts=None,\n                 count_txt_thickness=2,\n                 count_txt_color=(0, 0, 0),\n                 count_color=(255, 255, 255),\n                 count_reg_color=(255, 0, 255),\n                 region_thickness=5,\n                 line_dist_thresh=15,\n                 decay_factor=0.99,\n                 shape='circle'):\n        \n        \n        \n        self.imw = imw\n        self.imh = imh\n        self.heatmap_alpha = heatmap_alpha\n        self.view_img = view_img\n        self.view_in_counts = view_in_counts\n        self.view_out_counts = view_out_counts\n        self.colormap = colormap\n\n        # Region and line selection\n        if count_reg_pts is not None:\n\n            if len(count_reg_pts) == 2:\n                print('Line Counter Initiated.')\n                self.count_reg_pts = count_reg_pts\n                self.counting_region = LineString(count_reg_pts)\n\n            elif len(count_reg_pts) == 4:\n                print('Region Counter Initiated.')\n                self.count_reg_pts = count_reg_pts\n                self.counting_region = Polygon(self.count_reg_pts)\n\n            else:\n                print('Region or line points Invalid, 2 or 4 points supported')\n                print('Using Line Counter Now')\n                self.counting_region = Polygon([(20, 400), (1260, 400)])  # dummy points\n\n        # Heatmap new frame\n        self.heatmap = np.zeros((int(self.imh), int(self.imw)), dtype=np.float32)\n\n        self.count_txt_thickness = count_txt_thickness\n        self.count_txt_color = count_txt_color\n        self.count_color = count_color\n        self.region_color = count_reg_color\n        self.region_thickness = region_thickness\n        self.decay_factor = decay_factor\n        self.line_dist_thresh = line_dist_thresh\n        self.shape = shape\n\n        # shape of heatmap, if not selected\n        if self.shape not in ['circle', 'rect']:\n            print(\"Unknown shape value provided, 'circle' & 'rect' supported\")\n            print('Using Circular shape now')\n            self.shape = 'circle'\n```\n\nOriginal documentation:\n```python\nConfigures the heatmap colormap, width, height and display parameters.\n\n        Args:\n            colormap (cv2.COLORMAP): The colormap to be set.\n            imw (int): The width of the frame.\n            imh (int): The height of the frame.\n            heatmap_alpha (float): alpha value for heatmap display\n            view_img (bool): Flag indicating frame display\n            count_reg_pts (list): Object counting region points\n            count_txt_thickness (int): Text thickness for object counting display\n            count_txt_color (RGB color): count text color value\n            count_color (RGB color): count text background color value\n            count_reg_color (RGB color): Color of object counting region\n            region_thickness (int): Object counting Region thickness\n            line_dist_thresh (int): Euclidean Distance threshold for line counter\n            decay_factor (float): value for removing heatmap area after object passed\n            shape (str): Heatmap shape, rect or circle shape supported\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Configures the heatmap colormap, width, height and display parameters.\n\n        Args:\n            colormap (cv2.COLORMAP): The colormap to be set.\n            imw (int): The width of the frame.\n            imh (int): The height of the frame.\n            heatmap_alpha (float): alpha value for heatmap display\n            view_img (bool): Flag indicating frame display\n            view_in_counts (bool): Flag to control whether to display the incounts on video stream.\n            view_out_counts (bool): Flag to control whether to display the outcounts on video stream.\n            count_reg_pts (list): Object counting region points\n            count_txt_thickness (int): Text thickness for object counting display\n            count_txt_color (RGB color): count text color value\n            count_color (RGB color): count text background color value\n            count_reg_color (RGB color): Color of object counting region\n            region_thickness (int): Object counting Region thickness\n            line_dist_thresh (int): Euclidean Distance threshold for line counter\n            decay_factor (float): value for removing heatmap area after object passed\n            shape (str): Heatmap shape, rect or circle shape supported", "file": "ultralytics_solutions_heatmap.py", "function": "Heatmap.set_args", "project": "ultralytics", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run_cell(self, raw_cell, store_history=False):\n        \n        \n        if (not raw_cell) or raw_cell.isspace():\n            return\n\n        for line in raw_cell.splitlines():\n            self.input_splitter.push(line)\n        cell = self.input_splitter.source_reset()\n\n        with self.builtin_trap:\n            prefilter_failed = False\n            if len(cell.splitlines()) == 1:\n                try:\n                    # use prefilter_lines to handle trailing newlines\n                    # restore trailing newline for ast.parse\n                    cell = self.prefilter_manager.prefilter_lines(cell) + '\\n'\n                except AliasError as e:\n                    error(e)\n                    prefilter_failed = True\n                except Exception:\n                    # don't allow prefilter errors to crash IPython\n                    self.showtraceback()\n                    prefilter_failed = True\n\n            # Store raw and processed history\n            if store_history:\n                self.history_manager.store_inputs(self.execution_count,\n                                                  cell, raw_cell)\n\n            self.logger.log(cell, raw_cell)\n\n            if not prefilter_failed:\n                # don't run if prefilter failed\n                cell_name = self.compile.cache(cell, self.execution_count)\n\n                with self.display_trap:\n                    try:\n                        code_ast = self.compile.ast_parse(cell, filename=cell_name)\n                    except IndentationError:\n                        self.showindentationerror()\n                        if store_history:\n                            self.execution_count += 1\n                        return None\n                    except (OverflowError, SyntaxError, ValueError, TypeError,\n                            MemoryError):\n                        self.showsyntaxerror()\n                        if store_history:\n                            self.execution_count += 1\n                        return None\n\n                    self.run_ast_nodes(code_ast.body, cell_name,\n                                       interactivity=\"last_expr\")\n\n                    # Execute any registered post-execution functions.\n                    for func, status in self._post_execute.iteritems():\n                        if self.disable_failing_post_execute and not status:\n                            continue\n                        try:\n                            func()\n                        except KeyboardInterrupt:\n                            print >> io.stderr, \"\\nKeyboardInterrupt\"\n                        except Exception:\n                            # register as failing:\n                            self._post_execute[func] = False\n                            self.showtraceback()\n                            print >> io.stderr, '\\n'.join([\n                                \"post-execution function %r produced an error.\" % func,\n                                \"If this problem persists, you can disable failing post-exec functions with:\",\n                                \"\",\n                                \"    get_ipython().disable_failing_post_execute = True\"\n                            ])\n\n        if store_history:\n            # Write output to the database. Does nothing unless\n            # history output logging is enabled.\n            self.history_manager.store_output(self.execution_count)\n            # Each cell is a *single* input, regardless of how many lines it has\n            self.execution_count += 1```\n\nUpdated code:\n```python\ndef run_cell(self, raw_cell, store_history=False, silent=False):\n        \n        \n        if (not raw_cell) or raw_cell.isspace():\n            return\n        \n        if silent:\n            store_history = False\n\n        for line in raw_cell.splitlines():\n            self.input_splitter.push(line)\n        cell = self.input_splitter.source_reset()\n\n        with self.builtin_trap:\n            prefilter_failed = False\n            if len(cell.splitlines()) == 1:\n                try:\n                    # use prefilter_lines to handle trailing newlines\n                    # restore trailing newline for ast.parse\n                    cell = self.prefilter_manager.prefilter_lines(cell) + '\\n'\n                except AliasError as e:\n                    error(e)\n                    prefilter_failed = True\n                except Exception:\n                    # don't allow prefilter errors to crash IPython\n                    self.showtraceback()\n                    prefilter_failed = True\n\n            # Store raw and processed history\n            if store_history:\n                self.history_manager.store_inputs(self.execution_count,\n                                                  cell, raw_cell)\n            if not silent:\n                self.logger.log(cell, raw_cell)\n\n            if not prefilter_failed:\n                # don't run if prefilter failed\n                cell_name = self.compile.cache(cell, self.execution_count)\n\n                with self.display_trap:\n                    try:\n                        code_ast = self.compile.ast_parse(cell, filename=cell_name)\n                    except IndentationError:\n                        self.showindentationerror()\n                        if store_history:\n                            self.execution_count += 1\n                        return None\n                    except (OverflowError, SyntaxError, ValueError, TypeError,\n                            MemoryError):\n                        self.showsyntaxerror()\n                        if store_history:\n                            self.execution_count += 1\n                        return None\n                    \n                    interactivity = \"none\" if silent else \"last_expr\"\n                    self.run_ast_nodes(code_ast.body, cell_name,\n                                       interactivity=interactivity)\n                    \n                    # Execute any registered post-execution functions.\n                    # unless we are silent\n                    post_exec = [] if silent else self._post_execute.iteritems()\n                    \n                    for func, status in post_exec:\n                        if self.disable_failing_post_execute and not status:\n                            continue\n                        try:\n                            func()\n                        except KeyboardInterrupt:\n                            print >> io.stderr, \"\\nKeyboardInterrupt\"\n                        except Exception:\n                            # register as failing:\n                            self._post_execute[func] = False\n                            self.showtraceback()\n                            print >> io.stderr, '\\n'.join([\n                                \"post-execution function %r produced an error.\" % func,\n                                \"If this problem persists, you can disable failing post-exec functions with:\",\n                                \"\",\n                                \"    get_ipython().disable_failing_post_execute = True\"\n                            ])\n\n        if store_history:\n            # Write output to the database. Does nothing unless\n            # history output logging is enabled.\n            self.history_manager.store_output(self.execution_count)\n            # Each cell is a *single* input, regardless of how many lines it has\n            self.execution_count += 1\n```\n\nOriginal documentation:\n```python\nRun a complete IPython cell.\n\n        Parameters\n        ----------\n        raw_cell : str\n          The code (including IPython code such as %magic functions) to run.\n        store_history : bool\n          If True, the raw and translated cell will be stored in IPython's\n          history. For user code calling back into IPython's machinery, this\n          should be set to False.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run a complete IPython cell.\n\n        Parameters\n        ----------\n        raw_cell : str\n          The code (including IPython code such as %magic functions) to run.\n        store_history : bool\n          If True, the raw and translated cell will be stored in IPython's\n          history. For user code calling back into IPython's machinery, this\n          should be set to False.\n        silent : bool\n          If True, avoid side-effets, such as implicit displayhooks, history,\n          and logging.  silent=True forces store_history=False.", "file": "IPython_core_interactiveshell.py", "function": "InteractiveShell.run_cell", "project": "ipython", "change_stats": {"code_changes": 23, "doc_changes": 5, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_connection(self) -> StatusResponse:\n        \n        \n        \n\n        result = StatusResponse(False)\n        need_to_close = self.is_connected is False\n\n        try:\n            connection = self.connect()\n            result.success = connection.is_connected()\n        except Exception as e:\n            logger.error(f'Error connecting to MySQL {self.connection_data[\"database\"]}, {e}!')\n            result.error_message = str(e)\n\n        if result.success is True and need_to_close:\n            self.disconnect()\n        if result.success is False and self.is_connected is True:\n            self.is_connected = False\n\n        return result```\n\nUpdated code:\n```python\ndef check_connection(self) -> StatusResponse:\n        \n        \n        \n\n        result = StatusResponse(False)\n        need_to_close = not self.is_connected\n\n        try:\n            with self.connect() as connection:\n                result.success = connection.is_connected()\n        except mysql.connector.Error as e:\n            logger.error(f'Error connecting to MySQL {self.connection_data[\"database\"]}, {e}!')\n            result.error_message = str(e)\n\n        if result.success and need_to_close:\n            self.disconnect()\n        if not result.success and self.is_connected:\n            self.is_connected = False\n\n        return result\n```\n\nOriginal documentation:\n```python\nCheck the connection of the MySQL database\n        :return: success status and error message if error occurs\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks the status of the connection to the MySQL database\n        \n        Returns:\n            StatusResponse: An object containing the success status and an error message if an error occurs.", "file": "mindsdb_integrations_handlers_mysql_handler_mysql_handler.py", "function": "MySQLHandler.check_connection", "project": "mindsdb", "change_stats": {"code_changes": 14, "doc_changes": 8, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef handle_typed_errors(cls, errors: list, **extra):\n        \n        \n        return cls(errors=[e[0] for e in errors], **extra)```\n\nUpdated code:\n```python\ndef handle_typed_errors(cls, errors: list, **extra):\n        \n        if hasattr(cls, \"ERROR_TYPE_CLASS\") and hasattr(cls, \"ERROR_TYPE_FIELD\"):\n            typed_errors = [\n                cls.ERROR_TYPE_CLASS(field=e.field, message=e.message, code=code)\n                for e, code in errors\n            ]\n            extra.update({cls.ERROR_TYPE_FIELD: typed_errors})\n        return cls(errors=[e[0] for e in errors], **extra)\n```\n\nOriginal documentation:\n```python\nReturn class instance with errors.\n\n        Should be overridden in mutation that supports typed errors.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return class instance with errors.", "file": "saleor_graphql_core_mutations.py", "function": "BaseMutation.handle_typed_errors", "project": "saleor", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef na_arithmetic_op(left, right, op, str_rep: str):\n    \n    \n    \n    import pandas.core.computation.expressions as expressions\n\n    try:\n        result = expressions.evaluate(op, str_rep, left, right)\n    except TypeError:\n        result = masked_arith_op(left, right, op)\n\n    return missing.dispatch_fill_zeros(op, left, right, result)```\n\nUpdated code:\n```python\ndef na_arithmetic_op(left, right, op, str_rep: Optional[str], is_cmp: bool = False):\n    \n    \n    \n    import pandas.core.computation.expressions as expressions\n\n    try:\n        result = expressions.evaluate(op, str_rep, left, right)\n    except TypeError:\n        if is_cmp:\n            # numexpr failed on comparison op, e.g. ndarray[float] > datetime\n            #  In this case we do not fall back to the masked op, as that\n            #  will handle complex numbers incorrectly, see GH#32047\n            raise\n        result = masked_arith_op(left, right, op)\n\n    if is_cmp and (is_scalar(result) or result is NotImplemented):\n        # numpy returned a scalar instead of operating element-wise\n        # e.g. numeric array vs str\n        return invalid_comparison(left, right, op)\n\n    return missing.dispatch_fill_zeros(op, left, right, result)\n```\n\nOriginal documentation:\n```python\nReturn the result of evaluating op on the passed in values.\n\n    If native types are not compatible, try coersion to object dtype.\n\n    Parameters\n    ----------\n    left : np.ndarray\n    right : np.ndarray or scalar\n    str_rep : str or None\n\n    Returns\n    -------\n    array-like\n\n    Raises\n    ------\n    TypeError : invalid operation\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the result of evaluating op on the passed in values.\n\n    If native types are not compatible, try coersion to object dtype.\n\n    Parameters\n    ----------\n    left : np.ndarray\n    right : np.ndarray or scalar\n    str_rep : str or None\n    is_cmp : bool, default False\n        If this a comparison operation.\n\n    Returns\n    -------\n    array-like\n\n    Raises\n    ------\n    TypeError : invalid operation", "file": "pandas_core_ops_array_ops.py", "function": "na_arithmetic_op", "project": "pandas", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        vocab: Vocab,\n        name: str = \"attribute_ruler\",\n        *,\n        pattern_dicts: Optional[Iterable[AttributeRulerPatternType]] = None,\n        validate: bool = False,\n    ) -> None:\n        \n        \n        self.name = name\n        self.vocab = vocab\n        self.matcher = Matcher(self.vocab, validate=validate)\n        self.attrs = []\n        self._attrs_unnormed = []  # store for reference\n        self.indices = []```\n\nUpdated code:\n```python\ndef __init__(\n        self, vocab: Vocab, name: str = \"attribute_ruler\", *, validate: bool = False\n    ) -> None:\n        \n        \n        self.name = name\n        self.vocab = vocab\n        self.matcher = Matcher(self.vocab, validate=validate)\n        self.attrs = []\n        self._attrs_unnormed = []  # store for reference\n        self.indices = []\n```\n\nOriginal documentation:\n```python\nCreate the AttributeRuler. After creation, you can add patterns\n        with the `.initialize()` or `.add_patterns()` methods, or load patterns\n        with `.from_bytes()` or `.from_disk()`. Loading patterns will remove\n        any patterns you've added previously.\n\n        vocab (Vocab): The vocab.\n        name (str): The pipe name. Defaults to \"attribute_ruler\".\n        \n        RETURNS (AttributeRuler): The AttributeRuler component.\n\n        DOCS: https://nightly.spacy.io/api/attributeruler#init\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create the AttributeRuler. After creation, you can add patterns\n        with the `.initialize()` or `.add_patterns()` methods, or load patterns\n        with `.from_bytes()` or `.from_disk()`. Loading patterns will remove\n        any patterns you've added previously.\n\n        vocab (Vocab): The vocab.\n        name (str): The pipe name. Defaults to \"attribute_ruler\".\n\n        RETURNS (AttributeRuler): The AttributeRuler component.\n\n        DOCS: https://nightly.spacy.io/api/attributeruler#init", "file": "spacy_pipeline_attributeruler.py", "function": "AttributeRuler.__init__", "project": "spaCy", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_valuation_rate(self, args):\n\t\t \t\n\t\t\n\t\tfrom erpnext.stock.utils import get_incoming_rate\n\t\tdt = self.costing_date or nowdate()\n\t\ttime = self.costing_date == nowdate() and now().split()[1] or '23:59'\n\t\twarehouse = frappe.db.sql(\"select warehouse from `tabBin` where item_code = %s\", args['item_code'])\n\t\trate = []\n\t\tfor wh in warehouse:\n\t\t\tr = get_incoming_rate({\n\t\t\t\t\"item_code\": args.get(\"item_code\"),\n\t\t\t\t\"warehouse\": wh[0],\n\t\t\t\t\"posting_date\": dt,\n\t\t\t\t\"posting_time\": time,\n\t\t\t\t\"qty\": args.get(\"qty\") or 0\n\t\t\t})\n\t\t\tif r:\n\t\t\t\trate.append(r)\n\n\t\treturn rate and flt(sum(rate))/len(rate) or 0```\n\nUpdated code:\n```python\ndef get_valuation_rate(self, args):\n\t\t \n\t\t\n\t\tfrom erpnext.stock.utils import get_incoming_rate\n\t\tdt = self.costing_date or nowdate()\n\t\ttime = self.costing_date == nowdate() and now().split()[1] or '23:59'\n\t\twarehouse = frappe.db.sql(\"select warehouse from `tabBin` where item_code = %s\", args['item_code'])\n\t\trate = []\n\t\tfor wh in warehouse:\n\t\t\tr = get_incoming_rate({\n\t\t\t\t\"item_code\": args.get(\"item_code\"),\n\t\t\t\t\"warehouse\": wh[0],\n\t\t\t\t\"posting_date\": dt,\n\t\t\t\t\"posting_time\": time,\n\t\t\t\t\"qty\": args.get(\"qty\") or 0\n\t\t\t})\n\t\t\tif r:\n\t\t\t\trate.append(r)\n\n\t\treturn rate and flt(sum(rate))/len(rate) or 0\n```\n\nOriginal documentation:\n```python\nGet average valuation rate of relevant warehouses \n\t\t\tas per valuation method (MAR/FIFO) \n\t\t\tas on costing date\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get average valuation rate of relevant warehouses\n\t\t\tas per valuation method (MAR/FIFO)\n\t\t\tas on costing date", "file": "erpnext_manufacturing_doctype_bom_bom.py", "function": "BOM.get_valuation_rate", "project": "erpnext", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef query(self, *tags, **kwtags):\n        \n        \n        \n        opts = super(SequenceDB, self).query(*tags, **kwtags)\n\n        position_cutoff = kwtags.pop('position_cutoff', config.optdb.position_cutoff)\n        if len(tags)>=1 and isinstance(tags[0],Query):\n#the call to super should have raise an error with a good message\n            assert len(tags)==1\n            if getattr(tags[0],'position_cutoff', None):\n                position_cutoff = tags[0].position_cutoff\n\n        opts = [o for o in opts if self.__position__[o.name] < position_cutoff]\n        opts.sort(key = lambda obj: self.__position__[obj.name])\n        return opt.SeqOptimizer(opts, failure_callback = self.failure_callback)```\n\nUpdated code:\n```python\ndef query(self, *tags, **kwtags):\n        \n        \n        \n        opts = super(SequenceDB, self).query(*tags, **kwtags)\n\n        position_cutoff = kwtags.pop('position_cutoff',\n                                     config.optdb.position_cutoff)\n        if len(tags) >= 1 and isinstance(tags[0], Query):\n#the call to super should have raise an error with a good message\n            assert len(tags) == 1\n            if getattr(tags[0], 'position_cutoff', None):\n                position_cutoff = tags[0].position_cutoff\n\n        opts = [o for o in opts if self.__position__[o.name] < position_cutoff]\n        opts.sort(key=lambda obj: self.__position__[obj.name])\n        return opt.SeqOptimizer(opts, failure_callback=self.failure_callback)\n```\n\nOriginal documentation:\n```python\n:type position_cutoff: float or int\n        :param position_cutoff: only optimizations with position less than the cutoff are returned.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":type position_cutoff: float or int\n        :param position_cutoff: only optimizations with position less than\n                                the cutoff are returned.", "file": "theano_gof_optdb.py", "function": "SequenceDB.query", "project": "Theano", "change_stats": {"code_changes": 15, "doc_changes": 5, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_freqmismatch(self):\n        \n        \n        \n        with self.assertRaises(ValueError):\n            rf.OpenShort(self.open, self.short_1f)\n        \n        with self.assertRaises(ValueError):\n            self.dm_os.deembed(self.raw)```\n\nUpdated code:\n```python\ndef test_freqmismatch(self):\n        \n        \n        \n        with self.assertWarns(RuntimeWarning):\n            rf.OpenShort(self.open, self.short[0:len(self.short)//2])\n        \n        with self.assertWarns(RuntimeWarning):\n            self.dm.deembed(self.raw[0:len(self.raw)//2])\n```\n\nOriginal documentation:\n```python\nCheck that error is caught when networks are of different frequencies\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check that a warning is caught when networks are of different frequencies", "file": "skrf_calibration_tests_test_deembedding.py", "function": "DeembeddingTestCase.test_freqmismatch", "project": "scikit-rf", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _write(self, file_obj: BinaryIO, batch_size: int, encoding: str = \"utf-8\", **to_json_kwargs) -> int:\n        \n        \n        written = 0\n        _ = to_json_kwargs.pop(\"path_or_buf\", None)\n\n        for offset in range(0, len(self.dataset), batch_size):\n            batch = query_table(\n                table=self.dataset.data,\n                key=slice(offset, offset + batch_size),\n                indices=self.dataset._indices if self.dataset._indices is not None else None,\n            )\n            json_str = batch.to_pandas().to_json(path_or_buf=None, **to_json_kwargs)\n            written += file_obj.write(json_str.encode(encoding))\n        return written```\n\nUpdated code:\n```python\ndef _write(\n        self,\n        file_obj: BinaryIO,\n        batch_size: int,\n        encoding: str = \"utf-8\",\n        orient=\"records\",\n        lines=True,\n        **to_json_kwargs,\n    ) -> int:\n        \n        \n        written = 0\n        _ = to_json_kwargs.pop(\"path_or_buf\", None)\n\n        for offset in tqdm(range(0, len(self.dataset), batch_size)):\n            batch = query_table(\n                table=self.dataset.data,\n                key=slice(offset, offset + batch_size),\n                indices=self.dataset._indices if self.dataset._indices is not None else None,\n            )\n            json_str = batch.to_pandas().to_json(path_or_buf=None, orient=orient, lines=lines, **to_json_kwargs)\n            written += file_obj.write(json_str.encode(encoding))\n        return written\n```\n\nOriginal documentation:\n```python\nWrites the pyarrow table as JSON to a binary file handle.\n\n        Caller is responsible for opening and closing the handle.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Writes the pyarrow table as JSON lines to a binary file handle.\n\n        Caller is responsible for opening and closing the handle.", "file": "src_datasets_io_json.py", "function": "JsonDatasetWriter._write", "project": "datasets", "change_stats": {"code_changes": 16, "doc_changes": 4, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _ash_enter_from_map(self, skip_first_screenshot=True):\n        \n        \n        \n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if self.appear(IN_MAP, interval=2):\n                self.device.click(ASH_COLLECT_STATUS)\n                continue\n            if self.appear_then_click(ASH_ENTER_CONFIRM, offset=(20, 20), interval=2):\n                continue\n            if self.appear_then_click(BEACON_ENTER, offset=(20, 20), interval=2):\n                continue\n\n            # End:\n            if self.appear(ASH_START, offset=(30, 30)):\n                break```\n\nUpdated code:\n```python\ndef _ash_enter_from_map(self, skip_first_screenshot=True):\n        \n        \n        \n        in_map_timeout = Timer(2, count=3)\n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if in_map_timeout.reached() and self.is_in_map():\n                self.device.click(ASH_COLLECT_STATUS)\n                in_map_timeout.reset()\n                continue\n            if self.appear_then_click(ASH_ENTER_CONFIRM, offset=(20, 20), interval=2):\n                continue\n            if self.appear_then_click(BEACON_ENTER, offset=(20, 20), interval=2):\n                continue\n\n            # End:\n            if self.appear(ASH_START, offset=(30, 30)):\n                break\n```\n\nOriginal documentation:\n```python\nPages:\n            in: IN_MAP\n            out: is_in_ash\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Pages:\n            in: is_in_map\n            out: is_in_ash", "file": "module_os_ash_ash.py", "function": "OSAsh._ash_enter_from_map", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef identify_repo(repo_url):\n    \n    \n    \n    repo_url_values = repo_url.split('+')\n    if len(repo_url_values) == 2:\n        repo_type = repo_url_values[0]\n        if repo_type in [\"git\", \"hg\"]:\n            return repo_type, repo_url_values[1]\n        else:\n            raise UnknownRepoType\n    else:\n        if \"git\" in repo_url:\n            return \"git\", repo_url\n        elif \"bitbucket\" in repo_url:\n            return \"hg\", repo_url\n        else:\n            raise UnknownRepoType```\n\nUpdated code:\n```python\ndef identify_repo(repo_url):\n    \n    \n    \n    repo_url_values = repo_url.split('+')\n    if len(repo_url_values) == 2:\n        repo_type = repo_url_values[0]\n        if repo_type in [\"git\", \"hg\"]:\n            return repo_type, repo_url_values[1]\n        else:\n            raise UnknownRepoType\n    else:\n        if 'git' in repo_url:\n            return 'git', repo_url\n        elif 'bitbucket' in repo_url:\n            return 'hg', repo_url\n        else:\n            raise UnknownRepoType\n```\n\nOriginal documentation:\n```python\nDetermines if `repo_url` should be treated as a URL to a git or hg repo.\n    Repos can be identified prepeding \"hg+\" or \"git+\" to repo URL.\n\n    :param repo_url: Repo URL of unknown type.\n    :returns: (\"git\", repo_url), (\"hg\", repo_url), or None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Determines if `repo_url` should be treated as a URL to a git or hg repo.\n    Repos can be identified by prepending \"hg+\" or \"git+\" to the repo URL.\n\n    :param repo_url: Repo URL of unknown type.\n    :returns: ('git', repo_url), ('hg', repo_url), or None.", "file": "cookiecutter_vcs.py", "function": "identify_repo", "project": "cookiecutter", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_var(var_name):\n        \n        \n        \n        return SCacheton.cachevars.get(var_name, None)```\n\nUpdated code:\n```python\ndef get_var(key):\n        \n        \n        \n        return SCacheton.cachevars.get(key, None)\n```\n\nOriginal documentation:\n```python\nGet data cached by var_name\n        All cached data will be freed with cleanup()\n\n        You must not to store Tensor in SCacheton, use per-device cache vars\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get data cached by key\n        All cached data will be freed with cleanup()\n\n        You must not to store Tensor in SCacheton, use per-device cache vars", "file": "xlib_avecl__internal_SCacheton.py", "function": "SCacheton.get_var", "project": "DeepFaceLive", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef graph(model, args, verbose=False):\n    \n    \n    \n    with torch.onnx.select_model_mode_for_export(model, torch.onnx.TrainingMode.EVAL):  # TODO: move outside of torch.onnx?\n        try:\n            trace = torch.jit.trace(model, args)\n            graph = trace.graph\n            torch._C._jit_pass_inline(graph)\n        except RuntimeError as e:\n            print(e)\n            print('Error occurs, No graph saved')\n            raise e\n\n    if verbose:\n        print(graph)\n    list_of_nodes = parse(graph, trace, args)\n    # We are hardcoding that this was run on CPU even though it might have actually\n    # run on GPU. Note this is what is shown in TensorBoard and has no bearing\n    # on actual execution.\n    # TODO: See if we can extract GPU vs CPU information from the PyTorch model\n    # and pass it correctly to TensorBoard.\n    #\n    # Definition of StepStats and DeviceStepStats can be found at\n    # https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/graph/tf_graph_common/test/graph-test.ts\n    # and\n    # https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/step_stats.proto\n    stepstats = RunMetadata(step_stats=StepStats(dev_stats=[DeviceStepStats(device=\"/device:CPU:0\")]))\n    return GraphDef(node=list_of_nodes, versions=VersionDef(producer=22)), stepstats```\n\nUpdated code:\n```python\ndef graph(model, args, verbose=False, use_strict_trace=True):\n    \n    \n    \n    with torch.onnx.select_model_mode_for_export(model, torch.onnx.TrainingMode.EVAL):  # TODO: move outside of torch.onnx?\n        try:\n            trace = torch.jit.trace(model, args, strict=use_strict_trace)\n            graph = trace.graph\n            torch._C._jit_pass_inline(graph)\n        except RuntimeError as e:\n            print(e)\n            print('Error occurs, No graph saved')\n            raise e\n\n    if verbose:\n        print(graph)\n    list_of_nodes = parse(graph, trace, args)\n    # We are hardcoding that this was run on CPU even though it might have actually\n    # run on GPU. Note this is what is shown in TensorBoard and has no bearing\n    # on actual execution.\n    # TODO: See if we can extract GPU vs CPU information from the PyTorch model\n    # and pass it correctly to TensorBoard.\n    #\n    # Definition of StepStats and DeviceStepStats can be found at\n    # https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/graph/tf_graph_common/test/graph-test.ts\n    # and\n    # https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/step_stats.proto\n    stepstats = RunMetadata(step_stats=StepStats(dev_stats=[DeviceStepStats(device=\"/device:CPU:0\")]))\n    return GraphDef(node=list_of_nodes, versions=VersionDef(producer=22)), stepstats\n```\n\nOriginal documentation:\n```python\nThis method processes a PyTorch model and produces a `GraphDef` proto\n    that can be logged to TensorBoard.\n\n    Args:\n      model (PyTorch module): The model to be parsed.\n      args (tuple): input tensor[s] for the model.\n      verbose (bool): Whether to print out verbose information while\n        processing.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method processes a PyTorch model and produces a `GraphDef` proto\n    that can be logged to TensorBoard.\n\n    Args:\n      model (PyTorch module): The model to be parsed.\n      args (tuple): input tensor[s] for the model.\n      verbose (bool): Whether to print out verbose information while\n        processing.\n      use_strict_trace (bool): Whether to pass keyword argument `strict` to\n        `torch.jit.trace`. Pass False when you want the tracer to\n        record your mutable container types (list, dict)", "file": "torch_utils_tensorboard__pytorch_graph.py", "function": "graph", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lock_pair(pair: str, until: datetime, reason: str = None, *,\n                  now: datetime = None) -> PairLock:\n        \n        \n        \n        lock = PairLock(\n            pair=pair,\n            lock_time=now or datetime.now(timezone.utc),\n            lock_end_time=timeframe_to_next_date(PairLocks.timeframe, until),\n            reason=reason,\n            active=True\n        )\n        if PairLocks.use_db:\n            PairLock.query.session.add(lock)\n            PairLock.query.session.commit()\n        else:\n            PairLocks.locks.append(lock)\n        return lock```\n\nUpdated code:\n```python\ndef lock_pair(pair: str, until: datetime, reason: str = None, *,\n                  now: datetime = None, side: str) -> PairLock:\n        \n        \n        \n        lock = PairLock(\n            pair=pair,\n            lock_time=now or datetime.now(timezone.utc),\n            lock_end_time=timeframe_to_next_date(PairLocks.timeframe, until),\n            reason=reason,\n            direction=side,\n            active=True\n        )\n        if PairLocks.use_db:\n            PairLock.query.session.add(lock)\n            PairLock.query.session.commit()\n        else:\n            PairLocks.locks.append(lock)\n        return lock\n```\n\nOriginal documentation:\n```python\nCreate PairLock from now to \"until\".\n        Uses database by default, unless PairLocks.use_db is set to False,\n        in which case a list is maintained.\n        :param pair: pair to lock. use '*' to lock all pairs\n        :param until: End time of the lock. Will be rounded up to the next candle.\n        :param reason: Reason string that will be shown as reason for the lock\n        :param now: Current timestamp. Used to determine lock start time.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create PairLock from now to \"until\".\n        Uses database by default, unless PairLocks.use_db is set to False,\n        in which case a list is maintained.\n        :param pair: pair to lock. use '*' to lock all pairs\n        :param until: End time of the lock. Will be rounded up to the next candle.\n        :param reason: Reason string that will be shown as reason for the lock\n        :param now: Current timestamp. Used to determine lock start time.\n        :param side: Side to lock pair, can be 'long', 'short' or '*'", "file": "freqtrade_persistence_pairlock_middleware.py", "function": "PairLocks.lock_pair", "project": "freqtrade", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef auto_search_combat(self):\n        \n        \n        \n        logger.info('Auto search combat loading')\n        self.device.screenshot_interval_set(self.config.Optimization_CombatScreenshotInterval)\n        while 1:\n            self.device.screenshot()\n\n            if self.handle_combat_automation_confirm():\n                continue\n\n            # End\n            if self.handle_os_auto_search_map_option():\n                break\n            if self.is_combat_executing():\n                break\n            if self.is_in_map():\n                break\n\n        logger.info('Auto Search combat execute')\n        self.submarine_call_reset()\n        submarine_mode = 'do_not_use'\n        if self.config.Submarine_Fleet:\n            submarine_mode = self.config.Submarine_Mode\n\n        success = True\n        while 1:\n            self.device.screenshot()\n\n            if self.handle_submarine_call(submarine_mode):\n                continue\n            if self.handle_os_auto_search_map_option(enable=success):\n                continue\n\n            # End\n            if self.is_combat_executing():\n                continue\n            if self.handle_auto_search_battle_status():\n                success = False\n                continue\n            if self.handle_auto_search_exp_info():\n                success = False\n                continue\n            if self.handle_map_event():\n                continue\n            if self.is_in_map():\n                self.device.screenshot_interval_set(0)\n                break\n\n        logger.info('Combat end.')\n        return success```\n\nUpdated code:\n```python\ndef auto_search_combat(self, drop=None):\n        \n        \n        \n        logger.info('Auto search combat loading')\n        self.device.screenshot_interval_set(self.config.Optimization_CombatScreenshotInterval)\n        while 1:\n            self.device.screenshot()\n\n            if self.handle_combat_automation_confirm():\n                continue\n\n            # End\n            if self.handle_os_auto_search_map_option(drop=drop):\n                break\n            if self.is_combat_executing():\n                break\n            if self.is_in_map():\n                break\n\n        logger.info('Auto Search combat execute')\n        self.submarine_call_reset()\n        submarine_mode = 'do_not_use'\n        if self.config.Submarine_Fleet:\n            submarine_mode = self.config.Submarine_Mode\n\n        success = True\n        while 1:\n            self.device.screenshot()\n\n            if self.handle_submarine_call(submarine_mode):\n                continue\n            if self.handle_os_auto_search_map_option(drop=drop, enable=success):\n                continue\n\n            # End\n            if self.is_combat_executing():\n                continue\n            if self.handle_auto_search_battle_status():\n                success = False\n                continue\n            if self.handle_auto_search_exp_info():\n                success = False\n                continue\n            if self.handle_map_event():\n                continue\n            if self.is_in_map():\n                self.device.screenshot_interval_set(0)\n                break\n\n        logger.info('Combat end.')\n        return success\n```\n\nOriginal documentation:\n```python\nReturns:\n            bool: True if enemy cleared, False if fleet died.\n\n        Pages:\n            in: is_combat_loading()\n            out: combat status\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            drop (DropImage):\n\n        Returns:\n            bool: True if enemy cleared, False if fleet died.\n\n        Pages:\n            in: is_combat_loading()\n            out: combat status", "file": "module_os_combat_combat.py", "function": "Combat.auto_search_combat", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_datetime(\n    arg,\n    errors=\"raise\",\n    dayfirst=False,\n    yearfirst=False,\n    utc=None,\n    format=None,\n    exact=True,\n    unit=None,\n    infer_datetime_format=False,\n    origin=\"unix\",\n    cache=True,\n):\n    \n    \n    \n    if arg is None:\n        return None\n\n    if origin != \"unix\":\n        arg = _adjust_to_origin(arg, origin, unit)\n\n    tz = \"utc\" if utc else None\n    convert_listlike = partial(\n        _convert_listlike_datetimes,\n        tz=tz,\n        unit=unit,\n        dayfirst=dayfirst,\n        yearfirst=yearfirst,\n        errors=errors,\n        exact=exact,\n        infer_datetime_format=infer_datetime_format,\n    )\n\n    if isinstance(arg, Timestamp):\n        result = arg\n        if tz is not None:\n            if arg.tz is not None:\n                result = result.tz_convert(tz)\n            else:\n                result = result.tz_localize(tz)\n    elif isinstance(arg, ABCSeries):\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\n        if not cache_array.empty:\n            result = arg.map(cache_array)\n        else:\n            values = convert_listlike(arg._values, format)\n            result = arg._constructor(values, index=arg.index, name=arg.name)\n    elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):\n        result = _assemble_from_unit_mappings(arg, errors, tz)\n    elif isinstance(arg, ABCIndexClass):\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\n        if not cache_array.empty:\n            result = _convert_and_box_cache(arg, cache_array, name=arg.name)\n        else:\n            convert_listlike = partial(convert_listlike, name=arg.name)\n            result = convert_listlike(arg, format)\n    elif is_list_like(arg):\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\n        if not cache_array.empty:\n            result = _convert_and_box_cache(arg, cache_array)\n        else:\n            result = convert_listlike(arg, format)\n    else:\n        result = convert_listlike(np.array([arg]), format)[0]\n\n    return result```\n\nUpdated code:\n```python\ndef to_datetime(\n    arg,\n    errors=\"raise\",\n    dayfirst=False,\n    yearfirst=False,\n    utc=None,\n    format=None,\n    exact=True,\n    unit=None,\n    infer_datetime_format=False,\n    origin=\"unix\",\n    cache=True,\n):\n    \n    \n    \n    if arg is None:\n        return None\n\n    if origin != \"unix\":\n        arg = _adjust_to_origin(arg, origin, unit)\n\n    tz = \"utc\" if utc else None\n    convert_listlike = partial(\n        _convert_listlike_datetimes,\n        tz=tz,\n        unit=unit,\n        dayfirst=dayfirst,\n        yearfirst=yearfirst,\n        errors=errors,\n        exact=exact,\n        infer_datetime_format=infer_datetime_format,\n    )\n\n    if isinstance(arg, Timestamp):\n        result = arg\n        if tz is not None:\n            if arg.tz is not None:\n                result = result.tz_convert(tz)\n            else:\n                result = result.tz_localize(tz)\n    elif isinstance(arg, ABCSeries):\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\n        if not cache_array.empty:\n            result = arg.map(cache_array)\n        else:\n            values = convert_listlike(arg._values, format)\n            result = arg._constructor(values, index=arg.index, name=arg.name)\n    elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):\n        result = _assemble_from_unit_mappings(arg, errors, tz)\n    elif isinstance(arg, ABCIndexClass):\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\n        if not cache_array.empty:\n            result = _convert_and_box_cache(arg, cache_array, name=arg.name)\n        else:\n            convert_listlike = partial(convert_listlike, name=arg.name)\n            result = convert_listlike(arg, format)\n    elif is_list_like(arg):\n        try:\n            cache_array = _maybe_cache(arg, format, cache, convert_listlike)\n        except tslibs.OutOfBoundsDatetime:\n            # caching attempts to create a DatetimeIndex, which may raise\n            # an OOB. If that's the desired behavior, then just reraise...\n            if errors == \"raise\":\n                raise\n            # ... otherwise, continue without the cache.\n            from pandas import Series\n\n            cache_array = Series([], dtype=object)  # just an empty array\n        if not cache_array.empty:\n            result = _convert_and_box_cache(arg, cache_array)\n        else:\n            result = convert_listlike(arg, format)\n    else:\n        result = convert_listlike(np.array([arg]), format)[0]\n\n    return result\n```\n\nOriginal documentation:\n```python\nConvert argument to datetime.\n\n    Parameters\n    ----------\n    arg : int, float, str, datetime, list, tuple, 1-d array, Series DataFrame/dict-like\n        The object to convert to a datetime.\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n        - If 'raise', then invalid parsing will raise an exception.\n        - If 'coerce', then invalid parsing will be set as NaT.\n        - If 'ignore', then invalid parsing will return the input.\n    dayfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n        If True, parses dates with the day first, eg 10/11/12 is parsed as\n        2012-11-10.\n        Warning: dayfirst=True is not strict, but will prefer to parse\n        with day first (this is a known bug, based on dateutil behavior).\n    yearfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n\n        - If True parses dates with the year first, eg 10/11/12 is parsed as\n          2010-11-12.\n        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\n          as dateutil).\n\n        Warning: yearfirst=True is not strict, but will prefer to parse\n        with year first (this is a known bug, based on dateutil behavior).\n    utc : bool, default None\n        Return UTC DatetimeIndex if True (converting any tz-aware\n        datetime.datetime objects as well).\n    format : str, default None\n        The strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\n        all the way up to nanoseconds.\n        See strftime documentation for more information on choices:\n        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n    exact : bool, True by default\n        Behaves as:\n        - If True, require an exact format match.\n        - If False, allow the format to match anywhere in the target string.\n\n    unit : str, default 'ns'\n        The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n        integer or float number. This will be based off the origin.\n        Example, with unit='ms' and origin='unix' (the default), this\n        would calculate the number of milliseconds to the unix epoch start.\n    infer_datetime_format : bool, default False\n        If True and no `format` is given, attempt to infer the format of the\n        datetime strings, and if it can be inferred, switch to a faster\n        method of parsing them. In some cases this can increase the parsing\n        speed by ~5-10x.\n    origin : scalar, default 'unix'\n        Define the reference date. The numeric values would be parsed as number\n        of units (defined by `unit`) since this reference date.\n\n        - If 'unix' (or POSIX) time; origin is set to 1970-01-01.\n        - If 'julian', unit must be 'D', and origin is set to beginning of\n          Julian Calendar. Julian day number 0 is assigned to the day starting\n          at noon on January 1, 4713 BC.\n        - If Timestamp convertible, origin is set to Timestamp identified by\n          origin.\n    cache : bool, default True\n        If True, use a cache of unique, converted dates to apply the datetime\n        conversion. May produce significant speed-up when parsing duplicate\n        date strings, especially ones with timezone offsets.\n\n        .. versionadded:: 0.23.0\n\n        .. versionchanged:: 0.25.0\n            - changed default value from False to True.\n\n    Returns\n    -------\n    datetime\n        If parsing succeeded.\n        Return type depends on input:\n\n        - list-like: DatetimeIndex\n        - Series: Series of datetime64 dtype\n        - scalar: Timestamp\n\n        In case when it is not possible to return designated types (e.g. when\n        any element of input is before Timestamp.min or after Timestamp.max)\n        return will have datetime.datetime type (or corresponding\n        array/Series).\n\n    See Also\n    --------\n    DataFrame.astype : Cast argument to a specified dtype.\n    to_timedelta : Convert argument to timedelta.\n    convert_dtypes : Convert dtypes.\n\n    Examples\n    --------\n    Assembling a datetime from multiple columns of a DataFrame. The keys can be\n    common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n    'ms', 'us', 'ns']) or plurals of the same\n\n    >>> df = pd.DataFrame({'year': [2015, 2016],\n    ...                    'month': [2, 3],\n    ...                    'day': [4, 5]})\n    >>> pd.to_datetime(df)\n    0   2015-02-04\n    1   2016-03-05\n    dtype: datetime64[ns]\n\n    If a date does not meet the `timestamp limitations\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n    #timeseries-timestamp-limits>`_, passing errors='ignore'\n    will return the original input instead of raising any exception.\n\n    Passing errors='coerce' will force an out-of-bounds date to NaT,\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\n\n    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore')\n    datetime.datetime(1300, 1, 1, 0, 0)\n    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\n    NaT\n\n    Passing infer_datetime_format=True can often-times speedup a parsing\n    if its not an ISO8601 format exactly, but in a regular format.\n\n    >>> s = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n    >>> s.head()\n    0    3/11/2000\n    1    3/12/2000\n    2    3/13/2000\n    3    3/11/2000\n    4    3/12/2000\n    dtype: object\n\n    >>> %timeit pd.to_datetime(s, infer_datetime_format=True)  # doctest: +SKIP\n    100 loops, best of 3: 10.4 ms per loop\n\n    >>> %timeit pd.to_datetime(s, infer_datetime_format=False)  # doctest: +SKIP\n    1 loop, best of 3: 471 ms per loop\n\n    Using a unix epoch time\n\n    >>> pd.to_datetime(1490195805, unit='s')\n    Timestamp('2017-03-22 15:16:45')\n    >>> pd.to_datetime(1490195805433502912, unit='ns')\n    Timestamp('2017-03-22 15:16:45.433502912')\n\n    .. warning:: For float arg, precision rounding might happen. To prevent\n        unexpected behavior use a fixed-width exact type.\n\n    Using a non-unix epoch origin\n\n    >>> pd.to_datetime([1, 2, 3], unit='D',\n    ...                origin=pd.Timestamp('1960-01-01'))\n    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], \\\ndtype='datetime64[ns]', freq=None)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert argument to datetime.\n\n    Parameters\n    ----------\n    arg : int, float, str, datetime, list, tuple, 1-d array, Series DataFrame/dict-like\n        The object to convert to a datetime.\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n        - If 'raise', then invalid parsing will raise an exception.\n        - If 'coerce', then invalid parsing will be set as NaT.\n        - If 'ignore', then invalid parsing will return the input.\n    dayfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n        If True, parses dates with the day first, eg 10/11/12 is parsed as\n        2012-11-10.\n        Warning: dayfirst=True is not strict, but will prefer to parse\n        with day first (this is a known bug, based on dateutil behavior).\n    yearfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n\n        - If True parses dates with the year first, eg 10/11/12 is parsed as\n          2010-11-12.\n        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\n          as dateutil).\n\n        Warning: yearfirst=True is not strict, but will prefer to parse\n        with year first (this is a known bug, based on dateutil behavior).\n    utc : bool, default None\n        Return UTC DatetimeIndex if True (converting any tz-aware\n        datetime.datetime objects as well).\n    format : str, default None\n        The strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\n        all the way up to nanoseconds.\n        See strftime documentation for more information on choices:\n        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n    exact : bool, True by default\n        Behaves as:\n        - If True, require an exact format match.\n        - If False, allow the format to match anywhere in the target string.\n\n    unit : str, default 'ns'\n        The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n        integer or float number. This will be based off the origin.\n        Example, with unit='ms' and origin='unix' (the default), this\n        would calculate the number of milliseconds to the unix epoch start.\n    infer_datetime_format : bool, default False\n        If True and no `format` is given, attempt to infer the format of the\n        datetime strings, and if it can be inferred, switch to a faster\n        method of parsing them. In some cases this can increase the parsing\n        speed by ~5-10x.\n    origin : scalar, default 'unix'\n        Define the reference date. The numeric values would be parsed as number\n        of units (defined by `unit`) since this reference date.\n\n        - If 'unix' (or POSIX) time; origin is set to 1970-01-01.\n        - If 'julian', unit must be 'D', and origin is set to beginning of\n          Julian Calendar. Julian day number 0 is assigned to the day starting\n          at noon on January 1, 4713 BC.\n        - If Timestamp convertible, origin is set to Timestamp identified by\n          origin.\n    cache : bool, default True\n        If True, use a cache of unique, converted dates to apply the datetime\n        conversion. May produce significant speed-up when parsing duplicate\n        date strings, especially ones with timezone offsets. The cache is only\n        used when there are at least 50 values. The presence of out-of-bounds\n        values will render the cache unusable and may slow down parsing.\n\n        .. versionadded:: 0.23.0\n\n        .. versionchanged:: 0.25.0\n            - changed default value from False to True.\n\n    Returns\n    -------\n    datetime\n        If parsing succeeded.\n        Return type depends on input:\n\n        - list-like: DatetimeIndex\n        - Series: Series of datetime64 dtype\n        - scalar: Timestamp\n\n        In case when it is not possible to return designated types (e.g. when\n        any element of input is before Timestamp.min or after Timestamp.max)\n        return will have datetime.datetime type (or corresponding\n        array/Series).\n\n    See Also\n    --------\n    DataFrame.astype : Cast argument to a specified dtype.\n    to_timedelta : Convert argument to timedelta.\n    convert_dtypes : Convert dtypes.\n\n    Examples\n    --------\n    Assembling a datetime from multiple columns of a DataFrame. The keys can be\n    common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n    'ms', 'us', 'ns']) or plurals of the same\n\n    >>> df = pd.DataFrame({'year': [2015, 2016],\n    ...                    'month': [2, 3],\n    ...                    'day': [4, 5]})\n    >>> pd.to_datetime(df)\n    0   2015-02-04\n    1   2016-03-05\n    dtype: datetime64[ns]\n\n    If a date does not meet the `timestamp limitations\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n    #timeseries-timestamp-limits>`_, passing errors='ignore'\n    will return the original input instead of raising any exception.\n\n    Passing errors='coerce' will force an out-of-bounds date to NaT,\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\n\n    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore')\n    datetime.datetime(1300, 1, 1, 0, 0)\n    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\n    NaT\n\n    Passing infer_datetime_format=True can often-times speedup a parsing\n    if its not an ISO8601 format exactly, but in a regular format.\n\n    >>> s = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n    >>> s.head()\n    0    3/11/2000\n    1    3/12/2000\n    2    3/13/2000\n    3    3/11/2000\n    4    3/12/2000\n    dtype: object\n\n    >>> %timeit pd.to_datetime(s, infer_datetime_format=True)  # doctest: +SKIP\n    100 loops, best of 3: 10.4 ms per loop\n\n    >>> %timeit pd.to_datetime(s, infer_datetime_format=False)  # doctest: +SKIP\n    1 loop, best of 3: 471 ms per loop\n\n    Using a unix epoch time\n\n    >>> pd.to_datetime(1490195805, unit='s')\n    Timestamp('2017-03-22 15:16:45')\n    >>> pd.to_datetime(1490195805433502912, unit='ns')\n    Timestamp('2017-03-22 15:16:45.433502912')\n\n    .. warning:: For float arg, precision rounding might happen. To prevent\n        unexpected behavior use a fixed-width exact type.\n\n    Using a non-unix epoch origin\n\n    >>> pd.to_datetime([1, 2, 3], unit='D',\n    ...                origin=pd.Timestamp('1960-01-01'))\n    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], \\\ndtype='datetime64[ns]', freq=None)", "file": "pandas_core_tools_datetimes.py", "function": "to_datetime", "project": "pandas", "change_stats": {"code_changes": 14, "doc_changes": 6, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        in_blocks: BlockList,\n        stats: DatastreamStats,\n        datastream_uuid=None,\n        *,\n        run_by_consumer: bool,\n    ):\n        \n        \n        self._in_blocks = in_blocks\n        self._in_stats = stats\n        # A computed snapshot of some prefix of stages.\n        self._snapshot_blocks = None\n        self._snapshot_stats = None\n        # Chains of stages.\n        self._stages_before_snapshot = []\n        self._stages_after_snapshot = []\n        # Cache of optimized stages.\n        self._last_optimized_stages = None\n\n        self._datastream_uuid = datastream_uuid or uuid.uuid4().hex\n        if not stats.datastream_uuid:\n            stats.datastream_uuid = self._datastream_uuid\n\n        self._run_by_consumer = run_by_consumer```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        in_blocks: BlockList,\n        stats: DatasetStats,\n        dataset_uuid=None,\n        *,\n        run_by_consumer: bool,\n    ):\n        \n        \n        self._in_blocks = in_blocks\n        self._in_stats = stats\n        # A computed snapshot of some prefix of stages.\n        self._snapshot_blocks = None\n        self._snapshot_stats = None\n        # Chains of stages.\n        self._stages_before_snapshot = []\n        self._stages_after_snapshot = []\n        # Cache of optimized stages.\n        self._last_optimized_stages = None\n\n        self._dataset_uuid = dataset_uuid or uuid.uuid4().hex\n        if not stats.dataset_uuid:\n            stats.dataset_uuid = self._dataset_uuid\n\n        self._run_by_consumer = run_by_consumer\n```\n\nOriginal documentation:\n```python\nCreate a plan with no transformation stages.\n\n        Args:\n            in_blocks: Base list of blocks.\n            stats: Stats for the base blocks.\n            datastream_uuid: Datastream's UUID.\n            run_by_consumer: Whether this plan is invoked to run by the consumption\n            APIs (e.g. .iter_batches()).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a plan with no transformation stages.\n\n        Args:\n            in_blocks: Base list of blocks.\n            stats: Stats for the base blocks.\n            dataset_uuid: Dataset's UUID.\n            run_by_consumer: Whether this plan is invoked to run by the consumption\n            APIs (e.g. .iter_batches()).", "file": "python_ray_data__internal_plan.py", "function": "ExecutionPlan.__init__", "project": "ray", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_normalized_url(self, trigger):\n        '''\n        \n        '''\n        return trigger['parameters']['url'].strip('/')```\n\nUpdated code:\n```python\ndef _get_normalized_url(self, trigger):\n        \n        \n        \n        return trigger['parameters']['url'].strip('/')\n```\n\nOriginal documentation:\n```python\nremove the trailing and leading \"/\" so that the hook url and those coming\n        from trigger parameters end up being the same.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "remove the trailing and leading / so that the hook url and those coming\n        from trigger parameters end up being the same.", "file": "st2api_st2api_controllers_v1_webhooks.py", "function": "WebhooksController._get_normalized_url", "project": "st2", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef tamper(payload, headers=None):\n\t\n    \n\t\n\n# ASCII table:\n#   TAB     09      horizontal TAB\n\tblanks = '%09'\n\tretVal = payload\n\n\tif payload:\n\t\tfor commands in ['SELECT','UPDATE','INSERT','DELETE']:\n\t\t\tretVal = retVal.replace(commands + ' ', commands + blanks)\n\t\tretVal = re.sub(r\"\\s*=\\s*\", lambda match: process(match), retVal)\n\n\treturn retVal```\n\nUpdated code:\n```python\ndef tamper(payload, headers=None):\n    \n    \n    \n\n    retVal = payload\n\n    if payload:\n        retVal = re.sub(r\"(?i)(SELECT|UPDATE|INSERT|DELETE)\\s+\", r\"\\g<1>\\t\", payload)\n        retVal = re.sub(r\"\\s*=\\s*\", \" LIKE \", retVal)\n\n    return retVal\n```\n\nOriginal documentation:\n```python\nFirst Replaces the space after 'select ' with a valid random blank character.\n\t\tThen replace = with like\n\n    Example:\n        * Input: SELECT id FROM users where id = 1\n        * Output: SELECT%09id FROM users where id like 1\n\n    Requirement:\n        * MySQL, Bluecoat SGos with Waf activated as documented in\n        https://kb.bluecoat.com/index?page=content&id=FAQ2147\n\n    Tested against:\n        * MySQL 5.1, SGos Rules\n\n    Notes:\n        * Useful to bypass BlueCoat recommanded Waf rule configuration\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replaces space character after SQL statement with a valid random blank character.\n    Afterwards replace character = with LIKE operator\n\n    Example:\n        * Input: SELECT id FROM users where id = 1\n        * Output: SELECT%09id FROM users where id LIKE 1\n\n    Requirement:\n        * MySQL, Blue Coat SGOS with WAF activated as documented in\n        https://kb.bluecoat.com/index?page=content&id=FAQ2147\n\n    Tested against:\n        * MySQL 5.1, SGOS\n\n    Notes:\n        * Useful to bypass Blue Coat's recommended WAF rule configuration", "file": "tamper_bluecoat.py", "function": "tamper", "project": "sqlmap", "change_stats": {"code_changes": 20, "doc_changes": 14, "total_changes": 34}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef subplots(self, *, sharex=False, sharey=False, squeeze=True,\n                 subplot_kw=None):\n        \n        \n        \n\n        figure = self[0, 0].get_topmost_subplotspec().get_gridspec().figure\n\n        if figure is None:\n            raise ValueError(\"GridSpec.subplots() only works for GridSpecs \"\n                             \"created with a parent figure\")\n\n        if isinstance(sharex, bool):\n            sharex = \"all\" if sharex else \"none\"\n        if isinstance(sharey, bool):\n            sharey = \"all\" if sharey else \"none\"\n        # This check was added because it is very easy to type\n        # `subplots(1, 2, 1)` when `subplot(1, 2, 1)` was intended.\n        # In most cases, no error will ever occur, but mysterious behavior\n        # will result because what was intended to be the subplot index is\n        # instead treated as a bool for sharex.\n        if isinstance(sharex, Integral):\n            cbook._warn_external(\n                \"sharex argument to subplots() was an integer.  Did you \"\n                \"intend to use subplot() (without 's')?\")\n        cbook._check_in_list([\"all\", \"row\", \"col\", \"none\"],\n                             sharex=sharex, sharey=sharey)\n        if subplot_kw is None:\n            subplot_kw = {}\n        # don't mutate kwargs passed by user...\n        subplot_kw = subplot_kw.copy()\n\n        # Create array to hold all axes.\n        axarr = np.empty((self._nrows, self._ncols), dtype=object)\n        for row in range(self._nrows):\n            for col in range(self._ncols):\n                shared_with = {\"none\": None, \"all\": axarr[0, 0],\n                               \"row\": axarr[row, 0], \"col\": axarr[0, col]}\n                subplot_kw[\"sharex\"] = shared_with[sharex]\n                subplot_kw[\"sharey\"] = shared_with[sharey]\n                axarr[row, col] = figure.add_subplot(\n                    self[row, col], **subplot_kw)\n\n        # turn off redundant tick labeling\n        if sharex in [\"col\", \"all\"]:\n            # turn off all but the bottom row\n            for ax in axarr[:-1, :].flat:\n                ax.xaxis.set_tick_params(which='both',\n                                         labelbottom=False, labeltop=False)\n                ax.xaxis.offsetText.set_visible(False)\n        if sharey in [\"row\", \"all\"]:\n            # turn off all but the first column\n            for ax in axarr[:, 1:].flat:\n                ax.yaxis.set_tick_params(which='both',\n                                         labelleft=False, labelright=False)\n                ax.yaxis.offsetText.set_visible(False)\n\n        if squeeze:\n            # Discarding unneeded dimensions that equal 1.  If we only have one\n            # subplot, just return it instead of a 1-element array.\n            return axarr.item() if axarr.size == 1 else axarr.squeeze()\n        else:\n            # Returned axis array will be always 2-d, even if nrows=ncols=1.\n            return axarr```\n\nUpdated code:\n```python\ndef subplots(self, sharex=False, sharey=False, squeeze=True,\n                 subplot_kw=None):\n        \n        \n        \n\n        figure = self[0, 0].get_topmost_subplotspec().get_gridspec().figure\n\n        if figure is None:\n            raise ValueError(\"GridSpec.subplots() only works for GridSpecs \"\n                             \"created with a parent figure\")\n\n        if isinstance(sharex, bool):\n            sharex = \"all\" if sharex else \"none\"\n        if isinstance(sharey, bool):\n            sharey = \"all\" if sharey else \"none\"\n        # This check was added because it is very easy to type\n        # `subplots(1, 2, 1)` when `subplot(1, 2, 1)` was intended.\n        # In most cases, no error will ever occur, but mysterious behavior\n        # will result because what was intended to be the subplot index is\n        # instead treated as a bool for sharex.\n        if isinstance(sharex, Integral):\n            cbook._warn_external(\n                \"sharex argument to subplots() was an integer.  Did you \"\n                \"intend to use subplot() (without 's')?\")\n        cbook._check_in_list([\"all\", \"row\", \"col\", \"none\"],\n                             sharex=sharex, sharey=sharey)\n        if subplot_kw is None:\n            subplot_kw = {}\n        # don't mutate kwargs passed by user...\n        subplot_kw = subplot_kw.copy()\n\n        # Create array to hold all axes.\n        axarr = np.empty((self._nrows, self._ncols), dtype=object)\n        for row in range(self._nrows):\n            for col in range(self._ncols):\n                shared_with = {\"none\": None, \"all\": axarr[0, 0],\n                               \"row\": axarr[row, 0], \"col\": axarr[0, col]}\n                subplot_kw[\"sharex\"] = shared_with[sharex]\n                subplot_kw[\"sharey\"] = shared_with[sharey]\n                axarr[row, col] = figure.add_subplot(\n                    self[row, col], **subplot_kw)\n\n        # turn off redundant tick labeling\n        if sharex in [\"col\", \"all\"]:\n            # turn off all but the bottom row\n            for ax in axarr[:-1, :].flat:\n                ax.xaxis.set_tick_params(which='both',\n                                         labelbottom=False, labeltop=False)\n                ax.xaxis.offsetText.set_visible(False)\n        if sharey in [\"row\", \"all\"]:\n            # turn off all but the first column\n            for ax in axarr[:, 1:].flat:\n                ax.yaxis.set_tick_params(which='both',\n                                         labelleft=False, labelright=False)\n                ax.yaxis.offsetText.set_visible(False)\n\n        if squeeze:\n            # Discarding unneeded dimensions that equal 1.  If we only have one\n            # subplot, just return it instead of a 1-element array.\n            return axarr.item() if axarr.size == 1 else axarr.squeeze()\n        else:\n            # Returned axis array will be always 2-d, even if nrows=ncols=1.\n            return axarr\n```\n\nOriginal documentation:\n```python\nAdd all subplots specified by this `GridSpec` to its parent figure.\n\n        This utility wrapper makes it convenient to create common layouts of\n        subplots in a single call.\n\n        Parameters\n        ----------\n        sharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n            Controls sharing of properties among x (*sharex*) or y (*sharey*)\n            axes:\n\n                - True or 'all': x- or y-axis will be shared among all\n                  subplots.\n                - False or 'none': each subplot x- or y-axis will be\n                  independent.\n                - 'row': each subplot row will share an x- or y-axis.\n                - 'col': each subplot column will share an x- or y-axis.\n\n            When subplots have a shared x-axis along a column, only the x tick\n            labels of the bottom subplot are created. Similarly, when subplots\n            have a shared y-axis along a row, only the y tick labels of the\n            first column subplot are created. To later turn other subplots'\n            ticklabels on, use `~matplotlib.axes.Axes.tick_params`.\n\n        squeeze : bool, optional, default: True\n            - If True, extra dimensions are squeezed out from the returned\n              array of Axes:\n\n                - if only one subplot is constructed (nrows=ncols=1), the\n                  resulting single Axes object is returned as a scalar.\n                - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n                  object array of Axes objects.\n                - for NxM, subplots with N>1 and M>1 are returned\n                  as a 2D array.\n\n            - If False, no squeezing at all is done: the returned Axes object\n              is always a 2D array containing Axes instances, even if it ends\n              up being 1x1.\n\n        subplot_kw : dict, optional\n            Dict with keywords passed to the\n            :meth:`~matplotlib.figure.Figure.add_subplot` call used to create\n            each subplot.\n\n        Returns\n        -------\n        ax : `~.axes.Axes` object or array of Axes objects.\n            *ax* can be either a single `~matplotlib.axes.Axes` object or\n            an array of Axes objects if more than one subplot was created. The\n            dimensions of the resulting array can be controlled with the\n            squeeze keyword, see above.\n\n        See Also\n        --------\n        .pyplot.subplots\n        .Figure.add_subplot\n        .pyplot.subplot\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add all subplots specified by this `GridSpec` to its parent figure.\n\n        This utility wrapper makes it convenient to create common layouts of\n        subplots in a single call.\n\n        Parameters\n        ----------\n        sharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n            Controls sharing of properties among x (*sharex*) or y (*sharey*)\n            axes:\n\n            - True or 'all': x- or y-axis will be shared among all subplots.\n            - False or 'none': each subplot x- or y-axis will be independent.\n            - 'row': each subplot row will share an x- or y-axis.\n            - 'col': each subplot column will share an x- or y-axis.\n\n            When subplots have a shared x-axis along a column, only the x tick\n            labels of the bottom subplot are created. Similarly, when subplots\n            have a shared y-axis along a row, only the y tick labels of the\n            first column subplot are created. To later turn other subplots'\n            ticklabels on, use `~matplotlib.axes.Axes.tick_params`.\n\n        squeeze : bool, optional, default: True\n            - If True, extra dimensions are squeezed out from the returned\n              array of Axes:\n\n              - if only one subplot is constructed (nrows=ncols=1), the\n                resulting single Axes object is returned as a scalar.\n              - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n                object array of Axes objects.\n              - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n            - If False, no squeezing at all is done: the returned Axes object\n              is always a 2D array containing Axes instances, even if it ends\n              up being 1x1.\n\n        subplot_kw : dict, optional\n            Dict with keywords passed to the `~.Figure.add_subplot` call used\n            to create each subplot.\n\n        Returns\n        -------\n        ax : `~.axes.Axes` object or array of Axes objects.\n            *ax* can be either a single `~matplotlib.axes.Axes` object or\n            an array of Axes objects if more than one subplot was created. The\n            dimensions of the resulting array can be controlled with the\n            squeeze keyword, see above.\n\n        See Also\n        --------\n        .pyplot.subplots\n        .Figure.add_subplot\n        .pyplot.subplot", "file": "lib_matplotlib_gridspec.py", "function": "GridSpecBase.subplots", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 28, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef local_minima(image, selem=None):\n    \n    \n    if np.issubdtype(image.dtype, 'half'):\n        # find the minimal grey level difference\n        h = _find_min_diff(image)\n    else:\n        h = 1\n    local_min = h_minima(image, h, selem=selem)\n    return local_min```\n\nUpdated code:\n```python\ndef local_minima(image, selem=None):\n    \n    \n    # find the minimal grey level difference\n    h = _find_min_diff(image)\n    if h == 0:\n        return np.zeros(image.shape, np.uint8)\n    if not np.issubdtype(image.dtype, 'half'):\n        h = 1\n    local_min = h_minima(image, h, selem=selem)\n    return local_min\n```\n\nOriginal documentation:\n```python\nDetermine all local minima of the image.\n\n    The local minima are defined as connected sets of pixels with equal\n    grey level strictly smaller than the grey levels of all pixels in direct\n    neighborhood of the set.\n\n    For integer typed images, this corresponds to the h-minima with h=1.\n    For float typed images, h is determined as the smallest difference\n    between grey levels.\n\n    Parameters\n    ----------\n    image : ndarray\n        The input image for which the minima are to be calculated.\n    selem : ndarray, optional\n        The neighborhood expressed as an n-D array of 1's and 0's.\n        Default is the ball of radius 1 according to the maximum norm\n        (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)\n\n    Returns\n    -------\n    local_min : ndarray\n       All local minima of the image. The result image is a binary image,\n       where pixels belonging to local minima take value 1, the other pixels\n       take value 0.\n\n    See also\n    --------\n    skimage.morphology.extrema.h_minima\n    skimage.morphology.extrema.h_maxima\n    skimage.morphology.extrema.local_maxima\n\n    References\n    ----------\n    .. [1] Soille, P., \"Morphological Image Analysis: Principles and\n           Applications\" (Chapter 6), 2nd edition (2003), ISBN 3540429883.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.morphology import extrema\n\n    We create an image (quadratic function with a minimum in the center and\n    4 additional constant maxima.\n    The depth of the minima are: 1, 21, 41, 61, 81, 101\n\n    >>> w = 12\n    >>> x, y = np.mgrid[0:w,0:w]\n    >>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n    >>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n    >>> f[9:10,9:11] = 100; f[10,10] = 100\n    >>> f = f.astype(np.int)\n\n    We can calculate all local minima:\n\n    >>> minima = extrema.local_minima(f)\n\n    The resulting image will contain all 6 local minima.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Determine all local minima of the image.\n\n    The local minima are defined as connected sets of pixels with equal\n    grey level strictly smaller than the grey levels of all pixels in direct\n    neighborhood of the set.\n\n    For integer typed images, this corresponds to the h-minima with h=1.\n    For float typed images, h is determined as the smallest difference\n    between grey levels.\n\n    Parameters\n    ----------\n    image : ndarray\n        The input image for which the minima are to be calculated.\n    selem : ndarray, optional\n        The neighborhood expressed as an n-D array of 1's and 0's.\n        Default is the ball of radius 1 according to the maximum norm\n        (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)\n\n    Returns\n    -------\n    local_min : ndarray\n       All local minima of the image. The result image is a binary image,\n       where pixels belonging to local minima take value 1, the other pixels\n       take value 0.\n\n    See also\n    --------\n    skimage.morphology.extrema.h_minima\n    skimage.morphology.extrema.h_maxima\n    skimage.morphology.extrema.local_maxima\n\n    References\n    ----------\n    .. [1] Soille, P., \"Morphological Image Analysis: Principles and\n           Applications\" (Chapter 6), 2nd edition (2003), ISBN 3540429883.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.morphology import extrema\n\n    We create an image (quadratic function with a minimum in the center and\n    4 additional constant maxima.\n    The depth of the minima are: 1, 21, 41, 61, 81, 101\n\n    >>> w = 10\n    >>> x, y = np.mgrid[0:w,0:w]\n    >>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n    >>> f[2:4,2:4] = 160; f[2:4,7:9] = 140; f[7:9,2:4] = 120; f[7:9,7:9] = 100\n    >>> f = f.astype(np.int)\n\n    We can calculate all local minima:\n\n    >>> minima = extrema.local_minima(f)\n\n    The resulting image will contain all 6 local minima.", "file": "skimage_morphology_extrema.py", "function": "local_minima", "project": "scikit-image", "change_stats": {"code_changes": 11, "doc_changes": 7, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef url_escape(value):\n    \n    return urllib_parse.quote_plus(utf8(value))```\n\nUpdated code:\n```python\ndef url_escape(value, plus=True):\n    \n    \n    quote = urllib_parse.quote_plus if plus else urllib_parse.quote\n    return quote(utf8(value))\n```\n\nOriginal documentation:\n```python\nReturns a URL-encoded version of the given value.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a URL-encoded version of the given value.\n\n    If ``plus`` is true (the default), spaces will be represented\n    as \"+\" instead of \"%20\".  This is appropriate for query strings\n    but not for the path component of a URL.  Note that this default\n    is the reverse of Python's urllib module.", "file": "tornado_escape.py", "function": "url_escape", "project": "tornado", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef magic_notebook(self, s):\n        \n        \n        args = magic_arguments.parse_argstring(self.magic_notebook, s)\n\n        from IPython.nbformat import current\n        if args.export:\n            fname, name, format = current.parse_filename(args.filename)\n            cells = []\n            hist = list(self.history_manager.get_range())\n            for session, prompt_number, input in hist[:-1]:\n                cells.append(current.new_code_cell(prompt_number=prompt_number, input=input))\n            worksheet = current.new_worksheet(cells=cells)\n            nb = current.new_notebook(name=name,worksheets=[worksheet])\n            with open(fname, 'w') as f:\n                current.write(nb, f, format);\n        elif args.format is not None:\n            old_fname, old_name, old_format = current.parse_filename(args.filename)\n            new_format = args.format\n            if new_format == u'xml' or new_format == u'ipynb':\n                new_fname = old_name + u'.ipynb'\n                new_format = u'xml'\n            elif new_format == u'py':\n                new_fname = old_name + u'.py'\n            elif new_format == u'json':\n                new_fname = old_name + u'.json'\n            else:\n                raise ValueError('Invalid notebook format: %s' % newformat)\n            with open(old_fname, 'r') as f:\n                nb = current.read(f, old_format)\n            with open(new_fname, 'w') as f:\n                current.write(nb, f, new_format)```\n\nUpdated code:\n```python\ndef magic_notebook(self, s):\n        \n        \n        args = magic_arguments.parse_argstring(self.magic_notebook, s)\n\n        from IPython.nbformat import current\n        if args.export:\n            fname, name, format = current.parse_filename(args.filename)\n            cells = []\n            hist = list(self.history_manager.get_range())\n            for session, prompt_number, input in hist[:-1]:\n                cells.append(current.new_code_cell(prompt_number=prompt_number, input=input))\n            worksheet = current.new_worksheet(cells=cells)\n            nb = current.new_notebook(name=name,worksheets=[worksheet])\n            with open(fname, 'w') as f:\n                current.write(nb, f, format);\n        elif args.format is not None:\n            old_fname, old_name, old_format = current.parse_filename(args.filename)\n            new_format = args.format\n            if new_format == u'xml':\n                raise ValueError('Notebooks cannot be written as xml.')\n            elif new_format == u'ipynb' or new_format == u'json':\n                new_fname = old_name + u'.ipynb'\n                new_format = u'json'\n            elif new_format == u'py':\n                new_fname = old_name + u'.py'\n            else:\n                raise ValueError('Invalid notebook format: %s' % new_format)\n            with open(old_fname, 'r') as f:\n                s = f.read()\n                try:\n                    nb = current.reads(s, old_format)\n                except:\n                    nb = current.reads(s, u'xml')\n            with open(new_fname, 'w') as f:\n                current.write(nb, f, new_format)\n```\n\nOriginal documentation:\n```python\nExport and convert IPython notebooks.\n\n        This function can export the current IPython history to a notebook file\n        or can convert an existing notebook file into a different format. For\n        example, to export the history to \"foo.ipynb\" do \"%notebook -e foo.ipynb\".\n        To export the history to \"foo.py\" do \"%notebook -e foo.py\". To convert\n        \"foo.ipynb\" to \"foo.json\" do \"%notebook -f json foo.ipynb\". Possible\n        formats include (xml/ipynb, json, py).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Export and convert IPython notebooks.\n\n        This function can export the current IPython history to a notebook file\n        or can convert an existing notebook file into a different format. For\n        example, to export the history to \"foo.ipynb\" do \"%notebook -e foo.ipynb\".\n        To export the history to \"foo.py\" do \"%notebook -e foo.py\". To convert\n        \"foo.ipynb\" to \"foo.json\" do \"%notebook -f json foo.ipynb\". Possible\n        formats include (json/ipynb, py).", "file": "IPython_core_magic.py", "function": "Magic.magic_notebook", "project": "ipython", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lab2lch(lab):\n    \n    \n    lch = _prepare_colorarray(lab).copy()\n\n    a, b = lch[..., 1], lch[..., 2]\n    lch[..., 1], lch[..., 2] = np.sqrt(a ** 2 + b ** 2), np.arctan2(b, a)\n\n    H = lch[..., 2]\n    H[H < 0] += 2 * np.pi  # (-pi, pi) -> (0, 2*pi)\n    return lch```\n\nUpdated code:\n```python\ndef lab2lch(lab):\n    \n    \n    lch = _prepare_lab_array(lab)\n\n    a, b = lch[..., 1], lch[..., 2]\n    lch[..., 1], lch[..., 2] = np.sqrt(a ** 2 + b ** 2), np.arctan2(b, a)\n\n    H = lch[..., 2]\n    H += np.where(H < 0, 2*np.pi, 0)  # (-pi, pi) -> (0, 2*pi)\n    return lch\n```\n\nOriginal documentation:\n```python\nCIE-LAB to CIE-LCH color space conversion.\n\n    LCH is the cylindrical representation of the LAB (Cartesian) colorspace\n\n    Parameters\n    ----------\n    lab : array_like\n        The image in CIE-LAB format, in a 3-D array of shape (.., .., 3).\n\n    Returns\n    -------\n    out : ndarray\n        The image in LCH format, in a 3-D array of shape (.., .., 3).\n\n    Raises\n    ------\n    ValueError\n        If `rgb` is not a 3-D array of shape (.., .., 3).\n\n    References\n    ----------\n\n    Notes\n    -----\n    The Hue is expressed as an angle between (0, 2*pi)\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.color import rgb2lab, lab2lch\n    >>> lena = data.lena()\n    >>> lena_lab = rgb2lab(lena)\n    >>> lena_lch = lab2lch(lena_lab)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "CIE-LAB to CIE-LCH color space conversion.\n\n    LCH is the cylindrical representation of the LAB (Cartesian) colorspace\n\n    Parameters\n    ----------\n    lab : array_like\n        The N-D image in CIE-LAB format. The last (`N+1`th) dimension must have\n        at least 3 elements, corresponding to the `L`, `a`, and `b` color\n        channels.  Subsequent elements are copied.\n\n    Returns\n    -------\n    out : ndarray\n        The image in LCH format, in a 3-D array of shape (.., .., 3).\n\n    Raises\n    ------\n    ValueError\n        If `rgb` is not a 3-D array of shape (.., .., 3).\n\n    References\n    ----------\n\n    Notes\n    -----\n    The Hue is expressed as an angle between (0, 2*pi)\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.color import rgb2lab, lab2lch\n    >>> lena = data.lena()\n    >>> lena_lab = rgb2lab(lena)\n    >>> lena_lch = lab2lch(lena_lab)", "file": "skimage_color_colorconv.py", "function": "lab2lch", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, obj, **adapted_methods):\n        \n        self.obj = obj\n        self.__dict__.update(adapted_methods)```\n\nUpdated code:\n```python\ndef __init__(self, obj: T, **adapted_methods: Callable):\n        \n        self.obj = obj\n        self.__dict__.update(adapted_methods)\n```\n\nOriginal documentation:\n```python\nWe set the adapted methods in the object's dict\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "We set the adapted methods in the object's dict.", "file": "patterns_structural_adapter.py", "function": "Adapter.__init__", "project": "python-patterns", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef frangi(image, sigmas=range(1, 10, 2), scale_range=None,\n           scale_step=None, beta1=None, beta2=None, alpha=0.5,\n           beta=0.5, gamma=15, black_ridges=True, mode='reflect',\n           cval=0):\n    \n    \n    \n\n    # Check deprecated keyword parameters\n    if beta1 is not None:\n        warn('Use keyword parameter `beta` instead of `beta1` which '\n             'will be removed in version 0.17.', stacklevel=2)\n        beta = beta1\n\n    if beta2 is not None:\n        warn('Use keyword parameter `gamma` instead of `beta2` which '\n             'will be removed in version 0.17.', stacklevel=2)\n        gamma = beta2\n\n    if scale_range is not None and scale_step is not None:\n        warn('Use keyword parameter `sigmas` instead of `scale_range` and '\n             '`scale_range` which will be removed in version 0.17.',\n             stacklevel=2)\n        sigmas = np.arange(scale_range[0], scale_range[1], scale_step)\n\n    # Check image dimensions\n    check_nD(image, [2, 3])\n\n    # Check (sigma) scales\n    sigmas = _check_sigmas(sigmas)\n\n    # Rescale filter parameters\n    alpha_sq = 2 * alpha ** 2\n    beta_sq = 2 * beta ** 2\n    gamma_sq = 2 * gamma ** 2\n\n    # Get image dimensions\n    ndim = image.ndim\n\n    # Invert image to detect dark ridges on light background\n    if black_ridges:\n        image = invert(image)\n\n    # Generate empty (n+1)D arrays for storing auxiliary images filtered\n    # at different (sigma) scales\n    filtered_array = np.zeros(sigmas.shape + image.shape)\n    lambdas_array = np.zeros_like(filtered_array)\n\n    # Filtering for all (sigma) scales\n    for i, sigma in enumerate(sigmas):\n\n        # Calculate (abs sorted) eigenvalues\n        lambda1, *lambdas = compute_hessian_eigenvalues(image, sigma,\n                                                        sorting='abs',\n                                                        mode=mode, cval=cval)\n\n        # Compute sensitivity to deviation from a plate-like\n        # structure see equations (11) and (15) in reference [1]_\n        r_a = np.inf if ndim == 2 else _divide_nonzero(*lambdas) ** 2\n\n        # Compute sensitivity to deviation from a blob-like structure,\n        # see equations (10) and (15) in reference [1]_,\n        # np.abs(lambda2) in 2D, np.sqrt(np.abs(lambda2 * lambda3)) in 3D\n        filtered_raw = np.abs(np.multiply.reduce(lambdas)) ** (1/len(lambdas))\n        r_b = _divide_nonzero(lambda1, filtered_raw) ** 2\n\n        # Compute sensitivity to areas of high variance/texture/structure,\n        # see equation (12)in reference [1]_\n        r_g = sum([lambda1 ** 2] + [lambdai ** 2 for lambdai in lambdas])\n\n        # Compute output image for given (sigma) scale and store results in\n        # (n+1)D matrices, see equations (13) and (15) in reference [1]_\n        filtered_array[i] = ((1 - np.exp(-r_a / alpha_sq))\n                             * np.exp(-r_b / beta_sq)\n                             * (1 - np.exp(-r_g / gamma_sq)))\n\n        lambdas_array[i] = np.max(lambdas, axis=0)\n\n    # Remove background\n    filtered_array[lambdas_array > 0] = 0\n\n    # Return for every pixel the maximum value over all (sigma) scales\n    return np.max(filtered_array, axis=0)```\n\nUpdated code:\n```python\ndef frangi(image, sigmas=range(1, 10, 2), scale_range=None,\n           scale_step=None, alpha=0.5, beta=0.5, gamma=15,\n           black_ridges=True, mode='reflect', cval=0):\n    \n    \n    \n    if scale_range is not None and scale_step is not None:\n        warn('Use keyword parameter `sigmas` instead of `scale_range` and '\n             '`scale_range` which will be removed in version 0.17.',\n             stacklevel=2)\n        sigmas = np.arange(scale_range[0], scale_range[1], scale_step)\n\n    # Check image dimensions\n    check_nD(image, [2, 3])\n\n    # Check (sigma) scales\n    sigmas = _check_sigmas(sigmas)\n\n    # Rescale filter parameters\n    alpha_sq = 2 * alpha ** 2\n    beta_sq = 2 * beta ** 2\n    gamma_sq = 2 * gamma ** 2\n\n    # Get image dimensions\n    ndim = image.ndim\n\n    # Invert image to detect dark ridges on light background\n    if black_ridges:\n        image = invert(image)\n\n    # Generate empty (n+1)D arrays for storing auxiliary images filtered\n    # at different (sigma) scales\n    filtered_array = np.zeros(sigmas.shape + image.shape)\n    lambdas_array = np.zeros_like(filtered_array)\n\n    # Filtering for all (sigma) scales\n    for i, sigma in enumerate(sigmas):\n\n        # Calculate (abs sorted) eigenvalues\n        lambda1, *lambdas = compute_hessian_eigenvalues(image, sigma,\n                                                        sorting='abs',\n                                                        mode=mode, cval=cval)\n\n        # Compute sensitivity to deviation from a plate-like\n        # structure see equations (11) and (15) in reference [1]_\n        r_a = np.inf if ndim == 2 else _divide_nonzero(*lambdas) ** 2\n\n        # Compute sensitivity to deviation from a blob-like structure,\n        # see equations (10) and (15) in reference [1]_,\n        # np.abs(lambda2) in 2D, np.sqrt(np.abs(lambda2 * lambda3)) in 3D\n        filtered_raw = np.abs(np.multiply.reduce(lambdas)) ** (1/len(lambdas))\n        r_b = _divide_nonzero(lambda1, filtered_raw) ** 2\n\n        # Compute sensitivity to areas of high variance/texture/structure,\n        # see equation (12)in reference [1]_\n        r_g = sum([lambda1 ** 2] + [lambdai ** 2 for lambdai in lambdas])\n\n        # Compute output image for given (sigma) scale and store results in\n        # (n+1)D matrices, see equations (13) and (15) in reference [1]_\n        filtered_array[i] = ((1 - np.exp(-r_a / alpha_sq))\n                             * np.exp(-r_b / beta_sq)\n                             * (1 - np.exp(-r_g / gamma_sq)))\n\n        lambdas_array[i] = np.max(lambdas, axis=0)\n\n    # Remove background\n    filtered_array[lambdas_array > 0] = 0\n\n    # Return for every pixel the maximum value over all (sigma) scales\n    return np.max(filtered_array, axis=0)\n```\n\nOriginal documentation:\n```python\nFilter an image with the Frangi vesselness filter.\n\n    This filter can be used to detect continuous ridges, e.g. vessels,\n    wrinkles, rivers. It can be used to calculate the fraction of the\n    whole image containing such objects.\n\n    Defined only for 2-D and 3-D images. Calculates the eigenvectors of the\n    Hessian to compute the similarity of an image region to vessels, according\n    to the method described in [1]_.\n\n    Parameters\n    ----------\n    image : (N, M[, P]) ndarray\n        Array with input image data.\n    sigmas : iterable of floats, optional\n        Sigmas used as scales of filter, i.e.,\n        np.arange(scale_range[0], scale_range[1], scale_step)\n    scale_range : 2-tuple of floats, optional\n        The range of sigmas used.\n    scale_step : float, optional\n        Step size between sigmas.\n    alpha : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to deviation from a plate-like structure.\n    beta = beta1 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to deviation from a blob-like structure.\n    gamma = beta2 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to areas of high variance/texture/structure.\n    black_ridges : boolean, optional\n        When True (the default), the filter detects black ridges; when\n        False, it detects white ridges.\n    mode : {'constant', 'reflect', 'wrap', 'nearest', 'mirror'}, optional\n        How to handle values outside the image borders.\n    cval : float, optional\n        Used in conjunction with mode 'constant', the value outside\n        the image boundaries.\n\n    Returns\n    -------\n    out : (N, M[, P]) ndarray\n        Filtered image (maximum of pixels across all scales).\n\n    Notes\n    -----\n    Written by Marc Schrijver, November 2001\n    Re-Written by D. J. Kroon, University of Twente, May 2009, [2]_\n    Adoption of 3D version from D. G. Ellis, Januar 20017, [3]_\n\n    See also\n    --------\n    meijering\n    sato\n    hessian\n\n    References\n    ----------\n    .. [1] Frangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A.\n        (1998,). Multiscale vessel enhancement filtering. In International\n        Conference on Medical Image Computing and Computer-Assisted\n        Intervention (pp. 130-137). Springer Berlin Heidelberg.\n        :DOI:`10.1007/BFb0056195`\n    .. [2] Kroon, D. J.: Hessian based Frangi vesselness filter.\n    .. [3] Ellis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Filter an image with the Frangi vesselness filter.\n\n    This filter can be used to detect continuous ridges, e.g. vessels,\n    wrinkles, rivers. It can be used to calculate the fraction of the\n    whole image containing such objects.\n\n    Defined only for 2-D and 3-D images. Calculates the eigenvectors of the\n    Hessian to compute the similarity of an image region to vessels, according\n    to the method described in [1]_.\n\n    Parameters\n    ----------\n    image : (N, M[, P]) ndarray\n        Array with input image data.\n    sigmas : iterable of floats, optional\n        Sigmas used as scales of filter, i.e.,\n        np.arange(scale_range[0], scale_range[1], scale_step)\n    scale_range : 2-tuple of floats, optional\n        The range of sigmas used.\n    scale_step : float, optional\n        Step size between sigmas.\n    alpha : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to deviation from a plate-like structure.\n    beta : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to deviation from a blob-like structure.\n    gamma : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to areas of high variance/texture/structure.\n    black_ridges : boolean, optional\n        When True (the default), the filter detects black ridges; when\n        False, it detects white ridges.\n    mode : {'constant', 'reflect', 'wrap', 'nearest', 'mirror'}, optional\n        How to handle values outside the image borders.\n    cval : float, optional\n        Used in conjunction with mode 'constant', the value outside\n        the image boundaries.\n\n    Returns\n    -------\n    out : (N, M[, P]) ndarray\n        Filtered image (maximum of pixels across all scales).\n\n    Notes\n    -----\n    Written by Marc Schrijver, November 2001\n    Re-Written by D. J. Kroon, University of Twente, May 2009, [2]_\n    Adoption of 3D version from D. G. Ellis, Januar 20017, [3]_\n\n    See also\n    --------\n    meijering\n    sato\n    hessian\n\n    References\n    ----------\n    .. [1] Frangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A.\n        (1998,). Multiscale vessel enhancement filtering. In International\n        Conference on Medical Image Computing and Computer-Assisted\n        Intervention (pp. 130-137). Springer Berlin Heidelberg.\n        :DOI:`10.1007/BFb0056195`\n    .. [2] Kroon, D. J.: Hessian based Frangi vesselness filter.\n    .. [3] Ellis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi", "file": "skimage_filters_ridges.py", "function": "frangi", "project": "scikit-image", "change_stats": {"code_changes": 19, "doc_changes": 6, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef select_jinja_autoescape(self, filename: str) -> bool:\n        \n        \n        if filename is None:\n            return True\n        return filename.endswith((\".html\", \".htm\", \".xml\", \".xhtml\", \".svg\"))```\n\nUpdated code:\n```python\ndef select_jinja_autoescape(self, filename: str) -> bool:\n        \n        \n        if filename is None:\n            return True\n        return filename.endswith((\".html\", \".htm\", \".xml\", \".xhtml\"))\n```\n\nOriginal documentation:\n```python\nReturns ``True`` if autoescaping should be active for the given\n        template name. If no template name is given, returns `True`.\n\n        .. versionchanged:: 2.2\n            Autoescaping is now enabled by default for ``.svg`` files.\n\n        .. versionadded:: 0.5\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns ``True`` if autoescaping should be active for the given\n        template name. If no template name is given, returns `True`.\n\n        .. versionadded:: 0.5", "file": "src_flask_app.py", "function": "Flask.select_jinja_autoescape", "project": "flask", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,\n               optimizer: tf.keras.optimizers.Optimizer,\n               trainable_weights_only: bool = True,\n               average_decay: float = 0.99,\n               start_step: int = 0,\n               dynamic_decay: bool = True,\n               name: str = 'ExponentialMovingAverage',\n               **kwargs):\n    \n    \n    super().__init__(name, **kwargs)\n    self._average_decay = average_decay\n    self._trainable_weights_only = trainable_weights_only\n    self._start_step = tf.constant(start_step, tf.float32)\n    self._dynamic_decay = dynamic_decay\n    self._optimizer = optimizer\n    self._track_trackable(self._optimizer, 'ema_base_optimizer')\n    self._average_weights = None\n    self._model_weights = None```\n\nUpdated code:\n```python\ndef __init__(self,\n               optimizer: tf_keras.optimizers.Optimizer,\n               trainable_weights_only: bool = True,\n               average_decay: float = 0.99,\n               start_step: int = 0,\n               dynamic_decay: bool = True,\n               name: str = 'ExponentialMovingAverage',\n               **kwargs):\n    \n    \n    super().__init__(name, **kwargs)\n    self._average_decay = average_decay\n    self._trainable_weights_only = trainable_weights_only\n    self._start_step = tf.constant(start_step, tf.float32)\n    self._dynamic_decay = dynamic_decay\n    self._optimizer = optimizer\n    self._track_trackable(self._optimizer, 'ema_base_optimizer')\n    self._average_weights = None\n    self._model_weights = None\n```\n\nOriginal documentation:\n```python\nConstruct a new ExponentialMovingAverage optimizer.\n\n    Args:\n      optimizer: `tf.keras.optimizers.Optimizer` that will be\n        used to compute and apply gradients.\n      trainable_weights_only: 'bool', if True, only model trainable weights will\n        be updated. Otherwise, all model weights will be updated. This mainly\n        affects batch normalization parameters.\n      average_decay: float. Decay to use to maintain the moving averages\n        of trained variables.\n      start_step: int. What step to start the moving average.\n      dynamic_decay: bool. Whether to change the decay based on the number\n        of optimizer updates. Decay will start at 0.1 and gradually increase\n        up to `average_decay` after each optimizer update. This behavior is\n        similar to `tf.train.ExponentialMovingAverage` in TF 1.x.\n      name: Optional name for the operations created when applying\n        gradients. Defaults to \"moving_average\".\n      **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n        `clipvalue`, `lr`, `decay`}.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Construct a new ExponentialMovingAverage optimizer.\n\n    Args:\n      optimizer: `tf_keras.optimizers.Optimizer` that will be\n        used to compute and apply gradients.\n      trainable_weights_only: 'bool', if True, only model trainable weights will\n        be updated. Otherwise, all model weights will be updated. This mainly\n        affects batch normalization parameters.\n      average_decay: float. Decay to use to maintain the moving averages\n        of trained variables.\n      start_step: int. What step to start the moving average.\n      dynamic_decay: bool. Whether to change the decay based on the number\n        of optimizer updates. Decay will start at 0.1 and gradually increase\n        up to `average_decay` after each optimizer update. This behavior is\n        similar to `tf.train.ExponentialMovingAverage` in TF 1.x.\n      name: Optional name for the operations created when applying\n        gradients. Defaults to \"moving_average\".\n      **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n        `clipvalue`, `lr`, `decay`}.", "file": "official_modeling_optimization_ema_optimizer.py", "function": "ExponentialMovingAverage.__init__", "project": "models", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef atomic_write_in_dir(path, **kwargs):\n  \n  \n  writer = AtomicWriter(path, **kwargs)\n  return writer._open(_get_fileobject_func(writer, os.path.dirname(path)))```\n\nUpdated code:\n```python\ndef atomic_write_in_dir(path: str, mode: str = 'w', buffering: int = -1, encoding: Optional[str] = None, newline: Optional[str] = None, \n                        overwrite: bool = False):\n  \n  dir_name = os.path.dirname(path)\n\n  if not overwrite and os.path.exists(path):\n    raise FileExistsError(f\"File '{path}' already exists. To overwrite it, set 'overwrite' to True.\")\n\n  with tempfile.NamedTemporaryFile(mode=mode, buffering=buffering, encoding=encoding, newline=newline, dir=dir_name, delete=False) as tmp_file:\n    yield tmp_file\n    tmp_file_name = tmp_file.name\n  os.replace(tmp_file_name, path)\n```\n\nOriginal documentation:\n```python\nCreates an atomic writer using a temporary file in the same directory\n     as the destination file.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write to a file atomically using a temporary file in the same directory as the destination file.", "file": "common_file_helpers.py", "function": "atomic_write_in_dir", "project": "openpilot", "change_stats": {"code_changes": 17, "doc_changes": 5, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef preprocess(self, resized_inputs):\n    \n\n    \n    channel_means = [123.68, 116.779, 103.939]\n    return resized_inputs - [[channel_means]]```\n\nUpdated code:\n```python\ndef preprocess(self, resized_inputs):\n    \n\n    \n    if resized_inputs.shape.as_list()[3] == 3:\n      channel_means = [123.68, 116.779, 103.939]\n      return resized_inputs - [[channel_means]]\n    else:\n      return resized_inputs\n```\n\nOriginal documentation:\n```python\nFaster R-CNN Resnet V1 preprocessing.\n\n    VGG style channel mean subtraction as described here:\n    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n\n    Args:\n      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n        representing a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n        tensor representing a batch of images.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Faster R-CNN Resnet V1 preprocessing.\n\n    VGG style channel mean subtraction as described here:\n    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n    Note that if the number of channels is not equal to 3, the mean subtraction\n    will be skipped and the original resized_inputs will be returned.\n\n    Args:\n      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n        representing a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n        tensor representing a batch of images.", "file": "research_object_detection_models_faster_rcnn_resnet_v1_feature_extractor.py", "function": "FasterRCNNResnetV1FeatureExtractor.preprocess", "project": "models", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rank(self, method='average', na_option='keep'):\n        \n        \n        \n        from pandas.core.algorithms import rank\n        ranks = rank(self.values, method=method, na_option=na_option)\n        return Series(ranks, index=self.index, name=self.name)```\n\nUpdated code:\n```python\ndef rank(self, method='average', na_option='keep', ascending=True):\n        \n        \n        \n        from pandas.core.algorithms import rank\n        ranks = rank(self.values, method=method, na_option=na_option,\n                     ascending=ascending)\n        return Series(ranks, index=self.index, name=self.name)\n```\n\nOriginal documentation:\n```python\nCompute data ranks (1 through n). Equal values are assigned a rank that\n        is the average of the ranks of those values\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first'}\n            average: average rank of group\n            min: lowest rank in group\n            max: highest rank in group\n            first: ranks assigned in order they appear in the array\n        na_option : {'keep'}\n            keep: leave NA values where they are\n\n        Returns\n        -------\n        ranks : Series\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute data ranks (1 through n). Equal values are assigned a rank that\n        is the average of the ranks of those values\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first'}\n            average: average rank of group\n            min: lowest rank in group\n            max: highest rank in group\n            first: ranks assigned in order they appear in the array\n        na_option : {'keep'}\n            keep: leave NA values where they are\n        ascending : boolean, default True\n            False for ranks by high (1) to low (N)\n\n        Returns\n        -------\n        ranks : Series", "file": "pandas_core_series.py", "function": "Series.rank", "project": "pandas", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _schedule_event(self, handler, when, data=None, name=None,\n                        repeat=None):\n        \n        \n        if not name:\n            name = self.skill.name + handler.__name__\n        unique_name = self._unique_name(name)\n        if repeat:\n            self.scheduled_repeats.append(name)  # store \"friendly name\"\n\n        data = data or {}\n        self.skill.add_event(unique_name, handler, once=not repeat)\n        event_data = {}\n        event_data['time'] = time.mktime(when.timetuple())\n        event_data['event'] = unique_name\n        event_data['repeat'] = repeat\n        event_data['data'] = data\n        self.skill.bus.emit(Message('mycroft.scheduler.schedule_event',\n                                    data=event_data))```\n\nUpdated code:\n```python\ndef _schedule_event(self, handler, when, data, name, repeat_interval=None):\n        \n\n        \n        if isinstance(when, (int, float)) and when >= 0:\n            when = datetime.now() + timedelta(seconds=when)\n        if not name:\n            name = self.skill.name + handler.__name__\n        unique_name = self._create_unique_name(name)\n        if repeat_interval:\n            self.scheduled_repeats.append(name)  # store \"friendly name\"\n\n        data = data or {}\n        self.skill.add_event(unique_name, handler, once=not repeat_interval)\n        event_data = {'time': time.mktime(when.timetuple()),\n                      'event': unique_name,\n                      'repeat': repeat_interval,\n                      'data': data}\n        self.skill.bus.emit(Message('mycroft.scheduler.schedule_event',\n                                    data=event_data))\n```\n\nOriginal documentation:\n```python\nUnderlying method for schedule_event and schedule_repeating_event.\n\n        Takes scheduling information and sends it off on the message bus.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Underlying method for schedule_event and schedule_repeating_event.\n\n        Takes scheduling information and sends it off on the message bus.\n\n        Arguments:\n            handler:                method to be called\n            when (datetime):        time (in system timezone) for first\n                                    calling the handler, or None to\n                                    initially trigger <frequency> seconds\n                                    from now\n            data (dict, optional):  data to send when the handler is called\n            name (str, optional):   reference name, must be unique\n            repeat_interval (float/int):  time in seconds between calls", "file": "mycroft_skills_event_scheduler.py", "function": "EventSchedulerInterface._schedule_event", "project": "mycroft-core", "change_stats": {"code_changes": 23, "doc_changes": 12, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef send_empty_evaluation_to_go(ip: str, recipient: str, channel_name: str) -> None:\n    \n    \n    \n\n    message_raw = build_go_message(\"report\", \"ip\", ip, \"score_confidence\", evaluation=None)\n\n    message_json = json.dumps(message_raw)\n    message_b64 = base64.b64encode(bytes(message_json, \"ascii\")).decode()\n\n    send_b64_to_go(message_b64, recipient, channel_name)```\n\nUpdated code:\n```python\ndef send_empty_evaluation_to_go(ip: str, recipient: str, channel_name: str) -> None:\n    \n    \n    \n    message_raw = build_go_message(\"report\", \"ip\", ip, \"score_confidence\", evaluation=None)\n    send_message_to_go(ip, recipient, channel_name, message_raw)\n```\n\nOriginal documentation:\n```python\nSend empty data to other peer, to show that there is no data here to be shared.\n\n    :param ip: The IP that is being reported\n    :param recipient: The peer that should receive the report. Use \"*\" wildcard to broadcast to everyone\n    :return: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates empty message and sends it to recipient;ip\n\n    :param ip: The IP that is being reported\n    :param recipient: The peer that should receive the report. Use \"*\" wildcard to broadcast to everyone\n    :return: None", "file": "modules_p2ptrust_utils_utils.py", "function": "send_empty_evaluation_to_go", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update_stock_ledger(self):\n\t\t\t\n\t\tfrom erpnext.stock.stock_ledger import get_previous_sle\n\n\t\tsl_entries = []\n\t\thas_serial_no = False\n\t\thas_batch_no = False\n\t\tfor row in self.items:\n\t\t\titem = frappe.get_doc(\"Item\", row.item_code)\n\t\t\tif item.has_batch_no:\n\t\t\t\thas_batch_no = True\n\n\t\t\tif item.has_serial_no or item.has_batch_no:\n\t\t\t\thas_serial_no = True\n\t\t\t\tself.get_sle_for_serialized_items(row, sl_entries)\n\t\t\telse:\n\t\t\t\tif row.serial_no or row.batch_no:\n\t\t\t\t\tfrappe.throw(_(\"Row #{0}: Item {1} is not a Serialized/Batched Item. It cannot have a Serial No/Batch No against it.\") \\\n\t\t\t\t\t\t.format(row.idx, frappe.bold(row.item_code)))\n\n\t\t\t\tprevious_sle = get_previous_sle({\n\t\t\t\t\t\"item_code\": row.item_code,\n\t\t\t\t\t\"warehouse\": row.warehouse,\n\t\t\t\t\t\"posting_date\": self.posting_date,\n\t\t\t\t\t\"posting_time\": self.posting_time\n\t\t\t\t})\n\n\t\t\t\tif previous_sle:\n\t\t\t\t\tif row.qty in (\"\", None):\n\t\t\t\t\t\trow.qty = previous_sle.get(\"qty_after_transaction\", 0)\n\n\t\t\t\t\tif row.valuation_rate in (\"\", None):\n\t\t\t\t\t\trow.valuation_rate = previous_sle.get(\"valuation_rate\", 0)\n\n\t\t\t\tif row.qty and not row.valuation_rate and not row.allow_zero_valuation_rate:\n\t\t\t\t\tfrappe.throw(_(\"Valuation Rate required for Item {0} at row {1}\").format(row.item_code, row.idx))\n\n\t\t\t\tif ((previous_sle and row.qty == previous_sle.get(\"qty_after_transaction\")\n\t\t\t\t\tand (row.valuation_rate == previous_sle.get(\"valuation_rate\") or row.qty == 0))\n\t\t\t\t\tor (not previous_sle and not row.qty)):\n\t\t\t\t\t\tcontinue\n\n\t\t\t\tsl_entries.append(self.get_sle_for_items(row))\n\n\t\tif sl_entries:\n\t\t\tif has_serial_no:\n\t\t\t\tsl_entries = self.merge_similar_item_serial_nos(sl_entries)\n\n\t\t\tallow_negative_stock = False\n\t\t\tif has_batch_no:\n\t\t\t\tallow_negative_stock = True\n\n\t\t\tself.make_sl_entries(sl_entries, allow_negative_stock=allow_negative_stock)\n\n\t\tif has_serial_no and sl_entries:\n\t\t\tself.update_valuation_rate_for_serial_no()```\n\nUpdated code:\n```python\ndef update_stock_ledger(self):\n\t\t\n\t\tfrom erpnext.stock.stock_ledger import get_previous_sle\n\n\t\tsl_entries = []\n\t\thas_serial_no = False\n\t\thas_batch_no = False\n\t\tfor row in self.items:\n\t\t\titem = frappe.get_doc(\"Item\", row.item_code)\n\t\t\tif item.has_batch_no:\n\t\t\t\thas_batch_no = True\n\n\t\t\tif item.has_serial_no or item.has_batch_no:\n\t\t\t\thas_serial_no = True\n\t\t\t\tself.get_sle_for_serialized_items(row, sl_entries)\n\t\t\telse:\n\t\t\t\tif row.serial_no or row.batch_no:\n\t\t\t\t\tfrappe.throw(\n\t\t\t\t\t\t_(\n\t\t\t\t\t\t\t\"Row #{0}: Item {1} is not a Serialized/Batched Item. It cannot have a Serial No/Batch No against it.\"\n\t\t\t\t\t\t).format(row.idx, frappe.bold(row.item_code))\n\t\t\t\t\t)\n\n\t\t\t\tprevious_sle = get_previous_sle(\n\t\t\t\t\t{\n\t\t\t\t\t\t\"item_code\": row.item_code,\n\t\t\t\t\t\t\"warehouse\": row.warehouse,\n\t\t\t\t\t\t\"posting_date\": self.posting_date,\n\t\t\t\t\t\t\"posting_time\": self.posting_time,\n\t\t\t\t\t}\n\t\t\t\t)\n\n\t\t\t\tif previous_sle:\n\t\t\t\t\tif row.qty in (\"\", None):\n\t\t\t\t\t\trow.qty = previous_sle.get(\"qty_after_transaction\", 0)\n\n\t\t\t\t\tif row.valuation_rate in (\"\", None):\n\t\t\t\t\t\trow.valuation_rate = previous_sle.get(\"valuation_rate\", 0)\n\n\t\t\t\tif row.qty and not row.valuation_rate and not row.allow_zero_valuation_rate:\n\t\t\t\t\tfrappe.throw(\n\t\t\t\t\t\t_(\"Valuation Rate required for Item {0} at row {1}\").format(row.item_code, row.idx)\n\t\t\t\t\t)\n\n\t\t\t\tif (\n\t\t\t\t\tprevious_sle\n\t\t\t\t\tand row.qty == previous_sle.get(\"qty_after_transaction\")\n\t\t\t\t\tand (row.valuation_rate == previous_sle.get(\"valuation_rate\") or row.qty == 0)\n\t\t\t\t) or (not previous_sle and not row.qty):\n\t\t\t\t\tcontinue\n\n\t\t\t\tsl_entries.append(self.get_sle_for_items(row))\n\n\t\tif sl_entries:\n\t\t\tif has_serial_no:\n\t\t\t\tsl_entries = self.merge_similar_item_serial_nos(sl_entries)\n\n\t\t\tallow_negative_stock = False\n\t\t\tif has_batch_no:\n\t\t\t\tallow_negative_stock = True\n\n\t\t\tself.make_sl_entries(sl_entries, allow_negative_stock=allow_negative_stock)\n\n\t\tif has_serial_no and sl_entries:\n\t\t\tself.update_valuation_rate_for_serial_no()\n```\n\nOriginal documentation:\n```python\nfind difference between current and expected entries\n\t\t\tand create stock ledger entries based on the difference\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "find difference between current and expected entries\n\t\tand create stock ledger entries based on the difference", "file": "erpnext_stock_doctype_stock_reconciliation_stock_reconciliation.py", "function": "StockReconciliation.update_stock_ledger", "project": "erpnext", "change_stats": {"code_changes": 39, "doc_changes": 4, "total_changes": 43}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_stories(\n        self,\n        template_variables: Optional[Dict] = None,\n        exclusion_percentage: Optional[int] = None,\n    ) -> StoryGraph:\n        \n        \n        raise NotImplementedError()```\n\nUpdated code:\n```python\ndef get_stories(self, exclusion_percentage: Optional[int] = None,) -> StoryGraph:\n        \n        \n        raise NotImplementedError()\n```\n\nOriginal documentation:\n```python\nRetrieves the stories that should be used for training.\n\n        Args:\n            template_variables: Values of templates that should be replaced while\n                                reading the story files.\n            exclusion_percentage: Amount of training data that should be excluded.\n\n        Returns:\n            `StoryGraph` containing all loaded stories.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieves the stories that should be used for training.\n\n        Args:\n            exclusion_percentage: Amount of training data that should be excluded.\n\n        Returns:\n            `StoryGraph` containing all loaded stories.", "file": "rasa_shared_importers_importer.py", "function": "TrainingDataImporter.get_stories", "project": "rasa", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_columns(self, table_name: str) -> StatusResponse:\n         \n        \n        \n        result = self._tables[table_name].get_columns()\n        try:\n            if result:\n                response = Response(\n                    RESPONSE_TYPE.TABLE,\n                    data_frame=pd.DataFrame(\n                        [result[i]['COLUMN_NAME'] for column_name in self._tables[table_name]['columns'] ],\n                        columns=['COLUMN_NAME']\n                        \n                    )\n                )\n            else:\n                response = Response(RESPONSE_TYPE.OK)\n            \n        except Exception as e:\n            log.logger.error(f'Error running while getting table {e} on ')\n            response = Response(\n                RESPONSE_TYPE.ERROR,\n                error_message=str(e)\n            )\n\n\n\n        return response```\n\nUpdated code:\n```python\ndef get_columns(self, table_name: str) -> Response:\n         \n\n        result = self._get_table(table_name).get_columns()\n\n        df = pd.DataFrame(result, columns=['Field'])\n        df['Type'] = 'str'\n\n        return Response(RESPONSE_TYPE.TABLE, df)\n```\n\nOriginal documentation:\n```python\nReturns a list of entity columns\n        Args:\n            table_name (str): name of one of tables returned by self.get_tables()\n        Returns:\n            HandlerResponse: shoud have same columns as information_schema.columns\n                (https://dev.mysql.com/doc/refman/8.0/en/information-schema-columns-table.html)\n                Column 'COLUMN_NAME' is mandatory, other is optional. Hightly\n                recomended to define also 'DATA_TYPE': it should be one of\n                python data types (by default it str).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a list of entity columns", "file": "mindsdb_integrations_libs_api_handler.py", "function": "APIHandler.get_columns", "project": "mindsdb", "change_stats": {"code_changes": 31, "doc_changes": 10, "total_changes": 41}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef assemble_from_file_blob_ids(self, file_blob_ids, checksum, commit=True):\n        \n        \n        \n        with transaction.atomic():\n            file_blobs = FileBlob.objects.filter(id__in=file_blob_ids).all()\n            # Make sure the blobs are sorted with the order provided\n            file_blobs = sorted(file_blobs, key=lambda blob: file_blob_ids.index(blob.id))\n\n            new_checksum = sha1(b'')\n            offset = 0\n            for blob in file_blobs:\n                FileBlobIndex.objects.create(\n                    file=self,\n                    blob=blob,\n                    offset=offset,\n                )\n                for chunk in blob.getfile().chunks():\n                    new_checksum.update(chunk)\n                offset += blob.size\n\n            self.size = offset\n            self.checksum = new_checksum.hexdigest()\n\n            if checksum != self.checksum:\n                raise AssembleChecksumMismatch('Checksum mismatch')\n\n        metrics.timing('filestore.file-size', offset)\n        if commit:\n            self.save()```\n\nUpdated code:\n```python\ndef assemble_from_file_blob_ids(self, file_blob_ids, checksum, commit=True):\n        \n        \n        \n        tf = tempfile.NamedTemporaryFile()\n        with transaction.atomic():\n            file_blobs = FileBlob.objects.filter(id__in=file_blob_ids).all()\n            # Make sure the blobs are sorted with the order provided\n            file_blobs = sorted(file_blobs, key=lambda blob: file_blob_ids.index(blob.id))\n\n            new_checksum = sha1(b'')\n            offset = 0\n            for blob in file_blobs:\n                FileBlobIndex.objects.create(\n                    file=self,\n                    blob=blob,\n                    offset=offset,\n                )\n                for chunk in blob.getfile().chunks():\n                    new_checksum.update(chunk)\n                    tf.write(chunk)\n                offset += blob.size\n\n            self.size = offset\n            self.checksum = new_checksum.hexdigest()\n\n            if checksum != self.checksum:\n                raise AssembleChecksumMismatch('Checksum mismatch')\n\n        metrics.timing('filestore.file-size', offset)\n        if commit:\n            self.save()\n        tf.flush()\n        tf.seek(0)\n        return tf\n```\n\nOriginal documentation:\n```python\nThis creates a file, from file blobs\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This creates a file, from file blobs and returns a temp file with the\n        contents.", "file": "src_sentry_models_file.py", "function": "File.assemble_from_file_blob_ids", "project": "sentry", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_sharded_model(self, model: nn.Module, checkpoint: Path, strict: bool):\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef load_sharded_model(self, model: nn.Module, index_file_path: str, strict: bool):\n        \n        \n        \n        pass\n```\n\nOriginal documentation:\n```python\nLoad model from sharded checkpoint.\n\n        Args:\n            model (nn.Module): model to be loaded.\n            checkpoint (str): checkpoint path. It should be path to the .index.json file or a path to a directory which contains a .index.json file.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load model from sharded checkpoint.\n\n        Args:\n            model (nn.Module): model to be loaded.\n            index_file_path (str): checkpoint path. It should be path to the .index.json file or a path to a directory which contains a .index.json file.\n            strict (bool): whether to strictly enforce that the param name in\n                the checkpoint match the keys returned by this module's.", "file": "colossalai_checkpoint_io_checkpoint_io_base.py", "function": "CheckpointIO.load_sharded_model", "project": "ColossalAI", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_content_file_input_type(self):\n        \n        \n        \n        self.assertIsInstance(ContentFile(b\"content\").read(), bytes)\n        self.assertIsInstance(ContentFile(\"espa\u00f1ol\").read(), six.text_type)```\n\nUpdated code:\n```python\ndef test_content_file_input_type(self):\n        \n        \n        \n        self.assertIsInstance(ContentFile(b\"content\").read(), bytes)\n        self.assertIsInstance(ContentFile(\"espa\u00f1ol\").read(), str)\n```\n\nOriginal documentation:\n```python\nContentFile can accept both bytes and unicode and that the\n        retrieved content is of the same type.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "ContentFile can accept both bytes and strings and the retrieved content\n        is of the same type.", "file": "tests_files_tests.py", "function": "ContentFileTestCase.test_content_file_input_type", "project": "django", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_path(self):\n        \n        \n        \n        _path, fillable = self.get_path_in_displaycoord()\n\n        if cbook.iterable(fillable):\n            _path = concatenate_paths(_path)\n\n        return self.get_transform().inverted().transform_path(_path)```\n\nUpdated code:\n```python\ndef get_path(self):\n        \n          \n        \n        _path, fillable = self.get_path_in_displaycoord()\n\n        if cbook.iterable(fillable):\n            _path = concatenate_paths(_path)\n\n        return self.get_transform().inverted().transform_path(_path)\n```\n\nOriginal documentation:\n```python\nreturn the path of the arrow in the data coordinate. Use\n        get_path_in_displaycoord() medthod to retrieve the arrow path\n        in the disaply coord.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "return the path of the arrow in the data coordinate. Use\n        get_path_in_displaycoord() method to retrieve the arrow path\n        in the disaply coord.", "file": "lib_matplotlib_patches.py", "function": "FancyArrowPatch.get_path", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _maybe_disable_assert(disable_assert):\n  \n  if not disable_assert:\n    yield\n    return\n\n  original_assert = control_flow_ops.Assert\n  control_flow_ops.Assert = _dont_assert\n  yield\n  control_flow_ops.Assert = original_assert```\n\nUpdated code:\n```python\ndef _maybe_disable_assert(disable_assert):\n  \n  if not disable_assert:\n    yield\n    return\n\n  original_assert = control_flow_assert.Assert\n  control_flow_assert.Assert = _dont_assert\n  yield\n  control_flow_assert.Assert = original_assert\n```\n\nOriginal documentation:\n```python\nScoped monkey patch of control_flow_ops.Assert to a no-op.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Scoped monkey patch of control_flow_assert.Assert to a no-op.", "file": "official_nlp_tools_export_tfhub_lib.py", "function": "_maybe_disable_assert", "project": "models", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, cost=0.0015):\n        \n        \n        \n        self.cost = float(cost)```\n\nUpdated code:\n```python\ndef __init__(self, cost=0.0015):\n        \n        \n        \n        self.cost_per_dollar = float(cost)\n```\n\nOriginal documentation:\n```python\nCost parameter is the cost of a trade per-dollar. 0.0015\n        on $1 million means $1,500 commission (=1,000,000 x 0.0015)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Cost parameter is the cost of a trade per-dollar. 0.0015\n        on $1 million means $1,500 commission (=1M * 0.0015)", "file": "zipline_finance_commission.py", "function": "PerDollar.__init__", "project": "zipline", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_balance(self, params={}):\n        \n        \n        \n        sandboxMode = self.safe_value(self.options, 'sandboxMode', False)\n        self.load_markets()\n        marketType, query = self.handle_market_type_and_params('fetchBalance', None, params)\n        method = self.get_supported_mapping(marketType, {\n            'spot': 'privateSpotGetAccountAssets',\n            'swap': 'privateMixGetAccountAccounts',\n        })\n        request = {}\n        if marketType == 'swap':\n            subType = None\n            subType, params = self.handle_sub_type_and_params('fetchBalance', None, params)\n            productType = 'UMCBL' if (subType == 'linear') else 'DMCBL'\n            if sandboxMode:\n                productType = 'S' + productType\n            request['productType'] = productType\n        response = getattr(self, method)(self.extend(request, query))\n        # spot\n        #     {\n        #       code: '00000',\n        #       msg: 'success',\n        #       requestTime: 1645928868827,\n        #       data: [\n        #         {\n        #           coinId: 1,\n        #           coinName: 'BTC',\n        #           available: '0.00070000',\n        #           frozen: '0.00000000',\n        #           lock: '0.00000000',\n        #           uTime: '1645921706000'\n        #         }\n        #       ]\n        #     }\n        #\n        # swap\n        #     {\n        #       code: '00000',\n        #       msg: 'success',\n        #       requestTime: 1645928929251,\n        #       data: [\n        #         {\n        #           marginCoin: 'USDT',\n        #           locked: '0',\n        #           available: '8.078525',\n        #           crossMaxAvailable: '8.078525',\n        #           fixedMaxAvailable: '8.078525',\n        #           maxTransferOut: '8.078525',\n        #           equity: '10.02508',\n        #           usdtEquity: '10.02508',\n        #           btcEquity: '0.00026057027'\n        #         }\n        #       ]\n        #     }\n        data = self.safe_value(response, 'data')\n        return self.parse_balance(data)```\n\nUpdated code:\n```python\ndef fetch_balance(self, params={}):\n        \n        \n        \n        sandboxMode = self.safe_value(self.options, 'sandboxMode', False)\n        self.load_markets()\n        request = {}\n        marketType = None\n        marginMode = None\n        response = None\n        marketType, params = self.handle_market_type_and_params('fetchBalance', None, params)\n        marginMode, params = self.handle_margin_mode_and_params('fetchBalance', params)\n        if (marketType == 'swap') or (marketType == 'future'):\n            subType = None\n            subType, params = self.handle_sub_type_and_params('fetchBalance', None, params)\n            productType = 'UMCBL' if (subType == 'linear') else 'DMCBL'\n            if sandboxMode:\n                productType = 'S' + productType\n            request['productType'] = productType\n            response = self.privateMixGetAccountAccounts(self.extend(request, params))\n        elif marginMode == 'isolated':\n            response = self.privateMarginGetIsolatedAccountAssets(self.extend(request, params))\n        elif marginMode == 'cross':\n            response = self.privateMarginGetCrossAccountAssets(self.extend(request, params))\n        elif marketType == 'spot':\n            response = self.privateSpotGetAccountAssets(self.extend(request, params))\n        else:\n            raise NotSupported(self.id + ' fetchBalance() does not support ' + marketType + ' accounts')\n        # spot\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697507299139,\n        #         \"data\": [\n        #             {\n        #                 \"coinId\": 1,\n        #                 \"coinName\": \"BTC\",\n        #                 \"available\": \"0.00000000\",\n        #                 \"frozen\": \"0.00000000\",\n        #                 \"lock\": \"0.00000000\",\n        #                 \"uTime\": \"1697248128000\"\n        #             },\n        #         ]\n        #     }\n        #\n        # swap\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697507505367,\n        #         \"data\": [\n        #             {\n        #                 \"marginCoin\": \"STETH\",\n        #                 \"locked\": \"0\",\n        #                 \"available\": \"0\",\n        #                 \"crossMaxAvailable\": \"0\",\n        #                 \"fixedMaxAvailable\": \"0\",\n        #                 \"maxTransferOut\": \"0\",\n        #                 \"equity\": \"0\",\n        #                 \"usdtEquity\": \"0\",\n        #                 \"btcEquity\": \"0\",\n        #                 \"crossRiskRate\": \"0\",\n        #                 \"unrealizedPL\": \"0\",\n        #                 \"bonus\": \"0\"\n        #             },\n        #         ]\n        #     }\n        #\n        # isolated margin\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697501436571,\n        #         \"data\": [\n        #             {\n        #                 \"symbol\": \"BTCUSDT\",\n        #                 \"coin\": \"BTC\",\n        #                 \"totalAmount\": \"0.00021654\",\n        #                 \"available\": \"0.00021654\",\n        #                 \"transferable\": \"0.00021654\",\n        #                 \"frozen\": \"0\",\n        #                 \"borrow\": \"0\",\n        #                 \"interest\": \"0\",\n        #                 \"net\": \"0.00021654\",\n        #                 \"ctime\": \"1697248128071\"\n        #             },\n        #         ]\n        #     }\n        #\n        # cross margin\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1697515463804,\n        #         \"data\": [\n        #             {\n        #                 \"coin\": \"BTC\",\n        #                 \"totalAmount\": \"0.00024996\",\n        #                 \"available\": \"0.00024996\",\n        #                 \"transferable\": \"0.00004994\",\n        #                 \"frozen\": \"0\",\n        #                 \"borrow\": \"0.0001\",\n        #                 \"interest\": \"0.00000001\",\n        #                 \"net\": \"0.00014995\",\n        #                 \"ctime\": \"1697251265504\"\n        #             },\n        #         ]\n        #     }\n        #\n        data = self.safe_value(response, 'data', [])\n        return self.parse_balance(data)\n```\n\nOriginal documentation:\n```python\nquery for balance and get the amount of funds available for trading or funds locked in orders\n        see https://bitgetlimited.github.io/apidoc/en/spot/#get-account-assets\n        see https://bitgetlimited.github.io/apidoc/en/mix/#get-account-list\n        :param dict [params]: extra parameters specific to the bitget api endpoint\n        :returns dict: a `balance structure <https://github.com/ccxt/ccxt/wiki/Manual#balance-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "query for balance and get the amount of funds available for trading or funds locked in orders\n        see https://bitgetlimited.github.io/apidoc/en/spot/#get-account-assets\n        see https://bitgetlimited.github.io/apidoc/en/mix/#get-account-list\n        see https://bitgetlimited.github.io/apidoc/en/margin/#get-cross-assets\n        see https://bitgetlimited.github.io/apidoc/en/margin/#get-isolated-assets\n        :param dict [params]: extra parameters specific to the bitget api endpoint\n        :returns dict: a `balance structure <https://github.com/ccxt/ccxt/wiki/Manual#balance-structure>`", "file": "python_ccxt_bitget.py", "function": "bitget.fetch_balance", "project": "ccxt", "change_stats": {"code_changes": 133, "doc_changes": 4, "total_changes": 137}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef guess_n_streams(size, warn=True):\n    \n    \n    \n    # TODO: a smart way of choosing the number of streams, see #612.\n    # Note that this code was moved out of `MRG_RandomStreams` so that it can\n    # be easily accessed from tests, where we want to disable the warning.\n    if (isinstance(size, (tuple, list)) and\n        all([isinstance(i, int) for i in size])):\n        # We can make a guess.\n        r = 1\n        for s in size:\n            r *= s\n        if r > 6:\n            r = r/6 # chosen as fastest for rbm_benchmark\n        return min(r, 30 * 256)\n    else:\n        if warn:\n            warnings.warn((\n                    \"MRG_RandomStreams Can't determine #streams from \"\n                    \"size (%s), guessing 30*256\") % str(size),\n                    stacklevel=3)\n        return 30 * 256```\n\nUpdated code:\n```python\ndef guess_n_streams(size, warn=True):\n    \n    \n    \n    # TODO: a smart way of choosing the number of streams, see #612.\n    # Note that this code was moved out of `MRG_RandomStreams` so that it can\n    # be easily accessed from tests, where we want to disable the warning.\n    if (isinstance(size, (tuple, list)) and\n        all([isinstance(i, int) for i in size])):\n        # We can make a guess.\n        r = 1\n        for s in size:\n            r *= s\n        if r > 6:\n            r = r/6 # chosen as fastest for rbm_benchmark\n\n        # The purpose of sampling from many streams is to be able to use\n        # the GPU to its full capacity.  It just wastes RAM and stream-initialization time to\n        # allocate more streams than necessary for the GPU.\n        # XXX: This number is chosen to be good for 280 and 480 architectures,\n        #      Better would be to use pycuda to query the number of\n        #      processors on the GPU device,\n        #      rather than guessing 60.\n        return min(r, 60 * 256)\n    else:\n        if warn:\n            warnings.warn((\n                    \"MRG_RandomStreams Can't determine #streams from \"\n                    \"size (%s), guessing 60*256\") % str(size),\n                    stacklevel=3)\n        return 60 * 256\n```\n\nOriginal documentation:\n```python\nReturn a guess at a good number of streams.\n\n    :param warn: If True, warn when a guess cannot be made (in which case\n    we return 30 * 256).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a guess at a good number of streams.\n\n    :param warn: If True, warn when a guess cannot be made (in which case\n    we return 60 * 256).", "file": "theano_sandbox_rng_mrg.py", "function": "guess_n_streams", "project": "Theano", "change_stats": {"code_changes": 16, "doc_changes": 4, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sort(self, cost=True, weight=True):\n        \n\n        \n\n        \n        attr = []\n        if weight:\n            attr.append('weight')\n        if cost:\n            attr.append('cost')\n        if len(attr):\n            grids = sorted(self.grids, key=operator.attrgetter(*attr))\n            return SelectedGrids(grids)\n        else:\n            return self```\n\nUpdated code:\n```python\ndef sort(self, *args):\n        \n        \n        \n        if len(args):\n            grids = sorted(self.grids, key=operator.attrgetter(*args))\n            return SelectedGrids(grids)\n        else:\n            return self\n```\n\nOriginal documentation:\n```python\nArgs:\n            cost (bool):\n            weight (bool):\n\n        Returns:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            args (str): Attribute name to sort.\n\n        Returns:\n            SelectedGrids:", "file": "module_map_map_grids.py", "function": "SelectedGrids.sort", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 15, "doc_changes": 6, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, axis, nonpos='mask'):\n        \n        \n        \n        self._transform = LogitTransform(nonpos)```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        axis,\n        nonpos='mask',\n        *,\n        one_half=r\"\\frac{1}{2}\",\n        use_overline=False,\n    ):\n        \n        \n        self._transform = LogitTransform(nonpos)\n        self._use_overline = use_overline\n        self._one_half = one_half\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        axis : `matplotlib.axis.Axis`\n            Currently unused.\n        nonpos : {'mask', 'clip'}\n            Determines the behavior for values beyond the open interval ]0, 1[.\n            They can either be masked as invalid, or clipped to a number very\n            close to 0 or 1.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Parameters\n        ----------\n        axis : `matplotlib.axis.Axis`\n            Currently unused.\n        nonpos : {'mask', 'clip'}\n            Determines the behavior for values beyond the open interval ]0, 1[.\n            They can either be masked as invalid, or clipped to a number very\n            close to 0 or 1.\n        use_overline : bool, default: False\n            Indicate the usage of survival notation (\\overline{x}) in place of\n            standard notation (1-x) for probability close to one.\n        one_half : str, default: r\"\\frac{1}{2}\"\n            The string used for ticks formatter to represent 1/2.", "file": "lib_matplotlib_scale.py", "function": "LogitScale.__init__", "project": "matplotlib", "change_stats": {"code_changes": 14, "doc_changes": 10, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef response_delete(self, request, obj_display, obj_id):\n        \n        \n        \n\n        opts = self.model._meta\n\n        if IS_POPUP_VAR in request.POST:\n            popup_response_data = json.dumps({\n                'action': 'delete',\n                'value': str(obj_id),\n            })\n            return TemplateResponse(request, self.popup_response_template or [\n                'admin/%s/%s/popup_response.html' % (opts.app_label, opts.model_name),\n                'admin/%s/popup_response.html' % opts.app_label,\n                'admin/popup_response.html',\n            ], {\n                'popup_response_data': popup_response_data,\n            })\n\n        self.message_user(\n            request,\n            _('The %(name)s \"%(obj)s\" was deleted successfully.') % {\n                'name': force_text(opts.verbose_name),\n                'obj': force_text(obj_display),\n            },\n            messages.SUCCESS,\n        )\n\n        if self.has_change_permission(request, None):\n            post_url = reverse(\n                'admin:%s_%s_changelist' % (opts.app_label, opts.model_name),\n                current_app=self.admin_site.name,\n            )\n            preserved_filters = self.get_preserved_filters(request)\n            post_url = add_preserved_filters(\n                {'preserved_filters': preserved_filters, 'opts': opts}, post_url\n            )\n        else:\n            post_url = reverse('admin:index', current_app=self.admin_site.name)\n        return HttpResponseRedirect(post_url)```\n\nUpdated code:\n```python\ndef response_delete(self, request, obj_display, obj_id):\n        \n        \n        \n        opts = self.model._meta\n\n        if IS_POPUP_VAR in request.POST:\n            popup_response_data = json.dumps({\n                'action': 'delete',\n                'value': str(obj_id),\n            })\n            return TemplateResponse(request, self.popup_response_template or [\n                'admin/%s/%s/popup_response.html' % (opts.app_label, opts.model_name),\n                'admin/%s/popup_response.html' % opts.app_label,\n                'admin/popup_response.html',\n            ], {\n                'popup_response_data': popup_response_data,\n            })\n\n        self.message_user(\n            request,\n            _('The %(name)s \"%(obj)s\" was deleted successfully.') % {\n                'name': force_text(opts.verbose_name),\n                'obj': force_text(obj_display),\n            },\n            messages.SUCCESS,\n        )\n\n        if self.has_change_permission(request, None):\n            post_url = reverse(\n                'admin:%s_%s_changelist' % (opts.app_label, opts.model_name),\n                current_app=self.admin_site.name,\n            )\n            preserved_filters = self.get_preserved_filters(request)\n            post_url = add_preserved_filters(\n                {'preserved_filters': preserved_filters, 'opts': opts}, post_url\n            )\n        else:\n            post_url = reverse('admin:index', current_app=self.admin_site.name)\n        return HttpResponseRedirect(post_url)\n```\n\nOriginal documentation:\n```python\nDetermines the HttpResponse for the delete_view stage.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Determine the HttpResponse for the delete_view stage.", "file": "django_contrib_admin_options.py", "function": "ModelAdmin.response_delete", "project": "django", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_registry_conflict(ray_start_4_cpus, tmpdir, use_workaround, exit_same):\n    \n    \n    # Create file markers\n    run_1_running = tmpdir / \"run_1_running\"\n    run_1_finished = tmpdir / \"run_1_finished\"\n    run_2_finished = tmpdir / \"run_2_finished\"\n\n    run_1_running.write_text(\"\", encoding=\"utf-8\")\n    run_1_finished.write_text(\"\", encoding=\"utf-8\")\n    run_2_finished.write_text(\"\", encoding=\"utf-8\")\n\n    ray_address = ray_start_4_cpus.address_info[\"address\"]\n\n    run_1_env = {\n        \"RAY_ADDRESS\": ray_address,\n        \"WORKAROUND\": str(int(use_workaround)),\n        \"FIXED_VAL\": str(1),\n        \"VAL_1\": str(2),\n        \"VAL_2\": str(3),\n        # Run 1 can start immediately\n        \"HANG_RUN_MARKER\": \"\",\n        # Allow second run to start once first trial of first run is started\n        \"DELETE_TRIAL_MARKER\": str(run_1_running),\n        # Hang in first trial until the second run finished\n        \"HANG_TRIAL_MARKER\": str(run_2_finished),\n        # Mark run 1 as completed\n        \"DELETE_RUN_MARKER\": str(run_1_finished),\n        # Do not wait at end\n        \"HANG_END_MARKER\": \"\",\n    }\n\n    run_2_env = {\n        \"RAY_ADDRESS\": ray_address,\n        \"WORKAROUND\": str(int(use_workaround)),\n        \"FIXED_VAL\": str(4),\n        \"VAL_1\": str(5),\n        \"VAL_2\": str(6),\n        # Wait until first trial of first run is running\n        \"HANG_RUN_MARKER\": str(run_1_running),\n        # Don't delete during run\n        \"DELETE_TRIAL_MARKER\": \"\",\n        # No need to hang in trial\n        \"HANG_TRIAL_MARKER\": \"\",\n        # After full run finished, allow first run to continue\n        \"DELETE_RUN_MARKER\": str(run_2_finished),\n        # Wait until first run finished\n        # If we don't do this, we actually don't die because of parameter conflict\n        # but because of \"The object's owner has exited\" - so we test this\n        # separately\n        \"HANG_END_MARKER\": str(run_1_finished) if exit_same else \"\",\n    }\n\n    script_path = Path(__file__).parent / \"_test_multi_tenancy_run.py\"\n\n    run_1 = subprocess.Popen(\n        [sys.executable, script_path], env=run_1_env, stderr=subprocess.PIPE\n    )\n    print(\"Started run 1:\", run_1.pid)\n\n    run_2 = subprocess.Popen([sys.executable, script_path], env=run_2_env)\n    print(\"Started run 2:\", run_2.pid)\n\n    assert run_2.wait() == 0\n\n    if use_workaround:\n        assert run_1.wait() == 0\n    else:\n        assert run_1.wait() != 0\n\n        stderr = run_1.stderr.read().decode()\n\n        if not exit_same:\n            assert \"OwnerDiedError\" in stderr, stderr\n        else:\n            assert \"AssertionError\" in stderr, stderr```\n\nUpdated code:\n```python\ndef test_registry_conflict(ray_start_4_cpus, tmpdir, exit_same):\n    \n    \n    # Create file markers\n    run_1_running = tmpdir / \"run_1_running\"\n    run_1_finished = tmpdir / \"run_1_finished\"\n    run_2_finished = tmpdir / \"run_2_finished\"\n\n    run_1_running.write_text(\"\", encoding=\"utf-8\")\n    run_1_finished.write_text(\"\", encoding=\"utf-8\")\n    run_2_finished.write_text(\"\", encoding=\"utf-8\")\n\n    ray_address = ray_start_4_cpus.address_info[\"address\"]\n\n    run_1_env = {\n        \"RAY_ADDRESS\": ray_address,\n        \"FIXED_VAL\": str(1),\n        \"VAL_1\": str(2),\n        \"VAL_2\": str(3),\n        # Run 1 can start immediately\n        \"HANG_RUN_MARKER\": \"\",\n        # Allow second run to start once first trial of first run is started\n        \"DELETE_TRIAL_MARKER\": str(run_1_running),\n        # Hang in first trial until the second run finished\n        \"HANG_TRIAL_MARKER\": str(run_2_finished),\n        # Mark run 1 as completed\n        \"DELETE_RUN_MARKER\": str(run_1_finished),\n        # Do not wait at end\n        \"HANG_END_MARKER\": \"\",\n    }\n\n    run_2_env = {\n        \"RAY_ADDRESS\": ray_address,\n        \"FIXED_VAL\": str(4),\n        \"VAL_1\": str(5),\n        \"VAL_2\": str(6),\n        # Wait until first trial of first run is running\n        \"HANG_RUN_MARKER\": str(run_1_running),\n        # Don't delete during run\n        \"DELETE_TRIAL_MARKER\": \"\",\n        # No need to hang in trial\n        \"HANG_TRIAL_MARKER\": \"\",\n        # After full run finished, allow first run to continue\n        \"DELETE_RUN_MARKER\": str(run_2_finished),\n        # Wait until first run finished\n        # If we don't do this, we actually don't die because of parameter conflict\n        # but because of \"The object's owner has exited\" - so we test this\n        # separately\n        \"HANG_END_MARKER\": str(run_1_finished) if exit_same else \"\",\n    }\n\n    script_path = Path(__file__).parent / \"_test_multi_tenancy_run.py\"\n\n    run_1 = subprocess.Popen(\n        [sys.executable, script_path], env=run_1_env, stderr=subprocess.PIPE\n    )\n    print(\"Started run 1:\", run_1.pid)\n\n    run_2 = subprocess.Popen([sys.executable, script_path], env=run_2_env)\n    print(\"Started run 2:\", run_2.pid)\n\n    assert run_2.wait() == 0\n    assert run_1.wait() == 0\n```\n\nOriginal documentation:\n```python\nTwo concurrent Tune runs can conflict with each other when they\n    use a trainable with the same name.\n\n    This test starts two runs in parallel and asserts that a workaround used\n    in the docs can alleviate the problem.\n\n    This is how we schedule the runs:\n\n    - We have two runs. Every run starts two trials.\n    - Run 1 starts 1 trial immediately. This trial starts with\n      the correct parameters for the script. The trial hangs until the file\n      ``run_2_finished`` is deleted.\n    - Run 2 starts as soon as the first trial of Run 1 runs (by waiting\n      until the ``run_1_running`` file is deleted by that trial). It will overwrite\n      the global registry trainable with the same name.\n    - Run 2 finishes both trials. The script finishes with the expected\n      parameters.\n    - Run 2 then deletes the ``run_2_finished`` marker, allowing Run 1 trial 1\n      to continue training. When training finishes, the second trial launches.\n      This second trial then uses the overwritten trainable, that is, the wrong\n      parameters unless you use the workaround.\n    - Run 1 finally finishes, and we compare the expected results with the actual\n      results.\n\n    When you don't use the workaround, expect an assertion error (if ``exit_same=True``,\n    see below), otherwise a KeyError (because a trial failed).\n    When the workaround is used, we expect everything to run without error.\n\n    NOTE: Two errors can occur with registry conflicts. First,\n    the trainable can be overwritten and captured, for example, when a fixed value\n    is included in the trainable. The second trial of run 1 then has a wrong\n    parameter and reports a wrong metric (from run 2).\n\n    The second error occurs when the second run finishes fully and its objects\n    are garbage collected. In this case, the first run tries to find the trainable\n    registered by run 2, but fails lookup because the objects have been\n    removed already. Note that these objects are registered with\n    ``tune.with_parameters()`` (not the global registry store).\n    We test both scenarios using the ``exit_same`` parameter.\n\n    NOTE: If we resolve the registry issue (for example, with unique keys)\n    you can remove the test that expects the assertion error. We can remove\n    the parametrization and the workaround and assert that no conflict occurs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Two concurrent Tune runs can conflict with each other when they\n    use a trainable with the same name.\n\n    This test starts two runs in parallel and asserts that our fix in\n    https://github.com/ray-project/ray/pull/33095 resolves the issue.\n\n    This is how we schedule the runs:\n\n    - We have two runs. Every run starts two trials.\n    - Run 1 starts 1 trial immediately. This trial starts with\n      the correct parameters for the script. The trial hangs until the file\n      ``run_2_finished`` is deleted.\n    - Run 2 starts as soon as the first trial of Run 1 runs (by waiting\n      until the ``run_1_running`` file is deleted by that trial). It will overwrite\n      the global registry trainable with the same name.\n    - Run 2 finishes both trials. The script finishes with the expected\n      parameters.\n    - Run 2 then deletes the ``run_2_finished`` marker, allowing Run 1 trial 1\n      to continue training. When training finishes, the second trial launches.\n      This second trial then uses the overwritten trainable, that is, the wrong\n      parameters unless you use the workaround.\n    - Run 1 finally finishes, and we compare the expected results with the actual\n      results.\n\n    NOTE: Two errors can occur with registry conflicts. First,\n    the trainable can be overwritten and captured, for example, when a fixed value\n    is included in the trainable. The second trial of run 1 then has a wrong\n    parameter and reports a wrong metric (from run 2).\n\n    The second error occurs when the second run finishes fully and its objects\n    are garbage collected. In this case, the first run tries to find the trainable\n    registered by run 2, but fails lookup because the objects have been\n    removed already. Note that these objects are registered with\n    ``tune.with_parameters()`` (not the global registry store).\n    We test both scenarios using the ``exit_same`` parameter.", "file": "python_ray_tune_tests_test_multi_tenancy.py", "function": "test_registry_conflict", "project": "ray", "change_stats": {"code_changes": 19, "doc_changes": 14, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef gt_roidb(self):\n        \n        \n        \n        cache_file = osp.join(self.cache_path, self.name + '_gt_roidb.pkl')\n\n        gt_roidb = [self._load_gta_annotation(index)\n                    for index in self._image_index]\n\n        return gt_roidb```\n\nUpdated code:\n```python\ndef gt_roidb(self):\n        \n        \n        \n        gt_roidb = [self._load_gta_annotation(index)\n                    for index in self._image_index]\n\n        return gt_roidb\n```\n\nOriginal documentation:\n```python\nReturn the database of ground-truth regions of interest.\n        This function loads/saves from/to a cache file to speed up future calls.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the database of ground-truth regions of interest.", "file": "faster-rcnn.pytorch_lib_datasets_gta.py", "function": "gta_det.gt_roidb", "project": "3d-vehicle-tracking", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ioff():\n    \n    \n    \n    return _IoffContext()```\n\nUpdated code:\n```python\ndef ioff():\n    \n    \n    \n    stack = ExitStack()\n    stack.callback(ion if isinteractive() else ioff)\n    matplotlib.interactive(False)\n    uninstall_repl_displayhook()\n    return stack\n```\n\nOriginal documentation:\n```python\nDisable interactive mode.\n\n    See `.pyplot.isinteractive` for more details.\n\n    See Also\n    --------\n    ion : Enable interactive mode.\n    isinteractive : Whether interactive mode is enabled.\n    show : Show all figures (and maybe block).\n    pause : Show all figures, and block for a time.\n\n    Notes\n    -----\n    For a temporary change, this can be used as a context manager::\n\n        # if interactive mode is on\n        # then figures will be shown on creation\n        plt.ion()\n        # This figure will be shown immediately\n        fig = plt.figure()\n\n        with plt.ioff():\n            # interactive mode will be off\n            # figures will not automatically be shown\n            fig2 = plt.figure()\n            # ...\n\n    To enable usage as a context manager, this function returns an\n    ``_IoffContext`` object. The return value is not intended to be stored\n    or accessed by the user.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Disable interactive mode.\n\n    See `.pyplot.isinteractive` for more details.\n\n    See Also\n    --------\n    ion : Enable interactive mode.\n    isinteractive : Whether interactive mode is enabled.\n    show : Show all figures (and maybe block).\n    pause : Show all figures, and block for a time.\n\n    Notes\n    -----\n    For a temporary change, this can be used as a context manager::\n\n        # if interactive mode is on\n        # then figures will be shown on creation\n        plt.ion()\n        # This figure will be shown immediately\n        fig = plt.figure()\n\n        with plt.ioff():\n            # interactive mode will be off\n            # figures will not automatically be shown\n            fig2 = plt.figure()\n            # ...\n\n    To enable optional usage as a context manager, this function returns a\n    `~contextlib.ExitStack` object, which is not intended to be stored or\n    accessed by the user.", "file": "lib_matplotlib_pyplot.py", "function": "ioff", "project": "matplotlib", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef initialize(ep_size=1, mpu=None):\n    \n    \n\n    \n    if mpu is not None:\n        log_dist(message=\"initializing deepspeed groups using mpu\", ranks=[0])\n        initialize_model_and_expert_parallel(ep_size, mpu)\n    else:\n        log_dist(message=\"initializing deepspeed groups\", ranks=[0])\n        initialize_model_parallel(1)\n        initialize_expert_parallel(ep_size)```\n\nUpdated code:\n```python\ndef initialize(ep_size=1, mpu=None, num_ep_list=None):\n    \n    \n\n    \n\n    if num_ep_list is None:\n        num_ep_list = [ep_size]\n\n    assert max(num_ep_list) >= ep_size, f\"ep_size={ep_size} is larger than the largest num_ep_list={max(num_ep_list)}, you should reduce expert parallel size\"\n\n    num_ep_list = list(set(num_ep_list))  # remove duplicates\n    num_ep_list.sort()  # sort in ascending order\n    for num_ep in num_ep_list:\n        assert num_ep > 0, 'num_ep must be positive'\n        assert num_ep % ep_size == 0 or ep_size % num_ep == 0, 'num_ep must be divisible/divided by ep_size'\n\n    if mpu is not None:\n        log_dist(message=\"initializing deepspeed groups using mpu\", ranks=[0])\n        initialize_model_and_expert_parallel(ep_size, mpu, num_ep_list)\n    else:\n        log_dist(message=\"initializing deepspeed groups\", ranks=[0])\n        initialize_model_parallel(1)\n        initialize_expert_parallel(ep_size, num_ep_list)\n```\n\nOriginal documentation:\n```python\nProcess groups initialization supporting expert (E), data (D), and model (M) parallelism. DeepSpeed considers\n    the following scenarios w.r.t. process group creation.\n\n    * S1: There is no expert parallelism or model parallelism, only data (D)::\n\n        model = my_model(args)\n        engine = deepspeed.initialize(model) # initialize groups without mpu\n\n    * S2: There is expert parallelism but no model parallelism (E+D)::\n\n        deepspeed.utils.groups.initialize(ep_size) # groups will be initialized here\n        model = my_model(args)\n        engine = deepspeed.initialize(model)\n\n    * S3: There is model parallelism but no expert parallelism (M)::\n\n        mpu.init() # client initializes it's model parallel unit\n        model = my_model(args)\n        engine = deepspeed.initialize(model, mpu=mpu) # init w. mpu but ep_size = dp_world_size\n\n    * S4: There is model, data, and expert parallelism (E+D+M)::\n\n        mpu.init() # client initializes it's model parallel unit\n        deepspeed.utils.groups.initialize(ep_size, mpu) # initialize expert groups wrt mpu\n        model = my_model(args)\n        engine = deepspeed.initialize(model, mpu=mpu) # passing mpu is optional in this case\n\n    Arguments:\n        ep_size (int, optional): default=1, expert parallel size\n        mpu (module, optional): default=None, model parallel unit (e.g., from Megatron)\n            that describes model/data parallel ranks.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Process groups initialization supporting expert (E), data (D), and model (M) parallelism. DeepSpeed considers\n    the following scenarios w.r.t. process group creation.\n\n    * S1: There is no expert parallelism or model parallelism, only data (D)::\n\n        model = my_model(args)\n        engine = deepspeed.initialize(model) # initialize groups without mpu\n\n    * S2: There is expert parallelism but no model parallelism (E+D)::\n\n        deepspeed.utils.groups.initialize(ep_size) # groups will be initialized here\n        model = my_model(args)\n        engine = deepspeed.initialize(model)\n\n    * S3: There is model parallelism but no expert parallelism (M)::\n\n        mpu.init() # client initializes it's model parallel unit\n        model = my_model(args)\n        engine = deepspeed.initialize(model, mpu=mpu) # init w. mpu but ep_size = dp_world_size\n\n    * S4: There is model, data, and expert parallelism (E+D+M)::\n\n        mpu.init() # client initializes it's model parallel unit\n        deepspeed.utils.groups.initialize(ep_size, mpu) # initialize expert groups wrt mpu\n        model = my_model(args)\n        engine = deepspeed.initialize(model, mpu=mpu) # passing mpu is optional in this case\n\n    Arguments:\n        ep_size (int, optional): default=1, maximum expert parallel size, which should be divisible/divided by the world size.\n        by each element in num_ep_list.\n        mpu (module, optional): default=None, model parallel unit (e.g., from Megatron)\n            that describes model/data parallel ranks.\n        num_ep_list (list, optional): default=None, list of number of expert parallel sizes in each MoE layer.", "file": "deepspeed_utils_groups.py", "function": "initialize", "project": "DeepSpeed", "change_stats": {"code_changes": 20, "doc_changes": 6, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, axis, **kwargs):\n        \n        \n        \n        if axis.axis_name == 'x':\n            base = kwargs.pop('basex', 10.0)\n            linthresh = kwargs.pop('linthreshx', 2.0)\n            subs = kwargs.pop('subsx', None)\n        else:\n            base = kwargs.pop('basey', 10.0)\n            linthresh = kwargs.pop('linthreshy', 2.0)\n            subs = kwargs.pop('subsy', None)\n\n        self._transform = self.SymmetricalLogTransform(base, linthresh)\n\n        assert base > 0.0\n        assert linthresh > 0.0\n\n        self.base = base\n        self.linthresh = linthresh\n        self.subs = subs```\n\nUpdated code:\n```python\ndef __init__(self, axis, **kwargs):\n        \n        \n        \n        if axis.axis_name == 'x':\n            base = kwargs.pop('basex', 10.0)\n            linthresh = kwargs.pop('linthreshx', 2.0)\n            subs = kwargs.pop('subsx', None)\n            linscale = kwargs.pop('linscalex', 1.0)\n        else:\n            base = kwargs.pop('basey', 10.0)\n            linthresh = kwargs.pop('linthreshy', 2.0)\n            subs = kwargs.pop('subsy', None)\n            linscale = kwargs.pop('linscaley', 1.0)\n\n        assert base > 1.0\n        assert linthresh > 0.0\n        assert linscale >= 1.0\n\n        self._transform = self.SymmetricalLogTransform(base, linthresh, linscale)\n\n        self.base = base\n        self.linthresh = linthresh\n        self.linscale = linscale\n        self.subs = subs\n```\n\nOriginal documentation:\n```python\n*basex*/*basey*:\n           The base of the logarithm\n\n        *linthreshx*/*linthreshy*:\n          The range (-*x*, *x*) within which the plot is linear (to\n          avoid having the plot go to infinity around zero).\n\n        *subsx*/*subsy*:\n           Where to place the subticks between each major tick.\n           Should be a sequence of integers.  For example, in a log10\n           scale: ``[2, 3, 4, 5, 6, 7, 8, 9]``\n\n           will place 8 logarithmically spaced minor ticks between\n           each major tick.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "*basex*/*basey*:\n           The base of the logarithm\n\n        *linthreshx*/*linthreshy*:\n          The range (-*x*, *x*) within which the plot is linear (to\n          avoid having the plot go to infinity around zero).\n\n        *subsx*/*subsy*:\n           Where to place the subticks between each major tick.\n           Should be a sequence of integers.  For example, in a log10\n           scale: ``[2, 3, 4, 5, 6, 7, 8, 9]``\n\n           will place 8 logarithmically spaced minor ticks between\n           each major tick.\n\n        *linscalex*/*linscaley*:\n           This allows the linear range (-*linthresh* to *linthresh*)\n           to be stretched relative to the logarithmic range.  Its\n           value is the number of decades to use for each half of the\n           linear range.  For example, when *linscale* == 1.0 (the\n           default), the space used for the positive and negative\n           halves of the linear range will be equal to one decade in\n           the logarithmic range.", "file": "lib_matplotlib_scale.py", "function": "SymmetricalLogScale.__init__", "project": "matplotlib", "change_stats": {"code_changes": 12, "doc_changes": 11, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef predict(self, df=None):\n        \n        \n        if self.history is None:\n            raise Exception('Model has not been fit.')\n\n        if df is None:\n            df = self.history.copy()\n        else:\n            if df.shape[0] == 0:\n                raise ValueError('Dataframe has no rows.')\n            df = self.setup_dataframe(df.copy())\n\n        df['trend'] = self.predict_trend(df)\n        seasonal_components = self.predict_seasonal_components(df)\n        if self.uncertainty_samples:\n            intervals = self.predict_uncertainty(df)\n        else:\n            intervals = None\n\n        # Drop columns except ds, cap, floor, and trend\n        cols = ['ds', 'trend']\n        if 'cap' in df:\n            cols.append('cap')\n        if self.logistic_floor:\n            cols.append('floor')\n        # Add in forecast components\n        df2 = pd.concat((df[cols], intervals, seasonal_components), axis=1)\n        df2['yhat'] = (\n                df2['trend'] * (1 + df2['multiplicative_terms'])\n                + df2['additive_terms']\n        )\n        return df2```\n\nUpdated code:\n```python\ndef predict(self, df: pd.DataFrame = None, vectorized: bool = True) -> pd.DataFrame:\n        \n        \n        if self.history is None:\n            raise Exception('Model has not been fit.')\n\n        if df is None:\n            df = self.history.copy()\n        else:\n            if df.shape[0] == 0:\n                raise ValueError('Dataframe has no rows.')\n            df = self.setup_dataframe(df.copy())\n\n        df['trend'] = self.predict_trend(df)\n        seasonal_components = self.predict_seasonal_components(df)\n        if self.uncertainty_samples:\n            intervals = self.predict_uncertainty(df, vectorized)\n        else:\n            intervals = None\n\n        # Drop columns except ds, cap, floor, and trend\n        cols = ['ds', 'trend']\n        if 'cap' in df:\n            cols.append('cap')\n        if self.logistic_floor:\n            cols.append('floor')\n        # Add in forecast components\n        df2 = pd.concat((df[cols], intervals, seasonal_components), axis=1)\n        df2['yhat'] = (\n                df2['trend'] * (1 + df2['multiplicative_terms'])\n                + df2['additive_terms']\n        )\n        return df2\n```\n\nOriginal documentation:\n```python\nPredict using the prophet model.\n\n        Parameters\n        ----------\n        df: pd.DataFrame with dates for predictions (column ds), and capacity\n            (column cap) if logistic growth. If not provided, predictions are\n            made on the history.\n\n        Returns\n        -------\n        A pd.DataFrame with the forecast components.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Predict using the prophet model.\n\n        Parameters\n        ----------\n        df: pd.DataFrame with dates for predictions (column ds), and capacity\n            (column cap) if logistic growth. If not provided, predictions are\n            made on the history.\n        vectorized: Whether to use a vectorized method to compute uncertainty intervals. Suggest using\n            True (the default) for much faster runtimes in most cases,\n            except when (growth = 'logistic' and mcmc_samples > 0).\n\n        Returns\n        -------\n        A pd.DataFrame with the forecast components.", "file": "python_prophet_forecaster.py", "function": "Prophet.predict", "project": "prophet", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_ents(doc: Doc, options: Dict[str, Any] = {}) -> Dict[str, Any]:\n    \n    \n    ents = [\n        {\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_}\n        for ent in doc.ents\n    ]\n    if not ents:\n        warnings.warn(Warnings.W006)\n    title = doc.user_data.get(\"title\", None) if hasattr(doc, \"user_data\") else None\n    settings = get_doc_settings(doc)\n    return {\"text\": doc.text, \"ents\": ents, \"title\": title, \"settings\": settings}```\n\nUpdated code:\n```python\ndef parse_ents(doc: Doc, options: Dict[str, Any] = {}) -> Dict[str, Any]:\n    \n    \n    kb_url_template = options.get(\"kb_url_template\", None)\n    ents = [\n        {\n            \"start\": ent.start_char,\n            \"end\": ent.end_char,\n            \"label\": ent.label_,\n            \"kb_id\": ent.kb_id_ if ent.kb_id_ else \"\",\n            \"kb_url\": kb_url_template.format(ent.kb_id_) if kb_url_template else \"#\",\n        }\n        for ent in doc.ents\n    ]\n    if not ents:\n        warnings.warn(Warnings.W006)\n    title = doc.user_data.get(\"title\", None) if hasattr(doc, \"user_data\") else None\n    settings = get_doc_settings(doc)\n    return {\"text\": doc.text, \"ents\": ents, \"title\": title, \"settings\": settings}\n```\n\nOriginal documentation:\n```python\nGenerate named entities in [{start: i, end: i, label: 'label'}] format.\n\n    doc (Doc): Document do parse.\n    RETURNS (dict): Generated entities keyed by text (original text) and ents.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate named entities in [{start: i, end: i, label: 'label'}] format.\n\n    doc (Doc): Document to parse.\n    options (Dict[str, Any]): NER-specific visualisation options.\n    RETURNS (dict): Generated entities keyed by text (original text) and ents.", "file": "spacy_displacy___init__.py", "function": "parse_ents", "project": "spaCy", "change_stats": {"code_changes": 11, "doc_changes": 5, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _load(self) -> keras.models.Model:\n         \n        \n        logger.debug(\"Loading model: %s\", self._filename)\n        if self._is_predict and not self.model_exists:\n            logger.error(\"Model could not be found in folder '%s'. Exiting\", self._model_dir)\n            sys.exit(1)\n\n        try:\n            model = load_model(self._filename, compile=False)\n        except RuntimeError as err:\n            if \"unable to get link info\" in str(err).lower():\n                msg = (f\"Unable to load the model from '{self._filename}'. This may be a \"\n                       \"temporary error but most likely means that your model has corrupted.\\n\"\n                       \"You can try to load the model again but if the problem persists you \"\n                       \"should use the Restore Tool to restore your model from backup.\\n\"\n                       f\"Original error: {str(err)}\")\n                raise FaceswapError(msg) from err\n            raise err\n        except KeyError as err:\n            if \"unable to open object\" in str(err).lower():\n                msg = (f\"Unable to load the model from '{self._filename}'. This may be a \"\n                       \"temporary error but most likely means that your model has corrupted.\\n\"\n                       \"You can try to load the model again but if the problem persists you \"\n                       \"should use the Restore Tool to restore your model from backup.\\n\"\n                       f\"Original error: {str(err)}\")\n                raise FaceswapError(msg) from err\n            raise err\n\n        logger.info(\"Loaded model from disk: '%s'\", self._filename)\n        return model```\n\nUpdated code:\n```python\ndef _load(self) -> tf.keras.models.Model:\n         \n        \n        logger.debug(\"Loading model: %s\", self._filename)\n        if self._is_predict and not self.model_exists:\n            logger.error(\"Model could not be found in folder '%s'. Exiting\", self._model_dir)\n            sys.exit(1)\n\n        try:\n            model = kmodels.load_model(self._filename, compile=False)\n        except RuntimeError as err:\n            if \"unable to get link info\" in str(err).lower():\n                msg = (f\"Unable to load the model from '{self._filename}'. This may be a \"\n                       \"temporary error but most likely means that your model has corrupted.\\n\"\n                       \"You can try to load the model again but if the problem persists you \"\n                       \"should use the Restore Tool to restore your model from backup.\\n\"\n                       f\"Original error: {str(err)}\")\n                raise FaceswapError(msg) from err\n            raise err\n        except KeyError as err:\n            if \"unable to open object\" in str(err).lower():\n                msg = (f\"Unable to load the model from '{self._filename}'. This may be a \"\n                       \"temporary error but most likely means that your model has corrupted.\\n\"\n                       \"You can try to load the model again but if the problem persists you \"\n                       \"should use the Restore Tool to restore your model from backup.\\n\"\n                       f\"Original error: {str(err)}\")\n                raise FaceswapError(msg) from err\n            raise err\n\n        logger.info(\"Loaded model from disk: '%s'\", self._filename)\n        return model\n```\n\nOriginal documentation:\n```python\nLoads the model from disk\n\n        If the predict function is to be called and the model cannot be found in the model folder\n        then an error is logged and the process exits.\n\n        When loading the model, the plugin model folder is scanned for custom layers which are\n        added to Keras' custom objects.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The saved model loaded from disk\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads the model from disk\n\n        If the predict function is to be called and the model cannot be found in the model folder\n        then an error is logged and the process exits.\n\n        When loading the model, the plugin model folder is scanned for custom layers which are\n        added to Keras' custom objects.\n\n        Returns\n        -------\n        :class:`tensorflow.keras.models.Model`\n            The saved model loaded from disk", "file": "plugins_train_model__base_io.py", "function": "IO._load", "project": "faceswap", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_multiple_ssh_clients(\n        self,\n        starttime,\n        saddr,\n        used_software,\n        major_v,\n        minor_v,\n        twid,\n        uid,\n    ):\n        \n        \n        \n        if 'SSH::CLIENT' not in used_software:\n            return\n\n        profileid = f'profile_{saddr}'\n        # what software was used before for this profile?\n        # returns a dict with software, 'version-major', 'version-minor'\n        cached_ssh_versions: dict = __database__.get_software_from_profile(\n            profileid\n        )\n        if not cached_ssh_versions:\n            # we have no previous software info about this saddr in out db\n            return False\n\n        cached_software = cached_ssh_versions['software']\n        if cached_software != used_software:\n            # we need them both to be \"SSH::CLIENT\"\n            return False\n\n        cached_major_v = cached_ssh_versions['version-major']\n        cached_minor_v = cached_ssh_versions['version-minor']\n        cached_versions = f'{cached_major_v}_{cached_minor_v}'\n        current_versions = f'{major_v}_{minor_v}'\n        if cached_versions == current_versions:\n            # they're using the same ssh client version\n            return False\n\n        # get the uid of the cached versions, and the uid of the current used versions\n        uids = [cached_ssh_versions['uid'], uid]\n        self.helper.set_evidence_multiple_ssh_versions(\n            saddr, cached_versions, current_versions, starttime, twid, uids\n        )\n        return True```\n\nUpdated code:\n```python\ndef check_multiple_ssh_clients(\n        self,\n        starttime,\n        saddr,\n        used_software,\n        major_v,\n        minor_v,\n        twid,\n        uid,\n        role='SSH::CLIENT'\n    ):\n        \n        \n        \n        if role not in used_software:\n            return\n\n        profileid = f'profile_{saddr}'\n        # what software was used before for this profile?\n        # returns a dict with software, 'version-major', 'version-minor'\n        cached_ssh_versions: dict = __database__.get_software_from_profile(\n            profileid\n        )\n        if not cached_ssh_versions:\n            # we have no previous software info about this saddr in out db\n            return False\n\n        cached_software = cached_ssh_versions['software']\n        if cached_software != used_software:\n            # we need them both to be the same role, either client or server\n            return False\n\n        cached_major_v = cached_ssh_versions['version-major']\n        cached_minor_v = cached_ssh_versions['version-minor']\n        cached_versions = f'{cached_major_v}_{cached_minor_v}'\n        current_versions = f'{major_v}_{minor_v}'\n        if cached_versions == current_versions:\n            # they're using the same ssh client version\n            return False\n\n        # get the uid of the cached versions, and the uid of the current used versions\n        uids = [cached_ssh_versions['uid'], uid]\n        self.helper.set_evidence_multiple_ssh_versions(\n            saddr, cached_versions, current_versions, starttime, twid, uids, role=role\n        )\n        return True\n```\n\nOriginal documentation:\n```python\nchecks if this srcip was detected using a different\n         ssh client versions before\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "checks if this srcip was detected using a different\n         ssh client or server versions before\n        :param role: can be 'SSH::CLIENT' or 'SSH::SERVER' as seen in zeek software.log flows", "file": "modules_flowalerts_flowalerts.py", "function": "Module.check_multiple_ssh_clients", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 9, "doc_changes": 5, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _do_policy_eval_w_trajectory_view_api(\n        *,\n        to_eval: Dict[PolicyID, List[PolicyEvalData]],\n        policies: Dict[PolicyID, Policy],\n        _sample_collector,\n        active_episodes: Dict[str, MultiAgentEpisode],\n        tf_sess: Optional[\"tf.Session\"] = None,\n) -> Dict[PolicyID, Tuple[TensorStructType, StateBatch, dict]]:\n    \n    \n\n    eval_results: Dict[PolicyID, TensorStructType] = {}\n\n    if tf_sess:\n        builder = TFRunBuilder(tf_sess, \"policy_eval\")\n        pending_fetches: Dict[PolicyID, Any] = {}\n    else:\n        builder = None\n\n    if log_once(\"compute_actions_input\"):\n        logger.info(\"Inputs to compute_actions():\\n\\n{}\\n\".format(\n            summarize(to_eval)))\n\n    for policy_id, eval_data in to_eval.items():\n        policy: Policy = _get_or_raise(policies, policy_id)\n        input_dict = _sample_collector.get_inference_input_dict(policy_id)\n        eval_results[policy_id] = \\\n            policy.compute_actions_from_input_dict(\n                input_dict,\n                timestep=policy.global_timestep,\n                episodes=[active_episodes[t.env_id] for t in eval_data])\n\n    if builder:\n        # type: PolicyID, Tuple[TensorStructType, StateBatch, dict]\n        for pid, v in pending_fetches.items():\n            eval_results[pid] = builder.get(v)\n\n    if log_once(\"compute_actions_result\"):\n        logger.info(\"Outputs of compute_actions():\\n\\n{}\\n\".format(\n            summarize(eval_results)))\n\n    return eval_results```\n\nUpdated code:\n```python\ndef _do_policy_eval_w_trajectory_view_api(\n        *,\n        to_eval: Dict[PolicyID, List[PolicyEvalData]],\n        policies: Dict[PolicyID, Policy],\n        sample_collector,\n        active_episodes: Dict[str, MultiAgentEpisode],\n        tf_sess: Optional[\"tf.Session\"] = None,\n) -> Dict[PolicyID, Tuple[TensorStructType, StateBatch, dict]]:\n    \n    \n\n    eval_results: Dict[PolicyID, TensorStructType] = {}\n\n    if tf_sess:\n        builder = TFRunBuilder(tf_sess, \"policy_eval\")\n        pending_fetches: Dict[PolicyID, Any] = {}\n    else:\n        builder = None\n\n    if log_once(\"compute_actions_input\"):\n        logger.info(\"Inputs to compute_actions():\\n\\n{}\\n\".format(\n            summarize(to_eval)))\n\n    for policy_id, eval_data in to_eval.items():\n        policy: Policy = _get_or_raise(policies, policy_id)\n        input_dict = sample_collector.get_inference_input_dict(policy_id)\n        eval_results[policy_id] = \\\n            policy.compute_actions_from_input_dict(\n                input_dict,\n                timestep=policy.global_timestep,\n                episodes=[active_episodes[t.env_id] for t in eval_data])\n\n    if builder:\n        # type: PolicyID, Tuple[TensorStructType, StateBatch, dict]\n        for pid, v in pending_fetches.items():\n            eval_results[pid] = builder.get(v)\n\n    if log_once(\"compute_actions_result\"):\n        logger.info(\"Outputs of compute_actions():\\n\\n{}\\n\".format(\n            summarize(eval_results)))\n\n    return eval_results\n```\n\nOriginal documentation:\n```python\nCall compute_actions on collected episode/model data to get next action.\n\n    Args:\n        to_eval (Dict[PolicyID, List[PolicyEvalData]]): Mapping of policy\n            IDs to lists of PolicyEvalData objects (items in these lists will\n            be the batch's items for the model forward pass).\n        policies (Dict[PolicyID, Policy]): Mapping from policy ID to Policy\n            obj.\n        _sample_collector (SampleCollector): The SampleCollector object to use.\n        tf_sess (Optional[tf.Session]): Optional tensorflow session to use for\n            batching TF policy evaluations.\n\n    Returns:\n        eval_results: dict of policy to compute_action() outputs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Call compute_actions on collected episode/model data to get next action.\n\n    Args:\n        to_eval (Dict[PolicyID, List[PolicyEvalData]]): Mapping of policy\n            IDs to lists of PolicyEvalData objects (items in these lists will\n            be the batch's items for the model forward pass).\n        policies (Dict[PolicyID, Policy]): Mapping from policy ID to Policy\n            obj.\n        sample_collector (SampleCollector): The SampleCollector object to use.\n        tf_sess (Optional[tf.Session]): Optional tensorflow session to use for\n            batching TF policy evaluations.\n\n    Returns:\n        eval_results: dict of policy to compute_action() outputs.", "file": "rllib_evaluation_sampler.py", "function": "_do_policy_eval_w_trajectory_view_api", "project": "ray", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_primary_scenarios() -> List[Scenario]:\n    \n    \n    return _ScenarioManagerFactory._build_manager()._get_primary_scenarios()```\n\nUpdated code:\n```python\ndef get_primary_scenarios(\n    is_sorted: bool = False,\n    descending: bool = False,\n    sort_key: Literal[\"name\", \"id\", \"config_id\", \"creation_date\", \"tags\"] = \"name\",\n) -> List[Scenario]:\n    \n    \n    scenario_manager = _ScenarioManagerFactory._build_manager()\n    scenarios = scenario_manager._get_primary_scenarios()\n    if is_sorted:\n        scenario_manager._sort_scenarios(scenarios, descending, sort_key)\n    return scenarios\n```\n\nOriginal documentation:\n```python\nRetrieve a list of all primary scenarios.\n\n    Returns:\n        A list containing all primary scenarios.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieve a list of all primary scenarios.\n\n    Parameters:\n         is_sorted (bool): The option to sort scenarios. The default sorting key is name.\n         descending (bool): The option to sort scenarios on the sorting key in descending order.\n         sort_key (Literal[\"name\", \"id\", \"creation_date\", \"tags\"]): The optiononal sort_key to\n             decide upon what key scenarios are sorted. The sorting is in increasing order for\n             dates, in alphabetical order for name and id, in lexographical order for tags.\n\n    Returns:\n        The list containing all primary scenarios, optionally sorted by name, id, creation_date or tags.\n            The sorting is in increasing order for dates, in alphabetical order for name and\n            id, and in lexicographical order for tags. If sorted is set to true, but if an\n            incorrect or no sort_key is provided, the scenarios are sorted by name.", "file": "taipy_core_taipy.py", "function": "get_primary_scenarios", "project": "taipy", "change_stats": {"code_changes": 14, "doc_changes": 14, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef shutdown(self, *args, **kwargs) -> None:  # type: ignore\n        \n        with self._shutdown_lock:\n            self._shutdown = True\n            # Drain all work items from the queue, and then cancel their\n            # associated futures.\n            while True:\n                try:\n                    work_item = self._work_queue.get_nowait()\n                except queue.Empty:\n                    break\n                if work_item is not None:\n                    work_item.future.cancel()\n            # Send a wake-up to prevent threads calling\n            # _work_queue.get(block=True) from permanently blocking.\n            self._work_queue.put(None)  # type: ignore[arg-type]\n\n        # The above code is backported from python 3.9\n        #\n        # For maintainability join_threads_or_timeout is\n        # a separate function since it is not a backport from\n        # cpython itself\n        #\n        self.join_threads_or_timeout()```\n\nUpdated code:\n```python\ndef shutdown(self, *args: Any, **kwargs: Any) -> None:\n        \n        super().shutdown(wait=False, cancel_futures=True)\n        self.join_threads_or_timeout()\n```\n\nOriginal documentation:\n```python\nShutdown backport from cpython 3.9 with interrupt support added.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Shutdown with interrupt support added.", "file": "homeassistant_util_executor.py", "function": "InterruptibleThreadPoolExecutor.shutdown", "project": "core", "change_stats": {"code_changes": 26, "doc_changes": 4, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_col(self):\n          \n\n        # validate this column for string truncation (or reset to the max size)\n        if self.kind == 'string':\n\n            c = self.col\n            if c is not None:\n                if c.itemsize < self.itemsize:\n                    raise Exception(\"[%s] column has a min_itemsize of [%s] but itemsize [%s] is required!\" % (self.cname,self.itemsize,c.itemsize))```\n\nUpdated code:\n```python\ndef validate_col(self, itemsize = None):\n          \n\n        # validate this column for string truncation (or reset to the max size)\n        dtype = getattr(self,'dtype',None)\n        if self.kind == 'string' or (dtype is not None and dtype.startswith('string')):\n\n            c = self.col\n            if c is not None:\n                if itemsize is None:\n                    itemsize = self.itemsize\n                if c.itemsize < itemsize:\n                    raise Exception(\"[%s] column has a min_itemsize of [%s] but itemsize [%s] is required!\" % (self.cname,itemsize,c.itemsize))\n                return c.itemsize\n\n        return None\n```\n\nOriginal documentation:\n```python\nvalidate this column & set table data for it\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "validate this column: return the compared against itemsize", "file": "pandas_io_pytables.py", "function": "IndexCol.validate_col", "project": "pandas", "change_stats": {"code_changes": 16, "doc_changes": 4, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_password_path(self):\n        \n        \n        if LINUX or BSD:\n            app_path = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n        elif OSX:\n            app_path = os.path.join(os.environ.get('HOME'), 'Library')\n        elif WINDOWS:\n            app_path = os.environ.get('APPDATA')\n        else:\n            app_path = '.'\n\n        # Append the Glances folder\n        app_path = os.path.join(app_path, 'glances')\n\n        return app_path```\n\nUpdated code:\n```python\ndef get_password_path(self):\n        \n        \n        if LINUX or BSD:\n            app_path = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n        elif MACOS:\n            app_path = os.path.join(os.environ.get('HOME'), 'Library')\n        elif WINDOWS:\n            app_path = os.environ.get('APPDATA')\n        else:\n            app_path = '.'\n\n        # Append the Glances folder\n        app_path = os.path.join(app_path, 'glances')\n\n        return app_path\n```\n\nOriginal documentation:\n```python\nr\"\"\"Get the path where the password file will be stored.\n\n        * Linux and BSD: ~/.config/glances\n        * OS X: ~/Library/glances\n        * Windows: %APPDATA%\\glances\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Get the path where the password file will be stored.\n\n        * Linux and *BSD: ~/.config/glances\n        * macOS: ~/Library/glances\n        * Windows: %APPDATA%\\glances", "file": "glances_password.py", "function": "GlancesPassword.get_password_path", "project": "glances", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef frame_center(self, frame_center):\n        \n        \n        self.frame.move_to(frame_center)```\n\nUpdated code:\n```python\ndef frame_center(self, frame_center: np.ndarray | list | tuple | Mobject):\n        \n        \n        self.frame.move_to(frame_center)\n```\n\nOriginal documentation:\n```python\nSets the centerpoint of the frame.\n\n        Parameters\n        ----------\n        frame_center : np.array, list, tuple, Mobject\n            The point to which the frame must be moved.\n            If is of type mobject, the frame will be moved to\n            the center of that mobject.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sets the centerpoint of the frame.\n\n        Parameters\n        ----------\n        frame_center\n            The point to which the frame must be moved.\n            If is of type mobject, the frame will be moved to\n            the center of that mobject.", "file": "manim_camera_moving_camera.py", "function": "MovingCamera.frame_center", "project": "manim", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef handle_stoploss_on_exchange(self, trade: Trade) -> bool:\n        \n        \n        \n\n        result = False\n\n        # If trade is open and the buy order is fulfilled but there is no stoploss,\n        # then we add a stoploss on exchange\n        if not trade.open_order_id and not trade.stoploss_order_id:\n            if self.edge:\n                stoploss = self.edge.stoploss(pair=trade.pair)\n            else:\n                stoploss = self.strategy.stoploss\n\n            stop_price = trade.open_rate * (1 + stoploss)\n\n            # limit price should be less than stop price.\n            # 0.98 is arbitrary here.\n            limit_price = stop_price * 0.98\n\n            stoploss_order_id = self.exchange.stoploss_limit(\n                pair=trade.pair, amount=trade.amount, stop_price=stop_price, rate=limit_price\n            )['id']\n            trade.stoploss_order_id = str(stoploss_order_id)\n\n        # Or the trade open and there is already a stoploss on exchange.\n        # so we check if it is hit ...\n        elif trade.stoploss_order_id:\n            logger.debug('Handling stoploss on exchange %s ...', trade)\n            order = self.exchange.get_order(trade.stoploss_order_id, trade.pair)\n            if order['status'] == 'closed':\n                trade.sell_reason = SellType.STOPLOSS_ON_EXCHANGE.value\n                trade.update(order)\n                result = True\n            else:\n                result = False\n        return result```\n\nUpdated code:\n```python\ndef handle_stoploss_on_exchange(self, trade: Trade) -> bool:\n        \n        \n        \n\n        result = False\n\n        # If trade is open and the buy order is fulfilled but there is no stoploss,\n        # then we add a stoploss on exchange\n        if not trade.open_order_id and not trade.stoploss_order_id:\n            if self.edge:\n                stoploss = self.edge.stoploss(pair=trade.pair)\n            else:\n                stoploss = self.strategy.stoploss\n\n            stop_price = trade.open_rate * (1 + stoploss)\n\n            # limit price should be less than stop price.\n            # 0.99 is arbitrary here.\n            limit_price = stop_price * 0.99\n\n            stoploss_order_id = self.exchange.stoploss_limit(\n                pair=trade.pair, amount=trade.amount, stop_price=stop_price, rate=limit_price\n            )['id']\n            trade.stoploss_order_id = str(stoploss_order_id)\n            trade.stoploss_last_update = datetime.now()\n\n        # Or the trade open and there is already a stoploss on exchange.\n        # so we check if it is hit ...\n        elif trade.stoploss_order_id:\n            logger.debug('Handling stoploss on exchange %s ...', trade)\n            order = self.exchange.get_order(trade.stoploss_order_id, trade.pair)\n            if order['status'] == 'closed':\n                trade.sell_reason = SellType.STOPLOSS_ON_EXCHANGE.value\n                trade.update(order)\n                result = True\n            elif self.config.get('trailing_stop', False):\n                # if trailing stoploss is enabled we check if stoploss value has changed\n                # in which case we cancel stoploss order and put another one with new\n                # value immediately\n                self.handle_trailing_stoploss_on_exchange(trade, order)\n\n        return result\n```\n\nOriginal documentation:\n```python\nCheck if trade is fulfilled in which case the stoploss\n        on exchange should be added immediately if stoploss on exchnage\n        is enabled.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if trade is fulfilled in which case the stoploss\n        on exchange should be added immediately if stoploss on exchange\n        is enabled.", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.handle_stoploss_on_exchange", "project": "freqtrade", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef attenuator(self, s21: NumberLike, db: bool = True, d: Number = 0,\n                   unit: str = 'deg', name: str = '', **kwargs) -> Network:\n        \n        \n\n        \n        if db:\n            s21 = mf.db_2_magnitude(s21)\n\n        result = self.match(nports=2)\n        result.s[:,0,1] = s21\n        result.s[:,1,0] = s21\n        result = result**self.line(d=d, unit = unit, **kwargs)\n        result.name = name\n        return result```\n\nUpdated code:\n```python\ndef attenuator(self, s21: NumberLike, db: bool = True, d: Number = 0,\n                   unit: str = 'deg', name: str = '', **kwargs) -> Network:\n        \n\n        \n\n        s21 = npy.array(s21)\n        if db:\n            s21 = mf.db_2_magnitude(s21)\n\n        result = self.match(nports=2)\n        result.s[:, 0, 1] = s21\n        result.s[:, 1, 0] = s21\n        result = result ** self.line(d=d, unit=unit, **kwargs)\n        result.name = name\n        return result\n```\n\nOriginal documentation:\n```python\nIdeal matched attenuator of a given length.\n\n        Parameters\n        ----------\n        s21 : number, array-like\n            the attenuation\n        db : bool, optional\n            is s21 in dB? otherwise assumes linear. Default is True (dB).\n        d : number, optional\n            length of attenuator. Default is 0.\n        unit : ['deg','rad','m','cm','um','in','mil','s','us','ns','ps']\n            the units of d.  See :func:`to_meters`, for details. \n            Default is 'deg'\n\n        Returns\n        -------\n        ntwk : :class:`~skrf.network.Network` object\n            2-port attenuator\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Ideal matched attenuator of a given length.\n\n        Parameters\n        ----------\n        s21 : number, array-like\n            the attenuation\n        db : bool, optional\n            is s21 in dB? otherwise assumes linear. Default is True (dB).\n        d : number, optional\n            length of attenuator. Default is 0.\n        unit : ['deg','rad','m','cm','um','in','mil','s','us','ns','ps']\n            the units of d.  See :func:`to_meters`, for details. \n            Default is 'deg'\n        name : str\n            Name for the returned attenuator Network\n        \\*\\*kwargs : arguments, keyword arguments\n            passed to func:`line`\n\n        Returns\n        -------\n        ntwk : :class:`~skrf.network.Network` object\n            2-port attenuator", "file": "skrf_media_media.py", "function": "Media.attenuator", "project": "scikit-rf", "change_stats": {"code_changes": 11, "doc_changes": 9, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sort_key(self, key):\n        \n        self._sort_key = key```\n\nUpdated code:\n```python\ndef sort_reverse(self):\n        \n        if self.sort_key == 'name' or self.sort_key == 'username':\n            return False\n\n        return True\n```\n\nOriginal documentation:\n```python\nSet the current sort key.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return True to sort processes in reverse 'key' order, False instead.", "file": "glances_processes.py", "function": "GlancesProcesses.sort_key", "project": "glances", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cov(self, other, min_periods=None) -> float:\n        \n        \n        \n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values, min_periods=min_periods)```\n\nUpdated code:\n```python\ndef cov(\n        self,\n        other: \"Series\",\n        min_periods: Optional[int] = None,\n        ddof: Optional[int] = 1,\n    ) -> float:\n        \n        \n        \n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(\n            this.values, other.values, min_periods=min_periods, ddof=ddof\n        )\n```\n\nOriginal documentation:\n```python\nCompute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n\n            versionadded:: 1.1.0\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874", "file": "pandas_core_series.py", "function": "Series.cov", "project": "pandas", "change_stats": {"code_changes": 13, "doc_changes": 7, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_locked(self):\n        \n        return not self._nuki_device.is_rto_activated```\n\nUpdated code:\n```python\ndef is_locked(self):\n        \n        return not (\n            self._nuki_device.is_rto_activated\n            or self._nuki_device.mode == MODE_OPENER_CONTINUOUS\n        )\n```\n\nOriginal documentation:\n```python\nReturn true if ring-to-open is enabled.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return true if either ring-to-open or continuous mode is enabled.", "file": "homeassistant_components_nuki_lock.py", "function": "NukiOpenerEntity.is_locked", "project": "core", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef inference(\n        self,\n        text: torch.Tensor,\n        feats: Optional[torch.Tensor] = None,\n        label: Optional[Dict[str, torch.Tensor]] = None,\n        melody: Optional[Dict[str, torch.Tensor]] = None,\n        pitch: Optional[torch.Tensor] = None,\n        spembs: Optional[torch.Tensor] = None,\n        sids: Optional[torch.Tensor] = None,\n        lids: Optional[torch.Tensor] = None,\n        noise_scale: float = 0.667,\n        noise_scale_dur: float = 0.8,\n        alpha: float = 1.0,\n        max_len: Optional[int] = None,\n        use_teacher_forcing: bool = False,\n        duration: Optional[Dict[str, torch.Tensor]] = None,\n        slur: Optional[Dict[str, torch.Tensor]] = None,\n    ) -> Dict[str, torch.Tensor]:\n        \n\n        \n        # setup\n        label = label[\"lab\"]\n        melody = melody[\"lab\"]\n        beat = duration[\"score_syb\"]\n        text = text[None]\n        text_lengths = torch.tensor(\n            [text.size(1)],\n            dtype=torch.long,\n            device=text.device,\n        )\n        label_lengths = torch.tensor(\n            [label.size(1)],\n            dtype=torch.long,\n            device=text.device,\n        )\n\n        if sids is not None:\n            sids = sids.view(1)\n        if lids is not None:\n            lids = lids.view(1)\n\n        # inference\n        if use_teacher_forcing:\n            assert feats is not None\n            feats = feats[None].transpose(1, 2)\n            feats_lengths = torch.tensor(\n                [feats.size(2)],\n                dtype=torch.long,\n                device=feats.device,\n            )\n            wav = self.generator.inference(\n                text=text,\n                text_lengths=text_lengths,\n                feats=feats,\n                feats_lengths=feats_lengths,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                beat=beat,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n                max_len=max_len,\n                use_teacher_forcing=use_teacher_forcing,\n            )\n        else:\n            wav = self.generator.inference(\n                text=text,\n                text_lengths=text_lengths,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                beat=beat,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n                noise_scale=noise_scale,\n                noise_scale_dur=noise_scale_dur,\n                alpha=alpha,\n                max_len=max_len,\n            )\n        return dict(wav=wav.view(-1))```\n\nUpdated code:\n```python\ndef inference(\n        self,\n        text: torch.Tensor,\n        feats: Optional[torch.Tensor] = None,\n        label: Optional[Dict[str, torch.Tensor]] = None,\n        melody: Optional[Dict[str, torch.Tensor]] = None,\n        pitch: Optional[torch.Tensor] = None,\n        duration: Optional[Dict[str, torch.Tensor]] = None,\n        slur: Optional[Dict[str, torch.Tensor]] = None,\n        spembs: Optional[torch.Tensor] = None,\n        sids: Optional[torch.Tensor] = None,\n        lids: Optional[torch.Tensor] = None,\n        noise_scale: float = 0.667,\n        noise_scale_dur: float = 0.8,\n        alpha: float = 1.0,\n        max_len: Optional[int] = None,\n        use_teacher_forcing: bool = False,\n    ) -> Dict[str, torch.Tensor]:\n        \n\n        \n        # setup\n        label = label[\"lab\"]\n        melody = melody[\"lab\"]\n        beat = duration[\"score_syb\"]\n        text = text[None]\n        text_lengths = torch.tensor(\n            [text.size(1)],\n            dtype=torch.long,\n            device=text.device,\n        )\n        label_lengths = torch.tensor(\n            [label.size(1)],\n            dtype=torch.long,\n            device=text.device,\n        )\n\n        if sids is not None:\n            sids = sids.view(1)\n        if lids is not None:\n            lids = lids.view(1)\n\n        # inference\n        if use_teacher_forcing:\n            assert feats is not None\n            feats = feats[None].transpose(1, 2)\n            feats_lengths = torch.tensor(\n                [feats.size(2)],\n                dtype=torch.long,\n                device=feats.device,\n            )\n            wav = self.generator.inference(\n                text=text,\n                text_lengths=text_lengths,\n                feats=feats,\n                feats_lengths=feats_lengths,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                beat=beat,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n                max_len=max_len,\n                use_teacher_forcing=use_teacher_forcing,\n            )\n        else:\n            wav = self.generator.inference(\n                text=text,\n                text_lengths=text_lengths,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                beat=beat,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n                noise_scale=noise_scale,\n                noise_scale_dur=noise_scale_dur,\n                alpha=alpha,\n                max_len=max_len,\n            )\n        return dict(wav=wav.view(-1))\n```\n\nOriginal documentation:\n```python\nRun inference.\n\n        Args:\n            text (Tensor): Input text index tensor (T_text,).\n            feats (Tensor): Feature tensor (T_feats, aux_channels).\n            label (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded label ids (B, Tmax).\n            melody (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded melody (B, Tmax).\n            tempo (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded tempo (B, Tmax).\n            beat (Optional[Dict]): key is \"lab\", \"score_phn\" or \"score_syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            pitch (FloatTensor): Batch of padded f0 (B, Tmax).\n            sids (Tensor): Speaker index tensor (1,).\n            spembs (Optional[Tensor]): Speaker embedding tensor (spk_embed_dim,).\n            lids (Tensor): Language index tensor (1,).\n            noise_scale (float): Noise scale value for flow.\n            noise_scale_dur (float): Noise scale value for duration predictor.\n            alpha (float): Alpha parameter to control the speed of generated singing.\n            max_len (Optional[int]): Maximum length.\n            use_teacher_forcing (bool): Whether to use teacher forcing.\n            duration (Optional[Dict]): key is \"phn\", \"syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            slur (LongTensor): Batch of padded slur (B, Tmax).\n\n\n        Returns:\n            Dict[str, Tensor]:\n                * wav (Tensor): Generated waveform tensor (T_wav,).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Run inference.\n\n        Args:\n            text (Tensor): Input text index tensor (T_text,).\n            feats (Tensor): Feature tensor (T_feats, aux_channels).\n            label (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded label ids (B, Tmax).\n            melody (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded melody (B, Tmax).\n            tempo (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded tempo (B, Tmax).\n            beat (Optional[Dict]): key is \"lab\", \"score_phn\" or \"score_syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            pitch (FloatTensor): Batch of padded f0 (B, Tmax).\n            duration (Optional[Dict]): key is \"phn\", \"syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            slur (LongTensor): Batch of padded slur (B, Tmax).\n            sids (Tensor): Speaker index tensor (1,).\n            spembs (Optional[Tensor]): Speaker embedding tensor (spk_embed_dim,).\n            lids (Tensor): Language index tensor (1,).\n            noise_scale (float): Noise scale value for flow.\n            noise_scale_dur (float): Noise scale value for duration predictor.\n            alpha (float): Alpha parameter to control the speed of generated singing.\n            max_len (Optional[int]): Maximum length.\n            use_teacher_forcing (bool): Whether to use teacher forcing.\n\n        Returns:\n            Dict[str, Tensor]:\n                * wav (Tensor): Generated waveform tensor (T_wav,).", "file": "espnet2_gan_svs_vits_vits.py", "function": "VITS.inference", "project": "espnet", "change_stats": {"code_changes": 6, "doc_changes": 9, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_figure(self, figure):\n        \n        \n\n        self.figure = figure\n        self.navigation = figure.canvas.manager.navigation```\n\nUpdated code:\n```python\ndef set_figure(self, figure):\n        \n        \n\n        self.figure = figure\n```\n\nOriginal documentation:\n```python\nSet the figure and navigation\n\n        Set the figure to be affected by this tool\n\n        Parameters\n        ----------\n        figure : `Figure`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the figure\n\n        Set the figure to be affected by this tool\n\n        Parameters\n        ----------\n        figure : `Figure`", "file": "lib_matplotlib_backend_tools.py", "function": "ToolBase.set_figure", "project": "matplotlib", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _sample_next_token(self,\n                           sequence: torch.Tensor,\n                           cfg_conditions: CFGConditions,\n                           unconditional_state: State,\n                           use_sampling: bool = False,\n                           temp: float = 1.0,\n                           top_k: int = 0,\n                           top_p: float = 0.0,\n                           cfg_coef: tp.Optional[float] = None) -> torch.Tensor:\n        \n        \n        B = sequence.shape[0]\n        cfg_coef = self.cfg_coef if cfg_coef is None else cfg_coef\n        model = self if self._fsdp is None else self._fsdp\n        if self.two_step_cfg and cfg_conditions != {}:\n            assert isinstance(cfg_conditions, tuple)\n            condition_tensors, null_condition_tensors = cfg_conditions\n            cond_logits = model(sequence, conditions=[], condition_tensors=condition_tensors)\n            state = self.get_streaming_state()\n            self.set_streaming_state(unconditional_state)\n            uncond_logits = model(sequence, conditions=[], condition_tensors=null_condition_tensors)\n            unconditional_state.update(self.get_streaming_state())\n            self.set_streaming_state(state)\n            logits = uncond_logits + (cond_logits - uncond_logits) * self.cfg_coef\n        else:\n            assert isinstance(cfg_conditions, dict)\n            condition_tensors = cfg_conditions\n            if condition_tensors:\n                # Preparing for CFG, predicting both conditional and unconditional logits.\n                sequence = torch.cat([sequence, sequence], dim=0)\n            all_logits = model(\n                sequence,\n                conditions=[], condition_tensors=condition_tensors)\n            if condition_tensors:\n                cond_logits, uncond_logits = all_logits.split(B, dim=0)  # [B, K, T, card]\n                logits = uncond_logits + (cond_logits - uncond_logits) * cfg_coef\n            else:\n                logits = all_logits\n\n        logits = logits.permute(0, 1, 3, 2)  # [B, K, card, T]\n        logits = logits[..., -1]  # [B x K x card]\n\n        # Apply softmax for sampling if temp > 0. Else, do greedy sampling to avoid zero division error.\n        if use_sampling and temp > 0.0:\n            probs = torch.softmax(logits / temp, dim=-1)\n            if top_p > 0.0:\n                next_token = utils.sample_top_p(probs, p=top_p)\n            elif top_k > 0:\n                next_token = utils.sample_top_k(probs, k=top_k)\n            else:\n                next_token = utils.multinomial(probs, num_samples=1)\n        else:\n            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n        return next_token```\n\nUpdated code:\n```python\ndef _sample_next_token(self,\n                           sequence: torch.Tensor,\n                           cfg_conditions: CFGConditions,\n                           unconditional_state: State,\n                           use_sampling: bool = False,\n                           temp: float = 1.0,\n                           top_k: int = 0,\n                           top_p: float = 0.0,\n                           cfg_coef: tp.Optional[float] = None) -> torch.Tensor:\n        \n        \n        B = sequence.shape[0]\n        cfg_coef = self.cfg_coef if cfg_coef is None else cfg_coef\n        model = self if self._fsdp is None else self._fsdp\n        if self.two_step_cfg and cfg_conditions != {}:\n            assert isinstance(cfg_conditions, tuple), type(cfg_conditions)\n            condition_tensors, null_condition_tensors = cfg_conditions\n            cond_logits = model(sequence, conditions=[], condition_tensors=condition_tensors)\n            state = self.get_streaming_state()\n            self.set_streaming_state(unconditional_state)\n            uncond_logits = model(sequence, conditions=[], condition_tensors=null_condition_tensors)\n            unconditional_state.update(self.get_streaming_state())\n            self.set_streaming_state(state)\n            logits = uncond_logits + (cond_logits - uncond_logits) * self.cfg_coef\n        else:\n            assert isinstance(cfg_conditions, dict)\n            condition_tensors = cfg_conditions\n            if condition_tensors:\n                # Preparing for CFG, predicting both conditional and unconditional logits.\n                sequence = torch.cat([sequence, sequence], dim=0)\n            all_logits = model(\n                sequence,\n                conditions=[], condition_tensors=condition_tensors)\n            if condition_tensors:\n                cond_logits, uncond_logits = all_logits.split(B, dim=0)  # [B, K, T, card]\n                logits = uncond_logits + (cond_logits - uncond_logits) * cfg_coef\n            else:\n                logits = all_logits\n\n        logits = logits.permute(0, 1, 3, 2)  # [B, K, card, T]\n        logits = logits[..., -1]  # [B x K x card]\n\n        # Apply softmax for sampling if temp > 0. Else, do greedy sampling to avoid zero division error.\n        if use_sampling and temp > 0.0:\n            probs = torch.softmax(logits / temp, dim=-1)\n            if top_p > 0.0:\n                next_token = utils.sample_top_p(probs, p=top_p)\n            elif top_k > 0:\n                next_token = utils.sample_top_k(probs, k=top_k)\n            else:\n                next_token = utils.multinomial(probs, num_samples=1)\n        else:\n            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n        return next_token\n```\n\nOriginal documentation:\n```python\nSample next token from the model given a sequence and a set of conditions. The model supports\n        multiple sampling strategies (greedy sampling, softmax, top-k, top-p...).\n\n        Args:\n            sequence (torch.Tensor): Current sequence of shape [B, K, S]\n                with K corresponding to the number of codebooks and S the number of sequence steps.\n                S = 1 in streaming mode, except for the first step that contains a bigger prompt.\n            condition_tensors (Dict[str, ConditionType): Set of conditions. If CFG is used,\n                should be twice the batch size, being the concatenation of the conditions + null conditions.\n            use_sampling (bool): Whether to use a sampling strategy or not.\n            temp (float): Sampling temperature.\n            top_k (int): K for \"top-k\" sampling.\n            top_p (float): P for \"top-p\" sampling.\n            cfg_coef (float): classifier free guidance coefficient\n        Returns:\n            next_token (torch.Tensor): Next token tensor of shape [B, K, 1].\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sample next token from the model given a sequence and a set of conditions. The model supports\n        multiple sampling strategies (greedy sampling, softmax, top-k, top-p...).\n\n        Args:\n            sequence (torch.Tensor): Current sequence of shape [B, K, S]\n                with K corresponding to the number of codebooks and S the number of sequence steps.\n                S = 1 in streaming mode, except for the first step that contains a bigger prompt.\n            condition_tensors (dict[str, ConditionType): Set of conditions. If CFG is used,\n                should be twice the batch size, being the concatenation of the conditions + null conditions.\n            use_sampling (bool): Whether to use a sampling strategy or not.\n            temp (float): Sampling temperature.\n            top_k (int): K for \"top-k\" sampling.\n            top_p (float): P for \"top-p\" sampling.\n            cfg_coef (float, optional): classifier free guidance coefficient\n        Returns:\n            next_token (torch.Tensor): Next token tensor of shape [B, K, 1].", "file": "audiocraft_models_lm.py", "function": "LMModel._sample_next_token", "project": "audiocraft", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _cast_types(self, values, cast_type, column):\n        \n        \n        \n        if is_categorical_dtype(cast_type):\n            known_cats = (\n                isinstance(cast_type, CategoricalDtype)\n                and cast_type.categories is not None\n            )\n\n            if not is_object_dtype(values) and not known_cats:\n                # TODO: this is for consistency with\n                # c-parser which parses all categories\n                # as strings\n\n                values = astype_nansafe(values, np.dtype(str))\n\n            cats = Index(values).unique().dropna()\n            values = Categorical._from_inferred_categories(\n                cats, cats.get_indexer(values), cast_type, true_values=self.true_values\n            )\n\n        # use the EA's implementation of casting\n        elif is_extension_array_dtype(cast_type):\n            # ensure cast_type is an actual dtype and not a string\n            cast_type = pandas_dtype(cast_type)\n            array_type = cast_type.construct_array_type()\n            try:\n                if is_bool_dtype(cast_type):\n                    return array_type._from_sequence_of_strings(\n                        values,\n                        dtype=cast_type,\n                        true_values=self.true_values,\n                        false_values=self.false_values,\n                    )\n                else:\n                    return array_type._from_sequence_of_strings(values, dtype=cast_type)\n            except NotImplementedError as err:\n                raise NotImplementedError(\n                    f\"Extension Array: {array_type} must implement \"\n                    \"_from_sequence_of_strings in order to be used in parser methods\"\n                ) from err\n\n        else:\n            try:\n                values = astype_nansafe(values, cast_type, copy=True, skipna=True)\n            except ValueError as err:\n                raise ValueError(\n                    f\"Unable to convert column {column} to type {cast_type}\"\n                ) from err\n        return values```\n\nUpdated code:\n```python\ndef _cast_types(self, values: ArrayLike, cast_type: DtypeObj, column) -> ArrayLike:\n        \n        \n        \n        if is_categorical_dtype(cast_type):\n            known_cats = (\n                isinstance(cast_type, CategoricalDtype)\n                and cast_type.categories is not None\n            )\n\n            if not is_object_dtype(values.dtype) and not known_cats:\n                # TODO: this is for consistency with\n                # c-parser which parses all categories\n                # as strings\n\n                values = lib.ensure_string_array(\n                    values, skipna=False, convert_na_value=False\n                )\n\n            cats = Index(values).unique().dropna()\n            values = Categorical._from_inferred_categories(\n                cats, cats.get_indexer(values), cast_type, true_values=self.true_values\n            )\n\n        # use the EA's implementation of casting\n        elif isinstance(cast_type, ExtensionDtype):\n            array_type = cast_type.construct_array_type()\n            try:\n                if is_bool_dtype(cast_type):\n                    # error: Unexpected keyword argument \"true_values\" for\n                    # \"_from_sequence_of_strings\" of \"ExtensionArray\"\n                    return array_type._from_sequence_of_strings(  # type: ignore[call-arg]  # noqa:E501\n                        values,\n                        dtype=cast_type,\n                        true_values=self.true_values,\n                        false_values=self.false_values,\n                    )\n                else:\n                    return array_type._from_sequence_of_strings(values, dtype=cast_type)\n            except NotImplementedError as err:\n                raise NotImplementedError(\n                    f\"Extension Array: {array_type} must implement \"\n                    \"_from_sequence_of_strings in order to be used in parser methods\"\n                ) from err\n\n        elif isinstance(values, ExtensionArray):\n            values = values.astype(cast_type, copy=False)\n        else:\n            try:\n                values = astype_nansafe(values, cast_type, copy=True, skipna=True)\n            except ValueError as err:\n                raise ValueError(\n                    f\"Unable to convert column {column} to type {cast_type}\"\n                ) from err\n        return values\n```\n\nOriginal documentation:\n```python\nCast values to specified type\n\n        Parameters\n        ----------\n        values : ndarray\n        cast_type : string or np.dtype\n           dtype to cast values to\n        column : string\n            column name - used only for error reporting\n\n        Returns\n        -------\n        converted : ndarray\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Cast values to specified type\n\n        Parameters\n        ----------\n        values : ndarray or ExtensionArray\n        cast_type : np.dtype or ExtensionDtype\n           dtype to cast values to\n        column : string\n            column name - used only for error reporting\n\n        Returns\n        -------\n        converted : ndarray or ExtensionArray", "file": "pandas_io_parsers_base_parser.py", "function": "ParserBase._cast_types", "project": "pandas", "change_stats": {"code_changes": 20, "doc_changes": 8, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_geo_field(cls, opts, lookup):\n        \n        \n        \n        from django.contrib.gis.db.models.fields import GeometryField\n        # This takes into account the situation where the lookup is a\n        # lookup to a related geographic field, e.g., 'address__point'.\n        field_list = lookup.split(LOOKUP_SEP)\n\n        # Reversing so list operates like a queue of related lookups,\n        # and popping the top lookup.\n        field_list.reverse()\n        fld_name = field_list.pop()\n\n        try:\n            geo_fld = opts.get_field(fld_name)\n            # If the field list is still around, then it means that the\n            # lookup was for a geometry field across a relationship --\n            # thus we keep on getting the related model options and the\n            # model field associated with the next field in the list\n            # until there's no more left.\n            while len(field_list):\n                opts = geo_fld.remote_field.model._meta\n                geo_fld = opts.get_field(field_list.pop())\n        except (FieldDoesNotExist, AttributeError):\n            return False\n\n        # Finally, make sure we got a Geographic field and return.\n        if isinstance(geo_fld, GeometryField):\n            return geo_fld\n        else:\n            return False```\n\nUpdated code:\n```python\ndef _check_geo_field(cls, opts, lookup):\n        \n        \n        \n        from django.contrib.gis.db.models.fields import BaseSpatialField\n        # This takes into account the situation where the lookup is a\n        # lookup to a related geographic field, e.g., 'address__point'.\n        field_list = lookup.split(LOOKUP_SEP)\n\n        # Reversing so list operates like a queue of related lookups,\n        # and popping the top lookup.\n        field_list.reverse()\n        fld_name = field_list.pop()\n\n        try:\n            geo_fld = opts.get_field(fld_name)\n            # If the field list is still around, then it means that the\n            # lookup was for a geometry field across a relationship --\n            # thus we keep on getting the related model options and the\n            # model field associated with the next field in the list\n            # until there's no more left.\n            while len(field_list):\n                opts = geo_fld.remote_field.model._meta\n                geo_fld = opts.get_field(field_list.pop())\n        except (FieldDoesNotExist, AttributeError):\n            return False\n\n        # Finally, make sure we got a Geographic field and return.\n        if isinstance(geo_fld, BaseSpatialField):\n            return geo_fld\n        else:\n            return False\n```\n\nOriginal documentation:\n```python\nUtility for checking the given lookup with the given model options.\n        The lookup is a string either specifying the geographic field, e.g.\n        'point, 'the_geom', or a related lookup on a geographic field like\n        'address__point'.\n\n        If a GeometryField exists according to the given lookup on the model\n        options, it will be returned.  Otherwise returns None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Utility for checking the given lookup with the given model options.\n        The lookup is a string either specifying the geographic field, e.g.\n        'point, 'the_geom', or a related lookup on a geographic field like\n        'address__point'.\n\n        If a BaseSpatialField exists according to the given lookup on the model\n        options, it will be returned. Otherwise return None.", "file": "django_contrib_gis_db_models_lookups.py", "function": "GISLookup._check_geo_field", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_ignored_modules(\n        self,\n        _ignored_modules: Any,\n    ) -> Set[torch.nn.Module]:\n        \n        \n        \n        if _ignored_modules is None:\n            return set()\n        msg_prefix = \"`ignored_modules` should be an iterable of \" \\\n            \"`torch.nn.Module`s \"\n        try:\n            ignored_root_modules = set(_ignored_modules)\n        except TypeError:\n            raise TypeError(msg_prefix + f\"but got {type(_ignored_modules)}\")\n        for module in ignored_root_modules:\n            if not isinstance(module, torch.nn.Module):\n                raise TypeError(\n                    msg_prefix + f\"but got an iterable with {type(module)}\"\n                )\n            if isinstance(module, FullyShardedDataParallel):\n                raise ValueError(\n                    \"`ignored_modules` should not include FSDP modules\"\n                )\n        # Include child modules and exclude nested FSDP modules\n        ignored_modules = set(\n            child for module in ignored_root_modules\n            for child in module.modules()\n            if not isinstance(child, FullyShardedDataParallel) and\n            not isinstance(child, FlattenParamsWrapper)\n        )\n        return ignored_modules```\n\nUpdated code:\n```python\ndef _get_ignored_modules(\n        self,\n        root_module: torch.nn.Module,\n        _ignored_modules: Any,\n    ) -> Set[torch.nn.Module]:\n        \n        \n        \n        if _ignored_modules is None:\n            return set()\n        msg_prefix = \"`ignored_modules` should be an iterable of \" \\\n            \"`torch.nn.Module`s \"\n        try:\n            ignored_root_modules = set(_ignored_modules)\n        except TypeError:\n            raise TypeError(msg_prefix + f\"but got {type(_ignored_modules)}\")\n        for module in ignored_root_modules:\n            if not isinstance(module, torch.nn.Module):\n                raise TypeError(\n                    msg_prefix + f\"but got an iterable with {type(module)}\"\n                )\n            if isinstance(module, FullyShardedDataParallel):\n                raise ValueError(\n                    \"`ignored_modules` should not include FSDP modules\"\n                )\n        # Include child modules and exclude nested FSDP modules\n        ignored_modules = set(\n            child for module in ignored_root_modules\n            for child in module.modules()\n            if not isinstance(child, FullyShardedDataParallel) and\n            not isinstance(child, FlattenParamsWrapper)\n        )\n        if root_module in ignored_modules:\n            raise ValueError(\n                \"Trying to ignore the top-level module passed into the FSDP \"\n                \"constructor itself will result in all parameters being \"\n                f\"ignored and is not supported: {module}\"\n            )\n        return ignored_modules\n```\n\nOriginal documentation:\n```python\nChecks that ``_ignored_modules`` is an iterable of ``torch.nn.Module``\n        s without any :class:`FullyShardedDataParallel` instances and then\n        returns them and their children as a :class:`set`, excluding nested\n        :class:`FullyShardedDataParallel` instances.\n\n        We include the child modules to better match user intuition since\n        ignoring a module should ignore its child modules as well, and we\n        exclude :class:`FullyShardedDataParallel` instances since ``self`` may\n        be the intended root instance that manages them.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks that ``_ignored_modules`` (1) is an iterable of\n        ``torch.nn.Module`` s without any :class:`FullyShardedDataParallel`\n        instances and does not contain the top-level ``module`` itself, and\n        then returns them and their children as a :class:`set`, excluding\n        nested :class:`FullyShardedDataParallel` instances.\n\n        We include the child modules of modules in ``_ignored_modules`` to be\n        more intuitive since ignoring a module should ignore its child modules\n        as well, and we exclude :class:`FullyShardedDataParallel` instances\n        since ``self`` may be the intended root instance that manages them.", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel._get_ignored_modules", "project": "pytorch", "change_stats": {"code_changes": 9, "doc_changes": 19, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_pickradius(self, pickradius):\n        \n        \n        \n        self.pickradius = pickradius```\n\nUpdated code:\n```python\ndef set_pickradius(self, pickradius):\n        \n        \n        \n        if not isinstance(pickradius, Number) or pickradius < 0:\n            raise ValueError(\"pick radius should be a distance\")\n        self._pickradius = pickradius\n```\n\nOriginal documentation:\n```python\nSet the depth of the axis used by the picker.\n\n        Parameters\n        ----------\n        pickradius :  float\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the depth of the axis used by the picker.\n\n        Parameters\n        ----------\n        pickradius : float\n            The acceptance radius for containment tests.\n            See also `.Axis.contains`.", "file": "lib_matplotlib_axis.py", "function": "Axis.set_pickradius", "project": "matplotlib", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef elementwise(in_params, out_params, operation, name, **kwargs):\n    \n\n    \n    _check_cuda_available()\n    return cupy.elementwise.ElementwiseKernel(\n        in_params, out_params, operation, name, **kwargs)```\n\nUpdated code:\n```python\ndef elementwise(in_params, out_params, operation, name, **kwargs):\n    \n\n    \n    _check_cuda_available()\n    return cupy.ElementwiseKernel(\n        in_params, out_params, operation, name, **kwargs)\n```\n\nOriginal documentation:\n```python\nCreates an elementwise kernel function.\n\n    This function uses :func:`~chainer.cuda.memoize` to cache the\n    kernel object, i.e. the resulting kernel object is cached for each argument\n    combination and CUDA device.\n\n    The arguments are the same as those for\n    :func:`cupy.elementwise.ElementwiseKernel`, except that the ``name``\n    argument is mandatory.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates an elementwise kernel function.\n\n    This function uses :func:`~chainer.cuda.memoize` to cache the\n    kernel object, i.e. the resulting kernel object is cached for each argument\n    combination and CUDA device.\n\n    The arguments are the same as those for\n    :class:`cupy.ElementwiseKernel`, except that the ``name`` argument is\n    mandatory.", "file": "chainer_cuda.py", "function": "elementwise", "project": "chainer", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_current_user_id(token: str = Security(oauth2_scheme)) -> str | None:\n    \n    if not settings.use_auth:\n        return None\n\n    # Generate a key from the auth secret\n    key = derive_key()\n\n    # Decrypt the JWE token\n    try:\n        token: bytes = jwe.decrypt(token, key)\n    except jwe.exceptions.InvalidJWEData:\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n\n    payload: dict = json.loads(token.decode())\n    user_id = payload.get(\"user_id\")\n    exp = payload.get(\"exp\")\n\n    if not user_id or not exp:\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n    if datetime.utcnow() >= datetime.fromtimestamp(exp):\n        raise HTTPException(status_code=401, detail=\"Token expired\")\n\n    return user_id```\n\nUpdated code:\n```python\ndef get_current_user_id(token: str = Security(oauth2_scheme)) -> str | None:\n    \n    if not settings.use_auth:\n        return None\n    if token is None:\n        raise HTTPException(status_code=HTTP_403_FORBIDDEN, detail=\"Not authenticated\")\n\n    # Generate a key from the auth secret\n    key: bytes = derive_key()\n\n    # Decrypt the JWE token\n    try:\n        token: bytes = jwe.decrypt(token, key)\n    except JWEError:\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n\n    payload: dict = json.loads(token.decode())\n    user_id = payload.get(\"user_id\")\n    exp = payload.get(\"exp\")\n\n    if not user_id or not exp:\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n    if datetime.utcnow() >= datetime.fromtimestamp(exp):\n        raise HTTPException(status_code=401, detail=\"Token expired\")\n\n    return user_id\n```\n\nOriginal documentation:\n```python\nDecode the current user JWT token and return the payload.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the current user ID by decoding the JWT token.", "file": "inference_server_oasst_inference_server_auth.py", "function": "get_current_user_id", "project": "Open-Assistant", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update_trade_state(self, trade: Trade, order_id: str, action_order: Dict[str, Any] = None,\n                           stoploss_order: bool = False) -> bool:\n        \n        \n        \n        if not order_id:\n            logger.warning(f'Orderid for trade {trade} is empty.')\n            return False\n\n        # Update trade with order values\n        logger.info('Found open order for %s', trade)\n        try:\n            order = action_order or self.exchange.fetch_order_or_stoploss_order(order_id,\n                                                                                trade.pair,\n                                                                                stoploss_order)\n        except InvalidOrderException as exception:\n            logger.warning('Unable to fetch order %s: %s', order_id, exception)\n            return False\n\n        trade.update_order(order)\n\n        if self.exchange.check_order_canceled_empty(order):\n            # Trade has been cancelled on exchange\n            # Handling of this will happen in check_handle_timedout.\n            return True\n\n        # Try update amount (binance-fix)\n        try:\n            new_amount = self.get_real_amount(trade, order)\n            if not isclose(safe_value_fallback(order, 'filled', 'amount'), new_amount,\n                           abs_tol=constants.MATH_CLOSE_PREC):\n                order['amount'] = new_amount\n                order.pop('filled', None)\n                trade.recalc_open_trade_value()\n        except DependencyException as exception:\n            logger.warning(\"Could not update trade amount: %s\", exception)\n\n        trade.update(order)\n        Trade.commit()\n\n        # Updating wallets when order is closed\n        if not trade.is_open:\n            if not stoploss_order and not trade.open_order_id:\n                self._notify_exit(trade, '', True)\n            self.handle_protections(trade.pair)\n            self.wallets.update()\n        elif not trade.open_order_id:\n            # Buy fill\n            self._notify_enter(trade, fill=True)\n\n        return False```\n\nUpdated code:\n```python\ndef update_trade_state(self, trade: Trade, order_id: str, action_order: Dict[str, Any] = None,\n                           stoploss_order: bool = False, send_msg: bool = True) -> bool:\n        \n        \n        \n        if not order_id:\n            logger.warning(f'Orderid for trade {trade} is empty.')\n            return False\n\n        # Update trade with order values\n        logger.info('Found open order for %s', trade)\n        try:\n            order = action_order or self.exchange.fetch_order_or_stoploss_order(order_id,\n                                                                                trade.pair,\n                                                                                stoploss_order)\n        except InvalidOrderException as exception:\n            logger.warning('Unable to fetch order %s: %s', order_id, exception)\n            return False\n\n        trade.update_order(order)\n\n        if self.exchange.check_order_canceled_empty(order):\n            # Trade has been cancelled on exchange\n            # Handling of this will happen in check_handle_timedout.\n            return True\n\n        # Try update amount (binance-fix)\n        try:\n            new_amount = self.get_real_amount(trade, order)\n            if not isclose(safe_value_fallback(order, 'filled', 'amount'), new_amount,\n                           abs_tol=constants.MATH_CLOSE_PREC):\n                order['amount'] = new_amount\n                order.pop('filled', None)\n                trade.recalc_open_trade_value()\n        except DependencyException as exception:\n            logger.warning(\"Could not update trade amount: %s\", exception)\n\n        trade.update(order)\n        Trade.commit()\n\n        # Updating wallets when order is closed\n        if not trade.is_open:\n            if send_msg and not stoploss_order and not trade.open_order_id:\n                self._notify_exit(trade, '', True)\n            self.handle_protections(trade.pair)\n            self.wallets.update()\n        elif send_msg and not trade.open_order_id:\n            # Buy fill\n            self._notify_enter(trade, fill=True)\n\n        return False\n```\n\nOriginal documentation:\n```python\nChecks trades with open orders and updates the amount if necessary\n        Handles closing both buy and sell orders.\n        :param trade: Trade object of the trade we're analyzing\n        :param order_id: Order-id of the order we're analyzing\n        :param action_order: Already acquired order object\n        :return: True if order has been cancelled without being filled partially, False otherwise\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks trades with open orders and updates the amount if necessary\n        Handles closing both buy and sell orders.\n        :param trade: Trade object of the trade we're analyzing\n        :param order_id: Order-id of the order we're analyzing\n        :param action_order: Already acquired order object\n        :param send_msg: Send notification - should always be True except in \"recovery\" methods\n        :return: True if order has been cancelled without being filled partially, False otherwise", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.update_trade_state", "project": "freqtrade", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef quantize(name, x_real, min, max, spline_order=3):\n    \n    \n    \n    assert spline_order in [3, 5]\n    assert min < max\n    lb = x_real.detach().floor()\n\n    s = x_real - lb\n    ss = s * s\n    t = 1 - s\n    tt = t * t\n\n    if spline_order == 3:\n        # This cubic spline interpolates over the nearest four integers, ensuring\n        # piecewise quadratic gradients.\n        probs = torch.stack([\n            t * tt,\n            4 + ss * (3 * s - 6),\n            4 + tt * (3 * t - 6),\n            s * ss,\n        ], dim=-1) * (1/6)\n    elif spline_order == 5:\n        # This quintic spline interpolates over the nearest eight integers, ensuring\n        # piecewise quartic gradients.\n        sss = ss * s\n        ssss = ss * ss\n        sssss = sss * ss\n\n        ttt = tt * t\n        tttt = tt * tt\n        ttttt = ttt * tt\n\n        probs = torch.stack([\n            2 * ttttt,\n            2 + 10 * t + 20 * tt + 20 * ttt + 10 * tttt - 7 * ttttt,\n            55 + 115 * t + 70 * tt - 9 * ttt - 25 * tttt + 7 * ttttt,\n            302 - 100 * ss + 10 * ssss,\n            302 - 100 * tt + 10 * tttt,\n            55 + 115 * s + 70 * ss - 9 * sss - 25 * ssss + 7 * sssss,\n            2 + 10 * s + 20 * ss + 20 * sss + 10 * ssss - 7 * sssss,\n            2 * sssss\n        ], dim=-1) * (1/840)\n\n    q = pyro.sample(\"Q_\" + name, dist.Categorical(probs)).type_as(x_real)\n\n    x = lb + q - {3: 1, 5: 3}[spline_order]\n    x = torch.max(x, 2 * min - 1 - x)\n    x = torch.min(x, 2 * max + 1 - x)\n\n    return pyro.deterministic(name, x)```\n\nUpdated code:\n```python\ndef quantize(name, x_real, min, max):\n    \n    \n    \n    assert min < max\n    lb = x_real.detach().floor()\n\n    # This cubic spline interpolates over the nearest four integers, ensuring\n    # piecewise quadratic gradients.\n    s = x_real - lb\n    ss = s * s\n    t = 1 - s\n    tt = t * t\n    probs = torch.stack([\n        t * tt,\n        4 + ss * (3 * s - 6),\n        4 + tt * (3 * t - 6),\n        s * ss,\n    ], dim=-1) * (1/6)\n    q = pyro.sample(\"Q_\" + name, dist.Categorical(probs)).type_as(x_real)\n\n    x = lb + q - 1\n    x = torch.max(x, 2 * min - 1 - x)\n    x = torch.min(x, 2 * max + 1 - x)\n\n    return pyro.deterministic(name, x)\n```\n\nOriginal documentation:\n```python\nRandomly quantize in a way that preserves probability mass.\n    We use a piecewise polynomial spline of order 3 or 5.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Randomly quantize in a way that preserves probability mass.\n    We use a piecewise polynomial spline of order 3.", "file": "examples_sir_hmc.py", "function": "quantize", "project": "pyro", "change_stats": {"code_changes": 47, "doc_changes": 4, "total_changes": 51}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_order(self, id: str, symbol: Optional[str] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        stop = self.safe_value_2(params, 'is_stop_order', 'stop', False)\n        params = self.omit(params, ['is_stop_order', 'stop'])\n        clientOrderId = self.safe_string_2(params, 'text', 'clientOrderId')\n        orderId = id\n        if clientOrderId is not None:\n            params = self.omit(params, ['text', 'clientOrderId'])\n            if clientOrderId[0] != 't':\n                clientOrderId = 't-' + clientOrderId\n            orderId = clientOrderId\n        market = None if (symbol is None) else self.market(symbol)\n        type, query = self.handle_market_type_and_params('fetchOrder', market, params)\n        contract = (type == 'swap') or (type == 'future')\n        request, requestParams = self.prepare_request(market, type, query) if contract else self.spot_order_prepare_request(market, stop, query)\n        request['order_id'] = orderId\n        methodMiddle = 'PriceOrders' if stop else 'Orders'\n        method = self.get_supported_mapping(type, {\n            'spot': 'privateSpotGet' + methodMiddle + 'OrderId',\n            'margin': 'privateSpotGet' + methodMiddle + 'OrderId',\n            'swap': 'privateFuturesGetSettle' + methodMiddle + 'OrderId',\n            'future': 'privateDeliveryGetSettle' + methodMiddle + 'OrderId',\n        })\n        response = getattr(self, method)(self.extend(request, requestParams))\n        return self.parse_order(response, market)```\n\nUpdated code:\n```python\ndef fetch_order(self, id: str, symbol: Optional[str] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        stop = self.safe_value_2(params, 'is_stop_order', 'stop', False)\n        params = self.omit(params, ['is_stop_order', 'stop'])\n        clientOrderId = self.safe_string_2(params, 'text', 'clientOrderId')\n        orderId = id\n        if clientOrderId is not None:\n            params = self.omit(params, ['text', 'clientOrderId'])\n            if clientOrderId[0] != 't':\n                clientOrderId = 't-' + clientOrderId\n            orderId = clientOrderId\n        market = None if (symbol is None) else self.market(symbol)\n        type, query = self.handle_market_type_and_params('fetchOrder', market, params)\n        contract = (type == 'swap') or (type == 'future') or (type == 'option')\n        request, requestParams = self.prepare_request(market, type, query) if contract else self.spot_order_prepare_request(market, stop, query)\n        request['order_id'] = orderId\n        methodMiddle = 'PriceOrders' if stop else 'Orders'\n        method = self.get_supported_mapping(type, {\n            'spot': 'privateSpotGet' + methodMiddle + 'OrderId',\n            'margin': 'privateSpotGet' + methodMiddle + 'OrderId',\n            'swap': 'privateFuturesGetSettle' + methodMiddle + 'OrderId',\n            'future': 'privateDeliveryGetSettle' + methodMiddle + 'OrderId',\n            'option': 'privateOptionsGetOrdersOrderId',\n        })\n        response = getattr(self, method)(self.extend(request, requestParams))\n        return self.parse_order(response, market)\n```\n\nOriginal documentation:\n```python\nRetrieves information on an order\n        :param str id: Order id\n        :param str symbol: Unified market symbol, *required for spot and margin*\n        :param dict params: Parameters specified by the exchange api\n        :param bool params['stop']: True if the order being fetched is a trigger order\n        :param str params['marginMode']: 'cross' or 'isolated' - marginMode for margin trading if not provided self.options['defaultMarginMode'] is used\n        :param str params['type']: 'spot', 'swap', or 'future', if not provided self.options['defaultMarginMode'] is used\n        :param str params['settle']: 'btc' or 'usdt' - settle currency for perpetual swap and future - market settle currency is used if symbol is not None, default=\"usdt\" for swap and \"btc\" for future\n        :returns: An `order structure <https://docs.ccxt.com/#/?id=order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieves information on an order\n        see https://www.gate.io/docs/developers/apiv4/en/#get-a-single-order\n        see https://www.gate.io/docs/developers/apiv4/en/#get-a-single-order-2\n        see https://www.gate.io/docs/developers/apiv4/en/#get-a-single-order-3\n        see https://www.gate.io/docs/developers/apiv4/en/#get-a-single-order-4\n        :param str id: Order id\n        :param str symbol: Unified market symbol, *required for spot and margin*\n        :param dict params: Parameters specified by the exchange api\n        :param bool params['stop']: True if the order being fetched is a trigger order\n        :param str params['marginMode']: 'cross' or 'isolated' - marginMode for margin trading if not provided self.options['defaultMarginMode'] is used\n        :param str params['type']: 'spot', 'swap', or 'future', if not provided self.options['defaultMarginMode'] is used\n        :param str params['settle']: 'btc' or 'usdt' - settle currency for perpetual swap and future - market settle currency is used if symbol is not None, default=\"usdt\" for swap and \"btc\" for future\n        :returns: An `order structure <https://docs.ccxt.com/#/?id=order-structure>`", "file": "python_ccxt_gate.py", "function": "gate.fetch_order", "project": "ccxt", "change_stats": {"code_changes": 5, "doc_changes": 6, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\n        \n        \n        if isinstance(tensor, Tensor) and tensor.dim() == 0:\n            tensor = tensor.unsqueeze(0)\n\n        import torch_xla.core.functions as xf\n        import torch_xla.core.xla_model as xm\n\n        return xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)```\n\nUpdated code:\n```python\ndef all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\n        \n        \n        if not self._launched:\n            return tensor\n        if not isinstance(tensor, Tensor):\n            raise NotImplementedError(\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\n            )\n        if tensor.dim() == 0:\n            tensor = tensor.unsqueeze(0)\n        if tensor.device.type != \"xla\":\n            tensor = tensor.to(self.root_device)\n\n        import torch_xla.core.functions as xf\n        import torch_xla.core.xla_model as xm\n\n        return xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\n```\n\nOriginal documentation:\n```python\nFunction to gather a tensor from several distributed processes.\n\n        Args:\n            tensor: tensor of shape (batch, ...)\n            group: not available with TPUs\n            sync_grads: flag that allows users to synchronize gradients for the all_gather operation\n        Return:\n            A tensor of shape (world_size, batch, ...)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Function to gather a tensor from several distributed processes.\n\n        Args:\n            tensor: tensor to all-gather.\n            group: unused.\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\n        Return:\n            A tensor of shape (world_size, ...)", "file": "src_lightning_pytorch_strategies_xla.py", "function": "XLAStrategy.all_gather", "project": "pytorch-lightning", "change_stats": {"code_changes": 12, "doc_changes": 10, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef build_entity(\n    start: int, end: int, value: Text, entity_type: Text, **kwargs: Dict[Text, Any]\n) -> Dict[Text, Any]:\n    \n\n    entity = {\"start\": start, \"end\": end, \"value\": value, \"entity\": entity_type}\n\n    entity.update(kwargs)\n    return entity```\n\nUpdated code:\n```python\ndef build_entity(\n    start: int,\n    end: int,\n    value: Text,\n    entity_type: Text,\n    role: Optional[Text] = None,\n    group: Optional[Text] = None,\n    **kwargs: Any,\n) -> Dict[Text, Any]:\n    \n\n    \n\n    entity = {\n        ENTITY_ATTRIBUTE_START: start,\n        ENTITY_ATTRIBUTE_END: end,\n        ENTITY_ATTRIBUTE_VALUE: value,\n        ENTITY_ATTRIBUTE_TYPE: entity_type,\n    }\n\n    if role:\n        entity[ENTITY_ATTRIBUTE_ROLE] = role\n    if group:\n        entity[ENTITY_ATTRIBUTE_GROUP] = group\n\n    entity.update(kwargs)\n    return entity\n```\n\nOriginal documentation:\n```python\nBuilds a standard entity dictionary.\n\n    Adds additional keyword parameters.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Builds a standard entity dictionary.\n\n    Adds additional keyword parameters.\n\n    Args:\n        start: start position of entity\n        end: end position of entity\n        value: text value of the entity\n        entity_type: name of the entity type\n        role: role of the entity\n        group: group of the entity\n        **kwargs: additional parameters\n\n    Returns:\n        an entity dictionary", "file": "rasa_nlu_utils___init__.py", "function": "build_entity", "project": "rasa", "change_stats": {"code_changes": 24, "doc_changes": 14, "total_changes": 38}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sample_model(self, df, seasonal_features, iteration, s_a, s_m):\n        \n        \n        trend = self.sample_predictive_trend(df, iteration)\n\n        beta = self.params['beta'][iteration]\n        Xb_a = np.matmul(seasonal_features.values,\n                         beta * s_a.values) * self.y_scale\n        Xb_m = np.matmul(seasonal_features.values, beta * s_m.values)\n\n        sigma = self.params['sigma_obs'][iteration]\n        noise = np.random.normal(0, sigma, df.shape[0]) * self.y_scale\n\n        return pd.DataFrame({\n            'yhat': trend * (1 + Xb_m) + Xb_a + noise,\n            'trend': trend\n        })```\n\nUpdated code:\n```python\ndef sample_model(self, df, seasonal_features, iteration, s_a, s_m) -> Dict[str, np.ndarray]:\n        \n        \n        trend = self.sample_predictive_trend(df, iteration)\n\n        beta = self.params['beta'][iteration]\n        Xb_a = np.matmul(seasonal_features.values,\n                         beta * s_a.values) * self.y_scale\n        Xb_m = np.matmul(seasonal_features.values, beta * s_m.values)\n\n        sigma = self.params['sigma_obs'][iteration]\n        noise = np.random.normal(0, sigma, df.shape[0]) * self.y_scale\n\n        return {\n            'yhat': trend * (1 + Xb_m) + Xb_a + noise,\n            'trend': trend\n        }\n```\n\nOriginal documentation:\n```python\nSimulate observations from the extrapolated generative model.\n\n        Parameters\n        ----------\n        df: Prediction dataframe.\n        seasonal_features: pd.DataFrame of seasonal features.\n        iteration: Int sampling iteration to use parameters from.\n        s_a: Indicator vector for additive components\n        s_m: Indicator vector for multiplicative components\n\n        Returns\n        -------\n        Dataframe with trend and yhat, each like df['t'].\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Simulate observations from the extrapolated generative model.\n\n        Parameters\n        ----------\n        df: Prediction dataframe.\n        seasonal_features: pd.DataFrame of seasonal features.\n        iteration: Int sampling iteration to use parameters from.\n        s_a: Indicator vector for additive components\n        s_m: Indicator vector for multiplicative components\n\n        Returns\n        -------\n        Dictionary with `yhat` and `trend`, each like df['t'].", "file": "python_prophet_forecaster.py", "function": "Prophet.sample_model", "project": "prophet", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef train(self, unfiltered_dataframe: DataFrame, metadata: dict) -> Tuple[DataFrame, DataFrame]:\n        \n        \n        \n\n        return unfiltered_dataframe, unfiltered_dataframe```\n\nUpdated code:\n```python\ndef train(self, unfiltered_dataframe: DataFrame, metadata: dict) -> Any:\n        \n        \n        \n\n        return Any\n```\n\nOriginal documentation:\n```python\nFilter the training data and train a model to it. Train makes heavy use of the datahandler\n        for storing, saving, loading, and managed.\n        :params:\n        :unfiltered_dataframe: Full dataframe for the current training period\n        :metadata: pair metadata from strategy. \n        :returns:\n        :model: Trained model which can be used to inference (self.predict)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Filter the training data and train a model to it. Train makes heavy use of the datahandler\n        for storing, saving, loading, and analyzing the data.\n        :params:\n        :unfiltered_dataframe: Full dataframe for the current training period\n        :metadata: pair metadata from strategy. \n        :returns:\n        :model: Trained model which can be used to inference (self.predict)", "file": "freqtrade_freqai_freqai_interface.py", "function": "IFreqaiModel.train", "project": "freqtrade", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef limitQuery(self, num, query, fieldsList=None):\n        \n        \n        \n\n        limitedQuery  = query\n        limitStr      = queries[kb.dbms].limit\n        fromIndex     = limitedQuery.index(\" FROM \")\n        untilFrom     = limitedQuery[:fromIndex]\n        fromFrom      = limitedQuery[fromIndex+1:]\n\n        if kb.dbms in ( \"MySQL\", \"PostgreSQL\" ):\n            limitStr = queries[kb.dbms].limit % (num, 1)\n            limitedQuery += \" %s\" % limitStr\n\n        # TODO: fix for Partial UNION query SQL injection technique both\n        # Oracle and Microsoft SQL Server\n        elif kb.dbms == \"Oracle\":\n            if query.startswith(\"SELECT \"):\n                limitedQuery = \"%s FROM (%s, %s\" % (untilFrom, untilFrom, limitStr)\n            else:\n                limitedQuery = \"%s FROM (SELECT %s, %s\" % (untilFrom, \", \".join(field for field in fieldsList), limitStr) \n            limitedQuery  = limitedQuery % fromFrom\n            limitedQuery += \"=%d\" % (num + 1)\n\n        elif kb.dbms == \"Microsoft SQL Server\":\n            if re.search(\" ORDER BY \", limitedQuery, re.I):\n                untilOrderChar = limitedQuery.index(\" ORDER BY \")\n                limitedQuery   = limitedQuery[:untilOrderChar]\n\n            limitedQuery  = limitedQuery.replace(\"SELECT \", (limitStr % 1), 1)\n            limitedQuery  = \"%s WHERE %s \" % (limitedQuery, fieldsList[0])\n            limitedQuery += \"NOT IN (%s\" % (limitStr % num)\n            limitedQuery += \"%s %s)\" % (fieldsList[0], fromFrom)\n\n        return limitedQuery```\n\nUpdated code:\n```python\ndef limitQuery(self, num, query, field):\n        \n        \n        \n\n        limitedQuery  = query\n        limitStr      = queries[kb.dbms].limit\n        fromIndex     = limitedQuery.index(\" FROM \")\n        untilFrom     = limitedQuery[:fromIndex]\n        fromFrom      = limitedQuery[fromIndex+1:]\n\n        if kb.dbms in ( \"MySQL\", \"PostgreSQL\" ):\n            limitStr = queries[kb.dbms].limit % (num, 1)\n            limitedQuery += \" %s\" % limitStr\n\n        # TODO: fix Partial UNION query SQL injection technique for Oracle\n        elif kb.dbms == \"Oracle\":\n            if query.startswith(\"SELECT \"):\n                limitedQuery = \"%s FROM (%s, %s\" % (untilFrom, untilFrom, limitStr)\n            else:\n                limitedQuery = \"%s FROM (SELECT %s, %s\" % (untilFrom, field, limitStr)\n            limitedQuery  = limitedQuery % fromFrom\n            limitedQuery += \"=%d\" % (num + 1)\n\n        elif kb.dbms == \"Microsoft SQL Server\":\n            if re.search(\" ORDER BY \", limitedQuery, re.I):\n                untilOrderChar = limitedQuery.index(\" ORDER BY \")\n                limitedQuery   = limitedQuery[:untilOrderChar]\n\n            limitedQuery  = limitedQuery.replace(\"SELECT \", (limitStr % 1), 1)\n            limitedQuery  = \"%s WHERE %s \" % (limitedQuery, field)\n            limitedQuery += \"NOT IN (%s\" % (limitStr % num)\n            limitedQuery += \"%s %s)\" % (field, fromFrom)\n\n        return limitedQuery\n```\n\nOriginal documentation:\n```python\nTake in input a query string and return its limited query string.\n\n        Example:\n\n        Input:  SELECT user FROM mysql.users\n        Output: SELECT user FROM mysql.users LIMIT <num>, 1\n\n        @param num: limit number\n        @type num: C{int}\n\n        @param query: query to be processed\n        @type query: C{str}\n\n        @param fieldsList: list of fields within the query\n        @type fieldsList: C{list}\n\n        @return: limited query string\n        @rtype: C{str}\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Take in input a query string and return its limited query string.\n\n        Example:\n\n        Input:  SELECT user FROM mysql.users\n        Output: SELECT user FROM mysql.users LIMIT <num>, 1\n\n        @param num: limit number\n        @type num: C{int}\n\n        @param query: query to be processed\n        @type query: C{str}\n\n        @param field: field within the query\n        @type field: C{list}\n\n        @return: limited query string\n        @rtype: C{str}", "file": "lib_core_agent.py", "function": "Agent.limitQuery", "project": "sqlmap", "change_stats": {"code_changes": 13, "doc_changes": 6, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _walk_global_ops(code):\n    \n    \n    \n    for instr in dis.get_instructions(code):\n        op = instr.opcode\n        if op in GLOBAL_OPS:\n            yield op, instr.arg```\n\nUpdated code:\n```python\ndef _walk_global_ops(code):\n    \n    \n    \n    for instr in dis.get_instructions(code):\n        op = instr.opcode\n        if op in GLOBAL_OPS:\n            yield instr.argval\n```\n\nOriginal documentation:\n```python\nYield (opcode, argument number) tuples for all\n    global-referencing instructions in *code*.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Yield referenced name for all global-referencing instructions in *code*.", "file": "python_ray_cloudpickle_cloudpickle.py", "function": "_walk_global_ops", "project": "ray", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cyclegan_generator_resnet(images,\n                              arg_scope_fn=cyclegan_arg_scope,\n                              num_resnet_blocks=6,\n                              num_filters=64,\n                              upsample_fn=cyclegan_upsample,\n                              kernel_size=3,\n                              num_outputs=3,\n                              tanh_linear_slope=0.0,\n                              is_training=False):\n  \n  \n  # Neither dropout nor batch norm -> dont need is_training\n  del is_training\n\n  end_points = {}\n\n  input_size = images.shape.as_list()\n  height, width = input_size[1], input_size[2]\n  if height and height % 4 != 0:\n    raise ValueError('The input height must be a multiple of 4.')\n  if width and width % 4 != 0:\n    raise ValueError('The input width must be a multiple of 4.')\n\n  if not isinstance(kernel_size, (list, tuple)):\n    kernel_size = [kernel_size, kernel_size]\n\n  kernel_height = kernel_size[0]\n  kernel_width = kernel_size[1]\n  pad_top = (kernel_height - 1) // 2\n  pad_bottom = kernel_height // 2\n  pad_left = (kernel_width - 1) // 2\n  pad_right = kernel_width // 2\n  paddings = np.array(\n      [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]],\n      dtype=np.int32)\n  spatial_pad_3 = np.array([[0, 0], [3, 3], [3, 3], [0, 0]])\n\n  with tf.contrib.framework.arg_scope(arg_scope_fn()):\n\n    ###########\n    # Encoder #\n    ###########\n    with tf.variable_scope('input'):\n      # 7x7 input stage\n      net = tf.pad(images, spatial_pad_3, 'REFLECT')\n      net = layers.conv2d(net, num_filters, kernel_size=[7, 7], padding='VALID')\n      end_points['encoder_0'] = net\n\n    with tf.variable_scope('encoder'):\n      with tf.contrib.framework.arg_scope(\n          [layers.conv2d],\n          kernel_size=kernel_size,\n          stride=2,\n          activation_fn=tf.nn.relu,\n          padding='VALID'):\n\n        net = tf.pad(net, paddings, 'REFLECT')\n        net = layers.conv2d(net, num_filters * 2)\n        end_points['encoder_1'] = net\n        net = tf.pad(net, paddings, 'REFLECT')\n        net = layers.conv2d(net, num_filters * 4)\n        end_points['encoder_2'] = net\n\n    ###################\n    # Residual Blocks #\n    ###################\n    with tf.variable_scope('residual_blocks'):\n      with tf.contrib.framework.arg_scope(\n          [layers.conv2d],\n          kernel_size=kernel_size,\n          stride=1,\n          activation_fn=tf.nn.relu,\n          padding='VALID'):\n        for block_id in xrange(num_resnet_blocks):\n          with tf.variable_scope('block_{}'.format(block_id)):\n            res_net = tf.pad(net, paddings, 'REFLECT')\n            res_net = layers.conv2d(res_net, num_filters * 4)\n            res_net = tf.pad(res_net, paddings, 'REFLECT')\n            res_net = layers.conv2d(res_net, num_filters * 4,\n                                    activation_fn=None)\n            net += res_net\n\n            end_points['resnet_block_%d' % block_id] = net\n\n    ###########\n    # Decoder #\n    ###########\n    with tf.variable_scope('decoder'):\n\n      with tf.contrib.framework.arg_scope(\n          [layers.conv2d],\n          kernel_size=kernel_size,\n          stride=1,\n          activation_fn=tf.nn.relu):\n\n        with tf.variable_scope('decoder1'):\n          net = upsample_fn(net, num_outputs=num_filters * 2, stride=[2, 2])\n        end_points['decoder1'] = net\n\n        with tf.variable_scope('decoder2'):\n          net = upsample_fn(net, num_outputs=num_filters, stride=[2, 2])\n        end_points['decoder2'] = net\n\n    with tf.variable_scope('output'):\n      net = tf.pad(net, spatial_pad_3, 'REFLECT')\n      logits = layers.conv2d(\n          net,\n          num_outputs, [7, 7],\n          activation_fn=None,\n          normalizer_fn=None,\n          padding='valid')\n      logits = tf.reshape(logits, _dynamic_or_static_shape(images))\n\n      end_points['logits'] = logits\n      end_points['predictions'] = tf.tanh(logits) + logits * tanh_linear_slope\n\n  return end_points['predictions'], end_points```\n\nUpdated code:\n```python\ndef cyclegan_generator_resnet(images,\n                              arg_scope_fn=cyclegan_arg_scope,\n                              num_resnet_blocks=6,\n                              num_filters=64,\n                              upsample_fn=cyclegan_upsample,\n                              kernel_size=3,\n                              tanh_linear_slope=0.0,\n                              is_training=False):\n  \n  \n  # Neither dropout nor batch norm -> dont need is_training\n  del is_training\n\n  end_points = {}\n\n  input_size = images.shape.as_list()\n  height, width = input_size[1], input_size[2]\n  if height and height % 4 != 0:\n    raise ValueError('The input height must be a multiple of 4.')\n  if width and width % 4 != 0:\n    raise ValueError('The input width must be a multiple of 4.')\n  num_outputs = input_size[3]\n\n  if not isinstance(kernel_size, (list, tuple)):\n    kernel_size = [kernel_size, kernel_size]\n\n  kernel_height = kernel_size[0]\n  kernel_width = kernel_size[1]\n  pad_top = (kernel_height - 1) // 2\n  pad_bottom = kernel_height // 2\n  pad_left = (kernel_width - 1) // 2\n  pad_right = kernel_width // 2\n  paddings = np.array(\n      [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]],\n      dtype=np.int32)\n  spatial_pad_3 = np.array([[0, 0], [3, 3], [3, 3], [0, 0]])\n\n  with tf.contrib.framework.arg_scope(arg_scope_fn()):\n\n    ###########\n    # Encoder #\n    ###########\n    with tf.variable_scope('input'):\n      # 7x7 input stage\n      net = tf.pad(images, spatial_pad_3, 'REFLECT')\n      net = layers.conv2d(net, num_filters, kernel_size=[7, 7], padding='VALID')\n      end_points['encoder_0'] = net\n\n    with tf.variable_scope('encoder'):\n      with tf.contrib.framework.arg_scope(\n          [layers.conv2d],\n          kernel_size=kernel_size,\n          stride=2,\n          activation_fn=tf.nn.relu,\n          padding='VALID'):\n\n        net = tf.pad(net, paddings, 'REFLECT')\n        net = layers.conv2d(net, num_filters * 2)\n        end_points['encoder_1'] = net\n        net = tf.pad(net, paddings, 'REFLECT')\n        net = layers.conv2d(net, num_filters * 4)\n        end_points['encoder_2'] = net\n\n    ###################\n    # Residual Blocks #\n    ###################\n    with tf.variable_scope('residual_blocks'):\n      with tf.contrib.framework.arg_scope(\n          [layers.conv2d],\n          kernel_size=kernel_size,\n          stride=1,\n          activation_fn=tf.nn.relu,\n          padding='VALID'):\n        for block_id in xrange(num_resnet_blocks):\n          with tf.variable_scope('block_{}'.format(block_id)):\n            res_net = tf.pad(net, paddings, 'REFLECT')\n            res_net = layers.conv2d(res_net, num_filters * 4)\n            res_net = tf.pad(res_net, paddings, 'REFLECT')\n            res_net = layers.conv2d(res_net, num_filters * 4,\n                                    activation_fn=None)\n            net += res_net\n\n            end_points['resnet_block_%d' % block_id] = net\n\n    ###########\n    # Decoder #\n    ###########\n    with tf.variable_scope('decoder'):\n\n      with tf.contrib.framework.arg_scope(\n          [layers.conv2d],\n          kernel_size=kernel_size,\n          stride=1,\n          activation_fn=tf.nn.relu):\n\n        with tf.variable_scope('decoder1'):\n          net = upsample_fn(net, num_outputs=num_filters * 2, stride=[2, 2])\n        end_points['decoder1'] = net\n\n        with tf.variable_scope('decoder2'):\n          net = upsample_fn(net, num_outputs=num_filters, stride=[2, 2])\n        end_points['decoder2'] = net\n\n    with tf.variable_scope('output'):\n      net = tf.pad(net, spatial_pad_3, 'REFLECT')\n      logits = layers.conv2d(\n          net,\n          num_outputs, [7, 7],\n          activation_fn=None,\n          normalizer_fn=None,\n          padding='valid')\n      logits = tf.reshape(logits, _dynamic_or_static_shape(images))\n\n      end_points['logits'] = logits\n      end_points['predictions'] = tf.tanh(logits) + logits * tanh_linear_slope\n\n  return end_points['predictions'], end_points\n```\n\nOriginal documentation:\n```python\nDefines the cyclegan resnet network architecture.\n\n  As closely as possible following\n  https://github.com/junyanz/CycleGAN/blob/master/models/architectures.lua#L232\n\n  FYI: This network requires input height and width to be divisible by 4 in\n  order to generate an output with shape equal to input shape. Assertions will\n  catch this if input dimensions are known at graph construction time, but\n  there's no protection if unknown at graph construction time (you'll see an\n  error).\n\n  Args:\n    images: Input image tensor of shape [batch_size, h, w, 3].\n    arg_scope_fn: Function to create the global arg_scope for the network.\n    num_resnet_blocks: Number of ResNet blocks in the middle of the generator.\n    num_filters: Number of filters of the first hidden layer.\n    upsample_fn: Upsampling function for the decoder part of the generator.\n    kernel_size: Size w or list/tuple [h, w] of the filter kernels for all inner\n      layers.\n    num_outputs: Number of output layers. Defaults to 3 for RGB.\n    tanh_linear_slope: Slope of the linear function to add to the tanh over the\n      logits.\n    is_training: Whether the network is created in training mode or inference\n      only mode. Not actually needed, just for compliance with other generator\n      network functions.\n\n  Returns:\n    A `Tensor` representing the model output and a dictionary of model end\n      points.\n\n  Raises:\n    ValueError: If the input height or width is known at graph construction time\n      and not a multiple of 4.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Defines the cyclegan resnet network architecture.\n\n  As closely as possible following\n  https://github.com/junyanz/CycleGAN/blob/master/models/architectures.lua#L232\n\n  FYI: This network requires input height and width to be divisible by 4 in\n  order to generate an output with shape equal to input shape. Assertions will\n  catch this if input dimensions are known at graph construction time, but\n  there's no protection if unknown at graph construction time (you'll see an\n  error).\n\n  Args:\n    images: Input image tensor of shape [batch_size, h, w, 3].\n    arg_scope_fn: Function to create the global arg_scope for the network.\n    num_resnet_blocks: Number of ResNet blocks in the middle of the generator.\n    num_filters: Number of filters of the first hidden layer.\n    upsample_fn: Upsampling function for the decoder part of the generator.\n    kernel_size: Size w or list/tuple [h, w] of the filter kernels for all inner\n      layers.\n    tanh_linear_slope: Slope of the linear function to add to the tanh over the\n      logits.\n    is_training: Whether the network is created in training mode or inference\n      only mode. Not actually needed, just for compliance with other generator\n      network functions.\n\n  Returns:\n    A `Tensor` representing the model output and a dictionary of model end\n      points.\n\n  Raises:\n    ValueError: If the input height or width is known at graph construction time\n      and not a multiple of 4.", "file": "research_slim_nets_cyclegan.py", "function": "cyclegan_generator_resnet", "project": "models", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef axes(arg=None, **kwargs):\n    \n    \n\n    \n\n    if arg is None:\n        return subplot(111, **kwargs)\n\n    if isinstance(arg, Axes):\n        warn_deprecated(\"2.2\",\n                        message=\"Using pyplot.axes(ax) with ax an Axes \"\n                                \"argument is deprecated. Please use \"\n                                \"pyplot.sca(ax) instead.\")\n        ax = arg\n        sca(ax)\n        return ax\n    else:\n        rect = arg\n        return gcf().add_axes(rect, **kwargs)```\n\nUpdated code:\n```python\ndef axes(arg=None, **kwargs):\n    \n    \n    \n\n    if arg is None:\n        return subplot(111, **kwargs)\n\n    if isinstance(arg, Axes):\n        warn_deprecated(\"2.2\",\n                        message=\"Using pyplot.axes(ax) with ax an Axes \"\n                                \"argument is deprecated. Please use \"\n                                \"pyplot.sca(ax) instead.\")\n        ax = arg\n        sca(ax)\n        return ax\n    else:\n        rect = arg\n        return gcf().add_axes(rect, **kwargs)\n```\n\nOriginal documentation:\n```python\nAdd an axes to the current figure and make it the current axes.\n\n    Parameters\n    ----------\n    arg : None or 4-tuple or Axes\n        The exact behavior of this function depends on the type:\n\n        - *None*: A new full window axes is added using\n          ``subplot(111, **kwargs)``\n        - 4-tuple of floats *rect* = ``[left, bottom, width, height]``.\n          A new axes is added with dimensions *rect* in normalized\n          (0, 1) units using `~.Figure.add_axes` on the current figure.\n        - `~matplotlib.axes.Axes`: This is equivalent to `.pyplot.sca`.\n          It sets the current axes to *arg*. Note: This implicitly\n          changes the current figure to the parent of *arg*.\n\n          .. note:: The use of an Axes as an argument is deprecated and will be\n                    removed in v3.0. Please use `.pyplot.sca` instead.\n\n    Other Parameters\n    ----------------\n    **kwargs :\n        For allowed keyword arguments see `.pyplot.subplot` and\n        `.Figure.add_axes` respectively. Some common keyword arguments are\n        listed below:\n\n        ========= =========== =================================================\n        kwarg     Accepts     Description\n        ========= =========== =================================================\n        facecolor color       the axes background color\n        frameon   bool        whether to display the frame\n        sharex    otherax     share x-axis with *otherax*\n        sharey    otherax     share y-axis with *otherax*\n        polar     bool        whether to use polar axes\n        aspect    [str | num] ['equal', 'auto'] or a number.  If a number, the\n                              ratio of y-unit/x-unit in screen-space.  See also\n                              `~.Axes.set_aspect`.\n        ========= =========== =================================================\n\n    Returns\n    -------\n    axes : Axes\n        The created or activated axes.\n\n    Examples\n    --------\n    Creating a new full window axes::\n\n        >>> plt.axes()\n\n    Creating a new axes with specified dimensions and some kwargs::\n\n        >>> plt.axes((left, bottom, width, height), facecolor='w')\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add an axes to the current figure and make it the current axes.\n\n    Call signatures::\n\n        plt.axes()\n        plt.axes(rect, projection=None, polar=False, **kwargs)\n        plt.axes(ax)\n\n    Parameters\n    ----------\n    arg : { None, 4-tuple, Axes }\n        The exact behavior of this function depends on the type:\n\n        - *None*: A new full window axes is added using\n          ``subplot(111, **kwargs)``\n        - 4-tuple of floats *rect* = ``[left, bottom, width, height]``.\n          A new axes is added with dimensions *rect* in normalized\n          (0, 1) units using `~.Figure.add_axes` on the current figure.\n        - `~.axes.Axes`: This is equivalent to `.pyplot.sca`.\n          It sets the current axes to *arg*. Note: This implicitly\n          changes the current figure to the parent of *arg*.\n\n          .. note:: The use of an `.axes.Axes` as an argument is deprecated\n                    and will be removed in v3.0. Please use `.pyplot.sca`\n                    instead.\n\n    projection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', \\\n'polar', 'rectilinear', str}, optional\n        The projection type of the `~.axes.Axes`. *str* is the name of\n        a costum projection, see `~matplotlib.projections`. The default\n        None results in a 'rectilinear' projection.\n\n    polar : boolean, optional\n        If True, equivalent to projection='polar'.\n\n    sharex, sharey : `~.axes.Axes`, optional\n        Share the x or y `~matplotlib.axis` with sharex and/or sharey.\n        The axis will have the same limits, ticks, and scale as the axis\n        of the shared axes.\n\n\n    label : str\n        A label for the returned axes.\n\n    Other Parameters\n    ----------------\n    **kwargs\n        This method also takes the keyword arguments for\n        the returned axes class. The keyword arguments for the\n        rectilinear axes class `~.axes.Axes` can be found in\n        the following table but there might also be other keyword\n        arguments if another projection is used, see the actual axes\n        class.\n        %(Axes)s\n\n    Returns\n    -------\n    axes : `~.axes.Axes` (or a subclass of `~.axes.Axes`)\n        The returned axes class depends on the projection used. It is\n        `~.axes.Axes` if rectilinear projection are used and\n        `.projections.polar.PolarAxes` if polar projection\n        are used.\n\n    Notes\n    -----\n    If the figure already has a axes with key (*args*,\n    *kwargs*) then it will simply make that axes current and\n    return it.  This behavior is deprecated. Meanwhile, if you do\n    not want this behavior (i.e., you want to force the creation of a\n    new axes), you must use a unique set of args and kwargs.  The axes\n    *label* attribute has been exposed for this purpose: if you want\n    two axes that are otherwise identical to be added to the figure,\n    make sure you give them unique labels.\n\n    See Also\n    --------\n    .Figure.add_axes\n    .pyplot.subplot\n    .Figure.add_subplot\n    .Figure.subplots\n    .pyplot.subplots\n\n    Examples\n    --------\n    ::\n\n        #Creating a new full window axes\n        plt.axes()\n\n        #Creating a new axes with specified dimensions and some kwargs\n        plt.axes((left, bottom, width, height), facecolor='w')", "file": "lib_matplotlib_pyplot.py", "function": "axes", "project": "matplotlib", "change_stats": {"code_changes": 3, "doc_changes": 96, "total_changes": 99}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef color_mode(self):\n        \n        if self._feature.supports_white and self._feature.supports_color:\n            return ColorMode.RGBW\n        if self._feature.supports_brightness:\n            return ColorMode.BRIGHTNESS\n        return ColorMode.ONOFF```\n\nUpdated code:\n```python\ndef color_mode(self):\n        \n        \n        color_mode_tmp = COLOR_MODE_MAP.get(self._feature.color_mode, ColorMode.ONOFF)\n        if color_mode_tmp == ColorMode.COLOR_TEMP:\n            self._attr_min_mireds = 1\n            self._attr_max_mireds = 255\n\n        return color_mode_tmp\n```\n\nOriginal documentation:\n```python\nReturn the color mode.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the color mode.\n\n        Set values to _attr_ibutes if needed.", "file": "homeassistant_components_blebox_light.py", "function": "BleBoxLightEntity.color_mode", "project": "core", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ctc_align(args, device):\n    \n        \n    \n    model, train_args = load_trained_model(args.model)\n    assert isinstance(model, ASRInterface)\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=\"asr\",\n        load_output=True,\n        sort_in_input_length=False,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={\"train\": False},\n    )\n    logging.info(f\"Decoding device={device}\")\n    model.to(device=device).eval()\n    # read audio and text json data\n    with open(args.data_json, \"rb\") as f:\n        js = json.load(f)[\"utts\"]\n    with open(args.utt_text, \"r\") as f:\n        lines = f.readlines()\n        i = 0\n        text = {}\n        segment_names = {}\n        for name in js.keys():\n            text_per_audio = []\n            segment_names_per_audio = []\n            while i < len(lines) and lines[i].startswith(name):\n                text_per_audio.append(lines[i][lines[i].find(\" \") + 1 :])\n                segment_names_per_audio.append(lines[i][: lines[i].find(\" \")])\n                i += 1\n            text[name] = text_per_audio\n            segment_names[name] = segment_names_per_audio\n    # apply configuration\n    config = CtcSegmentationParameters()\n    if args.subsampling_factor is not None:\n        config.subsampling_factor = args.subsampling_factor\n    if args.frame_duration is not None:\n        config.frame_duration_ms = args.frame_duration\n    if args.min_window_size is not None:\n        config.min_window_size = args.min_window_size\n    if args.max_window_size is not None:\n        config.max_window_size = args.max_window_size\n    logging.debug(\n        f\"Frame timings: {config.frame_duration_ms}ms * {config.subsampling_factor}\"\n    )\n    # Iterate over audio files to decode and align\n    for idx, name in enumerate(js.keys(), 1):\n        logging.info(\"(%d/%d) Aligning \" + name, idx, len(js.keys()))\n        batch = [(name, js[name])]\n        feat, label = load_inputs_and_targets(batch)\n        feat = feat[0]\n        with torch.no_grad():\n            # Encode input frames\n            enc_output = model.encode(torch.as_tensor(feat).to(device)).unsqueeze(0)\n            # Apply ctc layer to obtain log character probabilities\n            lpz = model.ctc.log_softmax(enc_output)[0].cpu().numpy()\n        # Prepare the text for aligning\n        ground_truth_mat, utt_begin_indices = prepare_text(\n            config, text[name], train_args.char_list\n        )\n        # Align using CTC segmentation\n        timings, char_probs, char_list = ctc_segmentation(config, lpz, ground_truth_mat)\n        # Obtain list of utterances with corresponding time intervals and confidence score\n        segments = determine_utterance_segments(\n            config, utt_begin_indices, char_probs, timings, text[name]\n        )\n        # Write to \"segments\" file\n        for i, boundary in enumerate(segments):\n            utt_segment = f\"{segment_names[name][i]} {name} {boundary[0]:.2f} {boundary[1]:.2f} {boundary[2]:.9f}\\n\"\n            args.output.write(utt_segment)\n    return 0```\n\nUpdated code:\n```python\ndef ctc_align(args, device):\n    \n    \n    model, train_args = load_trained_model(args.model)\n    assert isinstance(model, ASRInterface)\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=\"asr\",\n        load_output=True,\n        sort_in_input_length=False,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={\"train\": False},\n    )\n    logging.info(f\"Decoding device={device}\")\n    model.to(device=device).eval()\n    # read audio and text json data\n    with open(args.data_json, \"rb\") as f:\n        js = json.load(f)[\"utts\"]\n    with open(args.utt_text, \"r\") as f:\n        lines = f.readlines()\n        i = 0\n        text = {}\n        segment_names = {}\n        for name in js.keys():\n            text_per_audio = []\n            segment_names_per_audio = []\n            while i < len(lines) and lines[i].startswith(name):\n                text_per_audio.append(lines[i][lines[i].find(\" \") + 1 :])\n                segment_names_per_audio.append(lines[i][: lines[i].find(\" \")])\n                i += 1\n            text[name] = text_per_audio\n            segment_names[name] = segment_names_per_audio\n    # apply configuration\n    config = CtcSegmentationParameters()\n    if args.subsampling_factor is not None:\n        config.subsampling_factor = args.subsampling_factor\n    if args.frame_duration is not None:\n        config.frame_duration_ms = args.frame_duration\n    if args.min_window_size is not None:\n        config.min_window_size = args.min_window_size\n    if args.max_window_size is not None:\n        config.max_window_size = args.max_window_size\n    logging.debug(\n        f\"Frame timings: {config.frame_duration_ms}ms * {config.subsampling_factor}\"\n    )\n    # Iterate over audio files to decode and align\n    for idx, name in enumerate(js.keys(), 1):\n        logging.info(\"(%d/%d) Aligning \" + name, idx, len(js.keys()))\n        batch = [(name, js[name])]\n        feat, label = load_inputs_and_targets(batch)\n        feat = feat[0]\n        with torch.no_grad():\n            # Encode input frames\n            enc_output = model.encode(torch.as_tensor(feat).to(device)).unsqueeze(0)\n            # Apply ctc layer to obtain log character probabilities\n            lpz = model.ctc.log_softmax(enc_output)[0].cpu().numpy()\n        # Prepare the text for aligning\n        ground_truth_mat, utt_begin_indices = prepare_text(\n            config, text[name], train_args.char_list\n        )\n        # Align using CTC segmentation\n        timings, char_probs, char_list = ctc_segmentation(config, lpz, ground_truth_mat)\n        # Obtain list of utterances with time intervals and confidence score\n        segments = determine_utterance_segments(\n            config, utt_begin_indices, char_probs, timings, text[name]\n        )\n        # Write to \"segments\" file\n        for i, boundary in enumerate(segments):\n            utt_segment = (\n                f\"{segment_names[name][i]} {name} {boundary[0]:.2f}\"\n                f\" {boundary[1]:.2f} {boundary[2]:.9f}\\n\"\n            )\n            args.output.write(utt_segment)\n    return 0\n```\n\nOriginal documentation:\n```python\nParses configuration, infers the CTC posterior probabilities,\n        and then aligns start and end of utterances using CTC segmentation.\n        Results are written to the output file given in the args\n\n    :param args: given configuration\n    :param device: for inference; one of ['cuda', 'cpu']\n    :return:  0 on success\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "ESPnet-specific interface for CTC segmentation.\n\n    Parses configuration, infers the CTC posterior probabilities,\n    and then aligns start and end of utterances using CTC segmentation.\n    Results are written to the output file given in the args.\n\n    :param args: given configuration\n    :param device: for inference; one of ['cuda', 'cpu']\n    :return:  0 on success", "file": "espnet_bin_asr_align.py", "function": "ctc_align", "project": "espnet", "change_stats": {"code_changes": 10, "doc_changes": 10, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_sql_query_chain(\n    llm: BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: Optional[BasePromptTemplate] = None,\n    k: int = 5,\n) -> Runnable[Union[SQLInput, SQLInputWithTables], str]:\n    \n    \n    if prompt is not None:\n        prompt_to_use = prompt\n    elif db.dialect in SQL_PROMPTS:\n        prompt_to_use = SQL_PROMPTS[db.dialect]\n    else:\n        prompt_to_use = PROMPT\n    inputs = {\n        \"input\": lambda x: x[\"question\"] + \"\\nSQLQuery: \",\n        \"top_k\": lambda _: k,\n        \"table_info\": lambda x: db.get_table_info(\n            table_names=x.get(\"table_names_to_use\")\n        ),\n    }\n    if \"dialect\" in prompt_to_use.input_variables:\n        inputs[\"dialect\"] = lambda _: (db.dialect, prompt_to_use)\n    return (\n        RunnableParallel(inputs)\n        | prompt_to_use\n        | llm.bind(stop=[\"\\nSQLResult:\"])\n        | StrOutputParser()\n        | _strip\n    )```\n\nUpdated code:\n```python\ndef create_sql_query_chain(\n    llm: BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: Optional[BasePromptTemplate] = None,\n    k: int = 5,\n) -> Runnable[Union[SQLInput, SQLInputWithTables, Dict[str, Any]], str]:\n    \n      # noqa: E501\n    if prompt is not None:\n        prompt_to_use = prompt\n    elif db.dialect in SQL_PROMPTS:\n        prompt_to_use = SQL_PROMPTS[db.dialect]\n    else:\n        prompt_to_use = PROMPT\n    if {\"input\", \"top_k\", \"table_info\"}.difference(prompt_to_use.input_variables):\n        raise ValueError(\n            f\"Prompt must have input variables: 'input', 'top_k', \"\n            f\"'table_info'. Received prompt with input variables: \"\n            f\"{prompt_to_use.input_variables}. Full prompt:\\n\\n{prompt_to_use}\"\n        )\n    if \"dialect\" in prompt_to_use.input_variables:\n        prompt_to_use = prompt_to_use.partial(dialect=db.dialect)\n\n    inputs = {\n        \"input\": lambda x: x[\"question\"] + \"\\nSQLQuery: \",\n        \"table_info\": lambda x: db.get_table_info(\n            table_names=x.get(\"table_names_to_use\")\n        ),\n    }\n    return (\n        RunnablePassthrough.assign(**inputs)  # type: ignore\n        | (\n            lambda x: {\n                k: v\n                for k, v in x.items()\n                if k not in (\"question\", \"table_names_to_use\")\n            }\n        )\n        | prompt_to_use.partial(top_k=str(k))\n        | llm.bind(stop=[\"\\nSQLResult:\"])\n        | StrOutputParser()\n        | _strip\n    )\n```\n\nOriginal documentation:\n```python\nCreate a chain that generates SQL queries.\n\n    *Security Note*: This chain generates SQL queries for the given database.\n\n        The SQLDatabase class provides a get_table_info method that can be used\n        to get column information as well as sample data from the table.\n\n        To mitigate risk of leaking sensitive data, limit permissions\n        to read and scope to the tables that are needed.\n\n        Optionally, use the SQLInputWithTables input type to specify which tables\n        are allowed to be accessed.\n\n        Control access to who can submit requests to this chain.\n\n        See https://python.langchain.com/docs/security for more information.\n\n    Args:\n        llm: The language model to use\n        db: The SQLDatabase to generate the query for\n        prompt: The prompt to use. If none is provided, will choose one\n            based on dialect. Defaults to None.\n        k: The number of results per select statement to return. Defaults to 5.\n\n    Returns:\n        A chain that takes in a question and generates a SQL query that answers\n        that question.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a chain that generates SQL queries.\n\n    *Security Note*: This chain generates SQL queries for the given database.\n\n        The SQLDatabase class provides a get_table_info method that can be used\n        to get column information as well as sample data from the table.\n\n        To mitigate risk of leaking sensitive data, limit permissions\n        to read and scope to the tables that are needed.\n\n        Optionally, use the SQLInputWithTables input type to specify which tables\n        are allowed to be accessed.\n\n        Control access to who can submit requests to this chain.\n\n        See https://python.langchain.com/docs/security for more information.\n\n    Args:\n        llm: The language model to use.\n        db: The SQLDatabase to generate the query for.\n        prompt: The prompt to use. If none is provided, will choose one\n            based on dialect. Defaults to None. See Prompt section below for more.\n        k: The number of results per select statement to return. Defaults to 5.\n\n    Returns:\n        A chain that takes in a question and generates a SQL query that answers\n        that question.\n\n    Example:\n\n        .. code-block:: python\n\n            # pip install -U langchain langchain-community langchain-openai\n            from langchain_openai import ChatOpenAI\n            from langchain.chains import create_sql_query_chain\n            from langchain_community.utilities import SQLDatabase\n\n            db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n            llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n            chain = create_sql_query_chain(llm, db)\n            response = chain.invoke({\"question\": \"How many employees are there\"})\n\n    Prompt:\n        If no prompt is provided, a default prompt is selected based on the SQLDatabase dialect. If one is provided, it must support input variables:\n            * input: The user question plus suffix \"\\nSQLQuery: \" is passed here.\n            * top_k: The number of results per select statement (the `k` argument to\n                this function) is passed in here.\n            * table_info: Table definitions and sample rows are passed in here. If the\n                user specifies \"table_names_to_use\" when invoking chain, only those\n                will be included. Otherwise, all tables are included.\n            * dialect (optional): If dialect input variable is in prompt, the db\n                dialect will be passed in here.\n\n        Here's an example prompt:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import PromptTemplate\n\n            template = '''Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n            Use the following format:\n\n            Question: \"Question here\"\n            SQLQuery: \"SQL Query to run\"\n            SQLResult: \"Result of the SQLQuery\"\n            Answer: \"Final answer here\"\n\n            Only use the following tables:\n\n            {table_info}.\n\n            Question: {input}'''\n            prompt = PromptTemplate.from_template(template)", "file": "libs_langchain_langchain_chains_sql_database_query.py", "function": "create_sql_query_chain", "project": "langchain", "change_stats": {"code_changes": 29, "doc_changes": 54, "total_changes": 83}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef smart_split(text):\n    \n    \n    text = force_unicode(text)\n    for bit in smart_split_re.finditer(text):\n        bit = bit.group(0)\n        if bit[0] == '\"' and bit[-1] == '\"':\n            yield '\"' + bit[1:-1].replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\') + '\"'\n        elif bit[0] == \"'\" and bit[-1] == \"'\":\n            yield \"'\" + bit[1:-1].replace(\"\\\\'\", \"'\").replace(\"\\\\\\\\\", \"\\\\\") + \"'\"\n        else:\n            yield bit```\n\nUpdated code:\n```python\ndef smart_split(text):\n    \n    \n    text = force_unicode(text)\n    for bit in smart_split_re.finditer(text):\n        yield bit.group(0)\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n    Generator that splits a string by spaces, leaving quoted phrases together.\n    Supports both single and double quotes, and supports escaping quotes with\n    backslashes. In the output, strings will keep their initial and trailing\n    quote marks.\n\n    >>> list(smart_split(r'This is \"a person\\'s\" test.'))\n    [u'This', u'is', u'\"a person\\\\\\'s\"', u'test.']\n    >>> list(smart_split(r\"Another 'person\\'s' test.\")) \n    [u'Another', u\"'person's'\", u'test.']\n    >>> list(smart_split(r'A \"\\\"funky\\\" style\" test.')) \n    [u'A', u'\"\"funky\" style\"', u'test.']\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n    Generator that splits a string by spaces, leaving quoted phrases together.\n    Supports both single and double quotes, and supports escaping quotes with\n    backslashes. In the output, strings will keep their initial and trailing\n    quote marks and escaped quotes will remain escaped (the results can then\n    be further processed with unescape_string_literal()).\n\n    >>> list(smart_split(r'This is \"a person\\'s\" test.'))\n    [u'This', u'is', u'\"a person\\\\\\'s\"', u'test.']\n    >>> list(smart_split(r\"Another 'person\\'s' test.\"))\n    [u'Another', u\"'person\\\\'s'\", u'test.']\n    >>> list(smart_split(r'A \"\\\"funky\\\" style\" test.'))\n    [u'A', u'\"\\\\\"funky\\\\\" style\"', u'test.']", "file": "django_utils_text.py", "function": "smart_split", "project": "django", "change_stats": {"code_changes": 10, "doc_changes": 13, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef site_config_dir(\n    appname: str | None = None,\n    appauthor: str | None | Literal[False] = None,\n    version: str | None = None,\n    multipath: bool = False,\n) -> str:\n    \n    \n    \n    return PlatformDirs(appname=appname, appauthor=appauthor, version=version, multipath=multipath).site_config_dir```\n\nUpdated code:\n```python\ndef site_config_dir(\n    appname: str | None = None,\n    appauthor: str | None | Literal[False] = None,\n    version: str | None = None,\n    multipath: bool = False,\n    ensure_exists: bool = False,\n) -> str:\n    \n    \n    \n    return PlatformDirs(\n        appname=appname,\n        appauthor=appauthor,\n        version=version,\n        multipath=multipath,\n        ensure_exists=ensure_exists,\n    ).site_config_dir\n```\n\nOriginal documentation:\n```python\n:param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.\n    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.\n    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.\n    :param multipath: See `roaming <platformdirs.api.PlatformDirsABC.multipath>`.\n    :returns: config directory shared by the users\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.\n    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.\n    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.\n    :param multipath: See `roaming <platformdirs.api.PlatformDirsABC.multipath>`.\n    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.\n    :returns: config directory shared by the users", "file": "pipenv_patched_pip__vendor_platformdirs___init__.py", "function": "site_config_dir", "project": "pipenv", "change_stats": {"code_changes": 11, "doc_changes": 3, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _create_tracker_loader(\n    endpoint_config: Text, strategy: Text, count: Optional[int], seed: Optional[int]\n) -> MarkerTrackerLoader:\n    \n    \n    endpoints = AvailableEndpoints.read_endpoints(endpoint_config)\n    tracker_store = TrackerStore.create(endpoints.tracker_store)\n    return MarkerTrackerLoader(tracker_store, strategy, count, seed,)```\n\nUpdated code:\n```python\ndef _create_tracker_loader(\n    endpoint_config: Text,\n    strategy: Text,\n    domain: Domain,\n    count: Optional[int],\n    seed: Optional[int],\n) -> MarkerTrackerLoader:\n    \n    \n    endpoints = AvailableEndpoints.read_endpoints(endpoint_config)\n    tracker_store = TrackerStore.create(endpoints.tracker_store, domain=domain)\n    return MarkerTrackerLoader(tracker_store, strategy, count, seed,)\n```\n\nOriginal documentation:\n```python\nCreate a tracker loader against the configured tracker store.\n\n    Args:\n        endpoint_config: Path to the endpoint configuration defining the tracker\n                         store to use.\n        strategy: Strategy to use when selecting trackers to extract from.\n        count: (Optional) Number of trackers to extract from (for any strategy\n               except 'all').\n        seed: (Optional) The seed to initialise the random number generator for\n              use with the 'sample_n' strategy.\n\n    Returns:\n        A MarkerTrackerLoader object configured with the specified strategy against\n        the configured tracker store.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a tracker loader against the configured tracker store.\n\n    Args:\n        endpoint_config: Path to the endpoint configuration defining the tracker\n                         store to use.\n        strategy: Strategy to use when selecting trackers to extract from.\n        domain: The domain to use when connecting to the tracker store.\n        count: (Optional) Number of trackers to extract from (for any strategy\n               except 'all').\n        seed: (Optional) The seed to initialise the random number generator for\n              use with the 'sample_n' strategy.\n\n    Returns:\n        A MarkerTrackerLoader object configured with the specified strategy against\n        the configured tracker store.", "file": "rasa_cli_evaluate.py", "function": "_create_tracker_loader", "project": "rasa", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef can_omit_invisible_parens(\n    line: Line,\n    line_length: int,\n) -> bool:\n    \n    \n    bt = line.bracket_tracker\n    if not bt.delimiters:\n        # Without delimiters the optional parentheses are useless.\n        return True\n\n    max_priority = bt.max_delimiter_priority()\n    if bt.delimiter_count_with_priority(max_priority) > 1:\n        # With more than one delimiter of a kind the optional parentheses read better.\n        return False\n\n    if max_priority == DOT_PRIORITY:\n        # A single stranded method call doesn't require optional parentheses.\n        return True\n\n    assert len(line.leaves) >= 2, \"Stranded delimiter\"\n\n    # With a single delimiter, omit if the expression starts or ends with\n    # a bracket.\n    first = line.leaves[0]\n    second = line.leaves[1]\n    if first.type in OPENING_BRACKETS and second.type not in CLOSING_BRACKETS:\n        if _can_omit_opening_paren(line, first=first, line_length=line_length):\n            return True\n\n        # Note: we are not returning False here because a line might have *both*\n        # a leading opening bracket and a trailing closing bracket.  If the\n        # opening bracket doesn't match our rule, maybe the closing will.\n\n    penultimate = line.leaves[-2]\n    last = line.leaves[-1]\n\n    if (\n        last.type == token.RPAR\n        or last.type == token.RBRACE\n        or (\n            # don't use indexing for omitting optional parentheses;\n            # it looks weird\n            last.type == token.RSQB\n            and last.parent\n            and last.parent.type != syms.trailer\n        )\n    ):\n        if penultimate.type in OPENING_BRACKETS:\n            # Empty brackets don't help.\n            return False\n\n        if is_multiline_string(first):\n            # Additional wrapping of a multiline string in this situation is\n            # unnecessary.\n            return True\n\n        if _can_omit_closing_paren(line, last=last, line_length=line_length):\n            return True\n\n    return False```\n\nUpdated code:\n```python\ndef can_omit_invisible_parens(\n    rhs: RHSResult,\n    line_length: int,\n) -> bool:\n    \n    \n    line = rhs.body\n    bt = line.bracket_tracker\n    if not bt.delimiters:\n        # Without delimiters the optional parentheses are useless.\n        return True\n\n    max_priority = bt.max_delimiter_priority()\n    delimiter_count = bt.delimiter_count_with_priority(max_priority)\n    if delimiter_count > 1:\n        # With more than one delimiter of a kind the optional parentheses read better.\n        return False\n\n    if delimiter_count == 1:\n        if (\n            Preview.wrap_multiple_context_managers_in_parens in line.mode\n            and max_priority == COMMA_PRIORITY\n            and rhs.head.is_with_stmt\n        ):\n            # For two context manager with statements, the optional parentheses read\n            # better. In this case, `rhs.body` is the context managers part of\n            # the with statement. `rhs.head` is the `with (` part on the previous\n            # line.\n            return False\n        # Otherwise it may also read better, but we don't do it today and requires\n        # careful considerations for all possible cases. See\n        # https://github.com/psf/black/issues/2156.\n\n    if max_priority == DOT_PRIORITY:\n        # A single stranded method call doesn't require optional parentheses.\n        return True\n\n    assert len(line.leaves) >= 2, \"Stranded delimiter\"\n\n    # With a single delimiter, omit if the expression starts or ends with\n    # a bracket.\n    first = line.leaves[0]\n    second = line.leaves[1]\n    if first.type in OPENING_BRACKETS and second.type not in CLOSING_BRACKETS:\n        if _can_omit_opening_paren(line, first=first, line_length=line_length):\n            return True\n\n        # Note: we are not returning False here because a line might have *both*\n        # a leading opening bracket and a trailing closing bracket.  If the\n        # opening bracket doesn't match our rule, maybe the closing will.\n\n    penultimate = line.leaves[-2]\n    last = line.leaves[-1]\n\n    if (\n        last.type == token.RPAR\n        or last.type == token.RBRACE\n        or (\n            # don't use indexing for omitting optional parentheses;\n            # it looks weird\n            last.type == token.RSQB\n            and last.parent\n            and last.parent.type != syms.trailer\n        )\n    ):\n        if penultimate.type in OPENING_BRACKETS:\n            # Empty brackets don't help.\n            return False\n\n        if is_multiline_string(first):\n            # Additional wrapping of a multiline string in this situation is\n            # unnecessary.\n            return True\n\n        if _can_omit_closing_paren(line, last=last, line_length=line_length):\n            return True\n\n    return False\n```\n\nOriginal documentation:\n```python\nDoes `line` have a shape safe to reformat without optional parens around it?\n\n    Returns True for only a subset of potentially nice looking formattings but\n    the point is to not return false positives that end up producing lines that\n    are too long.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Does `rhs.body` have a shape safe to reformat without optional parens around it?\n\n    Returns True for only a subset of potentially nice looking formattings but\n    the point is to not return false positives that end up producing lines that\n    are too long.", "file": "src_black_lines.py", "function": "can_omit_invisible_parens", "project": "black", "change_stats": {"code_changes": 23, "doc_changes": 4, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _from_folder(self):\n         \n        \n        with_hash = self._args[0]\n        logger.debug(\"Loading images from folder: '%s'. with_hash: %s\", self.location, with_hash)\n        for idx, filename in enumerate(self.file_list):\n            if idx in self._skip_list:\n                logger.trace(\"Skipping frame %s due to skip list\")\n                continue\n            image_read = read_image(filename, raise_error=False, with_hash=with_hash)\n            if with_hash:\n                retval = filename, *image_read\n            else:\n                retval = filename, image_read\n            if retval[1] is None:\n                logger.debug(\"Image not loaded: '%s'\", filename)\n                continue\n            yield retval```\n\nUpdated code:\n```python\ndef _from_folder(self):\n         \n        \n        logger.debug(\"Loading frames from folder: '%s'\", self.location)\n        for idx, filename in enumerate(self.file_list):\n            if idx in self._skip_list:\n                logger.trace(\"Skipping frame %s due to skip list\")\n                continue\n            image_read = read_image(filename, raise_error=False, with_hash=False)\n            retval = filename, image_read\n            if retval[1] is None:\n                logger.warning(\"Frame not loaded: '%s'\", filename)\n                continue\n            yield retval\n```\n\nOriginal documentation:\n```python\nGenerator for loading images from a folder\n\n        Yields\n        ------\n        filename: str\n            The filename of the loaded image.\n        image: numpy.ndarray\n            The loaded image.\n        sha1_hash: str, optional\n            The sha1 hash of the loaded image. Only yielded if :class:`ImageIO` was\n            initialized with :attr:`load_with_hash` set to ``True`` and the :attr:`location`\n            is a folder of images.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generator for loading images from a folder\n\n        Yields\n        ------\n        filename: str\n            The filename of the loaded image.\n        image: numpy.ndarray\n            The loaded image.", "file": "lib_image.py", "function": "ImagesLoader._from_folder", "project": "faceswap", "change_stats": {"code_changes": 14, "doc_changes": 6, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rectangle(start, end=None, extent=None, shape=None):\n    \n\n    \n    if extent is not None:\n        end = np.array(start) + np.array(extent)\n    elif end is None:\n        raise ValueError(\"Either `end` or `extent` must be given\")\n    tl = np.minimum(start, end)\n    br = np.maximum(start, end)\n    if extent is None:\n        br += 1\n    if shape is not None:\n        br = np.minimum(shape, br)\n        tl = np.maximum(np.zeros_like(shape), tl)\n    coords = np.meshgrid(*[np.arange(st, en) for st, en in zip(tuple(tl),\n                                                               tuple(br))])\n    return coords```\n\nUpdated code:\n```python\ndef rectangle(start, end=None, extent=None, shape=None):\n    \n\n    \n    tl, br = _rectangle_slice(start=start, end=end, extent=extent)\n\n    if shape is not None:\n        br = np.minimum(shape, br)\n        tl = np.maximum(np.zeros_like(shape), tl)\n    coords = np.meshgrid(*[np.arange(st, en) for st, en in zip(tuple(tl),\n                                                               tuple(br))])\n    return coords\n```\n\nOriginal documentation:\n```python\nGenerate coordinates of pixels within a rectangle.\n\n    Parameters\n    ----------\n    start : tuple\n        Origin point of the rectangle, e.g., ``([plane,] row, column)``.\n    end : tuple\n        End point of the rectangle ``([plane,] row, column)``.\n        Either `end` or `extent` must be specified.\n    extent : tuple\n        The extent (size) of the drawn rectangle.  E.g.,\n        ``([num_planes,] num_rows, num_cols)``.\n        Either `end` or `extent` must be specified.\n    shape : tuple, optional\n        Image shape used to determine the maximum bounds of the output\n        coordinates. This is useful for clipping rectangles that exceed\n        the image size. By default, no clipping is done.\n\n    Returns\n    -------\n    coords : array of int, shape (Ndim, Npoints)\n        The coordinates of all pixels in the rectangle.\n\n    Notes\n    -----\n    This function can be applied to N-dimensional images, by passing `start` and\n    `end` or `extent` as tuples of length N.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.draw import rectangle\n    >>> img = np.zeros((5, 5), dtype=np.uint8)\n    >>> start = (1, 1)\n    >>> extent = (3, 3)\n    >>> rr, cc = rectangle(start, extent=extent, shape=img.shape)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 0, 0, 0, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 0, 0, 0, 0]], dtype=uint8)\n\n\n    >>> img = np.zeros((5, 5), dtype=np.uint8)\n    >>> start = (0, 1)\n    >>> end = (3, 3)\n    >>> rr, cc = rectangle(start, end=end, shape=img.shape)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 0, 0, 0, 0]], dtype=uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate coordinates of pixels within a rectangle.\n\n    Parameters\n    ----------\n    start : tuple\n        Origin point of the rectangle, e.g., ``([plane,] row, column)``.\n    end : tuple\n        End point of the rectangle ``([plane,] row, column)``.\n        For a 2D matrix, the slice defined by the rectangle is\n        ``[start:(end+1)]``.\n        Either `end` or `extent` must be specified.\n    extent : tuple\n        The extent (size) of the drawn rectangle.  E.g.,\n        ``([num_planes,] num_rows, num_cols)``.\n        Either `end` or `extent` must be specified.\n        A negative extent is valid, and will result in a rectangle\n        going along the oposite direction. If extent is negative, the\n        `start` point is not included.\n    shape : tuple, optional\n        Image shape used to determine the maximum bounds of the output\n        coordinates. This is useful for clipping rectangles that exceed\n        the image size. By default, no clipping is done.\n\n    Returns\n    -------\n    coords : array of int, shape (Ndim, Npoints)\n        The coordinates of all pixels in the rectangle.\n\n    Notes\n    -----\n    This function can be applied to N-dimensional images, by passing `start` and\n    `end` or `extent` as tuples of length N.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.draw import rectangle\n    >>> img = np.zeros((5, 5), dtype=np.uint8)\n    >>> start = (1, 1)\n    >>> extent = (3, 3)\n    >>> rr, cc = rectangle(start, extent=extent, shape=img.shape)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 0, 0, 0, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 0, 0, 0, 0]], dtype=uint8)\n\n\n    >>> img = np.zeros((5, 5), dtype=np.uint8)\n    >>> start = (0, 1)\n    >>> end = (3, 3)\n    >>> rr, cc = rectangle(start, end=end, shape=img.shape)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 1, 1, 1, 0],\n           [0, 0, 0, 0, 0]], dtype=uint8)\n\n    >>> import numpy as np\n    >>> from skimage.draw import rectangle\n    >>> img = np.zeros((6, 6), dtype=np.uint8)\n    >>> start = (3, 3)\n    >>>\n    >>> rr, cc = rectangle(start, extent=(2, 2))\n    >>> img[rr, cc] = 1\n    >>> rr, cc = rectangle(start, extent=(-2, 2))\n    >>> img[rr, cc] = 2\n    >>> rr, cc = rectangle(start, extent=(-2, -2))\n    >>> img[rr, cc] = 3\n    >>> rr, cc = rectangle(start, extent=(2, -2))\n    >>> img[rr, cc] = 4\n    >>> print(img)\n    [[0 0 0 0 0 0]\n     [0 3 3 2 2 0]\n     [0 3 3 2 2 0]\n     [0 4 4 1 1 0]\n     [0 4 4 1 1 0]\n     [0 0 0 0 0 0]]", "file": "skimage_draw_draw.py", "function": "rectangle", "project": "scikit-image", "change_stats": {"code_changes": 12, "doc_changes": 28, "total_changes": 40}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_client_cert(master_ip, master_port, fname, token, username, group=None):\n    \n    \n    \n    info = \"/CN={}\".format(username)\n    if group:\n        info = \"{}/O={}\".format(info, group)\n\n    # the filenames must survive snap refreshes, so replace revision number with current\n    snapdata_current = os.path.abspath(os.path.join(snapdata_path, \"..\", \"current\"))\n\n    cer_req_file = \"{}/certs/{}.csr\".format(snapdata_current, fname)\n    cer_key_file = \"{}/certs/{}.key\".format(snapdata_current, fname)\n    cer_file = \"{}/certs/{}.crt\".format(snapdata_current, fname)\n    if not os.path.exists(cer_key_file):\n        cmd_gen_cert_key = \"{snap}/usr/bin/openssl genrsa -out {key} 2048\".format(\n            snap=snap_path, key=cer_key_file\n        )\n        subprocess.check_call(\n            cmd_gen_cert_key.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n        )\n        try_set_file_permissions(cer_key_file)\n\n    cmd_cert = \"{snap}/usr/bin/openssl req -new -sha256 -key {key} -out {csr} -subj {info}\".format(\n        snap=snap_path,\n        key=cer_key_file,\n        csr=cer_req_file,\n        info=info,\n    )\n    subprocess.check_call(cmd_cert.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    with open(cer_req_file) as fp:\n        csr = fp.read()\n        req_data = {\"token\": token, \"request\": csr}\n        # TODO: enable ssl verification\n        signed = requests.post(\n            \"https://{}:{}/{}/sign-cert\".format(master_ip, master_port, CLUSTER_API),\n            json=req_data,\n            verify=False,\n        )\n        if signed.status_code != 200:\n            error = \"Failed to sign {} certificate ({}).\".format(fname, signed.status_code)\n            try:\n                if \"error\" in signed.json():\n                    error = \"{} {}\".format(error, format(signed.json()[\"error\"]))\n            except ValueError:\n                print(\"Make sure the cluster you connect to supports joining worker nodes.\")\n            print(error)\n            exit(1)\n        info = signed.json()\n        with open(cer_file, \"w\") as cert_fp:\n            cert_fp.write(info[\"certificate\"])\n        try_set_file_permissions(cer_file)\n\n        return {\n            \"certificate_location\": cer_file,\n            \"certificate_key_location\": cer_key_file,\n        }```\n\nUpdated code:\n```python\ndef get_client_cert(master_ip, master_port, fname: str, token: str, subject: str, with_sans: bool):\n    \n    \n    \n\n    cert_crt = (snap_data() / \"certs\" / fname).with_suffix(\".crt\")\n    cert_key = (snap_data() / \"certs\" / fname).with_suffix(\".key\")\n    # generate csr\n    script = \"generate_csr_with_sans\" if with_sans else \"generate_csr\"\n    p = subprocess.run(\n        [f\"{snap()}/actions/common/utils.sh\", script, subject, cert_key],\n        check=True,\n        capture_output=True,\n    )\n    csr = p.stdout.decode()\n\n    req_data = {\"token\": token, \"request\": csr}\n    # TODO: enable ssl verification\n    signed = requests.post(\n        \"https://{}:{}/{}/sign-cert\".format(master_ip, master_port, CLUSTER_API),\n        json=req_data,\n        verify=False,\n    )\n    if signed.status_code != 200:\n        error = \"Failed to sign {} certificate ({}).\".format(fname, signed.status_code)\n        try:\n            if \"error\" in signed.json():\n                error = \"{} {}\".format(error, format(signed.json()[\"error\"]))\n        except ValueError:\n            print(\"Make sure the cluster you connect to supports joining worker nodes.\")\n        print(error)\n        exit(1)\n    info = signed.json()\n    cert_crt.write_text(info[\"certificate\"])\n    try_set_file_permissions(cert_crt)\n```\n\nOriginal documentation:\n```python\nGet a signed cert.\n    See https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs\n\n    :param master_ip: master ip\n    :param master_port: master port\n    :param fname: file name prefix for the certificate\n    :param token: token to contact the master with\n    :param username: the username of the cert's owner\n    :param group: the group the owner belongs to\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get a signed cert signed by a remote cluster-agent.\n    See https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs\n\n    :param master_ip: master ip\n    :param master_port: master port\n    :param fname: file name prefix for the certificate\n    :param token: token to contact the master with\n    :param subject: the subject of the certificate\n    :param with_sans: whether to include hostname and node IPs as subject alternate names", "file": "scripts_wrappers_join.py", "function": "get_client_cert", "project": "microk8s", "change_stats": {"code_changes": 82, "doc_changes": 8, "total_changes": 90}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef put_scalar(self, name, value, smoothing_hint=True):\n        \n        \n        \n        name = self._current_prefix + name\n        history = self._history[name]\n        value = float(value)\n        history.update(value, self._iter)\n        self._latest_scalars[name] = (value, self._iter)\n\n        existing_hint = self._smoothing_hints.get(name)\n        if existing_hint is not None:\n            assert (\n                existing_hint == smoothing_hint\n            ), \"Scalar {} was put with a different smoothing_hint!\".format(name)\n        else:\n            self._smoothing_hints[name] = smoothing_hint```\n\nUpdated code:\n```python\ndef put_scalar(self, name, value, smoothing_hint=True, cur_iter=None):\n        \n        \n        \n        name = self._current_prefix + name\n        cur_iter = self._iter if cur_iter is None else cur_iter\n        history = self._history[name]\n        value = float(value)\n        history.update(value, cur_iter)\n        self._latest_scalars[name] = (value, cur_iter)\n\n        existing_hint = self._smoothing_hints.get(name)\n\n        if existing_hint is not None:\n            assert (\n                existing_hint == smoothing_hint\n            ), \"Scalar {} was put with a different smoothing_hint!\".format(name)\n        else:\n            self._smoothing_hints[name] = smoothing_hint\n```\n\nOriginal documentation:\n```python\nAdd a scalar `value` to the `HistoryBuffer` associated with `name`.\n\n        Args:\n            smoothing_hint (bool): a 'hint' on whether this scalar is noisy and should be\n                smoothed when logged. The hint will be accessible through\n                :meth:`EventStorage.smoothing_hints`.  A writer may ignore the hint\n                and apply custom smoothing rule.\n\n                It defaults to True because most scalars we save need to be smoothed to\n                provide any useful signal.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add a scalar `value` to the `HistoryBuffer` associated with `name`.\n\n        Args:\n            smoothing_hint (bool): a 'hint' on whether this scalar is noisy and should be\n                smoothed when logged. The hint will be accessible through\n                :meth:`EventStorage.smoothing_hints`.  A writer may ignore the hint\n                and apply custom smoothing rule.\n\n                It defaults to True because most scalars we save need to be smoothed to\n                provide any useful signal.\n            cur_iter (int): an iteration number to set explicitly instead of current iteration", "file": "detectron2_utils_events.py", "function": "EventStorage.put_scalar", "project": "detectron2", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef gen_audio(is_clean, params, index, audio_samples_length=-1):\n    ''''''\n\n    clipped_files = []\n    low_activity_files = []\n    if audio_samples_length == -1:\n        audio_samples_length = int(params['audio_length']*params['fs'])\n    if is_clean:\n        activity_threshold = params['clean_activity_threshold']\n    else:\n        activity_threshold = params['noise_activity_threshold']\n\n    while True:\n        audio, source_files, new_clipped_files, index = \\\n            build_audio(is_clean, params, index, audio_samples_length)\n\n        clipped_files += new_clipped_files\n        if len(audio) < audio_samples_length:\n            continue\n\n        if activity_threshold == 0.0:\n            break\n\n        percactive = activitydetector(audio=audio)\n        if percactive > activity_threshold:\n            break\n        else:\n            low_activity_files += source_files\n\n    return audio, source_files, clipped_files, low_activity_files, index```\n\nUpdated code:\n```python\ndef gen_audio(is_clean, params, index, audio_samples_length=-1):\n    \n\n    clipped_files = []\n    low_activity_files = []\n    if audio_samples_length == -1:\n        audio_samples_length = int(params[\"audio_length\"] * params[\"fs\"])\n    if is_clean:\n        activity_threshold = params[\"clean_activity_threshold\"]\n    else:\n        activity_threshold = params[\"noise_activity_threshold\"]\n\n    while True:\n        audio, source_files, new_clipped_files, index = build_audio(\n            is_clean, params, index, audio_samples_length\n        )\n\n        clipped_files += new_clipped_files\n        if len(audio) < audio_samples_length:\n            continue\n\n        if activity_threshold == 0.0:\n            break\n\n        percactive = activitydetector(audio=audio)\n        if percactive > activity_threshold:\n            break\n        else:\n            low_activity_files += source_files\n\n    return audio, source_files, clipped_files, low_activity_files, index\n```\n\nOriginal documentation:\n```python\nCalls build_audio() to get an audio signal, and verify that it meets the\n       activity threshold\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calls build_audio() to get an audio signal, and verify that it meets the\n    activity threshold", "file": "egs2_dns_icassp22_enh1_local_noisyspeech_synthesizer.py", "function": "gen_audio", "project": "espnet", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_trial_checkpoints_paths(self, trial, metric=TRAINING_ITERATION):\n        \n        \n\n        if isinstance(trial, str):\n            trial_dir = os.path.expanduser(trial)\n\n            # get checkpoints from logdir\n            chkpt_df = TrainableUtil.get_checkpoints_paths(trial_dir)\n\n            # join with trial dataframe to get metrics\n            trial_df = self.trial_dataframes[trial_dir]\n            path_metric_df = chkpt_df.merge(\n                trial_df, on=\"training_iteration\", how=\"inner\")\n            return path_metric_df[[\"chkpt_path\", metric]].values.tolist()\n        elif isinstance(trial, Trial):\n            checkpoints = trial.checkpoint_manager.best_checkpoints()\n            # TODO(ujvl): Remove condition once the checkpoint manager is\n            #  modified to only track PERSISTENT checkpoints.\n            return [[c.value, c.result[metric]] for c in checkpoints\n                    if c.storage == Checkpoint.PERSISTENT]\n        else:\n            raise ValueError(\"trial should be a string or a Trial instance.\")```\n\nUpdated code:\n```python\ndef get_trial_checkpoints_paths(self, trial, metric=TRAINING_ITERATION):\n        \n        \n        if isinstance(trial, str):\n            trial_dir = os.path.expanduser(trial)\n            # Get checkpoints from logdir.\n            chkpt_df = TrainableUtil.get_checkpoints_paths(trial_dir)\n\n            # Join with trial dataframe to get metrics.\n            trial_df = self.trial_dataframes[trial_dir]\n            path_metric_df = chkpt_df.merge(\n                trial_df, on=\"training_iteration\", how=\"inner\")\n            return path_metric_df[[\"chkpt_path\", metric]].values.tolist()\n        elif isinstance(trial, Trial):\n            checkpoints = trial.checkpoint_manager.best_checkpoints()\n            return [[c.value, c.result[metric]] for c in checkpoints]\n        else:\n            raise ValueError(\"trial should be a string or a Trial instance.\")\n```\n\nOriginal documentation:\n```python\nReturns a list of [path, metric] lists for all disk checkpoints of\n         a trial.\n\n        Arguments:\n            trial(Trial): The log directory of a trial, or a trial instance.\n            metric (str): key for trial info to return, e.g. \"mean_accuracy\".\n                \"training_iteration\" is used by default.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets paths and metrics of all persistent checkpoints of a trial.\n\n        Args:\n            trial (Trial): The log directory of a trial, or a trial instance.\n            metric (str): key for trial info to return, e.g. \"mean_accuracy\".\n                \"training_iteration\" is used by default.\n\n        Returns:\n            A list of [path, metric] lists for all persistent checkpoints of\n                the trial.", "file": "python_ray_tune_analysis_experiment_analysis.py", "function": "Analysis.get_trial_checkpoints_paths", "project": "ray", "change_stats": {"code_changes": 13, "doc_changes": 13, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef thin(image, max_iter=None):\n    \n    \n    \n    # check that image is 2d\n    check_nD(image, 2)\n\n    # convert image to uint8 with values in {0, 1}\n    skel = np.asanyarray(image, dtype=bool).astype(np.uint8)\n\n    # neighborhood mask\n    mask = np.array([[ 8,  4,   2],\n                     [16,  0,   1],\n                     [32, 64, 128]], dtype=np.uint8)\n\n    # iterate until convergence, up to the iteration limit\n    max_iter = max_iter or np.inf\n    n_iter = 0\n    n_pts_old, n_pts_new = np.inf, np.sum(skel)\n    while n_pts_old != n_pts_new and n_iter < max_iter:\n        n_pts_old = n_pts_new\n\n        # perform the two \"subiterations\" described in the paper\n        for lut in [G123_LUT, G123P_LUT]:\n            # correlate image with neighborhood mask\n            N = ndi.correlate(skel, mask, mode='constant')\n            # take deletion decision from this subiteration's LUT\n            D = np.take(lut, N)\n            # perform deletion\n            skel[D] = 0\n\n        n_pts_new = np.sum(skel)  # count points after thinning\n        n_iter += 1\n\n    return skel.astype(bool)```\n\nUpdated code:\n```python\ndef thin(image, max_num_iter=None):\n    \n    \n    \n    # check that image is 2d\n    check_nD(image, 2)\n\n    # convert image to uint8 with values in {0, 1}\n    skel = np.asanyarray(image, dtype=bool).astype(np.uint8)\n\n    # neighborhood mask\n    mask = np.array([[ 8,  4,   2],\n                     [16,  0,   1],\n                     [32, 64, 128]], dtype=np.uint8)\n\n    # iterate until convergence, up to the iteration limit\n    max_num_iter = max_num_iter or np.inf\n    num_iter = 0\n    n_pts_old, n_pts_new = np.inf, np.sum(skel)\n    while n_pts_old != n_pts_new and num_iter < max_num_iter:\n        n_pts_old = n_pts_new\n\n        # perform the two \"subiterations\" described in the paper\n        for lut in [G123_LUT, G123P_LUT]:\n            # correlate image with neighborhood mask\n            N = ndi.correlate(skel, mask, mode='constant')\n            # take deletion decision from this subiteration's LUT\n            D = np.take(lut, N)\n            # perform deletion\n            skel[D] = 0\n\n        n_pts_new = np.sum(skel)  # count points after thinning\n        num_iter += 1\n\n    return skel.astype(bool)\n```\n\nOriginal documentation:\n```python\nPerform morphological thinning of a binary image.\n\n    Parameters\n    ----------\n    image : binary (M, N) ndarray\n        The image to be thinned.\n    max_iter : int, number of iterations, optional\n        Regardless of the value of this parameter, the thinned image\n        is returned immediately if an iteration produces no change.\n        If this parameter is specified it thus sets an upper bound on\n        the number of iterations performed.\n\n    Returns\n    -------\n    out : ndarray of bool\n        Thinned image.\n\n    See Also\n    --------\n    skeletonize, medial_axis\n\n    Notes\n    -----\n    This algorithm [1]_ works by making multiple passes over the image,\n    removing pixels matching a set of criteria designed to thin\n    connected regions while preserving eight-connected components and\n    2 x 2 squares [2]_. In each of the two sub-iterations the algorithm\n    correlates the intermediate skeleton image with a neighborhood mask,\n    then looks up each neighborhood in a lookup table indicating whether\n    the central pixel should be deleted in that sub-iteration.\n\n    References\n    ----------\n    .. [1] Z. Guo and R. W. Hall, \"Parallel thinning with\n           two-subiteration algorithms,\" Comm. ACM, vol. 32, no. 3,\n           pp. 359-373, 1989. :DOI:`10.1145/62065.62074`\n    .. [2] Lam, L., Seong-Whan Lee, and Ching Y. Suen, \"Thinning\n           Methodologies-A Comprehensive Survey,\" IEEE Transactions on\n           Pattern Analysis and Machine Intelligence, Vol 14, No. 9,\n           p. 879, 1992. :DOI:`10.1109/34.161346`\n\n    Examples\n    --------\n    >>> square = np.zeros((7, 7), dtype=np.uint8)\n    >>> square[1:-1, 2:-2] = 1\n    >>> square[0, 1] =  1\n    >>> square\n    array([[0, 1, 0, 0, 0, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n    >>> skel = thin(square)\n    >>> skel.astype(np.uint8)\n    array([[0, 1, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform morphological thinning of a binary image.\n\n    Parameters\n    ----------\n    image : binary (M, N) ndarray\n        The image to be thinned.\n    max_num_iter : int, number of iterations, optional\n        Regardless of the value of this parameter, the thinned image\n        is returned immediately if an iteration produces no change.\n        If this parameter is specified it thus sets an upper bound on\n        the number of iterations performed.\n\n    Returns\n    -------\n    out : ndarray of bool\n        Thinned image.\n\n    See Also\n    --------\n    skeletonize, medial_axis\n\n    Notes\n    -----\n    This algorithm [1]_ works by making multiple passes over the image,\n    removing pixels matching a set of criteria designed to thin\n    connected regions while preserving eight-connected components and\n    2 x 2 squares [2]_. In each of the two sub-iterations the algorithm\n    correlates the intermediate skeleton image with a neighborhood mask,\n    then looks up each neighborhood in a lookup table indicating whether\n    the central pixel should be deleted in that sub-iteration.\n\n    References\n    ----------\n    .. [1] Z. Guo and R. W. Hall, \"Parallel thinning with\n           two-subiteration algorithms,\" Comm. ACM, vol. 32, no. 3,\n           pp. 359-373, 1989. :DOI:`10.1145/62065.62074`\n    .. [2] Lam, L., Seong-Whan Lee, and Ching Y. Suen, \"Thinning\n           Methodologies-A Comprehensive Survey,\" IEEE Transactions on\n           Pattern Analysis and Machine Intelligence, Vol 14, No. 9,\n           p. 879, 1992. :DOI:`10.1109/34.161346`\n\n    Examples\n    --------\n    >>> square = np.zeros((7, 7), dtype=np.uint8)\n    >>> square[1:-1, 2:-2] = 1\n    >>> square[0, 1] =  1\n    >>> square\n    array([[0, 1, 0, 0, 0, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n    >>> skel = thin(square)\n    >>> skel.astype(np.uint8)\n    array([[0, 1, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)", "file": "skimage_morphology__skeletonize.py", "function": "thin", "project": "scikit-image", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef connect_wifi(addr:str) -> \"UIAutomatorServer\":\n    \n    \n    \n    if not re.match(r\"^https?://\", addr):\n        addr = \"http://\" + addr\n    # fixed_addr = fix_wifi_addr(addr)\n    # if fixed_addr is None:\n        # raise ConnectError(\"addr is invalid or atx-agent is not running\", addr)\n    u = urlparse.urlparse(addr)\n    host = u.hostname\n    port = u.port or 7912\n    return UIAutomatorServer(host, port)```\n\nUpdated code:\n```python\ndef connect_wifi(addr:str) -> \"Device\":\n    \n    \n    \n    if not re.match(r\"^https?://\", addr):\n        addr = \"http://\" + addr\n    # fixed_addr = fix_wifi_addr(addr)\n    # if fixed_addr is None:\n        # raise ConnectError(\"addr is invalid or atx-agent is not running\", addr)\n    u = urlparse.urlparse(addr)\n    host = u.hostname\n    port = u.port or 7912\n    return Device(host, port)\n```\n\nOriginal documentation:\n```python\nArgs:\n        addr (str) uiautomator server address.\n\n    Returns:\n        UIAutomatorServer\n\n    Raises:\n        ConnectError\n\n    Examples:\n        connect_wifi(\"10.0.0.1\")\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n        addr (str) uiautomator server address.\n\n    Returns:\n        Device\n\n    Raises:\n        ConnectError\n\n    Examples:\n        connect_wifi(\"10.0.0.1\")", "file": "uiautomator2___init__.py", "function": "connect_wifi", "project": "uiautomator2", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_config(\n    path: Union[Path, str],\n    *,\n    create_objects: bool = False,\n    schema: Type[BaseModel] = EmptySchema,\n    overrides: Dict[str, Any] = {},\n    validate: bool = True,\n) -> Dict[str, Any]:\n    \n    \n    config = thinc.config.Config().from_disk(path)\n    kwargs = {\"validate\": validate, \"schema\": schema, \"overrides\": overrides}\n    if create_objects:\n        return registry.make_from_config(config, **kwargs)\n    else:\n        # Just fill config here so we can validate and fail early\n        if validate and schema:\n            registry.fill_config(config, **kwargs)\n        return config```\n\nUpdated code:\n```python\ndef load_config(path, create_objects=False):\n    \n    \n    config = thinc.config.Config().from_disk(path)\n    if create_objects:\n        return registry.make_from_config(config, validate=True)\n    else:\n        return config\n```\n\nOriginal documentation:\n```python\nLoad a Thinc-formatted config file, optionally filling in objects where\n    the config references registry entries. See \"Thinc config files\" for details.\n\n    path (str / Path): Path to the config file\n    create_objects (bool): Whether to automatically create objects when the config\n        references registry entries. Defaults to False.\n    schema (BaseModel): Optional pydantic base schema to use for validation.\n    overrides (Dict[str, Any]): Optional overrides to substitute in config.\n    validate (bool): Whether to validate against schema.\n    RETURNS (dict): The objects from the config file.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a Thinc-formatted config file, optionally filling in objects where\n    the config references registry entries. See \"Thinc config files\" for details.\n\n    path (str / Path): Path to the config file\n    create_objects (bool): Whether to automatically create objects when the config\n        references registry entries. Defaults to False.\n\n    RETURNS (dict): The objects from the config file.", "file": "spacy_util.py", "function": "load_config", "project": "spaCy", "change_stats": {"code_changes": 17, "doc_changes": 6, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef inverse_transform(self, X, y='deprecated'):\n        \n        \n        if not isinstance(y, string_types) or y != 'deprecated':\n            warnings.warn(\"The parameter y on inverse_transform() is \"\n                          \"deprecated since 0.19 and will be removed in 0.21\",\n                          DeprecationWarning)\n        return self._transform(X, y=y, func=self.inverse_func,\n                               kw_args=self.inv_kw_args)```\n\nUpdated code:\n```python\ndef inverse_transform(self, X):\n        \n        \n        return self._transform(X, func=self.inverse_func,\n                               kw_args=self.inv_kw_args)\n```\n\nOriginal documentation:\n```python\nTransform X using the inverse function.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Input array.\n\n        y : (ignored)\n            .. deprecated::0.19\n\n        Returns\n        -------\n        X_out : array-like, shape (n_samples, n_features)\n            Transformed input.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Transform X using the inverse function.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Input array.\n\n        Returns\n        -------\n        X_out : array-like, shape (n_samples, n_features)\n            Transformed input.", "file": "sklearn_preprocessing__function_transformer.py", "function": "FunctionTransformer.inverse_transform", "project": "scikit-learn", "change_stats": {"code_changes": 10, "doc_changes": 5, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_llama_index_api_key(self, args, strict=True):\n        \n        \n        \n        key = \"OPENAI_API_KEY\"\n        for k in key, key.lower():\n            # 1\n            if args.get(k):\n                return args[k]\n            # 2\n            connection_args = self.engine_storage.get_connection_args()\n            if k in connection_args:\n                return connection_args[k]\n            # 3\n            api_key = os.getenv(k)\n            if api_key is not None:\n                return api_key\n            # 4\n            config = Config()\n            openai_cfg = config.get(\"llama_index\", {})\n            if k in openai_cfg:\n                return openai_cfg[k]\n\n        if strict:\n            raise Exception(\n                f'Missing API key \"{k}\". Either re-create this ML_ENGINE specifying the `{k}` parameter, or re-create this model and pass the API key with `USING` syntax.'\n            )```\n\nUpdated code:\n```python\ndef _get_llama_index_api_key(self, args, strict=True):\n        \n        \n        \n        key = 'OPENAI_API_KEY'\n        for k in key, key.lower():\n            # 1\n            if args.get(k):\n                return args[k]\n            # 2\n            connection_args = self.engine_storage.get_connection_args()\n            if k in connection_args:\n                return connection_args[k]\n            # 3\n            api_key = os.getenv(k)\n            if api_key is not None:\n                return api_key\n            # 4\n            config = Config()\n            openai_cfg = config.get('llama_index', {})\n            if k in openai_cfg:\n                return openai_cfg[k]\n\n        if strict:\n            raise Exception(f'Missing API key \"{k}\". Either re-create this ML_ENGINE specifying the `{k}` parameter, or re-create this model and pass the API key with `USING` syntax.')\n```\n\nOriginal documentation:\n```python\nAPI_KEY preference order:\n            1. provided at model creation\n            2. provided at engine creation\n            3. OPENAI_API_KEY env variable\n            4. llama_index.OPENAI_API_KEY setting in config.json\n\n        Note: method is not case-sensitive.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "API_KEY preference order:\n            1. provided at model creation\n            2. provided at engine creation\n            3. OPENAI_API_KEY env variable\n            4. llama_index.OPENAI_API_KEY setting in config.json\n\n        Note: method is not case sensitive.", "file": "mindsdb_integrations_handlers_llama_index_handler_llama_index_handler.py", "function": "LlamaIndexHandler._get_llama_index_api_key", "project": "mindsdb", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate_consumer(\n    filter_pattern: Optional[str] = None, unformatted: bool = False, resource_name: Optional[str] = None\n):\n    \n    \n    \n    if unformatted:\n        return generate_unformatted_consumer()\n\n    return generate_console_consumer(filter_pattern)```\n\nUpdated code:\n```python\ndef generate_consumer(\n    filter_pattern: Optional[str] = None, output: OutputOption = OutputOption.text, resource_name: Optional[str] = None\n):\n    \n    \n    \n    if output == OutputOption.json:\n        return generate_json_consumer()\n\n    return generate_text_consumer(filter_pattern)\n```\n\nOriginal documentation:\n```python\nGenerates consumer instance with the given variables.\n    If unformatted is True, then it will return consumer with formatters for just JSON.\n    If not, it will return console consumer\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates consumer instance with the given variables.\n    If output is JSON, then it will return consumer with formatters for just JSON.\n    Otherwise, it will return regular text console consumer", "file": "samcli_commands_logs_puller_factory.py", "function": "generate_consumer", "project": "aws-sam-cli", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef EncodedFile(file, input, output=None, errors='strict'):\n\n     \n\n    \n    if output is None:\n        output = input\n    encode, decode = lookup(input)[:2]\n    Reader, Writer = lookup(output)[2:]\n    return StreamRecoder(file,\n                         encode,decode,Reader,Writer,\n                         errors)```\n\nUpdated code:\n```python\ndef EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):\n\n     \n\n    \n    if file_encoding is None:\n        file_encoding = data_encoding\n    encode, decode = lookup(data_encoding)[:2]\n    Reader, Writer = lookup(file_encoding)[2:]\n    sr = StreamRecoder(file,\n                       encode,decode,Reader,Writer,\n                       errors)\n    # Add attributes to simplify introspection\n    sr.data_encoding = data_encoding\n    sr.file_encoding = file_encoding\n    return sr\n```\n\nOriginal documentation:\n```python\nReturn a wrapped version of file which provides transparent\n        encoding translation.\n\n        Strings written to the wrapped file are interpreted according\n        to the given input encoding and then written to the original\n        file as string using the output encoding. The intermediate\n        encoding will usually be Unicode but depends on the specified\n        codecs.\n\n        If output is not given, it defaults to input.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a wrapped version of file which provides transparent\n        encoding translation.\n\n        Strings written to the wrapped file are interpreted according\n        to the given data_encoding and then written to the original\n        file as string using file_encoding. The intermediate encoding\n        will usually be Unicode but depends on the specified codecs.\n\n        Strings are read from the file using file_encoding and then\n        passed back to the caller as string using data_encoding.\n\n        If file_encoding is not given, it defaults to data_encoding.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        data_encoding and file_encoding are added to the wrapped file\n        object as attributes .data_encoding and .file_encoding resp.", "file": "Lib_codecs.py", "function": "EncodedFile", "project": "cpython", "change_stats": {"code_changes": 22, "doc_changes": 17, "total_changes": 39}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef dump_follow_restriction(followRes, logfolder):\n    \n    filename = '{}followRestriction.json'.format(logfolder)\n\n    with open(filename, 'w') as followResFile:\n        json.dump(followRes, followResFile)```\n\nUpdated code:\n```python\ndef dump_follow_restriction(profile_name, logger, logfolder):\n      \n\n    try:\n        # get a DB and start a connection\n        db, id = get_db()\n        conn = sqlite3.connect(db)\n\n        with conn:\n            conn.row_factory = sqlite3.Row\n            cur = conn.cursor()\n\n            cur.execute(\"SELECT * FROM followRestriction WHERE profile_id=:var\", {\"var\":id})\n            data = cur.fetchall()\n\n        if data:\n            # get the existing data\n            filename = \"{}followRestriction.json\".format(logfolder)\n            if os.path.isfile(filename):\n                with open(filename) as followResFile:\n                    current_data = json.load(followResFile)\n            else:\n                current_data = {}\n\n            # pack the new data\n            follow_data = dict(user_data[1:3] for user_data in data or [])\n            current_data[profile_name] = follow_data\n\n            # dump the fresh follow data to a local human readable JSON\n            with open(filename, 'w') as followResFile:\n                json.dump(current_data, followResFile)\n\n    except Exception as exc:\n        logger.error(\"Pow! Error occured while dumping follow restriction data to a local JSON:\\n\\t{}\".format(str(exc).encode(\"utf-8\")))\n\n    finally:\n        if conn:\n            # close the open connection\n            conn.close()\n```\n\nOriginal documentation:\n```python\nDumps the given dictionary to a file using the json format\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Dump follow restriction data to a local human-readable JSON", "file": "instapy_unfollow_util.py", "function": "dump_follow_restriction", "project": "InstaPy", "change_stats": {"code_changes": 45, "doc_changes": 4, "total_changes": 49}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_interpolation_stage(self, s):\n        \n        \n        \n        if s is None:\n            s = \"data\"  # placeholder for maybe having rcParam\n        _api.check_in_list(['data', 'rgba'], s=s)\n        self._interpolation_stage = s\n        self.stale = True```\n\nUpdated code:\n```python\ndef set_interpolation_stage(self, s):\n        \n        \n        \n        s = mpl._val_or_rc(s, 'image.interpolation_stage')\n        _api.check_in_list(['data', 'rgba'], s=s)\n        self._interpolation_stage = s\n        self.stale = True\n```\n\nOriginal documentation:\n```python\nSet when interpolation happens during the transform to RGBA.\n\n        Parameters\n        ----------\n        s : {'data', 'rgba'} or None\n            Whether to apply up/downsampling interpolation in data or RGBA\n            space.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set when interpolation happens during the transform to RGBA.\n\n        Parameters\n        ----------\n        s : {'data', 'rgba'} or None\n            Whether to apply up/downsampling interpolation in data or RGBA\n            space.  If None, use :rc:`image.interpolation_stage`.", "file": "lib_matplotlib_image.py", "function": "_ImageBase.set_interpolation_stage", "project": "matplotlib", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_new_mask(self, x_mask: torch.Tensor) -> torch.Tensor:\n        \n\n        \n        x_t1 = x_mask.size(2) - (x_mask.size(2) % 3)\n        x_mask = x_mask[:, :, :x_t1][:, :, ::3]\n\n        x_t2 = x_mask.size(2) - (x_mask.size(2) % 2)\n        x_mask = x_mask[:, :, :x_t2][:, :, ::2]\n\n        return x_mask```\n\nUpdated code:\n```python\ndef create_new_mask(self, feats_mask: torch.Tensor) -> torch.Tensor:\n        \n\n        \n        vgg1_t_len = feats_mask.size(2) - (feats_mask.size(2) % 3)\n        vgg_mask = feats_mask[:, :, :vgg1_t_len][:, :, ::3]\n\n        vgg2_t_len = vgg_mask.size(2) - (vgg_mask.size(2) % 2)\n        vgg_mask = vgg_mask[:, :, :vgg2_t_len][:, :, ::2]\n\n        return vgg_mask\n```\n\nOriginal documentation:\n```python\nCreate a subsampled version of x_mask.\n\n        Args:\n            x_mask: Input mask (B, 1, T)\n\n        Returns:\n            x_mask: Output mask (B, 1, sub(T))\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a subsampled mask of feature sequences.\n\n        Args:\n            feats_mask: Mask of feature sequences. (B, 1, T)\n\n        Returns:\n            vgg_mask: Mask of VGG2L output sequences. (B, 1, sub(T))", "file": "espnet_nets_pytorch_backend_transducer_vgg2l.py", "function": "VGG2L.create_new_mask", "project": "espnet", "change_stats": {"code_changes": 14, "doc_changes": 8, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _assisted_decoding(\n        self,\n        input_ids: torch.LongTensor,\n        candidate_generator: Optional[\"CandidateGenerator\"] = None,\n        do_sample: bool = False,\n        logits_processor: Optional[LogitsProcessorList] = None,\n        logits_warper: Optional[LogitsProcessorList] = None,\n        stopping_criteria: Optional[StoppingCriteriaList] = None,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[Union[int, List[int]]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_scores: Optional[bool] = None,\n        output_logits: Optional[bool] = None,\n        return_dict_in_generate: Optional[bool] = None,\n        synced_gpus: bool = False,\n        streamer: Optional[\"BaseStreamer\"] = None,\n        **model_kwargs,\n    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n        \n        # init values\n        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n        logits_warper = logits_warper if logits_warper is not None else LogitsProcessorList()\n        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n        if eos_token_id is not None:\n            logger.warning_once(\n                \"`eos_token_id` is deprecated in this function and will be removed in v4.41, use\"\n                \" `stopping_criteria=StoppingCriteriaList([EosTokenCriteria(eos_token_id=eos_token_id)])` instead.\"\n                \" Otherwise make sure to set `model.generation_config.eos_token_id`\",\n                FutureWarning,\n            )\n            stopping_criteria.append(EosTokenCriteria(eos_token_id=eos_token_id))\n        else:\n            # TODO remove when the method is totally private and beam scorer refactored\n            # need to get `eos_token_id` and add stopping criteria, so that generation does not go forever\n            eos_token_id = [\n                criteria.eos_token_id.tolist() for criteria in stopping_criteria if hasattr(criteria, \"eos_token_id\")\n            ]\n            eos_token_id = eos_token_id[0] if eos_token_id else None\n            if eos_token_id is None and self.generation_config.eos_token_id is not None:\n                eos_token_id = self.generation_config.eos_token_id\n                stopping_criteria.append(EosTokenCriteria(eos_token_id=eos_token_id))\n\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        output_scores = output_scores if output_scores is not None else self.generation_config.output_scores\n        output_logits = output_logits if output_logits is not None else self.generation_config.output_logits\n        output_attentions = (\n            output_attentions if output_attentions is not None else self.generation_config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states\n        )\n        return_dict_in_generate = (\n            return_dict_in_generate\n            if return_dict_in_generate is not None\n            else self.generation_config.return_dict_in_generate\n        )\n\n        # init attention / hidden states / scores tuples\n        scores = () if (return_dict_in_generate and output_scores) else None\n        raw_logits = () if (return_dict_in_generate and output_logits) else None\n        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n\n        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n        if return_dict_in_generate and self.config.is_encoder_decoder:\n            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n            encoder_hidden_states = (\n                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n            )\n\n        # keep track of which sequences are already finished\n        batch_size = input_ids.shape[0]\n        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n\n        this_peer_finished = False\n        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n            cur_len = input_ids.shape[-1]\n\n            #  1. Fetch candidate sequences from a `CandidateGenerator`\n            candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)\n            candidate_input_ids = candidate_input_ids.to(self.device)\n            if candidate_logits is not None:\n                candidate_logits = candidate_logits.to(self.device)\n\n            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n            is_done_candidate = stopping_criteria(candidate_input_ids, None)\n\n            # 2. Use the original model to obtain the next token logits given the candidate sequence. We obtain\n            # `candidate_length + 1` relevant logits from this process: in the event that all candidates are correct,\n            # we use this forward pass to also pick the subsequent logits in the original model.\n\n            # 2.1. Prepare the model inputs\n            candidate_kwargs = copy.copy(model_kwargs)\n            candidate_kwargs = _prepare_attention_mask(\n                candidate_kwargs, candidate_input_ids.shape[1], self.config.is_encoder_decoder\n            )\n            candidate_kwargs = _prepare_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n            if \"cache_position\" in candidate_kwargs:\n                candidate_kwargs[\"cache_position\"] = torch.cat(\n                    (\n                        candidate_kwargs[\"cache_position\"],\n                        torch.arange(cur_len, cur_len + candidate_length, device=input_ids.device, dtype=torch.long),\n                    ),\n                    dim=0,\n                )\n\n            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n            if \"num_logits_to_keep\" in model_inputs:\n                model_inputs[\"num_logits_to_keep\"] = candidate_length + 1\n\n            # 2.2. Run a forward pass on the candidate sequence\n            outputs = self(\n                **model_inputs,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n            )\n\n            # 2.3. Process the new logits\n            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n            next_token_logits = new_logits.clone()\n            if len(logits_processor) > 0:\n                for i in range(candidate_length + 1):\n                    new_logits[:, i, :] = logits_processor(candidate_input_ids[:, : cur_len + i], new_logits[:, i, :])\n            if len(logits_warper) > 0:\n                for i in range(candidate_length + 1):\n                    new_logits[:, i, :] = logits_warper(candidate_input_ids[:, : cur_len + i], new_logits[:, i, :])\n\n            # 3. Select the accepted tokens. There are two possible cases:\n            # Case 1: `do_sample=True` and we have logits for the candidates (originally from speculative decoding)\n            # \ud83d\udc49 Apply algorithm 1 from the speculative decoding paper (https://arxiv.org/pdf/2211.17192.pdf).\n            if do_sample and candidate_logits is not None:\n                valid_tokens, n_matches = _speculative_sampling(\n                    candidate_input_ids,\n                    candidate_logits,\n                    candidate_length,\n                    new_logits,\n                    is_done_candidate,\n                )\n\n            # Case 2: all other cases (originally from assisted generation) \ud83d\udc49 Compare the tokens selected from the\n            # original model logits with the candidate tokens. We can keep the candidate tokens until the first\n            # mismatch, or until the max length is reached.\n            else:\n                if do_sample:\n                    probs = new_logits.softmax(dim=-1)\n                    selected_tokens = torch.multinomial(probs[0, :, :], num_samples=1).squeeze(1)[None, :]\n                else:\n                    selected_tokens = new_logits.argmax(dim=-1)\n\n                candidate_new_tokens = candidate_input_ids[:, cur_len:]\n                n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n\n                # Ensure we don't generate beyond max_len or an EOS token\n                if is_done_candidate and n_matches == candidate_length:\n                    n_matches -= 1\n                valid_tokens = selected_tokens[:, : n_matches + 1]\n\n            # 4. Update variables according to the number of matching assistant tokens. Remember: the token generated\n            # by the model after the last candidate match is also valid, as it is generated from a correct sequence.\n            # Because of this last token, assisted generation search reduces to a normal greedy search/sample if there\n            # is no match.\n\n            # 4.1. Get the valid continuation, after the matching tokens\n            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n            if streamer is not None:\n                streamer.put(valid_tokens.cpu())\n            new_cur_len = input_ids.shape[-1]\n\n            # 4.2. Discard past key values relative to unused assistant tokens\n            new_cache_size = new_cur_len - 1\n            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n\n            # 5. Update the candidate generation strategy if needed\n            candidate_generator.update_candidate_strategy(input_ids, new_logits, n_matches)\n\n            if synced_gpus and this_peer_finished:\n                continue  # don't waste resources running the code we don't need\n\n            # Store scores, attentions and hidden_states when required\n            # Assistant: modified to append one tuple element per token, as in the other generation methods.\n            if return_dict_in_generate:\n                if output_scores:\n                    scores += tuple(new_logits[:, i, :] for i in range(n_matches + 1))\n                if output_logits:\n                    raw_logits += (next_token_logits,)\n\n                if \"past_key_values\" not in model_kwargs:\n                    added_len = new_cur_len\n                else:\n                    added_len = n_matches + 1\n\n                if output_attentions:\n                    if self.config.is_encoder_decoder:\n                        cross_attentions = _split_model_outputs(\n                            cross_attentions, outputs.cross_attentions, cur_len, added_len\n                        )\n                        decoder_attentions = _split_model_outputs(\n                            decoder_attentions,\n                            outputs.decoder_attentions,\n                            cur_len,\n                            added_len,\n                            is_decoder_attention=True,\n                        )\n                    else:\n                        decoder_attentions = _split_model_outputs(\n                            decoder_attentions,\n                            outputs.attentions,\n                            cur_len,\n                            added_len,\n                            is_decoder_attention=True,\n                        )\n                if output_hidden_states:\n                    if self.config.is_encoder_decoder:\n                        decoder_hidden_states = _split_model_outputs(\n                            decoder_hidden_states, outputs.decoder_hidden_states, cur_len, added_len\n                        )\n                    else:\n                        decoder_hidden_states = _split_model_outputs(\n                            decoder_hidden_states, outputs.hidden_states, cur_len, added_len\n                        )\n\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs,\n                model_kwargs,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                num_new_tokens=n_matches + 1,\n            )\n\n            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n            this_peer_finished = unfinished_sequences.max() == 0\n\n        if streamer is not None:\n            streamer.end()\n\n        if (\n            hasattr(candidate_generator, \"assistant_model\")\n            and candidate_generator.assistant_model.generation_config.num_assistant_tokens_schedule == \"heuristic\"\n        ):\n            candidate_generator.assistant_model.generation_config.num_assistant_tokens = (\n                candidate_generator.num_assistant_tokens\n            )\n        if return_dict_in_generate:\n            if self.config.is_encoder_decoder:\n                return GenerateEncoderDecoderOutput(\n                    sequences=input_ids,\n                    scores=scores,\n                    logits=raw_logits,\n                    encoder_attentions=encoder_attentions,\n                    encoder_hidden_states=encoder_hidden_states,\n                    decoder_attentions=decoder_attentions,\n                    cross_attentions=cross_attentions,\n                    decoder_hidden_states=decoder_hidden_states,\n                    past_key_values=model_kwargs.get(\"past_key_values\"),\n                )\n            else:\n                return GenerateDecoderOnlyOutput(\n                    sequences=input_ids,\n                    scores=scores,\n                    logits=raw_logits,\n                    attentions=decoder_attentions,\n                    hidden_states=decoder_hidden_states,\n                    past_key_values=model_kwargs.get(\"past_key_values\"),\n                )\n        else:\n            return input_ids```\n\nUpdated code:\n```python\ndef _assisted_decoding(\n        self,\n        input_ids: torch.LongTensor,\n        candidate_generator: CandidateGenerator,\n        logits_processor: LogitsProcessorList,\n        logits_warper: LogitsProcessorList,\n        stopping_criteria: StoppingCriteriaList,\n        generation_config: GenerationConfig,\n        synced_gpus: bool,\n        streamer: Optional[\"BaseStreamer\"],\n        **model_kwargs,\n    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n        \n        \n        # init values\n        do_sample = logits_warper is not None\n        output_attentions = generation_config.output_attentions\n        output_hidden_states = generation_config.output_hidden_states\n        output_scores = generation_config.output_scores\n        output_logits = generation_config.output_logits\n        return_dict_in_generate = generation_config.return_dict_in_generate\n\n        # init attention / hidden states / scores tuples\n        scores = () if (return_dict_in_generate and output_scores) else None\n        raw_logits = () if (return_dict_in_generate and output_logits) else None\n        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n\n        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n        if return_dict_in_generate and self.config.is_encoder_decoder:\n            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n            encoder_hidden_states = (\n                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n            )\n\n        # keep track of which sequences are already finished\n        batch_size = input_ids.shape[0]\n        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n\n        this_peer_finished = False\n        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n            cur_len = input_ids.shape[-1]\n\n            #  1. Fetch candidate sequences from a `CandidateGenerator`\n            candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)\n            candidate_input_ids = candidate_input_ids.to(self.device)\n            if candidate_logits is not None:\n                candidate_logits = candidate_logits.to(self.device)\n\n            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n            is_done_candidate = stopping_criteria(candidate_input_ids, None)\n\n            # 2. Use the original model to obtain the next token logits given the candidate sequence. We obtain\n            # `candidate_length + 1` relevant logits from this process: in the event that all candidates are correct,\n            # we use this forward pass to also pick the subsequent logits in the original model.\n\n            # 2.1. Prepare the model inputs\n            candidate_kwargs = copy.copy(model_kwargs)\n            candidate_kwargs = _prepare_attention_mask(\n                candidate_kwargs, candidate_input_ids.shape[1], self.config.is_encoder_decoder\n            )\n            candidate_kwargs = _prepare_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n            if \"cache_position\" in candidate_kwargs:\n                candidate_kwargs[\"cache_position\"] = torch.cat(\n                    (\n                        candidate_kwargs[\"cache_position\"],\n                        torch.arange(cur_len, cur_len + candidate_length, device=input_ids.device, dtype=torch.long),\n                    ),\n                    dim=0,\n                )\n\n            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n            if \"num_logits_to_keep\" in model_inputs:\n                model_inputs[\"num_logits_to_keep\"] = candidate_length + 1\n\n            # 2.2. Run a forward pass on the candidate sequence\n            outputs = self(\n                **model_inputs,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n            )\n\n            # 2.3. Process the new logits\n            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n            next_token_logits = new_logits.clone()\n            if len(logits_processor) > 0:\n                for i in range(candidate_length + 1):\n                    new_logits[:, i, :] = logits_processor(candidate_input_ids[:, : cur_len + i], new_logits[:, i, :])\n            if do_sample and len(logits_warper) > 0:\n                for i in range(candidate_length + 1):\n                    new_logits[:, i, :] = logits_warper(candidate_input_ids[:, : cur_len + i], new_logits[:, i, :])\n\n            # 3. Select the accepted tokens. There are two possible cases:\n            # Case 1: `do_sample=True` and we have logits for the candidates (originally from speculative decoding)\n            # \ud83d\udc49 Apply algorithm 1 from the speculative decoding paper (https://arxiv.org/pdf/2211.17192.pdf).\n            if do_sample and candidate_logits is not None:\n                valid_tokens, n_matches = _speculative_sampling(\n                    candidate_input_ids,\n                    candidate_logits,\n                    candidate_length,\n                    new_logits,\n                    is_done_candidate,\n                )\n\n            # Case 2: all other cases (originally from assisted generation) \ud83d\udc49 Compare the tokens selected from the\n            # original model logits with the candidate tokens. We can keep the candidate tokens until the first\n            # mismatch, or until the max length is reached.\n            else:\n                if do_sample:\n                    probs = new_logits.softmax(dim=-1)\n                    selected_tokens = torch.multinomial(probs[0, :, :], num_samples=1).squeeze(1)[None, :]\n                else:\n                    selected_tokens = new_logits.argmax(dim=-1)\n\n                candidate_new_tokens = candidate_input_ids[:, cur_len:]\n                n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n\n                # Ensure we don't generate beyond max_len or an EOS token\n                if is_done_candidate and n_matches == candidate_length:\n                    n_matches -= 1\n                valid_tokens = selected_tokens[:, : n_matches + 1]\n\n            # 4. Update variables according to the number of matching assistant tokens. Remember: the token generated\n            # by the model after the last candidate match is also valid, as it is generated from a correct sequence.\n            # Because of this last token, assisted generation search reduces to a normal greedy search/sample if there\n            # is no match.\n\n            # 4.1. Get the valid continuation, after the matching tokens\n            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n            if streamer is not None:\n                streamer.put(valid_tokens.cpu())\n            new_cur_len = input_ids.shape[-1]\n\n            # 4.2. Discard past key values relative to unused assistant tokens\n            new_cache_size = new_cur_len - 1\n            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n\n            # 5. Update the candidate generation strategy if needed\n            candidate_generator.update_candidate_strategy(input_ids, new_logits, n_matches)\n\n            if synced_gpus and this_peer_finished:\n                continue  # don't waste resources running the code we don't need\n\n            # Store scores, attentions and hidden_states when required\n            # Assistant: modified to append one tuple element per token, as in the other generation methods.\n            if return_dict_in_generate:\n                if output_scores:\n                    scores += tuple(new_logits[:, i, :] for i in range(n_matches + 1))\n                if output_logits:\n                    raw_logits += (next_token_logits,)\n\n                if \"past_key_values\" not in model_kwargs:\n                    added_len = new_cur_len\n                else:\n                    added_len = n_matches + 1\n\n                if output_attentions:\n                    if self.config.is_encoder_decoder:\n                        cross_attentions = _split_model_outputs(\n                            cross_attentions, outputs.cross_attentions, cur_len, added_len\n                        )\n                        decoder_attentions = _split_model_outputs(\n                            decoder_attentions,\n                            outputs.decoder_attentions,\n                            cur_len,\n                            added_len,\n                            is_decoder_attention=True,\n                        )\n                    else:\n                        decoder_attentions = _split_model_outputs(\n                            decoder_attentions,\n                            outputs.attentions,\n                            cur_len,\n                            added_len,\n                            is_decoder_attention=True,\n                        )\n                if output_hidden_states:\n                    if self.config.is_encoder_decoder:\n                        decoder_hidden_states = _split_model_outputs(\n                            decoder_hidden_states, outputs.decoder_hidden_states, cur_len, added_len\n                        )\n                    else:\n                        decoder_hidden_states = _split_model_outputs(\n                            decoder_hidden_states, outputs.hidden_states, cur_len, added_len\n                        )\n\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs,\n                model_kwargs,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                num_new_tokens=n_matches + 1,\n            )\n\n            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n            this_peer_finished = unfinished_sequences.max() == 0\n\n        if streamer is not None:\n            streamer.end()\n\n        if (\n            hasattr(candidate_generator, \"assistant_model\")\n            and candidate_generator.assistant_model.generation_config.num_assistant_tokens_schedule == \"heuristic\"\n        ):\n            candidate_generator.assistant_model.generation_config.num_assistant_tokens = (\n                candidate_generator.num_assistant_tokens\n            )\n        if return_dict_in_generate:\n            if self.config.is_encoder_decoder:\n                return GenerateEncoderDecoderOutput(\n                    sequences=input_ids,\n                    scores=scores,\n                    logits=raw_logits,\n                    encoder_attentions=encoder_attentions,\n                    encoder_hidden_states=encoder_hidden_states,\n                    decoder_attentions=decoder_attentions,\n                    cross_attentions=cross_attentions,\n                    decoder_hidden_states=decoder_hidden_states,\n                    past_key_values=model_kwargs.get(\"past_key_values\"),\n                )\n            else:\n                return GenerateDecoderOnlyOutput(\n                    sequences=input_ids,\n                    scores=scores,\n                    logits=raw_logits,\n                    attentions=decoder_attentions,\n                    hidden_states=decoder_hidden_states,\n                    past_key_values=model_kwargs.get(\"past_key_values\"),\n                )\n        else:\n            return input_ids\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Generates sequences of token ids for models with a language modeling head using **greedy decoding** or\n        **sample** (depending on `do_sample`), assisted by candidate sequences. Assisted generation is an example of a\n        candidate decoding strategy. Can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text\n        models.\n\n        <Tip warning={true}>\n\n        In most cases, you do not need to call [`~generation.GenerationMixin._assisted_decoding`] directly. Use\n        generate() instead. For an overview of generation strategies and code examples, check the [following\n        guide](../generation_strategies).\n\n        </Tip>\n\n        Parameters:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                The sequence used as a prompt for the generation.\n            candidate_generator (`CandidateGenerator`, *optional*):\n                A derived instance of [`CandidateGenerator`] that defines how candidate sequences are generated. For\n                more information, the documentation of [`CandidateGenerator`] should be read.\n            do_sample (`bool`, *optional*, defaults to `False`):\n                Whether or not to use sampling ; use greedy decoding otherwise.\n            logits_processor (`LogitsProcessorList`, *optional*):\n                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n                used to modify the prediction scores of the language modeling head applied at each generation step.\n            logits_warper (`LogitsProcessorList`, *optional*):\n                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n                to warp the prediction score distribution of the language modeling head applied before multinomial\n                sampling at each generation step.\n            stopping_criteria (`StoppingCriteriaList`, *optional*):\n                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n                used to tell if the generation loop should stop.\n            pad_token_id (`int`, *optional*):\n                The id of the *padding* token.\n            eos_token_id (`Union[int, List[int]]`, *optional*):\n                The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n            output_attentions (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more details.\n            output_hidden_states (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more details.\n            output_scores (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n            output_logits (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the raw prediction logit scores. See `logits` under returned tensors for\n                more details.\n            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n            synced_gpus (`bool`, *optional*, defaults to `False`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            streamer (`BaseStreamer`, *optional*):\n                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n            model_kwargs:\n                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n\n        Return:\n            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or\n            `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n            `model.config.is_encoder_decoder=True`.\n\n        Examples:\n\n        ```python\n        >>> from transformers import (\n        ...     AutoTokenizer,\n        ...     AutoModelForCausalLM,\n        ...     LogitsProcessorList,\n        ...     MinLengthLogitsProcessor,\n        ...     StoppingCriteriaList,\n        ...     MaxLengthCriteria,\n        ... )\n        >>> from transformers.generation import AssistedCandidateGenerator\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n        >>> assistant_model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n        >>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n        >>> model.generation_config.pad_token_id = model.generation_config.eos_token_id\n        >>> input_prompt = \"It might be possible to\"\n        >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n        >>> model.generation_config.min_length = 10\n\n        >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n        >>> candidate_generator = AssistedCandidateGenerator(\n        ...     input_ids=input_ids,\n        ...     assistant_model=assistant_model,\n        ...     generation_config=model.generation_config,\n        ...     model_kwargs={},\n        ... )\n        >>> outputs = model._assisted_decoding(\n        ...     input_ids,\n        ...     candidate_generator=candidate_generator,\n        ...     stopping_criteria=stopping_criteria,\n        ... )\n        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n        ```\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Generates sequences of token ids for models with a language modeling head using **greedy decoding** or\n        **sample** (depending on `do_sample`), assisted by candidate sequences. Assisted generation is an example of a\n        candidate decoding strategy. Can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text\n        models.\n\n        Parameters:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                The sequence used as a prompt for the generation.\n            candidate_generator (`CandidateGenerator`):\n                A derived instance of [`CandidateGenerator`] that defines how candidate sequences are generated. For\n                more information, the documentation of [`CandidateGenerator`] should be read.\n            logits_processor (`LogitsProcessorList`):\n                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n                used to modify the prediction scores of the language modeling head applied at each generation step.\n            logits_warper (`LogitsProcessorList`):\n                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n                to warp the prediction score distribution of the language modeling head applied before multinomial\n                sampling at each generation step. Only used if sampling is active.\n            stopping_criteria (`StoppingCriteriaList`):\n                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n                used to tell if the generation loop should stop.\n            generation_config ([`~generation.GenerationConfig`]):\n                The generation configuration to be used as parametrization of the decoding method.\n            synced_gpus (`bool`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            streamer (`BaseStreamer`, *optional*):\n                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n            model_kwargs:\n                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n\n        Return:\n            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or\n            `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n            `model.config.is_encoder_decoder=True`.", "file": "src_transformers_generation_utils.py", "function": "GenerationMixin._assisted_decoding", "project": "transformers", "change_stats": {"code_changes": 70, "doc_changes": 81, "total_changes": 151}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _test_op(self, qmodule, subname=None, input_size=None, input_quantized=True,\n                 generate=False, prec=None, new_zipfile_serialization=False):\n        \n        \n        input_file, state_dict_file, scripted_module_file, traced_module_file, \\\n            expected_file, _package_file, _get_attr_targets_file = \\\n            get_filenames(self, subname)\n\n        # only generate once.\n        if generate and qengine_is_fbgemm():\n            input_tensor = torch.rand(*input_size).float()\n            if input_quantized:\n                input_tensor = torch.quantize_per_tensor(input_tensor, 0.5, 2, torch.quint8)\n            torch.save(input_tensor, input_file)\n            # Temporary fix to use _use_new_zipfile_serialization until #38379 lands.\n            torch.save(qmodule.state_dict(), state_dict_file, _use_new_zipfile_serialization=new_zipfile_serialization)\n            torch.jit.save(torch.jit.script(qmodule), scripted_module_file)\n            torch.jit.save(torch.jit.trace(qmodule, input_tensor), traced_module_file)\n            torch.save(qmodule(input_tensor), expected_file)\n\n        input_tensor = torch.load(input_file)\n        qmodule.load_state_dict(torch.load(state_dict_file))\n        qmodule_scripted = torch.jit.load(scripted_module_file)\n        qmodule_traced = torch.jit.load(traced_module_file)\n        expected = torch.load(expected_file)\n        self.assertEqual(qmodule(input_tensor), expected, atol=prec)\n        self.assertEqual(qmodule_scripted(input_tensor), expected, atol=prec)\n        self.assertEqual(qmodule_traced(input_tensor), expected, atol=prec)```\n\nUpdated code:\n```python\ndef _test_op(\n        self,\n        qmodule,\n        subname=None,\n        input_size=None,\n        input_quantized=True,\n        generate=False,\n        prec=None,\n        new_zipfile_serialization=False,\n    ):\n        \n        \n        (\n            input_file,\n            state_dict_file,\n            scripted_module_file,\n            traced_module_file,\n            expected_file,\n            _package_file,\n            _get_attr_targets_file,\n        ) = get_filenames(self, subname)\n\n        # only generate once.\n        if generate and qengine_is_fbgemm():\n            input_tensor = torch.rand(*input_size).float()\n            if input_quantized:\n                input_tensor = torch.quantize_per_tensor(\n                    input_tensor, 0.5, 2, torch.quint8\n                )\n            torch.save(input_tensor, input_file)\n            # Temporary fix to use _use_new_zipfile_serialization until #38379 lands.\n            torch.save(\n                qmodule.state_dict(),\n                state_dict_file,\n                _use_new_zipfile_serialization=new_zipfile_serialization,\n            )\n            torch.jit.save(torch.jit.script(qmodule), scripted_module_file)\n            torch.jit.save(torch.jit.trace(qmodule, input_tensor), traced_module_file)\n            torch.save(qmodule(input_tensor), expected_file)\n\n        input_tensor = torch.load(input_file)\n        qmodule.load_state_dict(torch.load(state_dict_file))\n        qmodule_scripted = torch.jit.load(scripted_module_file)\n        qmodule_traced = torch.jit.load(traced_module_file)\n        expected = torch.load(expected_file)\n        self.assertEqual(qmodule(input_tensor), expected, atol=prec)\n        self.assertEqual(qmodule_scripted(input_tensor), expected, atol=prec)\n        self.assertEqual(qmodule_traced(input_tensor), expected, atol=prec)\n```\n\nOriginal documentation:\n```python\nr\"\"\" Test quantized modules serialized previously can be loaded\n        with current code, make sure we don't break backward compatibility for the\n        serialization of quantized modules\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Test quantized modules serialized previously can be loaded\n        with current code, make sure we don't break backward compatibility for the\n        serialization of quantized modules", "file": "test_quantization_bc_test_backward_compatibility.py", "function": "TestSerialization._test_op", "project": "pytorch", "change_stats": {"code_changes": 36, "doc_changes": 4, "total_changes": 40}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef asfreq(self, freq, method=None):\n        \n        \n        \n        if isinstance(freq, datetools.DateOffset):\n            dateRange = DateRange(self.index[0], self.index[-1], offset=freq)\n        else:\n            dateRange = DateRange(self.index[0], self.index[-1], time_rule=freq)\n\n        return self.reindex(dateRange, method=method)```\n\nUpdated code:\n```python\ndef asfreq(self, freq, method=None):\n        \n        \n        \n        from pandas.core.daterange import date_range\n        if len(self.index) == 0:\n            return self.copy()\n        dti = date_range(self.index[0], self.index[-1], freq=freq)\n        return self.reindex(dti, method=method)\n```\n\nOriginal documentation:\n```python\nConvert this TimeSeries to the provided frequency using DateOffset\n        object or time rule. Optionally provide fill method to pad/backfill\n        missing values.\n\n        Parameters\n        ----------\n        offset : DateOffset object, or string in {'WEEKDAY', 'EOM'}\n            DateOffset object or subclass (e.g. monthEnd)\n        method : {'backfill', 'pad', None}\n            Method to use for filling holes in new index\n\n        Returns\n        -------\n        converted : TimeSeries\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert all TimeSeries inside to specified frequency using DateOffset\n        objects. Optionally provide fill method to pad/backfill missing values.\n\n        Parameters\n        ----------\n        freq : DateOffset object, or string\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill methdo\n\n        Returns\n        -------\n        converted : DataFrame", "file": "pandas_core_series.py", "function": "Series.asfreq", "project": "pandas", "change_stats": {"code_changes": 13, "doc_changes": 18, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef success_revocation(cert_path):\n    \n\n    \n    z_util(interfaces.IDisplay).notification(\n        \"Congratulations! You have successfully revoked the certificate \"\n        \"that was located at {0}{1}{1}\".format(\n            cert_path,\n            os.linesep),\n        pause=False)```\n\nUpdated code:\n```python\ndef success_revocation(cert_path):\n    \n\n    \n    display_util.notify(\n        \"Congratulations! You have successfully revoked the certificate \"\n        \"that was located at {0}.\".format(cert_path)\n    )\n```\n\nOriginal documentation:\n```python\nDisplay a box confirming a certificate has been revoked.\n\n    :param list cert_path: path to certificate which was revoked.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Display a message confirming a certificate has been revoked.\n\n    :param list cert_path: path to certificate which was revoked.", "file": "certbot_certbot_display_ops.py", "function": "success_revocation", "project": "certbot", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_fixed_in_frame_mobjects(self, *mobjects):\n        \n        \n        for mobject in extract_mobject_family_members(mobjects):\n            self.fixed_in_frame_mobjects.add(mobject)```\n\nUpdated code:\n```python\ndef add_fixed_in_frame_mobjects(self, *mobjects: Mobject):\n        \n        \n        for mobject in extract_mobject_family_members(mobjects):\n            self.fixed_in_frame_mobjects.add(mobject)\n```\n\nOriginal documentation:\n```python\nThis method allows the mobject to have a fixed position,\n        even when the camera moves around.\n        E.G If it was passed through this method, at the top of the frame, it\n        will continue to be displayed at the top of the frame.\n\n        Highly useful when displaying Titles or formulae or the like.\n\n        Parameters\n        ----------\n        **mobjects : Mobject\n            The mobject to fix in frame.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method allows the mobject to have a fixed position,\n        even when the camera moves around.\n        E.G If it was passed through this method, at the top of the frame, it\n        will continue to be displayed at the top of the frame.\n\n        Highly useful when displaying Titles or formulae or the like.\n\n        Parameters\n        ----------\n        **mobjects\n            The mobject to fix in frame.", "file": "manim_camera_three_d_camera.py", "function": "ThreeDCamera.add_fixed_in_frame_mobjects", "project": "manim", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef wait(self, timeout=None):\n        \n\n        \n        return self.backend.wait_for(self.task_id, timeout=timeout)```\n\nUpdated code:\n```python\ndef wait(self, timeout=None, propagate=True, interval=0.5):\n        \n\n        \n        return self.backend.wait_for(self.task_id, timeout=timeout,\n                                                   propagate=propagate,\n                                                   interval=interval)\n```\n\nOriginal documentation:\n```python\nWait for task, and return the result.\n\n        :keyword timeout: How long to wait, in seconds, before the\n                          operation times out.\n\n        :raises celery.exceptions.TimeoutError: if `timeout` is not\n            :const:`None` and the result does not arrive within `timeout`\n            seconds.\n\n        If the remote call raised an exception then that exception will\n        be re-raised.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Wait for task, and return the result.\n\n        .. warning::\n\n           Waiting for subtasks may lead to deadlocks.\n           Please read :ref:`task-synchronous-subtasks`.\n\n        :keyword timeout: How long to wait, in seconds, before the\n                          operation times out.\n        :keyword propagate: Re-raise exception if the task failed.\n        :keyword interval: Time to wait (in seconds) before retrying to\n           retrieve the result.  Note that this does not have any effect\n           when using the AMQP result store backend, as it does not\n           use polling.\n\n        :raises celery.exceptions.TimeoutError: if `timeout` is not\n            :const:`None` and the result does not arrive within `timeout`\n            seconds.\n\n        If the remote call raised an exception then that exception will\n        be re-raised.", "file": "celery_result.py", "function": "BaseAsyncResult.wait", "project": "celery", "change_stats": {"code_changes": 8, "doc_changes": 12, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef canny(image, sigma=1., low_threshold=None, high_threshold=None, mask=None,\n    quantile_threshold=False):\n    \n    \n\n    #\n    # The steps involved:\n    #\n    # * Smooth using the Gaussian with sigma above.\n    #\n    # * Apply the horizontal and vertical Sobel operators to get the gradients\n    #   within the image. The edge strength is the sum of the magnitudes\n    #   of the gradients in each direction.\n    #\n    # * Find the normal to the edge at each point using the arctangent of the\n    #   ratio of the Y sobel over the X sobel - pragmatically, we can\n    #   look at the signs of X and Y and the relative magnitude of X vs Y\n    #   to sort the points into 4 categories: horizontal, vertical,\n    #   diagonal and antidiagonal.\n    #\n    # * Look in the normal and reverse directions to see if the values\n    #   in either of those directions are greater than the point in question.\n    #   Use interpolation to get a mix of points instead of picking the one\n    #   that's the closest to the normal.\n    #\n    # * Label all points above the high threshold as edges.\n    # * Recursively label any point above the low threshold that is 8-connected\n    #   to a labeled point as an edge.\n    #\n    # Regarding masks, any point touching a masked point will have a gradient\n    # that is \"infected\" by the masked point, so it's enough to erode the\n    # mask by one and then mask the output. We also mask out the border points\n    # because who knows what lies beyond the edge of the image?\n    #\n    assert_nD(image, 2)\n\n    if low_threshold is None:\n        low_threshold = 0.1 * dtype_limits(image)[1]\n\n    if high_threshold is None:\n        high_threshold = 0.2 * dtype_limits(image)[1]\n\n    if mask is None:\n        mask = np.ones(image.shape, dtype=bool)\n    fsmooth = lambda x: gaussian_filter(x, sigma, mode='constant')\n    smoothed = smooth_with_function_and_mask(image, fsmooth, mask)\n    jsobel = ndi.sobel(smoothed, axis=1)\n    isobel = ndi.sobel(smoothed, axis=0)\n    abs_isobel = np.abs(isobel)\n    abs_jsobel = np.abs(jsobel)\n    magnitude = np.hypot(isobel, jsobel)\n\n    #\n    # Make the eroded mask. Setting the border value to zero will wipe\n    # out the image edges for us.\n    #\n    s = generate_binary_structure(2, 2)\n    eroded_mask = binary_erosion(mask, s, border_value=0)\n    eroded_mask = eroded_mask & (magnitude > 0)\n    #\n    #--------- Find local maxima --------------\n    #\n    # Assign each point to have a normal of 0-45 degrees, 45-90 degrees,\n    # 90-135 degrees and 135-180 degrees.\n    #\n    local_maxima = np.zeros(image.shape, bool)\n    #----- 0 to 45 degrees ------\n    pts_plus = (isobel >= 0) & (jsobel >= 0) & (abs_isobel >= abs_jsobel)\n    pts_minus = (isobel <= 0) & (jsobel <= 0) & (abs_isobel >= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    # Get the magnitudes shifted left to make a matrix of the points to the\n    # right of pts. Similarly, shift left and down to get the points to the\n    # top right of pts.\n    c1 = magnitude[1:, :][pts[:-1, :]]\n    c2 = magnitude[1:, 1:][pts[:-1, :-1]]\n    m = magnitude[pts]\n    w = abs_jsobel[pts] / abs_isobel[pts]\n    c_plus = c2 * w + c1 * (1 - w) <= m\n    c1 = magnitude[:-1, :][pts[1:, :]]\n    c2 = magnitude[:-1, :-1][pts[1:, 1:]]\n    c_minus = c2 * w + c1 * (1 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n    #----- 45 to 90 degrees ------\n    # Mix diagonal and vertical\n    #\n    pts_plus = (isobel >= 0) & (jsobel >= 0) & (abs_isobel <= abs_jsobel)\n    pts_minus = (isobel <= 0) & (jsobel <= 0) & (abs_isobel <= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    c1 = magnitude[:, 1:][pts[:, :-1]]\n    c2 = magnitude[1:, 1:][pts[:-1, :-1]]\n    m = magnitude[pts]\n    w = abs_isobel[pts] / abs_jsobel[pts]\n    c_plus = c2 * w + c1 * (1 - w) <= m\n    c1 = magnitude[:, :-1][pts[:, 1:]]\n    c2 = magnitude[:-1, :-1][pts[1:, 1:]]\n    c_minus = c2 * w + c1 * (1 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n    #----- 90 to 135 degrees ------\n    # Mix anti-diagonal and vertical\n    #\n    pts_plus = (isobel <= 0) & (jsobel >= 0) & (abs_isobel <= abs_jsobel)\n    pts_minus = (isobel >= 0) & (jsobel <= 0) & (abs_isobel <= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    c1a = magnitude[:, 1:][pts[:, :-1]]\n    c2a = magnitude[:-1, 1:][pts[1:, :-1]]\n    m = magnitude[pts]\n    w = abs_isobel[pts] / abs_jsobel[pts]\n    c_plus = c2a * w + c1a * (1.0 - w) <= m\n    c1 = magnitude[:, :-1][pts[:, 1:]]\n    c2 = magnitude[1:, :-1][pts[:-1, 1:]]\n    c_minus = c2 * w + c1 * (1.0 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n    #----- 135 to 180 degrees ------\n    # Mix anti-diagonal and anti-horizontal\n    #\n    pts_plus = (isobel <= 0) & (jsobel >= 0) & (abs_isobel >= abs_jsobel)\n    pts_minus = (isobel >= 0) & (jsobel <= 0) & (abs_isobel >= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    c1 = magnitude[:-1, :][pts[1:, :]]\n    c2 = magnitude[:-1, 1:][pts[1:, :-1]]\n    m = magnitude[pts]\n    w = abs_jsobel[pts] / abs_isobel[pts]\n    c_plus = c2 * w + c1 * (1 - w) <= m\n    c1 = magnitude[1:, :][pts[:-1, :]]\n    c2 = magnitude[1:, :-1][pts[:-1, 1:]]\n    c_minus = c2 * w + c1 * (1 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n\n    #\n    #---- If quantile_threshold is set then calculate the thresholds to use\n    #\n    if quantile_threshold:\n        if high_threshold > 1.0 or low_threshold > 1.0:\n            raise ValueError(\"Quantile thresholds must not be > 1.0\")\n        if high_threshold < 0.0 or low_threshold < 0.0:\n            raise ValueError(\"Quantile thresholds must not be < 0.0\")\n\n        high_threshold = np.percentile(magnitude, 100.0 * high_threshold)\n        low_threshold = np.percentile(magnitude, 100.0 * low_threshold)\n\n    #\n    #---- Create two masks at the two thresholds.\n    #\n    high_mask = local_maxima & (magnitude >= high_threshold)\n    low_mask = local_maxima & (magnitude >= low_threshold)\n    #\n    # Segment the low-mask, then only keep low-segments that have\n    # some high_mask component in them\n    #\n    strel = np.ones((3, 3), bool)\n    labels, count = label(low_mask, strel)\n    if count == 0:\n        return low_mask\n\n    sums = (np.array(ndi.sum(high_mask, labels,\n                             np.arange(count, dtype=np.int32) + 1),\n                     copy=False, ndmin=1))\n    good_label = np.zeros((count + 1,), bool)\n    good_label[1:] = sums > 0\n    output_mask = good_label[labels]\n    return output_mask```\n\nUpdated code:\n```python\ndef canny(image, sigma=1., low_threshold=None, high_threshold=None, mask=None,\n    use_quantiles=False):\n    \n    \n\n    #\n    # The steps involved:\n    #\n    # * Smooth using the Gaussian with sigma above.\n    #\n    # * Apply the horizontal and vertical Sobel operators to get the gradients\n    #   within the image. The edge strength is the sum of the magnitudes\n    #   of the gradients in each direction.\n    #\n    # * Find the normal to the edge at each point using the arctangent of the\n    #   ratio of the Y sobel over the X sobel - pragmatically, we can\n    #   look at the signs of X and Y and the relative magnitude of X vs Y\n    #   to sort the points into 4 categories: horizontal, vertical,\n    #   diagonal and antidiagonal.\n    #\n    # * Look in the normal and reverse directions to see if the values\n    #   in either of those directions are greater than the point in question.\n    #   Use interpolation to get a mix of points instead of picking the one\n    #   that's the closest to the normal.\n    #\n    # * Label all points above the high threshold as edges.\n    # * Recursively label any point above the low threshold that is 8-connected\n    #   to a labeled point as an edge.\n    #\n    # Regarding masks, any point touching a masked point will have a gradient\n    # that is \"infected\" by the masked point, so it's enough to erode the\n    # mask by one and then mask the output. We also mask out the border points\n    # because who knows what lies beyond the edge of the image?\n    #\n    assert_nD(image, 2)\n\n    if low_threshold is None:\n        low_threshold = 0.1 * dtype_limits(image)[1]\n\n    if high_threshold is None:\n        high_threshold = 0.2 * dtype_limits(image)[1]\n\n    if mask is None:\n        mask = np.ones(image.shape, dtype=bool)\n    fsmooth = lambda x: gaussian_filter(x, sigma, mode='constant')\n    smoothed = smooth_with_function_and_mask(image, fsmooth, mask)\n    jsobel = ndi.sobel(smoothed, axis=1)\n    isobel = ndi.sobel(smoothed, axis=0)\n    abs_isobel = np.abs(isobel)\n    abs_jsobel = np.abs(jsobel)\n    magnitude = np.hypot(isobel, jsobel)\n\n    #\n    # Make the eroded mask. Setting the border value to zero will wipe\n    # out the image edges for us.\n    #\n    s = generate_binary_structure(2, 2)\n    eroded_mask = binary_erosion(mask, s, border_value=0)\n    eroded_mask = eroded_mask & (magnitude > 0)\n    #\n    #--------- Find local maxima --------------\n    #\n    # Assign each point to have a normal of 0-45 degrees, 45-90 degrees,\n    # 90-135 degrees and 135-180 degrees.\n    #\n    local_maxima = np.zeros(image.shape, bool)\n    #----- 0 to 45 degrees ------\n    pts_plus = (isobel >= 0) & (jsobel >= 0) & (abs_isobel >= abs_jsobel)\n    pts_minus = (isobel <= 0) & (jsobel <= 0) & (abs_isobel >= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    # Get the magnitudes shifted left to make a matrix of the points to the\n    # right of pts. Similarly, shift left and down to get the points to the\n    # top right of pts.\n    c1 = magnitude[1:, :][pts[:-1, :]]\n    c2 = magnitude[1:, 1:][pts[:-1, :-1]]\n    m = magnitude[pts]\n    w = abs_jsobel[pts] / abs_isobel[pts]\n    c_plus = c2 * w + c1 * (1 - w) <= m\n    c1 = magnitude[:-1, :][pts[1:, :]]\n    c2 = magnitude[:-1, :-1][pts[1:, 1:]]\n    c_minus = c2 * w + c1 * (1 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n    #----- 45 to 90 degrees ------\n    # Mix diagonal and vertical\n    #\n    pts_plus = (isobel >= 0) & (jsobel >= 0) & (abs_isobel <= abs_jsobel)\n    pts_minus = (isobel <= 0) & (jsobel <= 0) & (abs_isobel <= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    c1 = magnitude[:, 1:][pts[:, :-1]]\n    c2 = magnitude[1:, 1:][pts[:-1, :-1]]\n    m = magnitude[pts]\n    w = abs_isobel[pts] / abs_jsobel[pts]\n    c_plus = c2 * w + c1 * (1 - w) <= m\n    c1 = magnitude[:, :-1][pts[:, 1:]]\n    c2 = magnitude[:-1, :-1][pts[1:, 1:]]\n    c_minus = c2 * w + c1 * (1 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n    #----- 90 to 135 degrees ------\n    # Mix anti-diagonal and vertical\n    #\n    pts_plus = (isobel <= 0) & (jsobel >= 0) & (abs_isobel <= abs_jsobel)\n    pts_minus = (isobel >= 0) & (jsobel <= 0) & (abs_isobel <= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    c1a = magnitude[:, 1:][pts[:, :-1]]\n    c2a = magnitude[:-1, 1:][pts[1:, :-1]]\n    m = magnitude[pts]\n    w = abs_isobel[pts] / abs_jsobel[pts]\n    c_plus = c2a * w + c1a * (1.0 - w) <= m\n    c1 = magnitude[:, :-1][pts[:, 1:]]\n    c2 = magnitude[1:, :-1][pts[:-1, 1:]]\n    c_minus = c2 * w + c1 * (1.0 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n    #----- 135 to 180 degrees ------\n    # Mix anti-diagonal and anti-horizontal\n    #\n    pts_plus = (isobel <= 0) & (jsobel >= 0) & (abs_isobel >= abs_jsobel)\n    pts_minus = (isobel >= 0) & (jsobel <= 0) & (abs_isobel >= abs_jsobel)\n    pts = pts_plus | pts_minus\n    pts = eroded_mask & pts\n    c1 = magnitude[:-1, :][pts[1:, :]]\n    c2 = magnitude[:-1, 1:][pts[1:, :-1]]\n    m = magnitude[pts]\n    w = abs_jsobel[pts] / abs_isobel[pts]\n    c_plus = c2 * w + c1 * (1 - w) <= m\n    c1 = magnitude[1:, :][pts[:-1, :]]\n    c2 = magnitude[1:, :-1][pts[:-1, 1:]]\n    c_minus = c2 * w + c1 * (1 - w) <= m\n    local_maxima[pts] = c_plus & c_minus\n\n    #\n    #---- If use_quantiles is set then calculate the thresholds to use\n    #\n    if use_quantiles:\n        if high_threshold > 1.0 or low_threshold > 1.0:\n            raise ValueError(\"Quantile thresholds must not be > 1.0\")\n        if high_threshold < 0.0 or low_threshold < 0.0:\n            raise ValueError(\"Quantile thresholds must not be < 0.0\")\n\n        high_threshold = np.percentile(magnitude, 100.0 * high_threshold)\n        low_threshold = np.percentile(magnitude, 100.0 * low_threshold)\n\n    #\n    #---- Create two masks at the two thresholds.\n    #\n    high_mask = local_maxima & (magnitude >= high_threshold)\n    low_mask = local_maxima & (magnitude >= low_threshold)\n    #\n    # Segment the low-mask, then only keep low-segments that have\n    # some high_mask component in them\n    #\n    strel = np.ones((3, 3), bool)\n    labels, count = label(low_mask, strel)\n    if count == 0:\n        return low_mask\n\n    sums = (np.array(ndi.sum(high_mask, labels,\n                             np.arange(count, dtype=np.int32) + 1),\n                     copy=False, ndmin=1))\n    good_label = np.zeros((count + 1,), bool)\n    good_label[1:] = sums > 0\n    output_mask = good_label[labels]\n    return output_mask\n```\n\nOriginal documentation:\n```python\nEdge filter an image using the Canny algorithm.\n\n    Parameters\n    -----------\n    image : 2D array\n        Greyscale input image to detect edges on; can be of any dtype.\n    sigma : float\n        Standard deviation of the Gaussian filter.\n    low_threshold : float\n        Lower bound for hysteresis thresholding (linking edges).\n        If None, low_threshold is set to 10% of dtype's max.\n    high_threshold : float\n        Upper bound for hysteresis thresholding (linking edges).\n        If None, high_threshold is set to 20% of dtype's max.\n    mask : array, dtype=bool, optional\n        Mask to limit the application of Canny to a certain area.\n    quantile_threshold : bool, optional\n        If True then treat low_threshold and high_threshold as quantiles of the\n        edge magnitude image, rather than absolute edge magnitude values. If True\n        then the thresholds must be in the range [0, 1].\n\n    Returns\n    -------\n    output : 2D array (image)\n        The binary edge map.\n\n    See also\n    --------\n    skimage.sobel\n\n    Notes\n    -----\n    The steps of the algorithm are as follows:\n\n    * Smooth the image using a Gaussian with ``sigma`` width.\n\n    * Apply the horizontal and vertical Sobel operators to get the gradients\n      within the image. The edge strength is the norm of the gradient.\n\n    * Thin potential edges to 1-pixel wide curves. First, find the normal\n      to the edge at each point. This is done by looking at the\n      signs and the relative magnitude of the X-Sobel and Y-Sobel\n      to sort the points into 4 categories: horizontal, vertical,\n      diagonal and antidiagonal. Then look in the normal and reverse\n      directions to see if the values in either of those directions are\n      greater than the point in question. Use interpolation to get a mix of\n      points instead of picking the one that's the closest to the normal.\n\n    * Perform a hysteresis thresholding: first label all points above the\n      high threshold as edges. Then recursively label any point above the\n      low threshold that is 8-connected to a labeled point as an edge.\n\n    References\n    -----------\n    Canny, J., A Computational Approach To Edge Detection, IEEE Trans.\n    Pattern Analysis and Machine Intelligence, 8:679-714, 1986\n\n    William Green's Canny tutorial\n    http://dasl.mem.drexel.edu/alumni/bGreen/www.pages.drexel.edu/_weg22/can_tut.html\n\n    Examples\n    --------\n    >>> from skimage import feature\n    >>> # Generate noisy image of a square\n    >>> im = np.zeros((256, 256))\n    >>> im[64:-64, 64:-64] = 1\n    >>> im += 0.2 * np.random.rand(*im.shape)\n    >>> # First trial with the Canny filter, with the default smoothing\n    >>> edges1 = feature.canny(im)\n    >>> # Increase the smoothing for better results\n    >>> edges2 = feature.canny(im, sigma=3)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Edge filter an image using the Canny algorithm.\n\n    Parameters\n    -----------\n    image : 2D array\n        Greyscale input image to detect edges on; can be of any dtype.\n    sigma : float\n        Standard deviation of the Gaussian filter.\n    low_threshold : float\n        Lower bound for hysteresis thresholding (linking edges).\n        If None, low_threshold is set to 10% of dtype's max.\n    high_threshold : float\n        Upper bound for hysteresis thresholding (linking edges).\n        If None, high_threshold is set to 20% of dtype's max.\n    mask : array, dtype=bool, optional\n        Mask to limit the application of Canny to a certain area.\n    use_quantiles : bool, optional\n        If True then treat low_threshold and high_threshold as quantiles of the\n        edge magnitude image, rather than absolute edge magnitude values. If True\n        then the thresholds must be in the range [0, 1].\n\n    Returns\n    -------\n    output : 2D array (image)\n        The binary edge map.\n\n    See also\n    --------\n    skimage.sobel\n\n    Notes\n    -----\n    The steps of the algorithm are as follows:\n\n    * Smooth the image using a Gaussian with ``sigma`` width.\n\n    * Apply the horizontal and vertical Sobel operators to get the gradients\n      within the image. The edge strength is the norm of the gradient.\n\n    * Thin potential edges to 1-pixel wide curves. First, find the normal\n      to the edge at each point. This is done by looking at the\n      signs and the relative magnitude of the X-Sobel and Y-Sobel\n      to sort the points into 4 categories: horizontal, vertical,\n      diagonal and antidiagonal. Then look in the normal and reverse\n      directions to see if the values in either of those directions are\n      greater than the point in question. Use interpolation to get a mix of\n      points instead of picking the one that's the closest to the normal.\n\n    * Perform a hysteresis thresholding: first label all points above the\n      high threshold as edges. Then recursively label any point above the\n      low threshold that is 8-connected to a labeled point as an edge.\n\n    References\n    -----------\n    Canny, J., A Computational Approach To Edge Detection, IEEE Trans.\n    Pattern Analysis and Machine Intelligence, 8:679-714, 1986\n\n    William Green's Canny tutorial\n    http://dasl.mem.drexel.edu/alumni/bGreen/www.pages.drexel.edu/_weg22/can_tut.html\n\n    Examples\n    --------\n    >>> from skimage import feature\n    >>> # Generate noisy image of a square\n    >>> im = np.zeros((256, 256))\n    >>> im[64:-64, 64:-64] = 1\n    >>> im += 0.2 * np.random.rand(*im.shape)\n    >>> # First trial with the Canny filter, with the default smoothing\n    >>> edges1 = feature.canny(im)\n    >>> # Increase the smoothing for better results\n    >>> edges2 = feature.canny(im, sigma=3)", "file": "skimage_feature__canny.py", "function": "canny", "project": "scikit-image", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef csc_row_median(csc):\n    \n     \n\n    \n    from ..preprocessing.imputation import _get_median\n\n    if not isinstance(csc, sp.csc_matrix):\n        raise TypeError(\"Expected matrix of CSC format, got %s\" % csc.format)\n\n    indptr = csc.indptr\n    n_samples, n_features = csc.shape\n    median = np.zeros(n_features)\n\n    for f_ind, ptr in enumerate(indptr[:-1]):\n\n        # Prevent modifying csc in place\n        data = np.copy(csc.data[ptr: indptr[f_ind + 1]])\n        nz = n_samples - data.size\n        median[f_ind] = _get_median(data, nz)\n\n    return median```\n\nUpdated code:\n```python\ndef csc_row_median(X):\n     \n\n    \n    if not isinstance(X, sp.csc_matrix):\n        raise TypeError(\"Expected matrix of CSC format, got %s\" % X.format)\n\n    indptr = X.indptr\n    n_samples, n_features = X.shape\n    median = np.zeros(n_features)\n\n    for f_ind, ptr in enumerate(indptr[:-1]):\n\n        # Prevent modifying X in place\n        data = np.copy(X.data[ptr: indptr[f_ind + 1]])\n        nz = n_samples - data.size\n        median[f_ind] = _get_median(data, nz)\n\n    return median\n```\n\nOriginal documentation:\n```python\nFind the median across axis 0 of a CSC matrix.\n    Wrapper aound the _get_median function in imputer and is equivalent\n    to doing np.median(X, axis=0)\n\n    Parameters\n    ----------\n    csc : CSC sparse matrix, shape (n_samples, n_features)\n        Input data.\n\n    Returns\n    -------\n    median : ndarray, shape = (n_features,)\n        Median.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Find the median across axis 0 of a CSC matrix.\n    It is equivalent to doing np.median(X, axis=0).\n\n    Parameters\n    ----------\n    X : CSC sparse matrix, shape (n_samples, n_features)\n        Input data.\n\n    Returns\n    -------\n    median : ndarray, shape = (n_features,)\n        Median.", "file": "sklearn_utils_sparsefuncs.py", "function": "csc_row_median", "project": "scikit-learn", "change_stats": {"code_changes": 19, "doc_changes": 7, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_timestamp_range_edges(first, last, offset, closed='left', base=0):\n    \n    \n    \n    if not all(isinstance(obj, pd.Timestamp) for obj in [first, last]):\n        raise TypeError(\"'first' and 'last' must be instances of type \"\n                        \"Timestamp\")\n\n    if isinstance(offset, Tick):\n        is_day = isinstance(offset, Day)\n        day_nanos = delta_to_nanoseconds(timedelta(1))\n\n        # #1165 and #24127\n        if (is_day and not offset.nanos % day_nanos) or not is_day:\n            return _adjust_dates_anchored(first, last, offset,\n                                          closed=closed, base=base)\n\n    else:\n        first = first.normalize()\n        last = last.normalize()\n\n    if closed == 'left':\n        first = Timestamp(offset.rollback(first))\n    else:\n        first = Timestamp(first - offset)\n\n    last = Timestamp(last + offset)\n\n    return first, last```\n\nUpdated code:\n```python\ndef _get_timestamp_range_edges(first, last, offset, closed='left', base=0):\n    \n    \n    \n    if isinstance(offset, Tick):\n        is_day = isinstance(offset, Day)\n        day_nanos = delta_to_nanoseconds(timedelta(1))\n\n        # #1165 and #24127\n        if (is_day and not offset.nanos % day_nanos) or not is_day:\n            first, last = _adjust_dates_anchored(first, last, offset,\n                                                 closed=closed, base=base)\n            if is_day and first.tz is not None:\n                # _adjust_dates_anchored assumes 'D' means 24H, but first/last\n                # might contain a DST transition (23H, 24H, or 25H).\n                # Ensure first/last snap to midnight.\n                first = first.normalize()\n                last = last.normalize()\n            return first, last\n\n    else:\n        first = first.normalize()\n        last = last.normalize()\n\n    if closed == 'left':\n        first = Timestamp(offset.rollback(first))\n    else:\n        first = Timestamp(first - offset)\n\n    last = Timestamp(last + offset)\n\n    return first, last\n```\n\nOriginal documentation:\n```python\nAdjust the `first` Timestamp to the preceeding Timestamp that resides on\n    the provided offset. Adjust the `last` Timestamp to the following\n    Timestamp that resides on the provided offset. Input Timestamps that\n    already reside on the offset will be adjusted depeding on the type of\n    offset and the `closed` parameter.\n\n    Parameters\n    ----------\n    first : pd.Timestamp\n        The beginning Timestamp of the range to be adjusted.\n    last : pd.Timestamp\n        The ending Timestamp of the range to be adjusted.\n    offset : pd.DateOffset\n        The dateoffset to which the Timestamps will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    base : int, default 0\n        The \"origin\" of the adjusted Timestamps.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Timestamp objects.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Adjust the `first` Timestamp to the preceeding Timestamp that resides on\n    the provided offset. Adjust the `last` Timestamp to the following\n    Timestamp that resides on the provided offset. Input Timestamps that\n    already reside on the offset will be adjusted depending on the type of\n    offset and the `closed` parameter.\n\n    Parameters\n    ----------\n    first : pd.Timestamp\n        The beginning Timestamp of the range to be adjusted.\n    last : pd.Timestamp\n        The ending Timestamp of the range to be adjusted.\n    offset : pd.DateOffset\n        The dateoffset to which the Timestamps will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    base : int, default 0\n        The \"origin\" of the adjusted Timestamps.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Timestamp objects.", "file": "pandas_core_resample.py", "function": "_get_timestamp_range_edges", "project": "pandas", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef vol_conical_frustum(height: float, radius_1: float, radius_2: float) -> float:\n    \n    \n    if radius_1 < 0 or radius_2 < 0 or height < 0:\n        raise ValueError(\"vol_conical_frustum() only accepts non-negative values\")\n    return (\n        1\n        / 3\n        * pi\n        * height\n        * (pow(radius_1, 2) + pow(radius_2, 2) + radius_1 * radius_2)\n    )```\n\nUpdated code:\n```python\ndef vol_conical_frustum(height: float, radius_1: float, radius_2: float) -> float:\n    \n    \n    # Volume is 1/3 * pi * height *\n    #           (radius_1 squared + radius_2 squared + radius_1 * radius_2)\n    if radius_1 < 0 or radius_2 < 0 or height < 0:\n        raise ValueError(\"vol_conical_frustum() only accepts non-negative values\")\n    return (\n        1\n        / 3\n        * pi\n        * height\n        * (pow(radius_1, 2) + pow(radius_2, 2) + radius_1 * radius_2)\n    )\n```\n\nOriginal documentation:\n```python\nCalculate the Volume of a Conical Frustum.\n    Wikipedia reference: https://en.wikipedia.org/wiki/Frustum\n    :return 1/3 * pi * height * (radius_1^2 + radius_top^2 + radius_1 * radius_2)\n    >>> vol_conical_frustum(45, 7, 28)\n    48490.482608158454\n    >>> vol_conical_frustum(1, 1, 2)\n    7.330382858376184\n    >>> vol_conical_frustum(1.6, 2.6, 3.6)\n    48.7240076620753\n    >>> vol_conical_frustum(0, 0, 0)\n    0.0\n    >>> vol_conical_frustum(-2, 2, 1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_conical_frustum() only accepts non-negative values\n    >>> vol_conical_frustum(2, -2, 1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_conical_frustum() only accepts non-negative values\n    >>> vol_conical_frustum(2, 2, -1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_conical_frustum() only accepts non-negative values\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculate the Volume of a Conical Frustum.\n    Wikipedia reference: https://en.wikipedia.org/wiki/Frustum\n\n    >>> vol_conical_frustum(45, 7, 28)\n    48490.482608158454\n    >>> vol_conical_frustum(1, 1, 2)\n    7.330382858376184\n    >>> vol_conical_frustum(1.6, 2.6, 3.6)\n    48.7240076620753\n    >>> vol_conical_frustum(0, 0, 0)\n    0.0\n    >>> vol_conical_frustum(-2, 2, 1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_conical_frustum() only accepts non-negative values\n    >>> vol_conical_frustum(2, -2, 1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_conical_frustum() only accepts non-negative values\n    >>> vol_conical_frustum(2, 2, -1)\n    Traceback (most recent call last):\n        ...\n    ValueError: vol_conical_frustum() only accepts non-negative values", "file": "maths_volume.py", "function": "vol_conical_frustum", "project": "Python", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef completions(self, text: str, offset: int)->Iterator[Completion]:\n        \n        \n\n        \n        warnings.warn(\"_complete is a provisional API (as of IPython 6.0). \"\n                      \"It may change without warnings. \"\n                      \"Use in corresponding context manager.\",\n                      category=ProvisionalCompleterWarning, stacklevel=2)\n\n        # Possible Improvements / Known limitation\n        ##########################################\n        # Completions may be identical even if they have different ranges and\n        # text. For example:\n        # >>> a=1\n        # >>> a.<tab>\n        # May returns:\n        #  - `a.real` from 0 to 2\n        #  - `.real` from 1 to 2\n        # the current code does not (yet) check for such equivalence\n        seen = set()\n        for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):\n            if c and (c in seen):\n                continue\n            yield c\n            seen.add(c)```\n\nUpdated code:\n```python\ndef completions(self, text: str, offset: int)->Iterator[Completion]:\n        \n        \n        \n        warnings.warn(\"_complete is a provisional API (as of IPython 6.0). \"\n                      \"It may change without warnings. \"\n                      \"Use in corresponding context manager.\",\n                      category=ProvisionalCompleterWarning, stacklevel=2)\n\n        seen = set()\n        for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):\n            if c and (c in seen):\n                continue\n            yield c\n            seen.add(c)\n```\n\nOriginal documentation:\n```python\nReturns an iterator over the possible completions\n\n        .. warning:: Unstable\n            \n            This function is unstable, API may change without warning.\n            It will also raise unless use in proper context manager.\n\n        Parameters\n        ----------\n        \n        text:str\n            Full text of the current input, multi line string.\n        offset:int\n            Integer representing the position of the cursor in ``text``. Offset\n            is 0-based indexed.\n\n        Yields\n        ------\n            :any:`Completion` object\n\n\n        The cursor on a text can either be seen as being \"in between\"\n        characters or \"On\" a character depending on the interface visible to\n        the user. For consistency the cursor being on \"in between\" characters X\n        and Y is equivalent to the cursor being \"on\" character Y, that is to say\n        the character the cursor is on is considered as being after the cursor.\n\n        Combining characters may span more that one position in the\n        text.\n\n\n        .. note::\n\n            If ``IPCompleter.debug`` is :any:`True` will yield a ``--jedi/ipython--``\n            fake Completion token to distinguish completion returned by Jedi\n            and usual IPython completion.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns an iterator over the possible completions\n\n        .. warning:: Unstable\n            \n            This function is unstable, API may change without warning.\n            It will also raise unless use in proper context manager.\n\n        Parameters\n        ----------\n        \n        text:str\n            Full text of the current input, multi line string.\n        offset:int\n            Integer representing the position of the cursor in ``text``. Offset\n            is 0-based indexed.\n\n        Yields\n        ------\n            :any:`Completion` object\n\n\n        The cursor on a text can either be seen as being \"in between\"\n        characters or \"On\" a character depending on the interface visible to\n        the user. For consistency the cursor being on \"in between\" characters X\n        and Y is equivalent to the cursor being \"on\" character Y, that is to say\n        the character the cursor is on is considered as being after the cursor.\n\n        Combining characters may span more that one position in the\n        text.\n\n\n        .. note::\n\n            If ``IPCompleter.debug`` is :any:`True` will yield a ``--jedi/ipython--``\n            fake Completion token to distinguish completion returned by Jedi\n            and usual IPython completion.\n\n        .. note::\n            \n            Completions are not completely deduplicated yet. If identical\n            completions are coming from different sources this function does not\n            ensure that each completion object will only be present once.", "file": "IPython_core_completer.py", "function": "IPCompleter.completions", "project": "ipython", "change_stats": {"code_changes": 13, "doc_changes": 8, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _handle_m2m_field_node(self, node, field):\n        \n        \n        \n        return [field.rel.to._meta.pk.to_python(\n                    c.getAttribute(\"pk\").encode(self.encoding)) \n                    for c in node.getElementsByTagName(\"object\")]```\n\nUpdated code:\n```python\ndef _handle_m2m_field_node(self, node, field):\n        \n        \n        \n        return [field.rel.to._meta.pk.to_python(\n                    c.getAttribute(\"pk\"))\n                    for c in node.getElementsByTagName(\"object\")]\n```\n\nOriginal documentation:\n```python\nHandle a <field> node for a ManyToManyField\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Handle a <field> node for a ManyToManyField.", "file": "django_core_serializers_xml_serializer.py", "function": "Deserializer._handle_m2m_field_node", "project": "django", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, output_prefix, corpus, num_features, num_best=None, chunksize=512, shardsize=5000):\n        \n        \n\n        \n        self.output_prefix = output_prefix\n        self.num_features = num_features\n        self.num_best = num_best\n        self.normalize = True\n        self.chunksize = int(chunksize)\n        self.shardsize = shardsize\n        self.shards = []\n        self.fresh_docs, self.fresh_nnz = [], 0\n\n        if corpus is not None:\n            self.add_documents(corpus)```\n\nUpdated code:\n```python\ndef __init__(self, output_prefix, corpus, num_features, num_best=None, chunksize=1024, shardsize=16384):\n        \n        \n\n        \n        if output_prefix:\n            outfile, self.output_prefix = tempfile.mkstemp(prefix='tmpindex')\n            outfile.close()\n        else:\n            self.output_prefix = output_prefix\n        self.num_features = num_features\n        self.num_best = num_best\n        self.normalize = True\n        self.chunksize = int(chunksize)\n        self.shardsize = shardsize\n        self.shards = []\n        self.fresh_docs, self.fresh_nnz = [], 0\n\n        if corpus is not None:\n            self.add_documents(corpus)\n```\n\nOriginal documentation:\n```python\nConstruct the index from `corpus`. The index can be later extended by calling\n        the `add_documents` method. Documents are split into shards of `shardsize`\n        documents each, converted to a matrix (for fast BLAS calls) and stored to disk\n        under `output_prefix.shard_number` (=you need write access to that location).\n\n        `shardsize` should be chosen so that a `shardsize x chunksize` matrix of floats\n        fits comfortably into main memory.\n\n        `num_features` is the number of features in the `corpus` (e.g. size of the\n        dictionary, or the number of latent topics for latent semantic models).\n\n        If `num_best` is left unspecified, similarity queries will return a full\n        vector with one float for every document in the index:\n\n        >>> index = Similarity('/tmp/index', corpus, num_features=400) # if corpus has 7 documents...\n        >>> index[query] # ... then result will have 7 floats\n        [0.0, 0.0, 0.2, 0.13, 0.8, 0.0, 0.1]\n\n        If `num_best` is set, queries return only the `num_best` most similar documents:\n\n        >>> index.num_best = 3\n        >>> index[query] # return at most \"num_best\" (index_of_document, similarity) tuples\n        [(4, 0.8), (2, 0.13), (3, 0.13)]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Construct the index from `corpus`. The index can be later extended by calling\n        the `add_documents` method. Documents are split into shards of `shardsize`\n        documents each, converted to a matrix (for fast BLAS calls) and stored to disk\n        under `output_prefix.shard_number` (=you need write access to that location).\n        If you don't specify an output prefix, a random filename in temp will be used.\n\n        `shardsize` should be chosen so that a `shardsize x chunksize` matrix of floats\n        fits comfortably into main memory.\n\n        `num_features` is the number of features in the `corpus` (e.g. size of the\n        dictionary, or the number of latent topics for latent semantic models).\n\n        If `num_best` is left unspecified, similarity queries will return a full\n        vector with one float for every document in the index:\n\n        >>> index = Similarity('/path/to/index', corpus, num_features=400) # if corpus has 7 documents...\n        >>> index[query] # ... then result will have 7 floats\n        [0.0, 0.0, 0.2, 0.13, 0.8, 0.0, 0.1]\n\n        If `num_best` is set, queries return only the `num_best` most similar documents:\n\n        >>> index.num_best = 3\n        >>> index[query] # return at most \"num_best\" of `(index_of_document, similarity)` tuples\n        [(4, 0.8), (2, 0.13), (3, 0.13)]", "file": "gensim_similarities_docsim.py", "function": "Similarity.__init__", "project": "gensim", "change_stats": {"code_changes": 10, "doc_changes": 7, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef repeat(self, n_repeat, mode='init'):\n        \n\n        \n        if n_repeat <= 0:\n            return ChainList()\n        if mode not in ['init', 'copy', 'share']:\n            raise ValueError(\n                'The \\'mode\\' argument should be either \\'init\\','\n                '\\'copy\\', or \\'share\\'. But {} was given.'.format(mode))\n        ret = ChainList()\n        link = self\n        for _ in range(n_repeat):\n            if mode in ['init', 'copy']:\n                link = copy.deepcopy(link)\n            if mode == 'init':\n                for param in link.params(include_uninit=False):\n                    param.initialize(param.shape)\n            ret.add_link(link)\n        return ret```\n\nUpdated code:\n```python\ndef repeat(self, n_repeat, mode='init'):\n        \n\n        \n        ret = chainer.Sequential()\n        if n_repeat <= 0:\n            return ret\n        if mode not in ['init', 'copy', 'share']:\n            raise ValueError(\n                'The \\'mode\\' argument should be either \\'init\\','\n                '\\'copy\\', or \\'share\\'. But {} was given.'.format(mode))\n        link = self\n        for _ in range(n_repeat):\n            if mode in ['init', 'copy']:\n                link = copy.deepcopy(link)\n            if mode == 'init':\n                for param in link.params(include_uninit=False):\n                    param.initialize(param.shape)\n            ret.append(link)\n        return ret\n```\n\nOriginal documentation:\n```python\nRepeat myself multiple times to make a :class:`~chainer.ChainList`.\n\n        This method returns a :class:`~chainer.ChainList` object which has\n        a same :class:`~chainer.Link` multiple times repeatedly. The ``mode``\n        argument means how to copy myself to repeat.\n\n        .. admonition:: Example\n\n            You can repeat a same link multiple times to create longer\n            :class:`~chainer.ChainList` block like this:\n\n                class ConvBNReLU(chainer.Chain):\n\n                    def __init__(self):\n                        super(ConvBNReLU, self).__init__()\n                        with self.init_scope():\n                            self.conv = L.Convolution2D(\n                                None, 64, 3, 1, 1, nobias=True)\n                            self.bn = L.BatchNormalization(64)\n                    \n                    def __call__(self, x):\n                        return F.relu(self.bn(self.conv(x)))\n                \n                net = ConvBNReLU().repeat(16, mode='init')\n\n            The ``net`` object contains 16 blocks, each of which is\n            ``ConvBNReLU``. And the ``mode`` was ``init``, so each block\n            is re-initialized with different parameters. If you give\n            ``copy`` to this argument, each block has same values for its\n            paramters but its object ID is different from others. If it is\n            ``share``, each block is same to others in terms of not only\n            paramters but also the object IDs because they are shallow-copied,\n            so that when the parameter of one block is changed, all the\n            parameters in the others also change.\n\n        Args:\n            n_repeat (int): Number of times to repeat.\n            mode (str): It should be either ``init``, ``copy``, or ``share``.\n                ``init`` means paramters of each repeated element in the\n                returned :class:`~chainer.ChainList` will be re-initialized,\n                so that all elements have different initial parameters.\n                ``copy`` means that the parameters will not be re-initialized\n                but object itself will be deep-copied, so that all elements\n                have same initial parameters but can be changed independently.\n                ``share`` means all the elements which construct the resulting\n                :class:`~chainer.ChainList` object are same object because they\n                are shallow-copied, so that all parameters of elements change\n                synchronously.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Repeat myself multiple times to make a :class:`~chainer.Sequential`.\n\n        This method returns a :class:`~chainer.Sequential` object which has\n        a same :class:`~chainer.Link` multiple times repeatedly. The ``mode``\n        argument means how to copy myself to repeat.\n\n        .. admonition:: Example\n\n            You can repeat a same link multiple times to create longer\n            :class:`~chainer.Sequential` block like this:\n\n                class ConvBNReLU(chainer.Chain):\n\n                    def __init__(self):\n                        super(ConvBNReLU, self).__init__()\n                        with self.init_scope():\n                            self.conv = L.Convolution2D(\n                                None, 64, 3, 1, 1, nobias=True)\n                            self.bn = L.BatchNormalization(64)\n\n                    def __call__(self, x):\n                        return F.relu(self.bn(self.conv(x)))\n\n                net = ConvBNReLU().repeat(16, mode='init')\n\n            The ``net`` object contains 16 blocks, each of which is\n            ``ConvBNReLU``. And the ``mode`` was ``init``, so each block\n            is re-initialized with different parameters. If you give\n            ``copy`` to this argument, each block has same values for its\n            paramters but its object ID is different from others. If it is\n            ``share``, each block is same to others in terms of not only\n            paramters but also the object IDs because they are shallow-copied,\n            so that when the parameter of one block is changed, all the\n            parameters in the others also change.\n\n        Args:\n            n_repeat (int): Number of times to repeat.\n            mode (str): It should be either ``init``, ``copy``, or ``share``.\n                ``init`` means paramters of each repeated element in the\n                returned :class:`~chainer.Sequential` will be re-initialized,\n                so that all elements have different initial parameters.\n                ``copy`` means that the parameters will not be re-initialized\n                but object itself will be deep-copied, so that all elements\n                have same initial parameters but can be changed independently.\n                ``share`` means all the elements which construct the resulting\n                :class:`~chainer.Sequential` object are same object because\n                they are shallow-copied, so that all parameters of elements\n                share each other.", "file": "chainer_link.py", "function": "Link.repeat", "project": "chainer", "change_stats": {"code_changes": 8, "doc_changes": 20, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate_histogram_visualization(self, feature_filter: str, module_fqn_filter: str = \"\"):\n        \n        \n        pass```\n\nUpdated code:\n```python\ndef generate_histogram_visualization(self, feature_filter: str, module_fqn_filter: str = \"\", num_bins: int = 10):\n        \n        \n        # checks if we have matplotlib and let's user know to install it if don't\n        if not got_matplotlib:\n            print(\"make sure to install matplotlib and try again.\")\n            return None\n\n        # get the x and y data and if per channel\n        x_data, y_data, data_per_channel = self._get_plottable_data(feature_filter, module_fqn_filter)\n\n        # for histogram, we just care about plotting the y data\n        # plot based on whether data is per channel or not\n        fig = plt.figure()\n        ax = plt.subplot()\n        ax.set_xlabel(feature_filter)\n        ax.set_ylabel(\"Frequency\")\n        ax.set_title(feature_filter + \" Histogram\")\n\n        if data_per_channel:\n            # set the legend as well\n            # combine all the data\n            all_data = []\n            for index, channel_info in enumerate(y_data):\n                all_data.extend(channel_info)\n            val, bins, _ = plt.hist(\n                all_data,\n                bins=num_bins,\n                stacked=True,\n                rwidth=0.8,\n            )\n            ax.legend(loc='upper right')\n            plt.xticks(bins)\n        else:\n            val, bins, _ = plt.hist(\n                y_data,\n                bins=num_bins,\n                stacked=False,\n                rwidth=0.8,\n            )\n            plt.xticks(bins)\n\n        plt.show()\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Takes in a feature and optional module_filter and plots the histogram of desired data.\n\n        Note:\n            Only features in the report that have tensor value data can be viewed as a histogram\n\n        Args:\n            feature_filter (str, optional): Filters the features presented to only those that\n                contain this filter substring\n                Default = \"\", results in all the features being printed\n            module_fqn_filter (str, optional): Only includes modules that contains this string\n                Default = \"\", results in all the modules in the reports to be visible in the table\n\n        Example Use:\n            >>> mod_report_visualizer.generategenerate_histogram_visualization_plot_visualization(\n                    feature_filter = \"per_channel_min\",\n                    module_fqn_filter = \"block1\"\n                )\n            # outputs histogram of per_channel_min information for all modules in block1 of model\n                information is gathered across all channels for all modules in block 1 for the\n                per_channel_min and is displayed in a histogram of equally sized bins\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Takes in a feature and optional module_filter and plots the histogram of desired data.\n\n        Note:\n            Only features in the report that have tensor value data can be viewed as a histogram\n            If you want to plot a histogram from all the channel values of a specific feature for\n                a specific model, make sure to specify both the model and the feature properly\n                in the filters and you should be able to see a distribution of the channel data\n\n        Args:\n            feature_filter (str, optional): Filters the features presented to only those that\n                contain this filter substring\n                Default = \"\", results in all the features being printed\n            module_fqn_filter (str, optional): Only includes modules that contains this string\n                Default = \"\", results in all the modules in the reports to be visible in the table\n            num_bins (int, optional): The number of bins to create the histogram with\n                Default = 10, the values will be split into 10 equal sized bins\n\n        Example Use:\n            >>> mod_report_visualizer.generategenerate_histogram_visualization_plot_visualization(\n                    feature_filter = \"per_channel_min\",\n                    module_fqn_filter = \"block1\"\n                )\n            # outputs histogram of per_channel_min information for all modules in block1 of model\n                information is gathered across all channels for all modules in block 1 for the\n                per_channel_min and is displayed in a histogram of equally sized bins", "file": "torch_ao_quantization_fx__model_report_model_report_visualizer.py", "function": "ModelReportVisualizer.generate_histogram_visualization", "project": "pytorch", "change_stats": {"code_changes": 45, "doc_changes": 7, "total_changes": 52}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convert_from_tiff_to_pdf(self):\n        \n        \n        \n        # Nothing to do, pdf_file is already assigned correctly\n        if self.mime != \"image/tiff\":\n            return\n\n        self.pdf_file = convert_from_tiff_to_pdf(self.file, Path(self.temp_dir.name))```\n\nUpdated code:\n```python\ndef convert_from_tiff_to_pdf(self):\n        \n        \n        \n        # Nothing to do, pdf_file is already assigned correctly\n        if self.mime != \"image/tiff\" or self._tiff_conversion_done:\n            return\n\n        self._tiff_conversion_done = True\n        self.pdf_file = convert_from_tiff_to_pdf(self.file, Path(self.temp_dir.name))\n```\n\nOriginal documentation:\n```python\nMay convert a TIFF image into a PDF, if the input is a TIFF\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "May convert a TIFF image into a PDF, if the input is a TIFF and\n        the TIFF has not been made into a PDF", "file": "src_documents_barcodes.py", "function": "BarcodeReader.convert_from_tiff_to_pdf", "project": "paperless-ngx", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef filings(\n        self,\n        start_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"Start date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        end_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"End date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        form_type: Annotated[\n            Optional[str],\n            OpenBBCustomParameter(\n                description=\"Fuzzy filter by form type. E.g. 10-K, 10, 8, 6-K, etc.\"\n            ),\n        ] = None,\n        limit: Annotated[\n            int,\n            OpenBBCustomParameter(description=\"The number of data entries to return.\"),\n        ] = 100,\n        provider: Optional[Literal[\"fmp\"]] = None,\n        **kwargs\n    ) -> OBBject[List[Data]]:\n        \n          # noqa: E501\n\n        inputs = filter_inputs(\n            provider_choices={\n                \"provider\": provider,\n            },\n            standard_params={\n                \"start_date\": start_date,\n                \"end_date\": end_date,\n                \"form_type\": form_type,\n                \"limit\": limit,\n            },\n            extra_params=kwargs,\n        )\n\n        return self._run(\n            \"/equity/discovery/filings\",\n            **inputs,\n        )```\n\nUpdated code:\n```python\ndef filings(\n        self,\n        start_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"Start date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        end_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"End date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        form_type: Annotated[\n            Optional[str],\n            OpenBBCustomParameter(\n                description=\"Filter by form type. Visit https://www.sec.gov/forms for a list of supported form types.\"\n            ),\n        ] = None,\n        limit: Annotated[\n            int,\n            OpenBBCustomParameter(description=\"The number of data entries to return.\"),\n        ] = 100,\n        provider: Optional[Literal[\"fmp\"]] = None,\n        **kwargs\n    ) -> OBBject[List[Data]]:\n        \n          # noqa: E501\n\n        inputs = filter_inputs(\n            provider_choices={\n                \"provider\": provider,\n            },\n            standard_params={\n                \"start_date\": start_date,\n                \"end_date\": end_date,\n                \"form_type\": form_type,\n                \"limit\": limit,\n            },\n            extra_params=kwargs,\n        )\n\n        return self._run(\n            \"/equity/discovery/filings\",\n            **inputs,\n        )\n```\n\nOriginal documentation:\n```python\nGet the most-recent filings submitted to the SEC.\n\n        Parameters\n        ----------\n        start_date : Optional[datetime.date]\n            Start date of the data, in YYYY-MM-DD format.\n        end_date : Optional[datetime.date]\n            End date of the data, in YYYY-MM-DD format.\n        form_type : Optional[str]\n            Fuzzy filter by form type. E.g. 10-K, 10, 8, 6-K, etc.\n        limit : int\n            The number of data entries to return.\n        provider : Optional[Literal['fmp']]\n            The provider to use for the query, by default None.\n            If None, the provider specified in defaults is selected or 'fmp' if there is\n            no default.\n        is_done : Optional[Literal['true', 'false']]\n            Flag for whether or not the filing is done. (provider: fmp)\n\n        Returns\n        -------\n        OBBject\n            results : List[DiscoveryFilings]\n                Serializable results.\n            provider : Optional[Literal['fmp']]\n                Provider name.\n            warnings : Optional[List[Warning_]]\n                List of warnings.\n            chart : Optional[Chart]\n                Chart object.\n            extra: Dict[str, Any]\n                Extra info.\n\n        DiscoveryFilings\n        ----------------\n        timestamp : datetime\n            The timestamp from when the filing was accepted.\n        symbol : Optional[str]\n            Symbol representing the entity requested in the data.\n        cik : str\n            The CIK of the filing\n        title : str\n            The title of the filing\n        form_type : str\n            The form type of the filing\n        url : Optional[str]\n            The URL of the filing\n        is_done : Optional[Literal['True', 'False']]\n            Whether or not the filing is done. (provider: fmp)\n\n        Example\n        -------\n        >>> from openbb import obb\n        >>> obb.equity.discovery.filings(limit=100)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the most-recent filings submitted to the SEC.\n\n        Parameters\n        ----------\n        start_date : Optional[datetime.date]\n            Start date of the data, in YYYY-MM-DD format.\n        end_date : Optional[datetime.date]\n            End date of the data, in YYYY-MM-DD format.\n        form_type : Optional[str]\n            Filter by form type. Visit https://www.sec.gov/forms for a list of supported form types.\n        limit : int\n            The number of data entries to return.\n        provider : Optional[Literal['fmp']]\n            The provider to use for the query, by default None.\n            If None, the provider specified in defaults is selected or 'fmp' if there is\n            no default.\n        is_done : Optional[bool]\n            Flag for whether or not the filing is done. (provider: fmp)\n\n        Returns\n        -------\n        OBBject\n            results : List[DiscoveryFilings]\n                Serializable results.\n            provider : Optional[Literal['fmp']]\n                Provider name.\n            warnings : Optional[List[Warning_]]\n                List of warnings.\n            chart : Optional[Chart]\n                Chart object.\n            extra: Dict[str, Any]\n                Extra info.\n\n        DiscoveryFilings\n        ----------------\n        symbol : str\n            Symbol representing the entity requested in the data.\n        cik : str\n            Central Index Key (CIK) for the requested entity.\n        title : str\n            Title of the filing.\n        date : datetime\n            The date of the data.\n        form_type : str\n            The form type of the filing\n        link : str\n            URL to the filing page on the SEC site.\n\n        Example\n        -------\n        >>> from openbb import obb\n        >>> obb.equity.discovery.filings(limit=100)", "file": "openbb_platform_openbb_package_equity_discovery.py", "function": "ROUTER_equity_discovery.filings", "project": "OpenBBTerminal", "change_stats": {"code_changes": 4, "doc_changes": 22, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, max_len, max_episode_step,\n                 state_dim, action_dim, if_on_policy, if_per, rollout_num):\n        \n        \n        self.now_len = 0\n        self.max_len = max_len\n        self.rollout_num = rollout_num\n\n        if if_on_policy:\n            buf_max_len = max_len + max_episode_step  # todo * 2\n        else:\n            buf_max_len = max_len // rollout_num\n\n        self.buffers = [ReplayBuffer(buf_max_len, state_dim, action_dim, if_on_policy, if_per)\n                        for _ in range(rollout_num)]```\n\nUpdated code:\n```python\ndef __init__(self, max_len, worker_num,\n                 state_dim, action_dim, if_discrete, if_on_policy, if_per_or_gae):\n        \n        \n        self.now_len = 0\n        self.max_len = max_len\n        self.worker_num = worker_num\n\n        buf_max_len = max_len // worker_num\n        self.buffers = [ReplayBuffer(buf_max_len, state_dim, action_dim, if_discrete, if_on_policy, if_per_or_gae)\n                        for _ in range(worker_num)]\n```\n\nOriginal documentation:\n```python\nExperience Replay Buffer for Multiple Processing\n\n        `int rollout_num` the rollout workers number\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Experience Replay Buffer for Multiple Processing\n\n        `int max_len` the max_len of ReplayBuffer, not the total len of ReplayBufferMP\n        `int worker_num` the rollout workers number", "file": "ElegantRL_replay.py", "function": "ReplayBufferMP.__init__", "project": "Python", "change_stats": {"code_changes": 18, "doc_changes": 5, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef corr(self, other, method='pearson'):\n        \n        \n        \n        this, other = self.align(other, join='inner', copy=False)\n        return nanops.nancorr(this.values, other.values, method=method)```\n\nUpdated code:\n```python\ndef corr(self, other, method='pearson',\n             min_periods=None):\n        \n        \n        \n        this, other = self.align(other, join='inner', copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancorr(this.values, other.values, method=method,\n                              min_periods=min_periods)\n```\n\nOriginal documentation:\n```python\nCompute correlation two Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n        method : {'pearson', 'kendall', 'spearman'}\n            pearson : standard correlation coefficient\n            kendall : Kendall Tau correlation coefficient\n            spearman : Spearman rank correlation\n\n        Returns\n        -------\n        correlation : float\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute correlation two Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n        method : {'pearson', 'kendall', 'spearman'}\n            pearson : standard correlation coefficient\n            kendall : Kendall Tau correlation coefficient\n            spearman : Spearman rank correlation\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result\n\n\n        Returns\n        -------\n        correlation : float", "file": "pandas_core_series.py", "function": "Series.corr", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 5, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unload_textual_inversion(\n        self,\n        tokens: Optional[Union[str, List[str]]] = None,\n    ):\n        \n        \n\n        tokenizer = getattr(self, \"tokenizer\", None)\n        text_encoder = getattr(self, \"text_encoder\", None)\n\n        # Get textual inversion tokens and ids\n        token_ids = []\n        last_special_token_id = None\n\n        if tokens:\n            if isinstance(tokens, str):\n                tokens = [tokens]\n            for added_token_id, added_token in tokenizer.added_tokens_decoder.items():\n                if not added_token.special:\n                    if added_token.content in tokens:\n                        token_ids.append(added_token_id)\n                else:\n                    last_special_token_id = added_token_id\n            if len(token_ids) == 0:\n                raise ValueError(\"No tokens to remove found\")\n        else:\n            tokens = []\n            for added_token_id, added_token in tokenizer.added_tokens_decoder.items():\n                if not added_token.special:\n                    token_ids.append(added_token_id)\n                    tokens.append(added_token.content)\n                else:\n                    last_special_token_id = added_token_id\n\n        # Delete from tokenizer\n        for token_id, token_to_remove in zip(token_ids, tokens):\n            del tokenizer._added_tokens_decoder[token_id]\n            del tokenizer._added_tokens_encoder[token_to_remove]\n\n        # Make all token ids sequential in tokenizer\n        key_id = 1\n        for token_id in tokenizer.added_tokens_decoder:\n            if token_id > last_special_token_id and token_id > last_special_token_id + key_id:\n                token = tokenizer._added_tokens_decoder[token_id]\n                tokenizer._added_tokens_decoder[last_special_token_id + key_id] = token\n                del tokenizer._added_tokens_decoder[token_id]\n                tokenizer._added_tokens_encoder[token.content] = last_special_token_id + key_id\n                key_id += 1\n        tokenizer._update_trie()\n\n        # Delete from text encoder\n        text_embedding_dim = text_encoder.get_input_embeddings().embedding_dim\n        temp_text_embedding_weights = text_encoder.get_input_embeddings().weight\n        text_embedding_weights = temp_text_embedding_weights[: last_special_token_id + 1]\n        to_append = []\n        for i in range(last_special_token_id + 1, temp_text_embedding_weights.shape[0]):\n            if i not in token_ids:\n                to_append.append(temp_text_embedding_weights[i].unsqueeze(0))\n        if len(to_append) > 0:\n            to_append = torch.cat(to_append, dim=0)\n            text_embedding_weights = torch.cat([text_embedding_weights, to_append], dim=0)\n        text_embeddings_filtered = nn.Embedding(text_embedding_weights.shape[0], text_embedding_dim)\n        text_embeddings_filtered.weight.data = text_embedding_weights\n        text_encoder.set_input_embeddings(text_embeddings_filtered)```\n\nUpdated code:\n```python\ndef unload_textual_inversion(\n        self,\n        tokens: Optional[Union[str, List[str]]] = None,\n        tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n        text_encoder: Optional[\"PreTrainedModel\"] = None,\n    ):\n        \n        \n\n        tokenizer = tokenizer or getattr(self, \"tokenizer\", None)\n        text_encoder = text_encoder or getattr(self, \"text_encoder\", None)\n\n        # Get textual inversion tokens and ids\n        token_ids = []\n        last_special_token_id = None\n\n        if tokens:\n            if isinstance(tokens, str):\n                tokens = [tokens]\n            for added_token_id, added_token in tokenizer.added_tokens_decoder.items():\n                if not added_token.special:\n                    if added_token.content in tokens:\n                        token_ids.append(added_token_id)\n                else:\n                    last_special_token_id = added_token_id\n            if len(token_ids) == 0:\n                raise ValueError(\"No tokens to remove found\")\n        else:\n            tokens = []\n            for added_token_id, added_token in tokenizer.added_tokens_decoder.items():\n                if not added_token.special:\n                    token_ids.append(added_token_id)\n                    tokens.append(added_token.content)\n                else:\n                    last_special_token_id = added_token_id\n\n        # Delete from tokenizer\n        for token_id, token_to_remove in zip(token_ids, tokens):\n            del tokenizer._added_tokens_decoder[token_id]\n            del tokenizer._added_tokens_encoder[token_to_remove]\n\n        # Make all token ids sequential in tokenizer\n        key_id = 1\n        for token_id in tokenizer.added_tokens_decoder:\n            if token_id > last_special_token_id and token_id > last_special_token_id + key_id:\n                token = tokenizer._added_tokens_decoder[token_id]\n                tokenizer._added_tokens_decoder[last_special_token_id + key_id] = token\n                del tokenizer._added_tokens_decoder[token_id]\n                tokenizer._added_tokens_encoder[token.content] = last_special_token_id + key_id\n                key_id += 1\n        tokenizer._update_trie()\n\n        # Delete from text encoder\n        text_embedding_dim = text_encoder.get_input_embeddings().embedding_dim\n        temp_text_embedding_weights = text_encoder.get_input_embeddings().weight\n        text_embedding_weights = temp_text_embedding_weights[: last_special_token_id + 1]\n        to_append = []\n        for i in range(last_special_token_id + 1, temp_text_embedding_weights.shape[0]):\n            if i not in token_ids:\n                to_append.append(temp_text_embedding_weights[i].unsqueeze(0))\n        if len(to_append) > 0:\n            to_append = torch.cat(to_append, dim=0)\n            text_embedding_weights = torch.cat([text_embedding_weights, to_append], dim=0)\n        text_embeddings_filtered = nn.Embedding(text_embedding_weights.shape[0], text_embedding_dim)\n        text_embeddings_filtered.weight.data = text_embedding_weights\n        text_encoder.set_input_embeddings(text_embeddings_filtered)\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Unload Textual Inversion embeddings from the text encoder of [`StableDiffusionPipeline`]\n\n        Example:\n        ```py\n        from diffusers import AutoPipelineForText2Image\n        import torch\n\n        pipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n        # Example 1\n        pipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\n        pipeline.load_textual_inversion(\"sd-concepts-library/moeb-style\")\n\n        # Remove all token embeddings\n        pipeline.unload_textual_inversion()\n\n        # Example 2\n        pipeline.load_textual_inversion(\"sd-concepts-library/moeb-style\")\n        pipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\n\n        # Remove just one token\n        pipeline.unload_textual_inversion(\"<moe-bius>\")\n        ```\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Unload Textual Inversion embeddings from the text encoder of [`StableDiffusionPipeline`]\n\n        Example:\n        ```py\n        from diffusers import AutoPipelineForText2Image\n        import torch\n\n        pipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n        # Example 1\n        pipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\n        pipeline.load_textual_inversion(\"sd-concepts-library/moeb-style\")\n\n        # Remove all token embeddings\n        pipeline.unload_textual_inversion()\n\n        # Example 2\n        pipeline.load_textual_inversion(\"sd-concepts-library/moeb-style\")\n        pipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\n\n        # Remove just one token\n        pipeline.unload_textual_inversion(\"<moe-bius>\")\n\n        # Example 3: unload from SDXL\n        pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n        embedding_path = hf_hub_download(repo_id=\"linoyts/web_y2k\", filename=\"web_y2k_emb.safetensors\", repo_type=\"model\")\n\n        # load embeddings to the text encoders\n        state_dict = load_file(embedding_path)\n\n        # load embeddings of text_encoder 1 (CLIP ViT-L/14)\n        pipeline.load_textual_inversion(state_dict[\"clip_l\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipeline.text_encoder, tokenizer=pipeline.tokenizer)\n        # load embeddings of text_encoder 2 (CLIP ViT-G/14)\n        pipeline.load_textual_inversion(state_dict[\"clip_g\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipeline.text_encoder_2, tokenizer=pipeline.tokenizer_2)\n\n        # Unload explicitly from both text encoders abd tokenizers\n        pipeline.unload_textual_inversion(tokens=[\"<s0>\", \"<s1>\"], text_encoder=pipeline.text_encoder, tokenizer=pipeline.tokenizer)\n        pipeline.unload_textual_inversion(tokens=[\"<s0>\", \"<s1>\"], text_encoder=pipeline.text_encoder_2, tokenizer=pipeline.tokenizer_2)\n\n        ```", "file": "src_diffusers_loaders_textual_inversion.py", "function": "TextualInversionLoaderMixin.unload_textual_inversion", "project": "diffusers", "change_stats": {"code_changes": 8, "doc_changes": 19, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef submit(\n    entity: Union[Scenario, Sequence, Task],\n    force: bool = False,\n    wait: bool = False,\n    timeout: Optional[Union[float, int]] = None,\n) -> Optional[Union[Job, List[Job]]]:\n    \n    \n    if isinstance(entity, Scenario):\n        return _ScenarioManagerFactory._build_manager()._submit(entity, force=force, wait=wait, timeout=timeout)\n    if isinstance(entity, Sequence):\n        return _SequenceManagerFactory._build_manager()._submit(entity, force=force, wait=wait, timeout=timeout)\n    if isinstance(entity, Task):\n        return _TaskManagerFactory._build_manager()._submit(entity, force=force, wait=wait, timeout=timeout)\n    return None```\n\nUpdated code:\n```python\ndef submit(\n    entity: Union[Scenario, Sequence, Task],\n    force: bool = False,\n    wait: bool = False,\n    timeout: Optional[Union[float, int]] = None,\n    **properties,\n) -> Submission:\n    \n    \n    if isinstance(entity, Scenario):\n        return _ScenarioManagerFactory._build_manager()._submit(\n            entity, force=force, wait=wait, timeout=timeout, **properties\n        )\n    if isinstance(entity, Sequence):\n        return _SequenceManagerFactory._build_manager()._submit(\n            entity, force=force, wait=wait, timeout=timeout, **properties\n        )\n    if isinstance(entity, Task):\n        return _TaskManagerFactory._build_manager()._submit(\n            entity, force=force, wait=wait, timeout=timeout, **properties\n        )\n    return None\n```\n\nOriginal documentation:\n```python\nSubmit a scenario, sequence or task entity for execution.\n\n    This function submits the given entity for execution and returns the created job(s).\n\n    If the entity is a sequence or a scenario, all the tasks of the entity are\n    submitted for execution.\n\n    Parameters:\n        entity (Union[Scenario^, Sequence^, Task^]): The scenario, sequence or task to submit.\n        force (bool): If True, the execution is forced even if for skippable tasks.\n        wait (bool): Wait for the orchestrated jobs created from the submission to be finished\n            in asynchronous mode.\n        timeout (Union[float, int]): The optional maximum number of seconds to wait\n            for the jobs to be finished before returning.\n\n    Returns:\n        The created `Job^` or a collection of the created `Job^` depends on the submitted entity.\n\n            - If a `Scenario^` or a `Sequence^` is provided, it will return a list of `Job^`.\n            - If a `Task^` is provided, it will return the created `Job^`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Submit a scenario, sequence or task entity for execution.\n\n    This function submits the given entity for execution and returns the created job(s).\n\n    If the entity is a sequence or a scenario, all the tasks of the entity are\n    submitted for execution.\n\n    Parameters:\n        entity (Union[Scenario^, Sequence^, Task^]): The scenario, sequence or task to submit.\n        force (bool): If True, the execution is forced even if for skippable tasks.\n        wait (bool): Wait for the orchestrated jobs created from the submission to be finished\n            in asynchronous mode.\n        timeout (Union[float, int]): The optional maximum number of seconds to wait\n            for the jobs to be finished before returning.\n        **properties (dict[str, any]): A keyworded variable length list of additional arguments.\n    Returns:\n        The created `Submission^` containing the information about the submission.", "file": "taipy_core_taipy.py", "function": "submit", "project": "taipy", "change_stats": {"code_changes": 17, "doc_changes": 9, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef dedent(text):\n    \n    \n    lines = text.expandtabs().split('\\n')\n    margin = None\n    for line in lines:\n        content = line.lstrip()\n        if not content:\n            continue\n        indent = len(line) - len(content)\n        if margin is None:\n            margin = indent\n        else:\n            margin = min(margin, indent)\n\n    if margin is not None and margin > 0:\n        for i in range(len(lines)):\n            lines[i] = lines[i][margin:]\n\n    return '\\n'.join(lines)```\n\nUpdated code:\n```python\ndef dedent(text):\n    \n    \n    # Look for the longest leading string of spaces and tabs common to\n    # all lines.\n    margin = None\n    text = _whitespace_only_re.sub('', text)\n    indents = _leading_whitespace_re.findall(text)\n    for indent in indents:\n        if margin is None:\n            margin = indent\n\n        # Current line more deeply indented than previous winner:\n        # no change (previous winner is still on top).\n        elif indent.startswith(margin): \n            pass                        \n\n        # Current line consistent with and no deeper than previous winner:\n        # it's the new winner.\n        elif margin.startswith(indent): \n            margin = indent             \n\n        # Current line and previous winner have no common whitespace:\n        # there is no margin.\n        else:\n            margin = \"\"\n            break\n\n    # sanity check (testing/debugging only)\n    if 0 and margin:\n        for line in text.split(\"\\n\"):\n            assert not line or line.startswith(margin), \\\n                   \"line = %r, margin = %r\" % (line, margin)\n\n    if margin:\n        text = re.sub(r'(?m)^' + margin, '', text)\n    return text\n```\n\nOriginal documentation:\n```python\ndedent(text : string) -> string\n\n    Remove any whitespace than can be uniformly removed from the left\n    of every line in `text`.\n\n    This can be used e.g. to make triple-quoted strings line up with\n    the left edge of screen/whatever, while still presenting it in the\n    source code in indented form.\n\n    For example:\n\n        def test():\n            # end first line with \\ to avoid the empty line!\n            s = '''\\\n            hello\n              world\n            '''\n            print repr(s)          # prints '    hello\\n      world\\n    '\n            print repr(dedent(s))  # prints 'hello\\n  world\\n'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Remove any common leading whitespace from every line in `text`.\n\n    This can be used to make triple-quoted strings line up with the left\n    edge of the display, while still presenting them in the source code\n    in indented form.\n\n    Note that tabs and spaces are both treated as whitespace, but they\n    are not equal: the lines \"  hello\" and \"\\thello\" are\n    considered to have no common leading whitespace.  (This behaviour is\n    new in Python 2.5; older versions of this module incorrectly\n    expanded tabs before searching for common leading whitespace.)", "file": "Lib_textwrap.py", "function": "dedent", "project": "cpython", "change_stats": {"code_changes": 41, "doc_changes": 28, "total_changes": 69}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _meow_enhance(self):\n        \n        \n        \n        logger.hr('Meowfficer enhance', level=1)\n        logger.attr('MeowfficerTrain_EnhanceIndex', self.config.MeowfficerTrain_EnhanceIndex)\n\n        # Base Cases\n        # - Config at least > 0 but less than or equal to 12\n        # - Coins at least > 1000\n        if not (1 <= self.config.MeowfficerTrain_EnhanceIndex <= 12):\n            logger.warning(f'Meowfficer_EnhanceIndex={self.config.MeowfficerTrain_EnhanceIndex} '\n                           f'is out of bounds. Please limit to 1~12, skip')\n            return\n\n        coins = MEOWFFICER_COINS.ocr(self.device.image)\n        if coins < 1000:\n            logger.info(f'Coins ({coins}) < 1000. Not enough coins to complete '\n                        f'enhancement, skip')\n            return\n\n        for _ in range(2):\n            # Select target meowfficer\n            # for enhancement\n            self._meow_select()\n\n            if self._meow_get_level() >= 30:\n                logger.info('Current meowfficer is already leveled max')\n                return\n\n            # Transition to MEOWFFICER_FEED after\n            # selection; broken up due to significant\n            # delayed behavior of meow_additional\n            if self.meow_enhance_enter():\n                break\n            else:\n                # Retreat from an existing battle\n                self.ui_goto_campaign()\n                self.ui_goto(page_meowfficer)\n                continue\n\n        # Initiate feed sequence; loop until exhaust all\n        # - Select Feed\n        # - Confirm/Cancel Feed\n        # - Confirm Enhancement\n        # - Check remaining coins after enhancement\n        while 1:\n            logger.hr('Enhance once', level=2)\n            if not self.meow_feed_enter():\n                # Exit back into page_meowfficer\n                self.ui_click(MEOWFFICER_GOTO_DORMMENU, check_button=MEOWFFICER_ENHANCE_ENTER,\n                              appear_button=MEOWFFICER_ENHANCE_CONFIRM, offset=None, skip_first_screenshot=True)\n                # Re-enter page_meowfficer\n                self.ui_goto_main()\n                self.ui_goto(page_meowfficer)\n                return False\n            if not self.meow_feed_select():\n                break\n            self.meow_enhance_confirm()\n\n            coins = MEOWFFICER_COINS.ocr(self.device.image)\n            if coins < 1000:\n                logger.info(f'Remaining coins ({coins}) < 1000. Not enough coins for next '\n                            f'enhancement, skip')\n                break\n\n        # Exit back into page_meowfficer\n        self.ui_click(MEOWFFICER_GOTO_DORMMENU, check_button=MEOWFFICER_ENHANCE_ENTER,\n                      appear_button=MEOWFFICER_ENHANCE_CONFIRM, offset=None, skip_first_screenshot=True)\n        return True```\n\nUpdated code:\n```python\ndef _meow_enhance(self):\n        \n        \n        \n        logger.hr('Meowfficer enhance', level=1)\n        logger.attr('MeowfficerTrain_EnhanceIndex', self.config.MeowfficerTrain_EnhanceIndex)\n\n        # Base Cases\n        # - Config at least > 0 but less than or equal to 12\n        # - Coins at least > 1000\n        if not (1 <= self.config.MeowfficerTrain_EnhanceIndex <= 12):\n            logger.warning(f'Meowfficer_EnhanceIndex={self.config.MeowfficerTrain_EnhanceIndex} '\n                           f'is out of bounds. Please limit to 1~12, skip')\n            return 'invalid'\n\n        coins = MEOWFFICER_COINS.ocr(self.device.image)\n        if coins < 1000:\n            logger.info(f'Coins ({coins}) < 1000. Not enough coins to complete '\n                        f'enhancement, skip')\n            return 'coin_limit'\n\n        for _ in range(2):\n            # Select target meowfficer\n            # for enhancement\n            self._meow_select()\n\n            if self._meow_get_level() >= 30:\n                logger.info('Current meowfficer is already leveled max')\n                return 'leveled_max'\n\n            # Transition to MEOWFFICER_FEED after\n            # selection; broken up due to significant\n            # delayed behavior of meow_additional\n            if self.meow_enhance_enter():\n                break\n            else:\n                # Retreat from an existing battle\n                self.ui_goto_campaign()\n                self.ui_goto(page_meowfficer)\n                continue\n\n        # Initiate feed sequence; loop until exhaust all\n        # - Select Feed\n        # - Confirm/Cancel Feed\n        # - Confirm Enhancement\n        # - Check remaining coins after enhancement\n        while 1:\n            logger.hr('Enhance once', level=2)\n            if not self.meow_feed_enter():\n                # Exit back into page_meowfficer\n                self.ui_click(MEOWFFICER_GOTO_DORMMENU, check_button=MEOWFFICER_ENHANCE_ENTER,\n                              appear_button=MEOWFFICER_ENHANCE_CONFIRM, offset=None, skip_first_screenshot=True)\n                # Re-enter page_meowfficer\n                self.ui_goto_main()\n                self.ui_goto(page_meowfficer)\n                return 'in_battle'\n            if not self.meow_feed_select():\n                break\n            self.meow_enhance_confirm()\n\n            coins = MEOWFFICER_COINS.ocr(self.device.image)\n            if coins < 1000:\n                logger.info(f'Remaining coins ({coins}) < 1000. Not enough coins for next '\n                            f'enhancement, skip')\n                break\n\n        # Exit back into page_meowfficer\n        self.ui_click(MEOWFFICER_GOTO_DORMMENU, check_button=MEOWFFICER_ENHANCE_ENTER,\n                      appear_button=MEOWFFICER_ENHANCE_CONFIRM, offset=None, skip_first_screenshot=True)\n        return 'success'\n```\n\nOriginal documentation:\n```python\nPerform meowfficer enhancement operations\n        involving using extraneous meowfficers to\n        donate XP into a meowfficer target\n\n        Returns:\n            bool: If success. False if failed,\n                probably because the meowfficer\n                to enhance has reached LV.30\n\n        Pages:\n            in: page_meowfficer\n            out: page_meowfficer\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform meowfficer enhancement operations\n        involving using extraneous meowfficers to\n        donate XP into a meowfficer target\n\n        Returns:\n            str:\n\n        Pages:\n            in: page_meowfficer\n            out: page_meowfficer", "file": "module_meowfficer_enhance.py", "function": "MeowfficerEnhance._meow_enhance", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef named_temporary_file(args, prefix, suffix, directory, content):\n    \n    \n    \n    if args.explain:\n        yield os.path.join(directory or '/tmp', '%stemp%s' % (prefix, suffix))\n    else:\n        with tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix, dir=directory) as tempfile_fd:\n            tempfile_fd.write(to_bytes(content))\n            tempfile_fd.flush()\n\n            yield tempfile_fd.name```\n\nUpdated code:\n```python\ndef named_temporary_file(args, prefix, suffix, directory, content):  # type: (CommonConfig, str, str, t.Optional[str], str) -> t.Iterator[str]\n    \n    if args.explain:\n        yield os.path.join(directory or '/tmp', '%stemp%s' % (prefix, suffix))\n    else:\n        with tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix, dir=directory) as tempfile_fd:\n            tempfile_fd.write(to_bytes(content))\n            tempfile_fd.flush()\n\n            yield tempfile_fd.name\n```\n\nOriginal documentation:\n```python\n:param args: CommonConfig\n    :param prefix: str\n    :param suffix: str\n    :param directory: str\n    :param content: str | bytes | unicode\n    :rtype: str\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Context manager for a named temporary file.", "file": "test_lib_ansible_test__internal_util_common.py", "function": "named_temporary_file", "project": "ansible", "change_stats": {"code_changes": 6, "doc_changes": 9, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _prepare_for_training(\n        self,\n        trackers: List[TrackerWithCachedStates],\n        domain: Domain,\n        interpreter: NaturalLanguageInterpreter,\n        **kwargs: Any,\n    ) -> Tuple[RasaModelData, np.ndarray]:\n        \n        \n        training_trackers = self._get_trackers_for_training(trackers)\n        # dealing with training data\n        tracker_state_features, label_ids, entity_tags = self._featurize_for_training(\n            training_trackers,\n            domain,\n            interpreter,\n            bilou_tagging=self.config[BILOU_FLAG],\n            **kwargs,\n        )\n\n        if not tracker_state_features:\n            return RasaModelData(), label_ids\n\n        self._label_data, encoded_all_labels = self._create_label_data(\n            domain, interpreter\n        )\n\n        # extract actual training data to feed to model\n        model_data = self._create_model_data(\n            tracker_state_features, label_ids, entity_tags, encoded_all_labels\n        )\n\n        if self.config[ENTITY_RECOGNITION]:\n            self._entity_tag_specs = self.featurizer.state_featurizer.entity_tag_specs\n\n        # keep one example for persisting and loading\n        self.data_example = model_data.first_data_example()\n\n        return model_data, label_ids```\n\nUpdated code:\n```python\ndef _prepare_for_training(\n        self,\n        trackers: List[TrackerWithCachedStates],\n        domain: Domain,\n        precomputations: MessageContainerForCoreFeaturization,\n        **kwargs: Any,\n    ) -> Tuple[RasaModelData, np.ndarray]:\n        \n        \n        training_trackers = self._get_trackers_for_training(trackers)\n        # dealing with training data\n        tracker_state_features, label_ids, entity_tags = self._featurize_for_training(\n            training_trackers,\n            domain,\n            precomputations=precomputations,\n            bilou_tagging=self.config[BILOU_FLAG],\n            **kwargs,\n        )\n\n        if not tracker_state_features:\n            return RasaModelData(), label_ids\n\n        self._label_data, encoded_all_labels = self._create_label_data(\n            domain, precomputations=precomputations\n        )\n\n        # extract actual training data to feed to model\n        model_data = self._create_model_data(\n            tracker_state_features, label_ids, entity_tags, encoded_all_labels\n        )\n\n        if self.config[ENTITY_RECOGNITION]:\n            self._entity_tag_specs = self.featurizer.state_featurizer.entity_tag_specs\n\n        # keep one example for persisting and loading\n        self.data_example = model_data.first_data_example()\n\n        return model_data, label_ids\n```\n\nOriginal documentation:\n```python\nPrepares data to be fed into the model.\n\n        Args:\n            trackers: List of training trackers to be featurized.\n            domain: Domain of the assistant.\n            interpreter: NLU interpreter to be used for featurizing states.\n            **kwargs: Any other arguments.\n\n        Returns:\n            Featurized data to be fed to the model and corresponding label ids.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Prepares data to be fed into the model.\n\n        Args:\n            trackers: List of training trackers to be featurized.\n            domain: Domain of the assistant.\n            precomputations: Contains precomputed features and attributes.\n            **kwargs: Any other arguments.\n\n        Returns:\n            Featurized data to be fed to the model and corresponding label ids.", "file": "rasa_core_policies_ted_policy.py", "function": "TEDPolicyGraphComponent._prepare_for_training", "project": "rasa", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(gpu_id, tol, max_iter):\n    '''\n\n    '''\n    for repeat in range(3):\n        print('Trial: %d' % repeat)\n        # Create the large symmetric matrix 'A'.\n        N = 2000\n        A = np.random.randint(-50, 50, size=(N, N))\n        A = (A + A.T).astype(np.float64)\n        x_ans = np.random.randint(-50, 50, size=N).astype(np.float64)\n        b = np.dot(A, x_ans)\n\n        print('Running CPU...')\n        with timer(' CPU '):\n            x_cpu = fit(A, b, tol, max_iter)\n        print(np.linalg.norm(x_cpu - x_ans))\n\n        with cupy.cuda.Device(gpu_id):\n            A_gpu = cupy.asarray(A)\n            b_gpu = cupy.asarray(b)\n            print('Running GPU...')\n            with timer(' GPU '):\n                x_gpu = fit(A_gpu, b_gpu, tol, max_iter)\n            print(np.linalg.norm(cupy.asnumpy(x_gpu) - x_ans))\n\n        print()```\n\nUpdated code:\n```python\ndef run(gpu_id, tol, max_iter):\n    \n\n    \n    for repeat in range(3):\n        print('Trial: %d' % repeat)\n        # Create the large symmetric matrix 'A'.\n        N = 2000\n        A = np.random.randint(-50, 50, size=(N, N))\n        A = (A + A.T).astype(np.float64)\n        x_ans = np.random.randint(-50, 50, size=N).astype(np.float64)\n        b = np.dot(A, x_ans)\n\n        print('Running CPU...')\n        with timer(' CPU '):\n            x_cpu = fit(A, b, tol, max_iter)\n        print(np.linalg.norm(x_cpu - x_ans))\n\n        with cupy.cuda.Device(gpu_id):\n            A_gpu = cupy.asarray(A)\n            b_gpu = cupy.asarray(b)\n            print('Running GPU...')\n            with timer(' GPU '):\n                x_gpu = fit(A_gpu, b_gpu, tol, max_iter)\n            print(np.linalg.norm(cupy.asnumpy(x_gpu) - x_ans))\n\n        print()\n```\n\nOriginal documentation:\n```python\nCuPy Congugate gradient example\n\n    Solve simultaneous linear equations, Ax = b.\n    'A' and 'x' are created randomly and 'b' is computed by 'Ax' at first.\n    Then, 'x' is computed from 'A' and 'b' in two ways, namely with CPU and\n    GPU. To evaluate the accuracy of computation, the Euclidean distances\n    between the answer 'x' and the reconstructed 'x' are computed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "CuPy Conjugate gradient example\n\n    Solve simultaneous linear equations, Ax = b.\n    'A' and 'x' are created randomly and 'b' is computed by 'Ax' at first.\n    Then, 'x' is computed from 'A' and 'b' in two ways, namely with CPU and\n    GPU. To evaluate the accuracy of computation, the Euclidean distances\n    between the answer 'x' and the reconstructed 'x' are computed.", "file": "examples_cg_cg.py", "function": "run", "project": "cupy", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef alignments_paths(self):\n          \n        alignments_paths = dict()\n        for side in (\"a\", \"b\"):\n            alignments_path = getattr(self.args, \"alignments_path_{}\".format(side))\n            if not alignments_path:\n                image_path = getattr(self.args, \"input_{}\".format(side))\n                alignments_path = os.path.join(image_path, \"alignments.json\")\n            alignments_paths[side] = alignments_path\n        logger.debug(\"Alignments paths: %s\", alignments_paths)\n        return alignments_paths```\n\nUpdated code:\n```python\ndef alignments_paths(self):\n          \n        alignments_paths = dict()\n        for side in (\"a\", \"b\"):\n            alignments_path = getattr(self.args, \"alignments_path_{}\".format(side))\n            if not alignments_path:\n                image_path = getattr(self.args, \"input_{}\".format(side))\n                alignments_path = os.path.join(image_path, \"alignments.fsa\")\n            alignments_paths[side] = alignments_path\n        logger.debug(\"Alignments paths: %s\", alignments_paths)\n        return alignments_paths\n```\n\nOriginal documentation:\n```python\nSet the alignments path to input dirs if not provided\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the alignments path to input folder if not provided", "file": "scripts_train.py", "function": "Train.alignments_paths", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef continuous_future(self,\n                          root_symbol_str,\n                          offset,\n                          roll,\n                          adjustment='mul'):\n        \n        \n        return self.asset_finder.create_continuous_future(\n            root_symbol_str,\n            offset,\n            roll,\n            adjustment,\n        )```\n\nUpdated code:\n```python\ndef continuous_future(self,\n                          root_symbol_str,\n                          offset=0,\n                          roll='volume',\n                          adjustment='mul'):\n        \n        \n        return self.asset_finder.create_continuous_future(\n            root_symbol_str,\n            offset,\n            roll,\n            adjustment,\n        )\n```\n\nOriginal documentation:\n```python\nCreate a specifier for a continuous contract.\n\n        Parameters\n        ----------\n        root_symbol_str : str\n            The root symbol for the future chain.\n\n        offset : int\n            The distance from the primary contract.\n\n        roll_style : str\n            How rolls are determined.\n\n        adjustment : str\n            Method for adjusting lookback prices between rolls. Options are\n            'mul', 'add', and None. Defaults to 'mul'.\n\n        Returns\n        -------\n        continuous_future : ContinuousFuture\n            The continuous future specifier.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a specifier for a continuous contract.\n\n        Parameters\n        ----------\n        root_symbol_str : str\n            The root symbol for the future chain.\n\n        offset : int, optional\n            The distance from the primary contract. Default is 0.\n\n        roll_style : str, optional\n            How rolls are determined. Default is 'volume'.\n\n        adjustment : str, optional\n            Method for adjusting lookback prices between rolls. Options are\n            'mul', 'add', and None. Default is 'mul'.\n\n        Returns\n        -------\n        continuous_future : ContinuousFuture\n            The continuous future specifier.", "file": "zipline_algorithm.py", "function": "TradingAlgorithm.continuous_future", "project": "zipline", "change_stats": {"code_changes": 6, "doc_changes": 14, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pad(self, *args, **kwargs):\n        \n        \n        \n        input_features = kwargs.pop(\"input_features\", None)\n        labels = kwargs.pop(\"labels\", None)\n\n        if len(args) > 0:\n            input_features = args[0]\n            args = args[1:]\n\n        if input_features is not None:\n            input_features = self.feature_extractor.pad(input_features, *args, **kwargs)\n        if labels is not None:\n            labels = self.tokenizer.pad(labels, **kwargs)\n\n        if labels is None:\n            return input_features\n        elif input_features is None:\n            return labels\n        else:\n            input_features[\"labels\"] = labels[\"input_ids\"]\n            return input_features```\n\nUpdated code:\n```python\ndef pad(self, *args, **kwargs):\n        \n        \n        \n        input_values = kwargs.pop(\"input_values\", None)\n        input_ids = kwargs.pop(\"input_ids\", None)\n        labels = kwargs.pop(\"labels\", None)\n\n        if input_values is not None and input_ids is not None:\n            raise ValueError(\"Cannot process both `input_values` and `input_ids` inputs.\")\n        if input_values is None and input_ids is None and labels is None:\n            raise ValueError(\n                \"You need to specify either an `input_values`, `input_ids`, or `labels` input to be padded.\"\n            )\n\n        if input_values is not None:\n            inputs = self.feature_extractor.pad(input_values, *args, **kwargs)\n        elif input_ids is not None:\n            inputs = self.tokenizer.pad(input_ids, **kwargs)\n        else:\n            inputs = None\n\n        if labels is not None:\n            if \"input_ids\" in labels or (isinstance(labels, list) and \"input_ids\" in labels[0]):\n                targets = self.tokenizer.pad(labels, **kwargs)\n                labels = targets[\"input_ids\"]\n            else:\n                feature_size_hack = self.feature_extractor.feature_size\n                self.feature_extractor.feature_size = self.feature_extractor.num_mel_bins\n                targets = self.feature_extractor.pad(labels, *args, **kwargs)\n                self.feature_extractor.feature_size = feature_size_hack\n                labels = targets[\"input_values\"]\n        else:\n            targets = None\n\n        if inputs is None:\n            return targets\n\n        if targets is not None:\n            inputs[\"labels\"] = labels\n\n            decoder_attention_mask = targets.get(\"attention_mask\")\n            if decoder_attention_mask is not None:\n                inputs[\"decoder_attention_mask\"] = decoder_attention_mask\n\n        return inputs\n```\n\nOriginal documentation:\n```python\nThis method forwards all its arguments to SpeechT5FeatureExtractor's [`~SpeechT5FeatureExtractor.pad`] and\n        returns its output.\n\n        You can process your labels by using the argument `text` (either in the same call as your audio inputs, or in a\n        separate call). This forwards its arguments to SpeechT5Tokenizer's [`~SpeechT5Tokenizer.pad`].\n\n        Please refer to the docstring of the above two methods for more information.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Collates the audio and text inputs, as well as their targets, into a padded batch.\n\n        Audio inputs are padded by SpeechT5FeatureExtractor's [`~SpeechT5FeatureExtractor.pad`]. Text inputs are padded\n        by SpeechT5Tokenizer's [`~SpeechT5Tokenizer.pad`].\n\n        Valid input combinations are:\n\n        - `input_ids` only\n        - `input_values` only\n        - `labels` only, either log-mel spectrograms or text tokens\n        - `input_ids` and log-mel spectrogram `labels`\n        - `input_values` and text `labels`\n\n        Please refer to the docstring of the above two methods for more information.", "file": "src_transformers_models_speecht5_processing_speecht5.py", "function": "SpeechT5Processor.pad", "project": "transformers", "change_stats": {"code_changes": 53, "doc_changes": 17, "total_changes": 70}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_login_shell(proc_cmd):\n    \n    \n    login_shell = os.environ.get('SHELL', '')\n    if login_shell:\n        proc_cmd = login_shell\n    else:\n        proc_cmd = proc_cmd[1:]\n    return (os.path.basename(proc_cmd).lower(), proc_cmd)```\n\nUpdated code:\n```python\ndef _get_login_shell(proc_cmd):\n    \n    login_shell = os.environ.get(\"SHELL\", \"\")\n    if login_shell:\n        proc_cmd = login_shell\n    else:\n        proc_cmd = proc_cmd[1:]\n    return (os.path.basename(proc_cmd).lower(), proc_cmd)\n```\n\nOriginal documentation:\n```python\nForm shell information from the SHELL environment variable if possible.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Form shell information from SHELL environ if possible.", "file": "pipenv_vendor_shellingham_posix___init__.py", "function": "_get_login_shell", "project": "pipenv", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_login(self, remote_url):\n        '''\n        '''\n        try:\n            statement = self.connection.cursor()\n            statement.execute('select user, token from %s where remote_url=\"%s\"'\n                              % (REMOTES_USER_TABLE, remote_url))\n            rs = statement.fetchone()\n            if not rs:\n                return None, None\n            name = rs[0]\n            token = rs[1]\n            return name, token\n        except Exception:\n            raise ConanException(\"Couldn't read login\\n Try removing '%s' file\" % self.dbfile)```\n\nUpdated code:\n```python\ndef get_login(self, remote_url):\n          \n        with self._connect() as connection:\n            try:\n                statement = connection.cursor()\n                statement.execute('select user, token from %s where remote_url=\"%s\"'\n                                  % (REMOTES_USER_TABLE, remote_url))\n                rs = statement.fetchone()\n                if not rs:\n                    return None, None\n                name = rs[0]\n                token = rs[1]\n                return name, token\n            except Exception:\n                raise ConanException(\"Couldn't read login\\n Try removing '%s' file\" % self.dbfile)\n```\n\nOriginal documentation:\n```python\nReturns login credentials.\n        This method is also in charge of expiring them.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns login credentials. This method is also in charge of expiring them.", "file": "conans_client_store_localdb.py", "function": "LocalDB.get_login", "project": "conan", "change_stats": {"code_changes": 30, "doc_changes": 5, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_permissions(cls, context, permissions=None):\n        \n        \n        permissions = permissions or cls._meta.permissions\n        if not permissions:\n            return True\n        if context.user.has_perms(permissions):\n            return True\n        service_account = getattr(context, \"service_account\", None)\n        if service_account and service_account.has_perms(permissions):\n            return True\n        return False```\n\nUpdated code:\n```python\ndef check_permissions(cls, context, permissions=None):\n        \n        \n        permissions = permissions or cls._meta.permissions\n        if not permissions:\n            return True\n        if context.user.has_perms(permissions):\n            return True\n        app = getattr(context, \"app\", None)\n        if app and app.has_perms(permissions):\n            return True\n        return False\n```\n\nOriginal documentation:\n```python\nDetermine whether user or service account has rights to perform this mutation.\n\n        Default implementation assumes that account is allowed to perform any\n        mutation. By overriding this method or defining required permissions\n        in the meta-class, you can restrict access to it.\n\n        The `context` parameter is the Context instance associated with the request.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Determine whether user or app has rights to perform this mutation.\n\n        Default implementation assumes that account is allowed to perform any\n        mutation. By overriding this method or defining required permissions\n        in the meta-class, you can restrict access to it.\n\n        The `context` parameter is the Context instance associated with the request.", "file": "saleor_graphql_core_mutations.py", "function": "BaseMutation.check_permissions", "project": "saleor", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_big_decorators_horizontal():\n    \n    fig, axs = plt.subplots(1, 2, figsize=(3, 2))\n    axs[0].set_xlabel('a' * 30)\n    axs[1].set_xlabel('b' * 30)\n    with pytest.warns(UserWarning):\n        fig.tight_layout()```\n\nUpdated code:\n```python\ndef test_big_decorators_horizontal():\n    \n    fig, axs = plt.subplots(1, 2, figsize=(3, 2))\n    axs[0].set_xlabel('a' * 30)\n    axs[1].set_xlabel('b' * 30)\n```\n\nOriginal documentation:\n```python\nTest that warning emitted when xlabel too big.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test that doesn't warn when xlabel too big.", "file": "lib_matplotlib_tests_test_tightlayout.py", "function": "test_big_decorators_horizontal", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef image_has_mask(image: np.ndarray) -> bool:\n    \n    \n        \n    return (\n        input_image.ndim == 3 and \n        input_image.shape[2] == 4 and \n        np.max(input_image[:, :, 3]) > 127\n    )```\n\nUpdated code:\n```python\ndef image_has_mask(input_image: np.ndarray) -> bool:\n    \n    \n        \n    return (\n        input_image.ndim == 3 and \n        input_image.shape[2] == 4 and \n        np.max(input_image[:, :, 3]) > 127\n    )\n```\n\nOriginal documentation:\n```python\nDetermine if an image has an alpha channel (mask) that is not empty.\n\n    The function checks if the input image has three dimensions (height, width, channels), \n    and if the third dimension (channel dimension) is of size 4 (presumably RGB + alpha). \n    Then it checks if the maximum value in the alpha channel is greater than 127. This is \n    presumably to check if there is any non-transparent (or semi-transparent) pixel in the \n    image. A pixel is considered non-transparent if its alpha value is above 127.\n\n    Args:\n        image (np.ndarray): A 3D numpy array representing an image. The dimensions should represent \n        [height, width, channels].\n\n    Returns:\n        bool: True if the image has a non-empty alpha channel, False otherwise.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Determine if an image has an alpha channel (mask) that is not empty.\n\n    The function checks if the input image has three dimensions (height, width, channels), \n    and if the third dimension (channel dimension) is of size 4 (presumably RGB + alpha). \n    Then it checks if the maximum value in the alpha channel is greater than 127. This is \n    presumably to check if there is any non-transparent (or semi-transparent) pixel in the \n    image. A pixel is considered non-transparent if its alpha value is above 127.\n\n    Args:\n        input_image (np.ndarray): A 3D numpy array representing an image. The dimensions \n        should represent [height, width, channels].\n\n    Returns:\n        bool: True if the image has a non-empty alpha channel, False otherwise.", "file": "scripts_controlnet.py", "function": "image_has_mask", "project": "sd-webui-controlnet", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef hvac_mode(self) -> str:\n        \n        \n        if self._device.boostMode:\n            return HVAC_MODE_HEAT\n        if self._device.controlMode == HMIP_MANUAL_CM:\n            return HVAC_MODE_HEAT\n\n        return HVAC_MODE_AUTO```\n\nUpdated code:\n```python\ndef hvac_mode(self) -> str:\n        \n        if self._disabled_by_cooling_mode:\n            return HVAC_MODE_OFF\n        if self._device.boostMode:\n            return HVAC_MODE_HEAT\n        if self._device.controlMode == HMIP_MANUAL_CM:\n            return HVAC_MODE_HEAT if self._heat_mode_enabled else HVAC_MODE_COOL\n\n        return HVAC_MODE_AUTO\n```\n\nOriginal documentation:\n```python\nReturn hvac operation ie. heat, cool mode.\n\n        Need to be one of HVAC_MODE_*.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return hvac operation ie.", "file": "homeassistant_components_homematicip_cloud_climate.py", "function": "HomematicipHeatingGroup.hvac_mode", "project": "core", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef scale(X, axis=0, with_mean=True, with_std=True, copy=True):\n    \n    \n    if sp.issparse(X):\n        raise NotImplementedError(\n            \"Scaling is not yet implement for sparse matrices\")\n    X = np.asarray(X)\n    if X.dtype.kind in ['i', 'u']:\n        warnings.warn('Data of type %s in scale. '\n                      'Converting to float is recommended' % X.dtype)\n    mean_, std_ = _mean_and_std(\n        X, axis, with_mean=with_mean, with_std=with_std)\n    if copy:\n        X = X.copy()\n    Xr = np.rollaxis(X, axis)\n    if with_mean:\n        Xr -= mean_\n    if with_std:\n        Xr /= std_\n    return X```\n\nUpdated code:\n```python\ndef scale(X, axis=0, with_mean=True, with_std=True, copy=True):\n    \n    \n    if sp.issparse(X):\n        X = X.tocsr()\n        if with_mean:\n            raise ValueError(\n                \"Cannot center sparse matrices: pass `with_mean=False` instead\"\n                \" See docstring for motivation and alternatives.\")\n        warn_if_not_float(X, estimator='The scale function')\n        if copy:\n            X = X.copy()\n        _, var = mean_variance_axis0(X)\n        var[var==0.0] = 1.0\n        inplace_csr_column_scale(X, np.sqrt(var))\n    else:\n        X = np.asarray(X)\n        warn_if_not_float(X, estimator='The scale function')\n        mean_, std_ = _mean_and_std(\n            X, axis, with_mean=with_mean, with_std=with_std)\n        if copy:\n            X = X.copy()\n        Xr = np.rollaxis(X, axis)\n        if with_mean:\n            Xr -= mean_\n        if with_std:\n            Xr /= std_\n    return X\n```\n\nOriginal documentation:\n```python\nStandardize a dataset along any axis\n\n    Center to the mean and component wise scale to unit variance.\n\n    Parameters\n    ----------\n    X : array-like\n        The data to center and scale.\n\n    axis : int (0 by default)\n        axis used to compute the means and standard deviations along. If 0,\n        independently standardize each feature, otherwise (if 1) standardize\n        each sample.\n\n    with_mean : boolean, True by default\n        If True, center the data before scaling.\n\n    with_std : boolean, True by default\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    copy : boolean, optional, default is True\n        set to False to perform inplace row normalization and avoid a\n        copy (if the input is already a numpy array or a scipy.sparse\n        CSR matrix and if axis is 1).\n\n    See also\n    --------\n    :class:`sklearn.preprocessing.Scaler` to perform centering and\n    scaling using the ``Transformer`` API (e.g. as part of a preprocessing\n    :class:`sklearn.pipeline.Pipeline`)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Standardize a dataset along any axis\n\n    Center to the mean and component wise scale to unit variance.\n\n    Parameters\n    ----------\n    X : array-like\n        The data to center and scale.\n\n    axis : int (0 by default)\n        axis used to compute the means and standard deviations along. If 0,\n        independently standardize each feature, otherwise (if 1) standardize\n        each sample.\n\n    with_mean : boolean, True by default\n        If True, center the data before scaling.\n\n    with_std : boolean, True by default\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    copy : boolean, optional, default is True\n        set to False to perform inplace row normalization and avoid a\n        copy (if the input is already a numpy array or a scipy.sparse\n        CSR matrix and if axis is 1).\n\n    Note\n    ----\n    This implementation will refuse to center scipy.sparse matrices\n    since it would make it non-sparse and would potentially crash the\n    program with memory exhaustion problems.\n\n    Instead the caller is expected to either set explicitly\n    `with_mean=False` (in that case, only variance scaling will be\n    performed on the features of the CSR matrix) or to pass `X.toarray()`\n    if he/she expects the materialized dense array to fit in memory.\n\n    To avoid memory copy the caller should pass a CSR matrix.\n\n    See also\n    --------\n    :class:`sklearn.preprocessing.Scaler` to perform centering and\n    scaling using the ``Transformer`` API (e.g. as part of a preprocessing\n    :class:`sklearn.pipeline.Pipeline`)", "file": "sklearn_preprocessing___init__.py", "function": "scale", "project": "scikit-learn", "change_stats": {"code_changes": 40, "doc_changes": 15, "total_changes": 55}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef evaluate_continuously(self,\n                            steps: int = -1,\n                            timeout: Optional[Union[int, float]] = None,\n                            timeout_fn: Optional[Callable[[], bool]] = None):\n    \n    \n    self._require(\"evaluator\", for_method=\"evaluate_continuously\")\n    self._require(\"checkpoint_manager\", for_method=\"evaluate_continuously\")\n\n    for checkpoint_path in tf.train.checkpoints_iterator(\n        self.checkpoint_manager.directory,\n        timeout=timeout,\n        timeout_fn=timeout_fn):\n      self.restore_checkpoint(checkpoint_path)\n      self.evaluate(steps)```\n\nUpdated code:\n```python\ndef evaluate_continuously(\n      self,\n      steps: int = -1,\n      timeout: Optional[Union[int, float]] = None,\n      timeout_fn: Optional[Callable[[], bool]] = None,\n  ) -> Optional[runner.Output]:\n    \n    \n    self._require(\"evaluator\", for_method=\"evaluate_continuously\")\n    self._require(\"checkpoint_manager\", for_method=\"evaluate_continuously\")\n\n    output = None\n    for checkpoint_path in tf.train.checkpoints_iterator(\n        self.checkpoint_manager.directory,\n        timeout=timeout,\n        timeout_fn=timeout_fn):\n      self.restore_checkpoint(checkpoint_path)\n      output = self.evaluate(steps)\n    return output\n```\n\nOriginal documentation:\n```python\nContinuously monitors a directory and evaluates new checkpoints in it.\n\n    This method continuously monitors a directory as specified by this\n    Controller's CheckpointManager init arg and runs evaluation on the\n    checkpoints found there.\n\n    Args:\n      steps: The number of steps to run when evaluating. If -1, this method will\n        evaluate over the entire evaluation dataset.\n      timeout: The maximum number of seconds to wait between checkpoints. See\n        tf.train.checkpoints_iterator documentation.\n      timeout_fn: Optional callable to call after a timeout. If the function\n        returns True, then it means that no new checkpoints will be generated\n        and the iterator will exit.\n\n    Raises:\n      ValueError: If no checkpoint found in `self.checkpoint_manager.directory`.\n      ValueError: If `evaluator` was not provided as a controller init arg.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Continuously monitors a directory and evaluates new checkpoints in it.\n\n    This method continuously monitors a directory as specified by this\n    Controller's CheckpointManager init arg and runs evaluation on the\n    checkpoints found there.\n\n    Args:\n      steps: The number of steps to run when evaluating. If -1, this method will\n        evaluate over the entire evaluation dataset.\n      timeout: The maximum number of seconds to wait between checkpoints. See\n        tf.train.checkpoints_iterator documentation.\n      timeout_fn: Optional callable to call after a timeout. If the function\n        returns True, then it means that no new checkpoints will be generated\n        and the iterator will exit.\n\n    Returns:\n      The evaluation results as a dictionary mapping names to NumPy values.\n\n    Raises:\n      ValueError: If no checkpoint found in `self.checkpoint_manager.directory`.\n      ValueError: If `evaluator` was not provided as a controller init arg.", "file": "orbit_controller.py", "function": "Controller.evaluate_continuously", "project": "models", "change_stats": {"code_changes": 16, "doc_changes": 5, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rm_rf(path):\n    \n    # Make sure all files are writeable and dirs executable to remove\n    chmod_r(path, 0o777)\n    if os.path.isfile(path):\n        os.remove(path)\n    else:\n        shutil.rmtree(path)```\n\nUpdated code:\n```python\ndef rm_rf(path):\n    \n    \n    \n    if not path or not os.path.exists(path):\n        return\n    # Make sure all files are writeable and dirs executable to remove\n    chmod_r(path, 0o777)\n    if os.path.isfile(path):\n        os.remove(path)\n    else:\n        shutil.rmtree(path)\n```\n\nOriginal documentation:\n```python\nRecursively removes file/directory\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Recursively removes a file or directory", "file": "localstack_utils_common.py", "function": "rm_rf", "project": "localstack", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef swipe(self, vector, box=(123, 159, 1193, 628), random_range=(0, 0, 0, 0), padding=15, duration=(0.1, 0.2),\n              name='SWIPE'):\n        \n        \n        self.click_record_check(name)\n        duration = ensure_time(duration)\n        start, end = random_rectangle_vector(vector, box, random_range=random_range, padding=padding)\n        logger.info(\n            'Swipe %s -> %s, %s' % (point2str(*start), point2str(*end), duration)\n        )\n        if np.linalg.norm(np.subtract(start, end)) < 10:\n            # Should swipe a certain distabce, otherwise AL will treat it as click.\n            # uiautomator2 should >= 6px, minitouch should >= 5px\n            logger.info('Swipe distance < 10px, dropped')\n        fx, fy, tx, ty = np.append(start, end).tolist()\n\n        method = self.config.Emulator_ControlMethod\n        if method == 'minitouch':\n            self._swipe_minitouch(fx, fy, tx, ty)\n        else:\n            self.device.swipe(fx, fy, tx, ty, duration=duration)```\n\nUpdated code:\n```python\ndef swipe(self, vector, box=(123, 159, 1193, 628), random_range=(0, 0, 0, 0), padding=15, duration=(0.1, 0.2),\n              whitelist_area=None, blacklist_area=None, name='SWIPE'):\n        \n        \n        self.click_record_check(name)\n        duration = ensure_time(duration)\n        start, end = random_rectangle_vector_opted(\n            vector,\n            box=box,\n            random_range=random_range,\n            padding=padding,\n            whitelist_area=whitelist_area,\n            blacklist_area=blacklist_area\n        )\n        logger.info(\n            'Swipe %s -> %s, %s' % (point2str(*start), point2str(*end), duration)\n        )\n        if np.linalg.norm(np.subtract(start, end)) < 10:\n            # Should swipe a certain distabce, otherwise AL will treat it as click.\n            # uiautomator2 should >= 6px, minitouch should >= 5px\n            logger.info('Swipe distance < 10px, dropped')\n        fx, fy, tx, ty = np.append(start, end).tolist()\n\n        method = self.config.Emulator_ControlMethod\n        if method == 'minitouch':\n            self._swipe_minitouch(fx, fy, tx, ty)\n        else:\n            self.device.swipe(fx, fy, tx, ty, duration=duration)\n```\n\nOriginal documentation:\n```python\nMethod to swipe.\n\n        Args:\n            box (tuple): Swipe in box (upper_left_x, upper_left_y, bottom_right_x, bottom_right_y).\n            vector (tuple): (x, y).\n            random_range (tuple): (x_min, y_min, x_max, y_max).\n            padding (int):\n            duration (int, float, tuple):\n            name (str): Swipe name\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Method to swipe.\n\n        Args:\n            box (tuple): Swipe in box (upper_left_x, upper_left_y, bottom_right_x, bottom_right_y).\n            vector (tuple): (x, y).\n            random_range (tuple): (x_min, y_min, x_max, y_max).\n            padding (int):\n            duration (int, float, tuple):\n            whitelist_area: (list[tuple[int]]):\n                A list of area that safe to click. Swipe path will end there.\n            blacklist_area: (list[tuple[int]]):\n                If none of the whitelist_area satisfies current vector, blacklist_area will be used.\n                Delete random path that ends in any blacklist_area.\n            name (str): Swipe name", "file": "module_device_control.py", "function": "Control.swipe", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 13, "doc_changes": 7, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _setup_entities(hass, dev_ids, platform):\n    \n    tuya = hass.data[DOMAIN][TUYA_DATA]\n    entities = []\n    for dev_id in dev_ids:\n        device = tuya.get_device_by_id(dev_id)\n        if device is None:\n            continue\n        entities.append(TuyaFanDevice(device, platform))\n    return entities```\n\nUpdated code:\n```python\ndef _setup_entities(\n    hass: HomeAssistant, entry: ConfigEntry, device_ids: list[str]\n) -> list[TuyaHaFan]:\n    \n    device_manager = hass.data[DOMAIN][entry.entry_id][TUYA_DEVICE_MANAGER]\n    entities = []\n    for device_id in device_ids:\n        device = device_manager.device_map[device_id]\n        if device is None:\n            continue\n        entities.append(TuyaHaFan(device, device_manager))\n        hass.data[DOMAIN][entry.entry_id][TUYA_HA_DEVICES].add(device_id)\n    return entities\n```\n\nOriginal documentation:\n```python\nSet up Tuya Fan device.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set up Tuya Fan.", "file": "homeassistant_components_tuya_fan.py", "function": "_setup_entities", "project": "core", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef expansive_install_req_from_line(\n    name: str,\n    comes_from: Optional[Union[str, InstallRequirement]] = None,\n    *,\n    use_pep517: Optional[bool] = None,\n    isolated: bool = False,\n    global_options: Optional[List[str]] = None,\n    hash_options: Optional[Dict[str, List[str]]] = None,\n    constraint: bool = False,\n    line_source: Optional[str] = None,\n    user_supplied: bool = False,\n    config_settings: Optional[Dict[str, Union[str, List[str]]]] = None,\n    expand_env: bool = False,\n) -> InstallRequirement:\n    \n    \n    name = name.strip(\"'\")\n    if name.startswith(\"-e \"):  # Editable requirements\n        name = name.split(\"-e \")[1]\n        return install_req_from_editable(name, line_source)\n    if has_name_with_extras(name):\n        name = name.split(\" @ \", 1)[1]\n\n    if expand_env:\n        name = expand_env_variables(name)\n\n    vcs_part = name\n    if \"@ \" in name:  # Check for new style vcs lines\n        vcs_part = name.split(\"@ \", 1)[1]\n    for vcs in VCS_LIST:\n        if vcs_part.startswith(f\"{vcs}+\"):\n            link = get_link_from_line(vcs_part)\n            return InstallRequirement(\n                None,\n                comes_from,\n                link=link,\n                use_pep517=use_pep517,\n                isolated=isolated,\n                global_options=global_options,\n                hash_options=hash_options,\n                constraint=constraint,\n                user_supplied=user_supplied,\n            )\n    if urlparse(name).scheme in (\"http\", \"https\", \"file\") or any(\n        name.endswith(s) for s in INSTALLABLE_EXTENSIONS\n    ):\n        parts = parse_req_from_line(name, line_source)\n    else:\n        # It's a requirement\n        if \"--index\" in name:\n            name = name.split(\"--index\")[0]\n        if \" -i \" in name:\n            name = name.split(\" -i \")[0]\n        # handle local version identifiers (like the ones torch uses in their public index)\n        if \"+\" in name:\n            name = name.split(\"+\")[0]\n        parts = parse_req_from_line(name, line_source)\n\n    return InstallRequirement(\n        parts.requirement,\n        comes_from,\n        link=parts.link,\n        markers=parts.markers,\n        use_pep517=use_pep517,\n        isolated=isolated,\n        global_options=global_options,\n        hash_options=hash_options,\n        config_settings=config_settings,\n        constraint=constraint,\n        extras=parts.extras,\n        user_supplied=user_supplied,\n    )```\n\nUpdated code:\n```python\ndef expansive_install_req_from_line(\n    pip_line: str,\n    comes_from: Optional[Union[str, InstallRequirement]] = None,\n    *,\n    use_pep517: Optional[bool] = None,\n    isolated: bool = False,\n    global_options: Optional[List[str]] = None,\n    hash_options: Optional[Dict[str, List[str]]] = None,\n    constraint: bool = False,\n    line_source: Optional[str] = None,\n    user_supplied: bool = False,\n    config_settings: Optional[Dict[str, Union[str, List[str]]]] = None,\n    expand_env: bool = False,\n) -> (InstallRequirement, str):\n    \n    \n    name = None\n    pip_line = pip_line.strip(\"'\")\n    for new_req_symbol in (\"@ \", \" @ \"):  # Check for new style pip lines\n        if new_req_symbol in pip_line:\n            pip_line_parts = pip_line.split(new_req_symbol, 1)\n            name = pip_line_parts[0]\n            pip_line = pip_line_parts[1]\n    if pip_line.startswith(\"-e \"):  # Editable requirements\n        pip_line = pip_line.split(\"-e \")[1]\n        return install_req_from_editable(pip_line, line_source), name\n\n    if expand_env:\n        pip_line = expand_env_variables(pip_line)\n\n    vcs_part = pip_line\n    for vcs in VCS_LIST:\n        if vcs_part.startswith(f\"{vcs}+\"):\n            link = get_link_from_line(vcs_part)\n            install_req = InstallRequirement(\n                None,\n                comes_from,\n                link=link,\n                use_pep517=use_pep517,\n                isolated=isolated,\n                global_options=global_options,\n                hash_options=hash_options,\n                constraint=constraint,\n                user_supplied=user_supplied,\n            )\n            return install_req, name\n    if urlparse(pip_line).scheme in (\"http\", \"https\", \"file\") or any(\n        pip_line.endswith(s) for s in INSTALLABLE_EXTENSIONS\n    ):\n        parts = parse_req_from_line(pip_line, line_source)\n    else:\n        # It's a requirement\n        if \"--index\" in pip_line:\n            pip_line = pip_line.split(\"--index\")[0]\n        if \" -i \" in pip_line:\n            pip_line = pip_line.split(\" -i \")[0]\n        # handle local version identifiers (like the ones torch uses in their public index)\n        if \"+\" in pip_line:\n            pip_line = pip_line.split(\"+\")[0]\n        parts = parse_req_from_line(pip_line, line_source)\n\n    install_req = InstallRequirement(\n        parts.requirement,\n        comes_from,\n        link=parts.link,\n        markers=parts.markers,\n        use_pep517=use_pep517,\n        isolated=isolated,\n        global_options=global_options,\n        hash_options=hash_options,\n        config_settings=config_settings,\n        constraint=constraint,\n        extras=parts.extras,\n        user_supplied=user_supplied,\n    )\n    return install_req, name\n```\n\nOriginal documentation:\n```python\nCreates an InstallRequirement from a name, which might be a\n    requirement, directory containing 'setup.py', filename, or URL.\n\n    :param line_source: An optional string describing where the line is from,\n        for logging purposes in case of an error.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create an InstallRequirement from a pip-style requirement line.\n    InstallRequirement is a pip internal construct that represents an installable requirement,\n    and is used as an intermediary between the pip command and the resolver.\n    :param pip_line: A pip-style requirement line.\n    :param comes_from: The path to the requirements file the line was found in.\n    :param use_pep517: Whether to use PEP 517/518 when installing the\n        requirement.\n    :param isolated: Whether to isolate the requirements when installing them. (likely unused)\n    :param global_options: Extra global options to be used when installing the install req (likely unused)\n    :param hash_options: Extra hash options to be used when installing the install req (likely unused)\n    :param constraint: Whether the requirement is a constraint.\n    :param line_source: The source of the line (e.g. \"requirements.txt\").\n    :param user_supplied: Whether the requirement was directly provided by the user.\n    :param config_settings: Configuration settings to be used when installing the install req (likely unused)\n    :param expand_env: Whether to expand environment variables in the line. (definitely used)\n    :return: A tuple of the InstallRequirement and the name of the package (if determined).", "file": "pipenv_utils_dependencies.py", "function": "expansive_install_req_from_line", "project": "pipenv", "change_stats": {"code_changes": 54, "doc_changes": 23, "total_changes": 77}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save_wav(*, wav: np.ndarray, path: str, sample_rate: int = None, **kwargs) -> None:\n    \n    \n    wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n    scipy.io.wavfile.write(path, sample_rate, wav_norm.astype(np.int16))```\n\nUpdated code:\n```python\ndef save_wav(*, wav: np.ndarray, path: str, sample_rate: int = None, pipe_out = None, **kwargs) -> None:\n    \n    \n    wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n\n    wav_norm = wav_norm.astype(np.int16)\n    if pipe_out:\n        wav_buffer = BytesIO()\n        scipy.io.wavfile.write(wav_buffer, sample_rate, wav_norm)\n        wav_buffer.seek(0)\n        pipe_out.buffer.write(wav_buffer.read())\n    scipy.io.wavfile.write(path, sample_rate, wav_norm)\n```\n\nOriginal documentation:\n```python\nSave float waveform to a file using Scipy.\n\n    Args:\n        wav (np.ndarray): Waveform with float values in range [-1, 1] to save.\n        path (str): Path to a output file.\n        sr (int, optional): Sampling rate used for saving to the file. Defaults to None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save float waveform to a file using Scipy.\n\n    Args:\n        wav (np.ndarray): Waveform with float values in range [-1, 1] to save.\n        path (str): Path to a output file.\n        sr (int, optional): Sampling rate used for saving to the file. Defaults to None.\n        pipe_out (BytesIO, optional): Flag to stdout the generated TTS wav file for shell pipe.", "file": "TTS_utils_audio_numpy_transforms.py", "function": "save_wav", "project": "TTS", "change_stats": {"code_changes": 13, "doc_changes": 3, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_for_video_urls_or_video_files(cls, values):\n        \n        \n        \n        \n\n        video_urls = values.get(\"video_urls\")\n        video_urls_col = values.get(\"video_urls_col\")\n        video_files = values.get(\"video_files\")\n        video_files_col = values.get(\"video_files_col\")\n\n        if not video_urls and not video_files and not video_urls_col and not video_files_col:\n            raise ValueError(\n                \"Neither video_urls, video_files, video_urls_col nor video_files_col have been provided. Please provide one of them.\"\n            )\n\n        return values```\n\nUpdated code:\n```python\ndef check_for_video_urls_or_video_files(cls, values):\n        \n        \n        \n        \n\n        video_urls = values.get(\"video_urls\")\n        video_urls_column = values.get(\"video_urls_column\")\n        video_files = values.get(\"video_files\")\n        video_files_column = values.get(\"video_files_column\")\n\n        if not video_urls and not video_files and not video_urls_column and not video_files_column:\n            raise ValueError(\n                \"Neither video_urls, video_files, video_urls_column nor video_files_column have been provided. Please provide one of them.\"\n            )\n\n        return values\n```\n\nOriginal documentation:\n```python\nRoot validator to check if video_urls or video_files have been provided.\n\n        Parameters\n        ----------\n        values : Dict\n            Dictionary containing the attributes of the model.\n\n        Raises\n        ------\n        ValueError\n            If neither video_urls, video_files, video_urls_col nor video_files_col have been provided.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Root validator to check if video_urls or video_files have been provided.\n\n        Parameters\n        ----------\n        values : Dict\n            Dictionary containing the attributes of the model.\n\n        Raises\n        ------\n        ValueError\n            If neither video_urls, video_files, video_urls_column nor video_files_column have been provided.", "file": "mindsdb_integrations_handlers_twelve_labs_handler_settings.py", "function": "TwelveLabsHandlerModel.check_for_video_urls_or_video_files", "project": "mindsdb", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef chown_r(path, user):\n    \n    # keep these imports here for Windows compatibility\n    import grp\n    import pwd\n\n    uid = pwd.getpwnam(user).pw_uid\n    gid = grp.getgrnam(user).gr_gid\n    os.chown(path, uid, gid)\n    for root, dirs, files in os.walk(path):\n        for dirname in dirs:\n            os.chown(os.path.join(root, dirname), uid, gid)\n        for filename in files:\n            os.chown(os.path.join(root, filename), uid, gid)```\n\nUpdated code:\n```python\ndef chown_r(path: str, user: str):\n    \n    # keep these imports here for Windows compatibility\n    import grp\n    import pwd\n\n    uid = pwd.getpwnam(user).pw_uid\n    gid = grp.getgrnam(user).gr_gid\n    os.chown(path, uid, gid)\n    for root, dirs, files in os.walk(path):\n        for dirname in dirs:\n            os.chown(os.path.join(root, dirname), uid, gid)\n        for filename in files:\n            os.chown(os.path.join(root, filename), uid, gid)\n```\n\nOriginal documentation:\n```python\nRecursive chown\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Recursive chown on the given file/directory path.", "file": "localstack_utils_common.py", "function": "chown_r", "project": "localstack", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef consumer(self):\n        \n\n        \n        consumer = Consumer(self.connection.channel(),\n                            queues=[self.queue], no_ack=True)\n        consumer.register_callback(self._receive)\n        with consumer:\n            yield consumer\n        consumer.channel.close()```\n\nUpdated code:\n```python\ndef consumer(self, wakeup=True):\n        \n        consumer = Consumer(self.connection,\n                            queues=[self.queue], no_ack=True)\n        consumer.register_callback(self._receive)\n        with consumer:\n            if wakeup:\n                self.wakeup_workers(channel=consumer.channel)\n            yield consumer\n```\n\nOriginal documentation:\n```python\nCreate event consumer.\n\n        .. warning::\n\n            This creates a new channel that needs to be closed\n            by calling `consumer.channel.close()`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create event consumer.", "file": "celery_events___init__.py", "function": "EventReceiver.consumer", "project": "celery", "change_stats": {"code_changes": 11, "doc_changes": 7, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _source_mask(self, ilens):\n        \n\n        \n        x_masks = make_non_pad_mask(ilens).to(next(self.parameters()).device)\n        return x_masks.unsqueeze(-2) & x_masks.unsqueeze(-1)```\n\nUpdated code:\n```python\ndef _source_mask(self, ilens):\n        \n\n        \n        x_masks = make_non_pad_mask(ilens).to(next(self.parameters()).device)\n        return x_masks.unsqueeze(-2)\n```\n\nOriginal documentation:\n```python\nMake masks for self-attention.\n\n        Args:\n            ilens (LongTensor or List): Batch of lengths (B,).\n\n        Returns:\n            Tensor: Mask tensor for self-attention.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n        Examples:\n            >>> ilens = [5, 3]\n            >>> self._source_mask(ilens)\n            tensor([[[1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1]],\n                    [[1, 1, 1, 0, 0],\n                     [1, 1, 1, 0, 0],\n                     [1, 1, 1, 0, 0],\n                     [0, 0, 0, 0, 0],\n                     [0, 0, 0, 0, 0]]], dtype=torch.uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Make masks for self-attention.\n\n        Args:\n            ilens (LongTensor or List): Batch of lengths (B,).\n\n        Returns:\n            Tensor: Mask tensor for self-attention.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n        Examples:\n            >>> ilens = [5, 3]\n            >>> self._source_mask(ilens)\n            tensor([[[1, 1, 1, 1, 1],\n                     [1, 1, 1, 0, 0]]], dtype=torch.uint8)", "file": "espnet_nets_pytorch_backend_e2e_tts_fastspeech.py", "function": "FeedForwardTransformer._source_mask", "project": "espnet", "change_stats": {"code_changes": 4, "doc_changes": 12, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef imsave(fname, arr, plugin=None, **plugin_args):\n    \n    \n    if plugin is None and hasattr(fname, 'lower'):\n        if fname.lower().endswith(('.tiff', '.tif')):\n            plugin = 'tifffile'\n    if is_low_contrast(arr):\n        warn('%s is a low contrast image' % fname)\n    if arr.dtype == bool:\n        warn('%s is a boolean image: setting True to 1 and False to 0' % fname)\n    return call_plugin('imsave', fname, arr, plugin=plugin, **plugin_args)```\n\nUpdated code:\n```python\ndef imsave(fname, arr, plugin=None, check_contrast=True, **plugin_args):\n    \n    \n    if plugin is None and hasattr(fname, 'lower'):\n        if fname.lower().endswith(('.tiff', '.tif')):\n            plugin = 'tifffile'\n    if check_contrast and is_low_contrast(arr):\n        warn('%s is a low contrast image' % fname)\n    if arr.dtype == bool:\n        warn('%s is a boolean image: setting True to 1 and False to 0' % fname)\n    return call_plugin('imsave', fname, arr, plugin=plugin, **plugin_args)\n```\n\nOriginal documentation:\n```python\nSave an image to file.\n\n    Parameters\n    ----------\n    fname : str\n        Target filename.\n    arr : ndarray of shape (M,N) or (M,N,3) or (M,N,4)\n        Image data.\n    plugin : str\n        Name of plugin to use.  By default, the different plugins are\n        tried (starting with imageio) until a suitable\n        candidate is found.  If not given and fname is a tiff file, the\n        tifffile plugin will be used.\n\n    Other parameters\n    ----------------\n    plugin_args : keywords\n        Passed to the given plugin.\n\n    Notes\n    -----\n    When saving a JPEG, the compression ratio may be controlled using the\n    ``quality`` keyword argument which is an integer with values in [1, 100]\n    where 1 is worst quality and smallest file size, and 100 is best quality\n    and largest file size (default 75).  This is only available when using\n    the PIL and imageio plugins.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save an image to file.\n\n    Parameters\n    ----------\n    fname : str\n        Target filename.\n    arr : ndarray of shape (M,N) or (M,N,3) or (M,N,4)\n        Image data.\n    plugin : str, optional\n        Name of plugin to use.  By default, the different plugins are\n        tried (starting with imageio) until a suitable\n        candidate is found.  If not given and fname is a tiff file, the\n        tifffile plugin will be used.\n    check_contrast : bool, optional\n        Check for low contrast and print warning (default: True).\n\n    Other parameters\n    ----------------\n    plugin_args : keywords\n        Passed to the given plugin.\n\n    Notes\n    -----\n    When saving a JPEG, the compression ratio may be controlled using the\n    ``quality`` keyword argument which is an integer with values in [1, 100]\n    where 1 is worst quality and smallest file size, and 100 is best quality\n    and largest file size (default 75).  This is only available when using\n    the PIL and imageio plugins.", "file": "skimage_io__io.py", "function": "imsave", "project": "scikit-image", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_get_home_dir_3():\n    \n    env[\"HOME\"] = HOME_TEST_DIR\n    home_dir = path.get_home_dir()\n    nt.assert_equal(home_dir, env[\"HOME\"])```\n\nUpdated code:\n```python\ndef test_get_home_dir_3():\n    \n    env[\"HOME\"] = HOME_TEST_DIR\n    home_dir = path.get_home_dir(True)\n    # get_home_dir expands symlinks\n    nt.assert_equal(home_dir, os.path.realpath(env[\"HOME\"]))\n```\n\nOriginal documentation:\n```python\nTestcase $HOME is set, then use its value as home directory.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "get_home_dir() uses $HOME if set", "file": "IPython_utils_tests_test_path.py", "function": "test_get_home_dir_3", "project": "ipython", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef change_geometry(self, numrows, numcols, num):\n        ''\n        self._subplotspec = GridSpec(numrows, numcols)[num-1]\n        self.update_params()\n        self.set_position(self.figbox)```\n\nUpdated code:\n```python\ndef change_geometry(self, numrows, numcols, num):\n        \n        self._subplotspec = GridSpec(numrows, numcols)[num-1]\n        self.update_params()\n        self.set_position(self.figbox)\n```\n\nOriginal documentation:\n```python\nchange subplot geometry, e.g., from 1,1,1 to 2,2,3\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Change subplot geometry, e.g., from (1, 1, 1) to (2, 2, 3).", "file": "lib_mpl_toolkits_axes_grid1_axes_divider.py", "function": "SubplotDivider.change_geometry", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n        \n        \n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func != nn.BatchNorm2d\n        else:\n            use_bias = norm_layer != nn.BatchNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.model = nn.Sequential(*sequence)```\n\nUpdated code:\n```python\ndef __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n        \n        \n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func != nn.BatchNorm2d\n        else:\n            use_bias = norm_layer != nn.BatchNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.model = nn.Sequential(*sequence)\n```\n\nOriginal documentation:\n```python\nConstruct a PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the first conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator;\n            norm_layer      -- normalization layer\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Construct a PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer", "file": "models_networks.py", "function": "NLayerDiscriminator.__init__", "project": "pytorch-CycleGAN-and-pix2pix", "change_stats": {"code_changes": 14, "doc_changes": 6, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef module(self) -> FlattenParamsWrapper:\n        \n        assert isinstance(self._fsdp_wrapped_module, FlattenParamsWrapper)\n        return self._fsdp_wrapped_module```\n\nUpdated code:\n```python\ndef module(self) -> nn.Module:\n        \n        \n        assert isinstance(self._fsdp_wrapped_module, FlattenParamsWrapper)\n        return self._fsdp_wrapped_module.module\n```\n\nOriginal documentation:\n```python\nmake model.module accessible, just like DDP.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Make model.module accessible, just like DDP. Return the\n        underlying module without the flatten_params_wrapper", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel.module", "project": "pytorch", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_buy_rate(self, pair: str, refresh: bool, tick: Dict = None) -> float:\n        \n        \n        \n        config_bid_strategy = self.config.get('bid_strategy', {})\n        if 'use_order_book' in config_bid_strategy and\\\n                config_bid_strategy.get('use_order_book', False):\n            logger.info('Getting price from order book')\n            order_book_top = config_bid_strategy.get('order_book_top', 1)\n            order_book = self.exchange.get_order_book(pair, order_book_top)\n            logger.debug('order_book %s', order_book)\n            # top 1 = index 0\n            order_book_rate = order_book['bids'][order_book_top - 1][0]\n            logger.info('...top %s order book buy rate %0.8f', order_book_top, order_book_rate)\n            used_rate = order_book_rate\n        else:\n            if not tick:\n                logger.info('Using Last Ask / Last Price')\n                ticker = self.exchange.fetch_ticker(pair, refresh)\n            else:\n                ticker = tick\n            if ticker['ask'] < ticker['last']:\n                ticker_rate = ticker['ask']\n            else:\n                balance = self.config['bid_strategy']['ask_last_balance']\n                ticker_rate = ticker['ask'] + balance * (ticker['last'] - ticker['ask'])\n            used_rate = ticker_rate\n\n        return used_rate```\n\nUpdated code:\n```python\ndef get_buy_rate(self, pair: str, refresh: bool) -> float:\n        \n        \n        \n        if not refresh:\n            rate = self._buy_rate_cache.get(pair)\n            # Check if cache has been invalidated\n            if rate:\n                logger.info(f\"Using cached buy rate for {pair}.\")\n                return rate\n\n        config_bid_strategy = self.config.get('bid_strategy', {})\n        if 'use_order_book' in config_bid_strategy and\\\n                config_bid_strategy.get('use_order_book', False):\n            logger.info('Getting price from order book')\n            order_book_top = config_bid_strategy.get('order_book_top', 1)\n            order_book = self.exchange.get_order_book(pair, order_book_top)\n            logger.debug('order_book %s', order_book)\n            # top 1 = index 0\n            order_book_rate = order_book['bids'][order_book_top - 1][0]\n            logger.info('...top %s order book buy rate %0.8f', order_book_top, order_book_rate)\n            used_rate = order_book_rate\n        else:\n            logger.info('Using Last Ask / Last Price')\n            ticker = self.exchange.fetch_ticker(pair)\n            if ticker['ask'] < ticker['last']:\n                ticker_rate = ticker['ask']\n            else:\n                balance = self.config['bid_strategy']['ask_last_balance']\n                ticker_rate = ticker['ask'] + balance * (ticker['last'] - ticker['ask'])\n            used_rate = ticker_rate\n\n        self._buy_rate_cache[pair] = used_rate\n\n        return used_rate\n```\n\nOriginal documentation:\n```python\nCalculates bid target between current ask price and last price\n        :return: float: Price\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculates bid target between current ask price and last price\n        :param pair: Pair to get rate for\n        :param refresh: allow cached data\n        :return: float: Price", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.get_buy_rate", "project": "freqtrade", "change_stats": {"code_changes": 20, "doc_changes": 4, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef books(self, oncard=False, end_session=True):\n         \n         \n            \n        raise NotImplementedError()```\n\nUpdated code:\n```python\ndef books(self, oncard=None, end_session=True):\n         \n         \n            \n        raise NotImplementedError()\n```\n\nOriginal documentation:\n```python\nReturn a list of ebooks on the device.\n        @param oncard:  If True return a list of ebooks on the storage card, \n                        otherwise return list of ebooks in main memory of device.\n                        If True and no books on card return empty list. \n        @return: A BookList.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a list of ebooks on the device.\n        @param oncard:  If 'carda' or 'cardb' return a list of ebooks on the\n                        specific storage card, otherwise return list of ebooks\n                        in main memory of device. If a card is specified and no\n                        books are on the card return empty list. \n        @return: A BookList.", "file": "src_calibre_devices_interface.py", "function": "Device.books", "project": "calibre", "change_stats": {"code_changes": 4, "doc_changes": 9, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _wrap(text, wrap_max=80, indent=4):\n    \n    \n    \n    indent = indent * ' '\n    wrap_width = wrap_max - len(indent)\n    if isinstance(text, Path):\n        text = path2str(text)\n    return textwrap.fill(text, width=wrap_width, initial_indent=indent,\n                         subsequent_indent=indent, break_long_words=False,\n                         break_on_hyphens=False)```\n\nUpdated code:\n```python\ndef _wrap(text, wrap_max=80, indent=4):\n    \n    \n    indent = indent * ' '\n    wrap_width = wrap_max - len(indent)\n    if isinstance(text, Path):\n        text = path2str(text)\n    return textwrap.fill(text, width=wrap_width, initial_indent=indent,\n                         subsequent_indent=indent, break_long_words=False,\n                         break_on_hyphens=False)\n```\n\nOriginal documentation:\n```python\nWrap text at given width using textwrap module. Indent should consist of\n    spaces. Its length is deducted from wrap width to ensure exact wrapping.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Wrap text at given width using textwrap module.\n\n    Args:\n        text (unicode): Text to wrap. If it's a Path, it's converted to string.\n        wrap_max (int): Maximum line length (indent is deducted).\n        indent (int): Number of spaces for indentation.\n    Returns:\n        unicode: Wrapped text.", "file": "spacy_util.py", "function": "_wrap", "project": "spaCy", "change_stats": {"code_changes": 3, "doc_changes": 12, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef wait_for_handshake(self, callback=None):\n        \n        \n        if (self._ssl_connect_callback is not None or\n                self._ssl_connect_future is not None):\n            raise RuntimeError(\"Already waiting\")\n        if callback is not None:\n            self._ssl_connect_callback = stack_context.wrap(callback)\n            future = None\n        else:\n            future = self._ssl_connect_future = Future()\n        if not self._ssl_accepting:\n            self._run_ssl_connect_callback()\n        return future```\n\nUpdated code:\n```python\ndef wait_for_handshake(self, callback=None):\n        \n\n        \n        if (self._ssl_connect_callback is not None or\n                self._ssl_connect_future is not None):\n            raise RuntimeError(\"Already waiting\")\n        if callback is not None:\n            warnings.warn(\"callback argument is deprecated, use returned Future instead\",\n                          DeprecationWarning)\n            self._ssl_connect_callback = stack_context.wrap(callback)\n            future = None\n        else:\n            future = self._ssl_connect_future = Future()\n        if not self._ssl_accepting:\n            self._run_ssl_connect_callback()\n        return future\n```\n\nOriginal documentation:\n```python\nWait for the initial SSL handshake to complete.\n\n        If a ``callback`` is given, it will be called with no\n        arguments once the handshake is complete; otherwise this\n        method returns a `.Future` which will resolve to the\n        stream itself after the handshake is complete.\n\n        Once the handshake is complete, information such as\n        the peer's certificate and NPN/ALPN selections may be\n        accessed on ``self.socket``.\n\n        This method is intended for use on server-side streams\n        or after using `IOStream.start_tls`; it should not be used\n        with `IOStream.connect` (which already waits for the\n        handshake to complete). It may only be called once per stream.\n\n        .. versionadded:: 4.2\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Wait for the initial SSL handshake to complete.\n\n        If a ``callback`` is given, it will be called with no\n        arguments once the handshake is complete; otherwise this\n        method returns a `.Future` which will resolve to the\n        stream itself after the handshake is complete.\n\n        Once the handshake is complete, information such as\n        the peer's certificate and NPN/ALPN selections may be\n        accessed on ``self.socket``.\n\n        This method is intended for use on server-side streams\n        or after using `IOStream.start_tls`; it should not be used\n        with `IOStream.connect` (which already waits for the\n        handshake to complete). It may only be called once per stream.\n\n        .. versionadded:: 4.2\n\n        .. deprecated:: 5.1\n\n           The ``callback`` argument is deprecated and will be removed\n           in Tornado 6.0. Use the returned `.Future` instead.", "file": "tornado_iostream.py", "function": "SSLIOStream.wait_for_handshake", "project": "tornado", "change_stats": {"code_changes": 5, "doc_changes": 7, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _inverse_stft(self, stft, time_crop=None):\n         \n        \n        inversed = inverse_stft(\n            tf.transpose(stft, perm=[2, 0, 1]),\n            self._frame_length,\n            self._frame_step,\n            window_fn=lambda frame_length, dtype: (\n                hann_window(frame_length, periodic=True, dtype=dtype))\n        ) * self.WINDOW_COMPENSATION_FACTOR\n        reshaped = tf.transpose(inversed)\n        if time_crop is None:\n            time_crop = tf.shape(self._features['waveform'])[0]\n        return reshaped[:time_crop, :]```\n\nUpdated code:\n```python\ndef _inverse_stft(self, stft_t, time_crop=None):\n         \n        \n        inversed = inverse_stft(\n            tf.transpose(stft_t, perm=[2, 0, 1]),\n            self._frame_length,\n            self._frame_step,\n            window_fn=lambda frame_length, dtype: (\n                hann_window(frame_length, periodic=True, dtype=dtype))\n        ) * self.WINDOW_COMPENSATION_FACTOR\n        reshaped = tf.transpose(inversed)\n        if time_crop is None:\n            time_crop = tf.shape(self._features['waveform'])[0]\n        return reshaped[:time_crop, :]\n```\n\nOriginal documentation:\n```python\nInverse and reshape the given STFT\n\n        :param stft: input STFT\n        :returns: inverse STFT (waveform)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inverse and reshape the given STFT\n\n        :param stft_t: input STFT\n        :returns: inverse STFT (waveform)", "file": "spleeter_model___init__.py", "function": "EstimatorSpecBuilder._inverse_stft", "project": "spleeter", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef colorstr(*input):\n    \n    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {\n        'black': '\\033[30m',  # basic colors\n        'red': '\\033[31m',\n        'green': '\\033[32m',\n        'yellow': '\\033[33m',\n        'blue': '\\033[34m',\n        'magenta': '\\033[35m',\n        'cyan': '\\033[36m',\n        'white': '\\033[37m',\n        'bright_black': '\\033[90m',  # bright colors\n        'bright_red': '\\033[91m',\n        'bright_green': '\\033[92m',\n        'bright_yellow': '\\033[93m',\n        'bright_blue': '\\033[94m',\n        'bright_magenta': '\\033[95m',\n        'bright_cyan': '\\033[96m',\n        'bright_white': '\\033[97m',\n        'end': '\\033[0m',  # misc\n        'bold': '\\033[1m',\n        'underline': '\\033[4m'}\n    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']```\n\nUpdated code:\n```python\ndef colorstr(*input):\n    \n    \n    \n    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {\n        'black': '\\033[30m',  # basic colors\n        'red': '\\033[31m',\n        'green': '\\033[32m',\n        'yellow': '\\033[33m',\n        'blue': '\\033[34m',\n        'magenta': '\\033[35m',\n        'cyan': '\\033[36m',\n        'white': '\\033[37m',\n        'bright_black': '\\033[90m',  # bright colors\n        'bright_red': '\\033[91m',\n        'bright_green': '\\033[92m',\n        'bright_yellow': '\\033[93m',\n        'bright_blue': '\\033[94m',\n        'bright_magenta': '\\033[95m',\n        'bright_cyan': '\\033[96m',\n        'bright_white': '\\033[97m',\n        'end': '\\033[0m',  # misc\n        'bold': '\\033[1m',\n        'underline': '\\033[4m'}\n    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n```\n\nOriginal documentation:\n```python\nColors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world').\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Colors a string based on the provided color and style arguments. Utilizes ANSI escape codes.\n    See https://en.wikipedia.org/wiki/ANSI_escape_code for more details.\n\n    This function can be called in two ways:\n        - colorstr('color', 'style', 'your string')\n        - colorstr('your string')\n\n    In the second form, 'blue' and 'bold' will be applied by default.\n\n    Args:\n        *input (str): A sequence of strings where the first n-1 strings are color and style arguments,\n                      and the last string is the one to be colored.\n\n    Supported Colors and Styles:\n        Basic Colors: 'black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'\n        Bright Colors: 'bright_black', 'bright_red', 'bright_green', 'bright_yellow',\n                       'bright_blue', 'bright_magenta', 'bright_cyan', 'bright_white'\n        Misc: 'end', 'bold', 'underline'\n\n    Returns:\n        (str): The input string wrapped with ANSI escape codes for the specified color and style.\n\n    Examples:\n        >>> colorstr('blue', 'bold', 'hello world')\n        >>> '\\033[34m\\033[1mhello world\\033[0m'", "file": "ultralytics_utils___init__.py", "function": "colorstr", "project": "ultralytics", "change_stats": {"code_changes": 4, "doc_changes": 28, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_fixed_in_frame_mobjects(self, *mobjects):\n        \n        \n        \n        if config.renderer != \"opengl\":\n            self.add(*mobjects)\n            self.camera: ThreeDCamera\n            self.camera.add_fixed_in_frame_mobjects(*mobjects)\n        else:\n            for mob in mobjects:\n                mob: OpenGLMobject\n                mob.fix_in_frame()\n                self.add(mob)```\n\nUpdated code:\n```python\ndef add_fixed_in_frame_mobjects(self, *mobjects: Mobject):\n        \n        \n        \n        if config.renderer != \"opengl\":\n            self.add(*mobjects)\n            self.camera: ThreeDCamera\n            self.camera.add_fixed_in_frame_mobjects(*mobjects)\n        else:\n            for mob in mobjects:\n                mob: OpenGLMobject\n                mob.fix_in_frame()\n                self.add(mob)\n```\n\nOriginal documentation:\n```python\nThis method is used to prevent the rotation and movement\n        of mobjects as the camera moves around. The mobject is\n        essentially overlaid, and is not impacted by the camera's\n        movement in any way.\n\n        Parameters\n        ----------\n        *mobjects : Mobjects\n            The Mobjects whose orientation must be fixed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method is used to prevent the rotation and movement\n        of mobjects as the camera moves around. The mobject is\n        essentially overlaid, and is not impacted by the camera's\n        movement in any way.\n\n        Parameters\n        ----------\n        *mobjects\n            The Mobjects whose orientation must be fixed.", "file": "manim_scene_three_d_scene.py", "function": "ThreeDScene.add_fixed_in_frame_mobjects", "project": "manim", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef text(self, value):\n        \n        \n\n        \n        if not self.started:\n            self.start()\n        # \u66f4\u591a\u7684\u8f93\u5165\u7528\u6cd5\u8bf7\u89c1 https://github.com/macacajs/android-unicode#use-in-adb-shell\n        value = ensure_unicode(value)\n        if '\\'' in value:\n            # \u5982\u679cvalue\u4e2d\u6709\u7279\u6b8a\u7b26\u53f7\uff0c\u5fc5\u987b\u7528\u5355\u5f15\u53f7\u5305\u88f9\u5185\u5bb9\uff0c\u56e0\u6b64\u5c06\u5355\u5f15\u53f7\u66ff\u6362\u4e3a\u53cc\u5f15\u53f7\n            value = value.replace('\\'', '\"')\n        self.adb.shell(f\"am broadcast -a ADB_INPUT_TEXT --es msg '{value}'\")```\n\nUpdated code:\n```python\ndef text(self, value):\n        \n        \n\n        \n        if not self.started:\n            self.start()\n        # \u66f4\u591a\u7684\u8f93\u5165\u7528\u6cd5\u8bf7\u89c1 https://github.com/macacajs/android-unicode#use-in-adb-shell\n        value = ensure_unicode(value)\n        value = escape_special_char(value)\n        self.adb.shell(f\"am broadcast -a ADB_INPUT_TEXT --es msg {value}\")\n```\n\nOriginal documentation:\n```python\nInput text with Yosemite input method\n\n        Args:\n            value: text to be inputted, will automatically replace single quotes with double quotes\n\n        Returns:\n            output form `adb shell` command\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Input text with Yosemite input method\n\n        Args:\n            value: text to be inputted\n\n        Returns:\n            output form `adb shell` command", "file": "airtest_core_android_ime.py", "function": "YosemiteIme.text", "project": "Airtest", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef schedule_event(self, event, sched_time, repeat=None, data=None, context=None):\n        \n        \n        data = data or {}\n        with self.event_lock:\n            # get current list of scheduled times for event, [] if missing\n            event_list = self.events.get(event, [])\n\n            # Don't schedule if the event is repeating and already scheduled\n            if repeat and event in self.events:\n                LOG.debug('Repeating event {} is already scheduled, discarding'\n                          .format(event))\n            else:\n                # add received event and time\n                event_list.append((sched_time, repeat, data, context))\n                self.events[event] = event_list```\n\nUpdated code:\n```python\ndef schedule_event(self, event, sched_time, repeat=None,\n                       data=None, context=None):\n        \n        \n        data = data or {}\n        with self.event_lock:\n            # get current list of scheduled times for event, [] if missing\n            event_list = self.events.get(event, [])\n\n            # Don't schedule if the event is repeating and already scheduled\n            if repeat and event in self.events:\n                LOG.debug('Repeating event {} is already scheduled, discarding'\n                          .format(event))\n            else:\n                # add received event and time\n                event_list.append((sched_time, repeat, data, context))\n                self.events[event] = event_list\n```\n\nOriginal documentation:\n```python\nAdd event to pending event schedule.\n\n        Arguments:\n            event (str): Handler for the event\n            sched_time ([type]): [description]\n            repeat ([type], optional): Defaults to None. [description]\n            data ([type], optional): Defaults to None. [description]\n            context (dict, optional): message context to send when the handler is called\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add event to pending event schedule.\n\n        Arguments:\n            event (str): Handler for the event\n            sched_time ([type]): [description]\n            repeat ([type], optional): Defaults to None. [description]\n            data ([type], optional): Defaults to None. [description]\n            context (dict, optional): context (dict, optional): message\n                                      context to send when the\n                                      handler is called", "file": "mycroft_skills_event_scheduler.py", "function": "EventScheduler.schedule_event", "project": "mycroft-core", "change_stats": {"code_changes": 5, "doc_changes": 6, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef value_counts(\n    values, sort=True, ascending=False, normalize=False, bins=None, dropna=True\n):\n    \n    \n\n    \n    from pandas.core.series import Series, Index\n\n    name = getattr(values, \"name\", None)\n\n    if bins is not None:\n        try:\n            from pandas.core.reshape.tile import cut\n\n            values = Series(values)\n            ii = cut(values, bins, include_lowest=True)\n        except TypeError:\n            raise TypeError(\"bins argument only works with numeric data.\")\n\n        # count, remove nulls (from the index), and but the bins\n        result = ii.value_counts(dropna=dropna)\n        result = result[result.index.notna()]\n        result.index = result.index.astype(\"interval\")\n        result = result.sort_index()\n\n        # if we are dropna and we have NO values\n        if dropna and (result.values == 0).all():\n            result = result.iloc[0:0]\n\n        # normalizing is by len of all (regardless of dropna)\n        counts = np.array([len(ii)])\n\n    else:\n\n        if is_extension_array_dtype(values):\n\n            # handle Categorical and sparse,\n            result = Series(values)._values.value_counts(dropna=dropna)\n            result.name = name\n            counts = result.values\n\n        else:\n            keys, counts = _value_counts_arraylike(values, dropna)\n\n            if not isinstance(keys, Index):\n                keys = Index(keys)\n            result = Series(counts, index=keys, name=name)\n\n    if sort:\n        result = result.sort_values(ascending=ascending)\n\n    if normalize:\n        result = result / float(counts.sum())\n\n    return result```\n\nUpdated code:\n```python\ndef value_counts(\n    values,\n    sort: bool = True,\n    ascending: bool = False,\n    normalize: bool = False,\n    bins=None,\n    dropna: bool = True,\n):\n    \n    \n    \n    from pandas.core.series import Series, Index\n\n    name = getattr(values, \"name\", None)\n\n    if bins is not None:\n        try:\n            from pandas.core.reshape.tile import cut\n\n            values = Series(values)\n            ii = cut(values, bins, include_lowest=True)\n        except TypeError:\n            raise TypeError(\"bins argument only works with numeric data.\")\n\n        # count, remove nulls (from the index), and but the bins\n        result = ii.value_counts(dropna=dropna)\n        result = result[result.index.notna()]\n        result.index = result.index.astype(\"interval\")\n        result = result.sort_index()\n\n        # if we are dropna and we have NO values\n        if dropna and (result.values == 0).all():\n            result = result.iloc[0:0]\n\n        # normalizing is by len of all (regardless of dropna)\n        counts = np.array([len(ii)])\n\n    else:\n\n        if is_extension_array_dtype(values):\n\n            # handle Categorical and sparse,\n            result = Series(values)._values.value_counts(dropna=dropna)\n            result.name = name\n            counts = result.values\n\n        else:\n            keys, counts = _value_counts_arraylike(values, dropna)\n\n            if not isinstance(keys, Index):\n                keys = Index(keys)\n            result = Series(counts, index=keys, name=name)\n\n    if sort:\n        result = result.sort_values(ascending=ascending)\n\n    if normalize:\n        result = result / float(counts.sum())\n\n    return result\n```\n\nOriginal documentation:\n```python\nCompute a histogram of the counts of non-null values.\n\n    Parameters\n    ----------\n    values : ndarray (1-d)\n    sort : boolean, default True\n        Sort by values\n    ascending : boolean, default False\n        Sort in ascending order\n    normalize: boolean, default False\n        If True then compute a relative histogram\n    bins : integer, optional\n        Rather than count values, group them into half-open bins,\n        convenience for pd.cut, only works with numeric data\n    dropna : boolean, default True\n        Don't include counts of NaN\n\n    Returns\n    -------\n    value_counts : Series\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute a histogram of the counts of non-null values.\n\n    Parameters\n    ----------\n    values : ndarray (1-d)\n    sort : bool, default True\n        Sort by values\n    ascending : bool, default False\n        Sort in ascending order\n    normalize: bool, default False\n        If True then compute a relative histogram\n    bins : integer, optional\n        Rather than count values, group them into half-open bins,\n        convenience for pd.cut, only works with numeric data\n    dropna : bool, default True\n        Don't include counts of NaN\n\n    Returns\n    -------\n    Series", "file": "pandas_core_algorithms.py", "function": "value_counts", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 12, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _find_symbolic_in_registry(\n    domain: str,\n    op_name: str,\n    opset_version: int,\n    operator_export_type: torch.onnx.OperatorExportTypes,\n) -> Optional[Callable]:\n    \n    \n\n    if not symbolic_registry.is_registered_op(op_name, domain, opset_version):\n        if operator_export_type == torch.onnx.OperatorExportTypes.ONNX_FALLTHROUGH:\n            # Use the original node directly\n            return None\n    return symbolic_registry.get_registered_op(op_name, domain, opset_version)```\n\nUpdated code:\n```python\ndef _find_symbolic_in_registry(\n    domain: str,\n    op_name: str,\n    opset_version: int,\n    operator_export_type: _C_onnx.OperatorExportTypes,\n) -> Optional[Callable]:\n    \n    \n\n    if not symbolic_registry.is_registered_op(op_name, domain, opset_version):\n        if operator_export_type == _C_onnx.OperatorExportTypes.ONNX_FALLTHROUGH:\n            # Use the original node directly\n            return None\n    return symbolic_registry.get_registered_op(op_name, domain, opset_version)\n```\n\nOriginal documentation:\n```python\nLooks up for the symbolic function in the registry.\n\n    Args:\n        domain: The domain of the symbolic function.\n        op_name: The name of the op.\n        opset_version: Currect opset used.\n        operator_export_type: An enum in torch.onnx.OperatorExportTypes.\n\n    Returns:\n        The symbolic function if found, None otherwise.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Looks up for the symbolic function in the registry.\n\n    Args:\n        domain: The domain of the symbolic function.\n        op_name: The name of the op.\n        opset_version: Currect opset used.\n        operator_export_type: An enum in _C_onnx.OperatorExportTypes.\n\n    Returns:\n        The symbolic function if found, None otherwise.", "file": "torch_onnx_utils.py", "function": "_find_symbolic_in_registry", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_mean_grad_case_1a(self):\n         \n        \n        d = torch.tensor([[0, 1, 2], [3, 4, 5.0]], requires_grad=True)\n        m = torch.tensor([[True, False, False], [False, True, False]])\n        with self.assertWarnsRegex(UserWarning, \"It is not recommended to create a MaskedTensor\"):\n            mt = MaskedTensor(d, m, requires_grad=True)\n        mt.mean().backward()\n        self.assertIsNone(d.grad)\n        _compare_mts(mt.grad, MaskedTensor(torch.tensor([[0.5, 0, 0], [0, 0.5, 0]]), m))```\n\nUpdated code:\n```python\ndef test_mean_grad_case_1a(self):\n         \n        \n        d = torch.tensor([[0, 1, 2], [3, 4, 5.0]], requires_grad=True)\n        m = torch.tensor([[True, False, False], [False, True, False]])\n        with self.assertWarnsRegex(UserWarning, \"It is not recommended to create a MaskedTensor\"):\n            mt = masked_tensor(d, m, requires_grad=True)\n        mt.mean().backward()\n        self.assertIsNone(d.grad)\n        _compare_mts(mt.grad, masked_tensor(torch.tensor([[0.5, 0, 0], [0, 0.5, 0]]), m))\n```\n\nOriginal documentation:\n```python\nvalues.requires_grad = True\n            mt = MaskedTensor(values, mask, requires_grad=True)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "values.requires_grad = True\n            mt = masked_tensor(values, mask, requires_grad=True)", "file": "test_test_maskedtensor.py", "function": "TestReductions.test_mean_grad_case_1a", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef stock_zt_pool_dtgc_em(date: str = '20210903') -> pd.DataFrame:\n    \n    \n    \n    url = 'http://push2ex.eastmoney.com/getTopicDTPool'\n    params = {\n        'ut': '7eea3edcaed734bea9cbfc24409ed989',\n        'dpt': 'wz.ztzt',\n        'Pageindex': '0',\n        'pagesize': '170',\n        'sort': 'fund:asc',\n        'date': date,\n        '_': '1621590489736',\n    }\n    r = requests.get(url, params=params)\n    data_json = r.json()\n    if data_json['data'] is None:\n        return pd.DataFrame()\n    temp_df = pd.DataFrame(data_json['data']['pool'])\n    temp_df.reset_index(inplace=True)\n    temp_df['index'] = range(1, len(temp_df)+1)\n    temp_df.columns = [\n        '\u5e8f\u53f7',\n        '\u4ee3\u7801',\n        '_',\n        '\u540d\u79f0',\n        '\u6700\u65b0\u4ef7',\n        '\u6da8\u8dcc\u5e45',\n        '\u6210\u4ea4\u989d',\n        '\u6d41\u901a\u5e02\u503c',\n        '\u603b\u5e02\u503c',\n        '\u52a8\u6001\u5e02\u76c8\u7387',\n        '\u6362\u624b\u7387',\n        '\u5c01\u5355\u8d44\u91d1',\n        '\u6700\u540e\u5c01\u677f\u65f6\u95f4',\n        '\u677f\u4e0a\u6210\u4ea4\u989d',\n        '\u8fde\u7eed\u8dcc\u505c',\n        '\u5f00\u677f\u6b21\u6570',\n        '\u6240\u5c5e\u884c\u4e1a',\n    ]\n    temp_df = temp_df[[\n        '\u5e8f\u53f7',\n        '\u4ee3\u7801',\n        '\u540d\u79f0',\n        '\u6da8\u8dcc\u5e45',\n        '\u6700\u65b0\u4ef7',\n        '\u6210\u4ea4\u989d',\n        '\u6d41\u901a\u5e02\u503c',\n        '\u603b\u5e02\u503c',\n        '\u52a8\u6001\u5e02\u76c8\u7387',\n        '\u6362\u624b\u7387',\n        '\u5c01\u5355\u8d44\u91d1',\n        '\u6700\u540e\u5c01\u677f\u65f6\u95f4',\n        '\u677f\u4e0a\u6210\u4ea4\u989d',\n        '\u8fde\u7eed\u8dcc\u505c',\n        '\u5f00\u677f\u6b21\u6570',\n        '\u6240\u5c5e\u884c\u4e1a',\n    ]]\n    temp_df['\u6700\u65b0\u4ef7'] = temp_df['\u6700\u65b0\u4ef7'] / 1000\n    temp_df['\u6700\u540e\u5c01\u677f\u65f6\u95f4'] = temp_df['\u6700\u540e\u5c01\u677f\u65f6\u95f4'].astype(str).str.zfill(6)\n    return temp_df```\n\nUpdated code:\n```python\ndef stock_zt_pool_dtgc_em(date: str = '20220425') -> pd.DataFrame:\n    \n    \n    \n    url = 'http://push2ex.eastmoney.com/getTopicDTPool'\n    params = {\n        'ut': '7eea3edcaed734bea9cbfc24409ed989',\n        'dpt': 'wz.ztzt',\n        'Pageindex': '0',\n        'pagesize': '10000',\n        'sort': 'fund:asc',\n        'date': date,\n        '_': '1621590489736',\n    }\n    r = requests.get(url, params=params)\n    data_json = r.json()\n    if data_json['data'] is None:\n        return pd.DataFrame()\n    temp_df = pd.DataFrame(data_json['data']['pool'])\n    temp_df.reset_index(inplace=True)\n    temp_df['index'] = range(1, len(temp_df)+1)\n    temp_df.columns = [\n        '\u5e8f\u53f7',\n        '\u4ee3\u7801',\n        '_',\n        '\u540d\u79f0',\n        '\u6700\u65b0\u4ef7',\n        '\u6da8\u8dcc\u5e45',\n        '\u6210\u4ea4\u989d',\n        '\u6d41\u901a\u5e02\u503c',\n        '\u603b\u5e02\u503c',\n        '\u52a8\u6001\u5e02\u76c8\u7387',\n        '\u6362\u624b\u7387',\n        '\u5c01\u5355\u8d44\u91d1',\n        '\u6700\u540e\u5c01\u677f\u65f6\u95f4',\n        '\u677f\u4e0a\u6210\u4ea4\u989d',\n        '\u8fde\u7eed\u8dcc\u505c',\n        '\u5f00\u677f\u6b21\u6570',\n        '\u6240\u5c5e\u884c\u4e1a',\n    ]\n    temp_df = temp_df[[\n        '\u5e8f\u53f7',\n        '\u4ee3\u7801',\n        '\u540d\u79f0',\n        '\u6da8\u8dcc\u5e45',\n        '\u6700\u65b0\u4ef7',\n        '\u6210\u4ea4\u989d',\n        '\u6d41\u901a\u5e02\u503c',\n        '\u603b\u5e02\u503c',\n        '\u52a8\u6001\u5e02\u76c8\u7387',\n        '\u6362\u624b\u7387',\n        '\u5c01\u5355\u8d44\u91d1',\n        '\u6700\u540e\u5c01\u677f\u65f6\u95f4',\n        '\u677f\u4e0a\u6210\u4ea4\u989d',\n        '\u8fde\u7eed\u8dcc\u505c',\n        '\u5f00\u677f\u6b21\u6570',\n        '\u6240\u5c5e\u884c\u4e1a',\n    ]]\n    temp_df['\u6700\u65b0\u4ef7'] = temp_df['\u6700\u65b0\u4ef7'] / 1000\n    temp_df['\u6700\u540e\u5c01\u677f\u65f6\u95f4'] = temp_df['\u6700\u540e\u5c01\u677f\u65f6\u95f4'].astype(str).str.zfill(6)\n\n    temp_df['\u6da8\u8dcc\u5e45'] = pd.to_numeric(temp_df['\u6da8\u8dcc\u5e45'])\n    temp_df['\u6700\u65b0\u4ef7'] = pd.to_numeric(temp_df['\u6700\u65b0\u4ef7'])\n    temp_df['\u6210\u4ea4\u989d'] = pd.to_numeric(temp_df['\u6210\u4ea4\u989d'])\n    temp_df['\u6d41\u901a\u5e02\u503c'] = pd.to_numeric(temp_df['\u6d41\u901a\u5e02\u503c'])\n    temp_df['\u603b\u5e02\u503c'] = pd.to_numeric(temp_df['\u603b\u5e02\u503c'])\n    temp_df['\u52a8\u6001\u5e02\u76c8\u7387'] = pd.to_numeric(temp_df['\u52a8\u6001\u5e02\u76c8\u7387'])\n    temp_df['\u6362\u624b\u7387'] = pd.to_numeric(temp_df['\u6362\u624b\u7387'])\n    temp_df['\u5c01\u5355\u8d44\u91d1'] = pd.to_numeric(temp_df['\u5c01\u5355\u8d44\u91d1'])\n    temp_df['\u677f\u4e0a\u6210\u4ea4\u989d'] = pd.to_numeric(temp_df['\u677f\u4e0a\u6210\u4ea4\u989d'])\n    temp_df['\u8fde\u7eed\u8dcc\u505c'] = pd.to_numeric(temp_df['\u8fde\u7eed\u8dcc\u505c'])\n    temp_df['\u5f00\u677f\u6b21\u6570'] = pd.to_numeric(temp_df['\u5f00\u677f\u6b21\u6570'])\n    temp_df['\u5f00\u677f\u6b21\u6570'] = pd.to_numeric(temp_df['\u5f00\u677f\u6b21\u6570'])\n\n    return temp_df\n```\n\nOriginal documentation:\n```python\n\u4e1c\u65b9\u8d22\u5bcc\u7f51-\u884c\u60c5\u4e2d\u5fc3-\u6da8\u505c\u677f\u884c\u60c5-\u8dcc\u505c\u80a1\u6c60\n    http://quote.eastmoney.com/ztb/detail#type=dtgc\n    :return: \u8dcc\u505c\u80a1\u6c60\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u4e1c\u65b9\u8d22\u5bcc\u7f51-\u884c\u60c5\u4e2d\u5fc3-\u6da8\u505c\u677f\u884c\u60c5-\u8dcc\u505c\u80a1\u6c60\n    http://quote.eastmoney.com/ztb/detail#type=dtgc\n    :param date: \u4ea4\u6613\u65e5\n    :type date: str\n    :return: \u8dcc\u505c\u80a1\u6c60\n    :rtype: pandas.DataFrame", "file": "akshare_stock_feature_stock_ztb_em.py", "function": "stock_zt_pool_dtgc_em", "project": "akshare", "change_stats": {"code_changes": 20, "doc_changes": 4, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_use_explicit_o2o_to_parent_as_pk(self):\n        \n        \n        \n        self.assertEqual(ParkingLot2._meta.pk.name, \"parent\")\n\n        # However, the connector from child to parent need not be the pk on\n        # the child at all.\n        self.assertEqual(ParkingLot3._meta.pk.name, \"primary_key\")\n        # the child->parent link\n        self.assertEqual(\n            ParkingLot3._meta.get_ancestor_link(Place).name,\n            \"parent\")```\n\nUpdated code:\n```python\ndef test_use_explicit_o2o_to_parent_as_pk(self):\n        \n        \n        \n        self.assertEqual(ParkingLot3._meta.pk.name, \"primary_key\")\n        # the child->parent link\n        self.assertEqual(ParkingLot3._meta.get_ancestor_link(Place).name, \"parent\")\n```\n\nOriginal documentation:\n```python\nRegression tests for #10406\n        If there's a one-to-one link between a child model and the parent and\n        no explicit pk declared, we can use the one-to-one link as the pk on\n        the child.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "The connector from child to parent need not be the pk on the child.", "file": "tests_model_inheritance_regress_tests.py", "function": "ModelInheritanceTest.test_use_explicit_o2o_to_parent_as_pk", "project": "django", "change_stats": {"code_changes": 10, "doc_changes": 7, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_all(self, **kw):\n        \n            \n        \n        runnertype_dbs = RuleType.get_all(**kw)\n        runnertype_apis = [RuleTypeAPI.from_model(runnertype_db)\n                           for runnertype_db in runnertype_dbs]\n        return runnertype_apis```\n\nUpdated code:\n```python\ndef get_all(self, **kw):\n        \n            \n        \n        ruletype_dbs = RuleType.get_all(**kw)\n        ruletype_apis = [RuleTypeAPI.from_model(runnertype_db)\n                         for runnertype_db in ruletype_dbs]\n        return ruletype_apis\n```\n\nOriginal documentation:\n```python\nList all RuleType objects.\n\n            Handles requests:\n                GET /runnertypes/\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "List all RuleType objects.\n\n            Handles requests:\n                GET /ruletypes/", "file": "st2api_st2api_controllers_v1_ruletypes.py", "function": "RuleTypesController.get_all", "project": "st2", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef listen(self, port, **kwargs):\n        \n        \n        # import is here rather than top level because HTTPServer\n        # is not importable on appengine\n        from tornado.httpserver import HTTPServer\n        server = HTTPServer(self, **kwargs)\n        server.listen(port)```\n\nUpdated code:\n```python\ndef listen(self, port, address=\"\", **kwargs):\n        \n        \n        # import is here rather than top level because HTTPServer\n        # is not importable on appengine\n        from tornado.httpserver import HTTPServer\n        server = HTTPServer(self, **kwargs)\n        server.listen(port, address)\n```\n\nOriginal documentation:\n```python\nStarts an HTTP server for this application on the given port.\n\n        This is a convenience alias for creating an HTTPServer object\n        and calling its listen method.  Keyword arguments are passed to\n        the HTTPServer constructor.  For advanced uses (e.g. preforking),\n        do not use this method; create an HTTPServer and call its\n        bind/start methods directly.\n\n        Note that after calling this method you still need to call\n        IOLoop.instance().start() to start the server.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Starts an HTTP server for this application on the given port.\n\n        This is a convenience alias for creating an HTTPServer object\n        and calling its listen method.  Keyword arguments not\n        supported by HTTPServer.listen are passed to the HTTPServer\n        constructor.  For advanced uses (e.g. preforking), do not use\n        this method; create an HTTPServer and call its bind/start\n        methods directly.\n\n        Note that after calling this method you still need to call\n        IOLoop.instance().start() to start the server.", "file": "tornado_web.py", "function": "Application.listen", "project": "tornado", "change_stats": {"code_changes": 6, "doc_changes": 11, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, boundaries, ncolors, clip=False, *, extend='neither'):\n        \n        \n        \n        if clip and extend != 'neither':\n            raise ValueError(\"'clip=True' is not compatible with 'extend'\")\n        self.clip = clip\n        self.vmin = boundaries[0]\n        self.vmax = boundaries[-1]\n        self.boundaries = np.asarray(boundaries)\n        self.N = len(self.boundaries)\n        self.Ncmap = ncolors\n        self.extend = extend\n\n        self._N = self.N - 1  # number of colors needed\n        self._offset = 0\n        if extend in ('min', 'both'):\n            self._N += 1\n            self._offset = 1\n        if extend in ('max', 'both'):\n            self._N += 1\n        if self._N > self.Ncmap:\n            raise ValueError(f\"There are {self._N} color bins including \"\n                             f\"extensions, but ncolors = {ncolors}; \"\n                             \"ncolors must equal or exceed the number of \"\n                             \"bins\")```\n\nUpdated code:\n```python\ndef __init__(self, boundaries, ncolors, clip=False, *, extend='neither'):\n        \n        \n        \n        if clip and extend != 'neither':\n            raise ValueError(\"'clip=True' is not compatible with 'extend'\")\n        self.clip = clip\n        self.vmin = boundaries[0]\n        self.vmax = boundaries[-1]\n        self.boundaries = np.asarray(boundaries)\n        self.N = len(self.boundaries)\n        if self.N < 2:\n            raise ValueError(\"You must provide at least 2 boundaries \"\n                             f\"(1 region) but you passed in {boundaries!r}\")\n        self.Ncmap = ncolors\n        self.extend = extend\n\n        self._n_regions = self.N - 1  # number of colors needed\n        self._offset = 0\n        if extend in ('min', 'both'):\n            self._n_regions += 1\n            self._offset = 1\n        if extend in ('max', 'both'):\n            self._n_regions += 1\n        if self._n_regions > self.Ncmap:\n            raise ValueError(f\"There are {self._n_regions} color bins \"\n                             \"including extensions, but ncolors = \"\n                             f\"{ncolors}; ncolors must equal or exceed the \"\n                             \"number of bins\")\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        boundaries : array-like\n            Monotonically increasing sequence of boundaries\n        ncolors : int\n            Number of colors in the colormap to be used\n        clip : bool, optional\n            If clip is ``True``, out of range values are mapped to 0 if they\n            are below ``boundaries[0]`` or mapped to ``ncolors - 1`` if they\n            are above ``boundaries[-1]``.\n\n            If clip is ``False``, out of range values are mapped to -1 if\n            they are below ``boundaries[0]`` or mapped to *ncolors* if they are\n            above ``boundaries[-1]``. These are then converted to valid indices\n            by `Colormap.__call__`.\n        extend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n            Extend the number of bins to include one or both of the\n            regions beyond the boundaries.  For example, if ``extend``\n            is 'min', then the color to which the region between the first\n            pair of boundaries is mapped will be distinct from the first\n            color in the colormap, and by default a\n            `~matplotlib.colorbar.Colorbar` will be drawn with\n            the triangle extension on the left or lower end.\n\n        Returns\n        -------\n        int16 scalar or array\n\n        Notes\n        -----\n        *boundaries* defines the edges of bins, and data falling within a bin\n        is mapped to the color with the same index.\n\n        If the number of bins, including any extensions, is less than\n        *ncolors*, the color index is chosen by linear interpolation, mapping\n        the ``[0, nbins - 1]`` range onto the ``[0, ncolors - 1]`` range.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        boundaries : array-like\n            Monotonically increasing sequence of at least 2 boundaries.\n        ncolors : int\n            Number of colors in the colormap to be used.\n        clip : bool, optional\n            If clip is ``True``, out of range values are mapped to 0 if they\n            are below ``boundaries[0]`` or mapped to ``ncolors - 1`` if they\n            are above ``boundaries[-1]``.\n\n            If clip is ``False``, out of range values are mapped to -1 if\n            they are below ``boundaries[0]`` or mapped to *ncolors* if they are\n            above ``boundaries[-1]``. These are then converted to valid indices\n            by `Colormap.__call__`.\n        extend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n            Extend the number of bins to include one or both of the\n            regions beyond the boundaries.  For example, if ``extend``\n            is 'min', then the color to which the region between the first\n            pair of boundaries is mapped will be distinct from the first\n            color in the colormap, and by default a\n            `~matplotlib.colorbar.Colorbar` will be drawn with\n            the triangle extension on the left or lower end.\n\n        Returns\n        -------\n        int16 scalar or array\n\n        Notes\n        -----\n        *boundaries* defines the edges of bins, and data falling within a bin\n        is mapped to the color with the same index.\n\n        If the number of bins, including any extensions, is less than\n        *ncolors*, the color index is chosen by linear interpolation, mapping\n        the ``[0, nbins - 1]`` range onto the ``[0, ncolors - 1]`` range.", "file": "lib_matplotlib_colors.py", "function": "BoundaryNorm.__init__", "project": "matplotlib", "change_stats": {"code_changes": 21, "doc_changes": 6, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_indexes(cls):\n        \n        errors = []\n        for index in cls._meta.indexes:\n            # Index name can't start with an underscore or a number, restricted\n            # for cross-database compatibility with Oracle.\n            if index.name[0] == '_' or index.name[0].isdigit():\n                errors.append(\n                    checks.Error(\n                        \"The index name '%s' cannot start with an underscore \"\n                        \"or a number.\" % index.name,\n                        obj=cls,\n                        id='models.E033',\n                    ),\n                )\n            if len(index.name) > index.max_name_length:\n                errors.append(\n                    checks.Error(\n                        \"The index name '%s' cannot be longer than %d \"\n                        \"characters.\" % (index.name, index.max_name_length),\n                        obj=cls,\n                        id='models.E034',\n                    ),\n                )\n        fields = [field for index in cls._meta.indexes for field, _ in index.fields_orders]\n        errors.extend(cls._check_local_fields(fields, 'indexes'))\n        return errors```\n\nUpdated code:\n```python\ndef _check_indexes(cls, databases):\n        \n        errors = []\n        for index in cls._meta.indexes:\n            # Index name can't start with an underscore or a number, restricted\n            # for cross-database compatibility with Oracle.\n            if index.name[0] == '_' or index.name[0].isdigit():\n                errors.append(\n                    checks.Error(\n                        \"The index name '%s' cannot start with an underscore \"\n                        \"or a number.\" % index.name,\n                        obj=cls,\n                        id='models.E033',\n                    ),\n                )\n            if len(index.name) > index.max_name_length:\n                errors.append(\n                    checks.Error(\n                        \"The index name '%s' cannot be longer than %d \"\n                        \"characters.\" % (index.name, index.max_name_length),\n                        obj=cls,\n                        id='models.E034',\n                    ),\n                )\n        for db in databases:\n            if not router.allow_migrate_model(db, cls):\n                continue\n            connection = connections[db]\n            if (\n                connection.features.supports_partial_indexes or\n                'supports_partial_indexes' in cls._meta.required_db_features\n            ):\n                continue\n            if any(index.condition is not None for index in cls._meta.indexes):\n                errors.append(\n                    checks.Warning(\n                        '%s does not support indexes with conditions.'\n                        % connection.display_name,\n                        hint=(\n                            \"Conditions will be ignored. Silence this warning \"\n                            \"if you don't care about it.\"\n                        ),\n                        obj=cls,\n                        id='models.W037',\n                    )\n                )\n        fields = [field for index in cls._meta.indexes for field, _ in index.fields_orders]\n        errors.extend(cls._check_local_fields(fields, 'indexes'))\n        return errors\n```\n\nOriginal documentation:\n```python\nCheck the fields and names of indexes.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check fields, names, and conditions of indexes.", "file": "django_db_models_base.py", "function": "Model._check_indexes", "project": "django", "change_stats": {"code_changes": 26, "doc_changes": 4, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef register(router: Router[Handler]):\n    \n    \n    \n    router.add(path_strategy_handler)\n    router.add(domain_strategy_handler)\n    router.add(legacy_handler)```\n\nUpdated code:\n```python\ndef register(router: Router[Handler]):\n    \n    \n    \n    router.add(standard_strategy_handler)\n    router.add(path_strategy_handler)\n    router.add(domain_strategy_handler)\n    router.add(legacy_handler)\n```\n\nOriginal documentation:\n```python\nRegisters the query API handlers into the given router. There are three routes, one for each SQS_ENDPOINT_STRATEGY.\n\n    :param router: the router to add the handlers into.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Registers the query API handlers into the given router. There are four routes, one for each SQS_ENDPOINT_STRATEGY.\n\n    :param router: the router to add the handlers into.", "file": "localstack_services_sqs_query_api.py", "function": "register", "project": "localstack", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _merge_tests(\n        current_tests: List[str], new_tests: List[str], exclude_tests: List[str]\n    ) -> List[str]:\n        \n        \n        \n        current_tests.extend(new_tests)\n        current_tests = [\n            test for test in current_tests if test not in exclude_tests\n        ]  # skip the excluded tests\n        current_tests = list(dict.fromkeys(current_tests))  # remove dupes\n\n        return current_tests```\n\nUpdated code:\n```python\ndef _merge_tests(\n        current_tests: List[str],\n        new_tests: List[str],\n        exclude_tests: List[str],\n        orig_tests: List[str],\n    ) -> List[str]:\n        \n        \n        \n        current_tests.extend(new_tests)\n        current_tests = [\n            test\n            for test in current_tests\n            if test not in exclude_tests and test in orig_tests\n        ]  # skip the excluded tests\n        current_tests = list(dict.fromkeys(current_tests))  # remove dupes\n\n        return current_tests\n```\n\nOriginal documentation:\n```python\nWe append all new tests to the current tests, while preserving the sorting on the new_tests\n        However, exclude any specified tests which have now moved to a higher priority list\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "We append all new tests to the current tests, while preserving the sorting on the new_tests\n        However, exclude any specified tests which have now moved to a higher priority list or tests\n        that weren't originally in the self's TestPrioritizations", "file": "tools_testing_target_determination_heuristics_interface.py", "function": "TestPrioritizations._merge_tests", "project": "pytorch", "change_stats": {"code_changes": 11, "doc_changes": 5, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef at_time(self, time, tz=None, asof=False):\n        \n        \n        \n        from pandas.tseries.resample import values_at_time\n        return values_at_time(self, time, tz=tz, asof=asof)```\n\nUpdated code:\n```python\ndef at_time(self, time, asof=False):\n        \n        \n        \n        indexer = self.index.indexer_at_time(time, asof=asof)\n        return self.take(indexer)\n```\n\nOriginal documentation:\n```python\nSelect values at particular time of day (e.g. 9:30AM)\n\n        Parameters\n        ----------\n        time : datetime.time or string\n        tz : string or pytz.timezone\n            Time zone for time. Corresponding timestamps would be converted to\n            time zone of the TimeSeries\n\n        Returns\n        -------\n        values_at_time : TimeSeries\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Select values at particular time of day (e.g. 9:30AM)\n\n        Parameters\n        ----------\n        time : datetime.time or string\n\n        Returns\n        -------\n        values_at_time : TimeSeries", "file": "pandas_core_series.py", "function": "TimeSeries.at_time", "project": "pandas", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef skeletonize_3d(image):\n    \n\n    \n    # make sure the image is 3D or 2D\n    if image.ndim < 2 or image.ndim > 3:\n        raise ValueError(\n            \"skeletonize_3d can only handle 2D or 3D images; \"\n            f\"got image.ndim = {image.ndim} instead.\"\n        )\n    image = np.ascontiguousarray(image)\n    image = img_as_ubyte(image, force_copy=False)\n\n    # make an in image 3D and pad it w/ zeros to simplify dealing w/ boundaries\n    # NB: careful here to not clobber the original *and* minimize copying\n    image_o = image\n    if image.ndim == 2:\n        image_o = image[np.newaxis, ...]\n    image_o = np.pad(image_o, pad_width=1, mode='constant')\n\n    # normalize to binary\n    maxval = image_o.max()\n    image_o[image_o != 0] = 1\n\n    # do the computation\n    image_o = np.asarray(_compute_thin_image(image_o))\n\n    # crop it back and restore the original intensity range\n    image_o = crop(image_o, crop_width=1)\n    if image.ndim == 2:\n        image_o = image_o[0]\n    image_o *= maxval\n\n    return image_o```\n\nUpdated code:\n```python\ndef skeletonize_3d(image):\n    \n\n    \n    # make sure the image is 3D or 2D\n    if image.ndim < 2 or image.ndim > 3:\n        raise ValueError(\n            \"skeletonize_3d can only handle 2D or 3D images; \"\n            f\"got image.ndim = {image.ndim} instead.\"\n        )\n\n    image_io = image.astype(bool, copy=False)\n\n    # make a 2D input image 3D and pad it w/ zeros to simplify dealing w/ boundaries\n    # NB: careful here to not clobber the original *and* minimize copying\n    if image.ndim == 2:\n        image_io = image_io[np.newaxis, ...]\n    image_o = np.pad(image_io, pad_width=1, mode='constant')\n\n    # do the computation\n    image_o = _compute_thin_image(image_o)\n\n    # crop it back and restore the original intensity range\n    image_o = crop(image_o, crop_width=1)\n    if image.ndim == 2:\n        image_o = image_o[0]\n\n    return image_o\n```\n\nOriginal documentation:\n```python\nCompute the skeleton of a binary image.\n\n    Thinning is used to reduce each connected component in a binary image\n    to a single-pixel wide skeleton.\n\n    Parameters\n    ----------\n    image : ndarray, 2D or 3D\n        A binary image containing the objects to be skeletonized. Zeros\n        represent background, nonzero values are foreground.\n\n    Returns\n    -------\n    skeleton : ndarray\n        The thinned image.\n\n    See Also\n    --------\n    skeletonize, medial_axis\n\n    Notes\n    -----\n    The method of [Lee94]_ uses an octree data structure to examine a 3x3x3\n    neighborhood of a pixel. The algorithm proceeds by iteratively sweeping\n    over the image, and removing pixels at each iteration until the image\n    stops changing. Each iteration consists of two steps: first, a list of\n    candidates for removal is assembled; then pixels from this list are\n    rechecked sequentially, to better preserve connectivity of the image.\n\n    The algorithm this function implements is different from the algorithms\n    used by either `skeletonize` or `medial_axis`, thus for 2D images the\n    results produced by this function are generally different.\n\n    References\n    ----------\n    .. [Lee94] T.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models\n           via 3-D medial surface/axis thinning algorithms.\n           Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute the skeleton of a binary image.\n\n    Thinning is used to reduce each connected component in a binary image\n    to a single-pixel wide skeleton.\n\n    Parameters\n    ----------\n    image : ndarray, 2D or 3D\n        A binary image containing the objects to be skeletonized. Zeros\n        represent background, nonzero values are foreground.\n\n    Returns\n    -------\n    skeleton : ndarray of bool\n        The thinned image.\n\n    See Also\n    --------\n    skeletonize, medial_axis\n\n    Notes\n    -----\n    The method of [Lee94]_ uses an octree data structure to examine a 3x3x3\n    neighborhood of a pixel. The algorithm proceeds by iteratively sweeping\n    over the image, and removing pixels at each iteration until the image\n    stops changing. Each iteration consists of two steps: first, a list of\n    candidates for removal is assembled; then pixels from this list are\n    rechecked sequentially, to better preserve connectivity of the image.\n\n    The algorithm this function implements is different from the algorithms\n    used by either `skeletonize` or `medial_axis`, thus for 2D images the\n    results produced by this function are generally different.\n\n    References\n    ----------\n    .. [Lee94] T.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models\n           via 3-D medial surface/axis thinning algorithms.\n           Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.", "file": "skimage_morphology__skeletonize.py", "function": "skeletonize_3d", "project": "scikit-image", "change_stats": {"code_changes": 20, "doc_changes": 4, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef focus_to_grid_center(self):\n        \n        \n        \n        if np.any(np.abs(self.grids.center_offset - 0.5) > self.config.MAP_GRID_CENTER_TOLERANCE):\n            logger.info('Re-focus to grid center.')\n            self.map_swipe((0, 0))\n            return True\n\n        return False```\n\nUpdated code:\n```python\ndef focus_to_grid_center(self, tolerance=None):\n        \n        \n        \n        if not tolerance:\n            tolerance = self.config.MAP_GRID_CENTER_TOLERANCE\n        if np.any(np.abs(self.view.center_offset - 0.5) > tolerance):\n            logger.info('Re-focus to grid center.')\n            self.map_swipe((0, 0))\n            return True\n\n        return False\n```\n\nOriginal documentation:\n```python\nRe-focus to the center of a grid.\n\n        Returns:\n            bool: Map swiped.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Re-focus to the center of a grid.\n\n        Args:\n            tolerance (float): 0 to 0.5. If None, use MAP_GRID_CENTER_TOLERANCE\n\n        Returns:\n            bool: Map swiped.", "file": "module_map_camera.py", "function": "Camera.focus_to_grid_center", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef code_gen(blocks):\n    \n    \n\n    \n    decl = \"\"\n    head = \"\"\n    tail = \"\"\n    for block in blocks:\n        decl += block.declare\n        head = head + (\"\\n{\\n%s\" % block.behavior)\n        tail = (\"%s\\n}\\n\" % block.cleanup) + tail\n    return decl + head + tail```\n\nUpdated code:\n```python\ndef code_gen(blocks):\n    \n    \n    \n    decl = \"\"\n    head = \"\"\n    tail = \"\"\n    for block in blocks:\n        decl += block.declare\n        head = head + (\"\\n{\\n%s\" % block.behavior)\n        tail = (\"%s\\n}\\n\" % block.cleanup) + tail\n    return decl + head + tail\n```\n\nOriginal documentation:\n```python\nWRITEME\n\n    From a list of L{CodeBlock} instances, returns a string\n    that executes them all in sequence. eg for C{(decl1, task1,\n    cleanup1)} and C{(decl2, task2, cleanup2)} the returned string\n    will be of the form:\n\n        decl1\n        decl2\n        {\n         task1\n         {\n          task2\n          cleanup2\n         }\n         cleanup1\n        }\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "From a list of L{CodeBlock} instances, returns a string\n    that executes them all in sequence. \n    \n    Eg for C{(decl1, task1,\n    cleanup1)} and C{(decl2, task2, cleanup2)} the returned string\n    will be of the form:\n\n        decl1\n        decl2\n        {\n         task1\n         {\n          task2\n          cleanup2\n         }\n         cleanup1\n        }\n\n    Parameters:\n    ----------------\n    blocks \n         List of CodeBlock instances such that\n         * declarations, behavior and cleanup are in the run()\n         method of the struct", "file": "theano_gof_cc.py", "function": "code_gen", "project": "Theano", "change_stats": {"code_changes": 3, "doc_changes": 17, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef predict(self, preprocessed_inputs, true_image_shapes):\n    \n    \n    pass```\n\nUpdated code:\n```python\ndef predict(self, preprocessed_inputs, true_image_shapes, **side_inputs):\n    \n    \n    pass\n```\n\nOriginal documentation:\n```python\nPredict prediction tensors from inputs tensor.\n\n    Outputs of this function can be passed to loss or postprocess functions.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding prediction tensors to be\n        passed to the Loss or Postprocess functions.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Predict prediction tensors from inputs tensor.\n\n    Outputs of this function can be passed to loss or postprocess functions.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n      **side_inputs: additional tensors that are required by the network.\n\n    Returns:\n      prediction_dict: a dictionary holding prediction tensors to be\n        passed to the Loss or Postprocess functions.", "file": "research_object_detection_core_model.py", "function": "DetectionModel.predict", "project": "models", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _generate_auth_url(self, query_parameters_dict: Dict[str, str], force_browser_logout: bool) -> str:\n        \n        \n        \n        auth_url = \"{}?{}\".format(self._auth_url, urlencode(query_parameters_dict))\n        if force_browser_logout:\n            # The url after '?next=' should be urlencoded\n            auth_url = \"{}?next={}\".format(MYCLOUD_LOGOFF, quote_plus(auth_url))\n        return auth_url```\n\nUpdated code:\n```python\ndef _generate_auth_url(self, query_parameters_dict: Dict[str, str], force_browser_logout: bool) -> str:\n        \n        \n        \n        auth_url = \"{}?{}\".format(self._auth_url, urlencode(query_parameters_dict))\n        if force_browser_logout:\n            # The url after '?next=' should be urlencoded\n            auth_url = \"{}?next={}\".format(MYCLOUD_LOGOFF_URL, quote_plus(auth_url))\n        return auth_url\n```\n\nOriginal documentation:\n```python\nGenerates the authentications url based on the original auth_url and the query_parameters_dict to be included.\n        If there is a request to force logging out of mycloud in the browser, the link to logoff from mycloud is\n        prepended in order to force the browser to logoff from mycloud and then redirect to the authentication url to\n        login again. This case is used to sync the accounts between Cura and the browser.\n        :param query_parameters_dict:\n        :param force_browser_logout:\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates the authentications url based on the original auth_url and the query_parameters_dict to be included.\n        If there is a request to force logging out of mycloud in the browser, the link to logoff from mycloud is\n        prepended in order to force the browser to logoff from mycloud and then redirect to the authentication url to\n        login again. This case is used to sync the accounts between Cura and the browser.\n\n        :param query_parameters_dict: A dictionary with the query parameters to be url encoded and added to the\n                                      authentication link\n        :param force_browser_logout: If True, Cura will prepend the MYCLOUD_LOGOFF_URL link before the authentication\n                                     link to force the a browser logout from mycloud.ultimaker.com\n        :return: The authentication URL, properly formatted and encoded", "file": "cura_OAuth2_AuthorizationService.py", "function": "AuthorizationService._generate_auth_url", "project": "Cura", "change_stats": {"code_changes": 4, "doc_changes": 11, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=0.5,\n             overlap=.5, *, threshold_rel=None, exclude_border=False):\n    \n    \n    image = img_as_float(image)\n    float_dtype = _supported_float_type(image.dtype)\n    image = image.astype(float_dtype, copy=False)\n\n    # if both min and max sigma are scalar, function returns only one sigma\n    scalar_sigma = np.isscalar(max_sigma) and np.isscalar(min_sigma)\n\n    # Gaussian filter requires that sequence-type sigmas have same\n    # dimensionality as image. This broadcasts scalar kernels\n    if np.isscalar(max_sigma):\n        max_sigma = np.full(image.ndim, max_sigma, dtype=float_dtype)\n    if np.isscalar(min_sigma):\n        min_sigma = np.full(image.ndim, min_sigma, dtype=float_dtype)\n\n    # Convert sequence types to array\n    min_sigma = np.asarray(min_sigma, dtype=float_dtype)\n    max_sigma = np.asarray(max_sigma, dtype=float_dtype)\n\n    if sigma_ratio <= 1.0:\n        raise ValueError('sigma_ratio must be > 1.0')\n\n    # k such that min_sigma*(sigma_ratio**k) > max_sigma\n    k = int(np.mean(np.log(max_sigma / min_sigma) / np.log(sigma_ratio) + 1))\n\n    # a geometric progression of standard deviations for gaussian kernels\n    sigma_list = np.array([min_sigma * (sigma_ratio ** i)\n                           for i in range(k + 1)])\n\n    gaussian_images = [ndi.gaussian_filter(image, s) for s in sigma_list]\n\n    # normalization factor for consistency in DoG magnitude\n    sf = 1 / (sigma_ratio - 1)\n\n    # computing difference between two successive Gaussian blurred images\n    # to obtain an approximation of the scale invariant Laplacian of the\n    # Gaussian operator\n    dog_images = [\n        (gaussian_images[i] - gaussian_images[i + 1]) * sf for i in range(k)\n    ]\n\n\n    image_cube = np.stack(dog_images, axis=-1)\n\n    exclude_border = _format_exclude_border(image.ndim, exclude_border)\n    local_maxima = peak_local_max(\n        image_cube,\n        threshold_abs=threshold,\n        threshold_rel=threshold_rel,\n        exclude_border=exclude_border,\n        footprint=np.ones((3,) * (image.ndim + 1)),\n    )\n\n    # Catch no peaks\n    if local_maxima.size == 0:\n        return np.empty((0, 3))\n\n    # Convert local_maxima to float64\n    lm = local_maxima.astype(float_dtype)\n\n    # translate final column of lm, which contains the index of the\n    # sigma that produced the maximum intensity value, into the sigma\n    sigmas_of_peaks = sigma_list[local_maxima[:, -1]]\n\n    if scalar_sigma:\n        # select one sigma column, keeping dimension\n        sigmas_of_peaks = sigmas_of_peaks[:, 0:1]\n\n    # Remove sigma index and replace with sigmas\n    lm = np.hstack([lm[:, :-1], sigmas_of_peaks])\n\n    sigma_dim = sigmas_of_peaks.shape[1]\n\n    return _prune_blobs(lm, overlap, sigma_dim=sigma_dim)```\n\nUpdated code:\n```python\ndef blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=0.5,\n             overlap=.5, *, threshold_rel=None, exclude_border=False):\n    \n    \n    image = img_as_float(image)\n    float_dtype = _supported_float_type(image.dtype)\n    image = image.astype(float_dtype, copy=False)\n\n    # if both min and max sigma are scalar, function returns only one sigma\n    scalar_sigma = np.isscalar(max_sigma) and np.isscalar(min_sigma)\n\n    # Gaussian filter requires that sequence-type sigmas have same\n    # dimensionality as image. This broadcasts scalar kernels\n    if np.isscalar(max_sigma):\n        max_sigma = np.full(image.ndim, max_sigma, dtype=float_dtype)\n    if np.isscalar(min_sigma):\n        min_sigma = np.full(image.ndim, min_sigma, dtype=float_dtype)\n\n    # Convert sequence types to array\n    min_sigma = np.asarray(min_sigma, dtype=float_dtype)\n    max_sigma = np.asarray(max_sigma, dtype=float_dtype)\n\n    if sigma_ratio <= 1.0:\n        raise ValueError('sigma_ratio must be > 1.0')\n\n    # k such that min_sigma*(sigma_ratio**k) > max_sigma\n    k = int(np.mean(np.log(max_sigma / min_sigma) / np.log(sigma_ratio) + 1))\n\n    # a geometric progression of standard deviations for gaussian kernels\n    sigma_list = np.array([min_sigma * (sigma_ratio ** i)\n                           for i in range(k + 1)])\n\n    gaussian_images = [ndi.gaussian_filter(image, s) for s in sigma_list]\n\n    # normalization factor for consistency in DoG magnitude\n    sf = 1 / (sigma_ratio - 1)\n\n    # computing difference between two successive Gaussian blurred images\n    # to obtain an approximation of the scale invariant Laplacian of the\n    # Gaussian operator\n    dog_images = [\n        (gaussian_images[i] - gaussian_images[i + 1]) * sf for i in range(k)\n    ]\n\n    image_cube = np.stack(dog_images, axis=-1)\n\n    exclude_border = _format_exclude_border(image.ndim, exclude_border)\n    local_maxima = peak_local_max(\n        image_cube,\n        threshold_abs=threshold,\n        threshold_rel=threshold_rel,\n        exclude_border=exclude_border,\n        footprint=np.ones((3,) * (image.ndim + 1)),\n    )\n\n    # Catch no peaks\n    if local_maxima.size == 0:\n        return np.empty((0, 3))\n\n    # Convert local_maxima to float64\n    lm = local_maxima.astype(float_dtype)\n\n    # translate final column of lm, which contains the index of the\n    # sigma that produced the maximum intensity value, into the sigma\n    sigmas_of_peaks = sigma_list[local_maxima[:, -1]]\n\n    if scalar_sigma:\n        # select one sigma column, keeping dimension\n        sigmas_of_peaks = sigmas_of_peaks[:, 0:1]\n\n    # Remove sigma index and replace with sigmas\n    lm = np.hstack([lm[:, :-1], sigmas_of_peaks])\n\n    sigma_dim = sigmas_of_peaks.shape[1]\n\n    return _prune_blobs(lm, overlap, sigma_dim=sigma_dim)\n```\n\nOriginal documentation:\n```python\nr\"\"\"Finds blobs in the given grayscale image.\n\n    Blobs are found using the Difference of Gaussian (DoG) method [1]_, [2]_.\n    For each blob found, the method returns its coordinates and the standard\n    deviation of the Gaussian kernel that detected the blob.\n\n    Parameters\n    ----------\n    image : 2D or 3D ndarray\n        Input grayscale image, blobs are assumed to be light on dark\n        background (white on black).\n    min_sigma : scalar or sequence of scalars, optional\n        The minimum standard deviation for Gaussian kernel. Keep this low to\n        detect smaller blobs. The standard deviations of the Gaussian filter\n        are given for each axis as a sequence, or as a single number, in\n        which case it is equal for all axes.\n    max_sigma : scalar or sequence of scalars, optional\n        The maximum standard deviation for Gaussian kernel. Keep this high to\n        detect larger blobs. The standard deviations of the Gaussian filter\n        are given for each axis as a sequence, or as a single number, in\n        which case it is equal for all axes.\n    sigma_ratio : float, optional\n        The ratio between the standard deviation of Gaussian Kernels used for\n        computing the Difference of Gaussians\n    threshold : float or None, optional.\n        The absolute lower bound for scale space maxima. Local maxima smaller\n        than `threshold` are ignored. Reduce this to detect blobs with lower\n        intensities. If `threshold_rel` is also specified, whichever threshold\n        is larger will be used. If None, `threshold_rel` is used instead.\n    overlap : float, optional\n        A value between 0 and 1. If the area of two blobs overlaps by a\n        fraction greater than `threshold`, the smaller blob is eliminated.\n    threshold_rel : float or None, optional\n        Minimum intensity of peaks, calculated as\n        ``max(dog_space) * threshold_rel``. Where ``dog_space`` refers to the\n        stack of difference-of-Gaussian (DoG) images computed internally. This\n        should have a value between 0 and 1. If None, `threshold_abs` is used\n        instead.\n    exclude_border : tuple of ints, int, or False, optional\n        If tuple of ints, the length of the tuple must match the input array's\n        dimensionality.  Each element of the tuple will exclude peaks from\n        within `exclude_border`-pixels of the border of the image along that\n        dimension.\n        If nonzero int, `exclude_border` excludes peaks from within\n        `exclude_border`-pixels of the border of the image.\n        If zero or False, peaks are identified regardless of their\n        distance from the border.\n\n    Returns\n    -------\n    A : (n, image.ndim + sigma) ndarray\n        A 2d array with each row representing 2 coordinate values for a 2D\n        image, and 3 coordinate values for a 3D image, plus the sigma(s) used.\n        When a single sigma is passed, outputs are:\n        ``(r, c, sigma)`` or ``(p, r, c, sigma)`` where ``(r, c)`` or\n        ``(p, r, c)`` are coordinates of the blob and ``sigma`` is the standard\n        deviation of the Gaussian kernel which detected the blob. When an\n        anisotropic gaussian is used (sigmas per dimension), the detected sigma\n        is returned for each dimension.\n\n    See also\n    --------\n    skimage.filters.difference_of_gaussians\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach\n    .. [2] Lowe, D. G. \"Distinctive Image Features from Scale-Invariant\n        Keypoints.\" International Journal of Computer Vision 60, 91\u2013110 (2004).\n        https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n        :DOI:`10.1023/B:VISI.0000029664.99615.94`\n\n    Examples\n    --------\n    >>> from skimage import data, feature\n    >>> coins = data.coins()\n    >>> feature.blob_dog(coins, threshold=.05, min_sigma=10, max_sigma=40)\n    array([[128., 155.,  10.],\n           [198., 155.,  10.],\n           [124., 338.,  10.],\n           [127., 102.,  10.],\n           [193., 281.,  10.],\n           [126., 208.,  10.],\n           [267., 115.,  10.],\n           [197., 102.,  10.],\n           [198., 215.,  10.],\n           [123., 279.,  10.],\n           [126.,  46.,  10.],\n           [259., 247.,  10.],\n           [196.,  43.,  10.],\n           [ 54., 276.,  10.],\n           [267., 358.,  10.],\n           [ 58., 100.,  10.],\n           [259., 305.,  10.],\n           [185., 347.,  16.],\n           [261., 174.,  16.],\n           [ 46., 336.,  16.],\n           [ 54., 217.,  10.],\n           [ 55., 157.,  10.],\n           [ 57.,  41.,  10.],\n           [260.,  47.,  16.]])\n\n    Notes\n    -----\n    The radius of each blob is approximately :math:`\\sqrt{2}\\sigma` for\n    a 2-D image and :math:`\\sqrt{3}\\sigma` for a 3-D image.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Finds blobs in the given grayscale image.\n\n    Blobs are found using the Difference of Gaussian (DoG) method [1]_, [2]_.\n    For each blob found, the method returns its coordinates and the standard\n    deviation of the Gaussian kernel that detected the blob.\n\n    Parameters\n    ----------\n    image : 2D or 3D ndarray\n        Input grayscale image, blobs are assumed to be light on dark\n        background (white on black).\n    min_sigma : scalar or sequence of scalars, optional\n        The minimum standard deviation for Gaussian kernel. Keep this low to\n        detect smaller blobs. The standard deviations of the Gaussian filter\n        are given for each axis as a sequence, or as a single number, in\n        which case it is equal for all axes.\n    max_sigma : scalar or sequence of scalars, optional\n        The maximum standard deviation for Gaussian kernel. Keep this high to\n        detect larger blobs. The standard deviations of the Gaussian filter\n        are given for each axis as a sequence, or as a single number, in\n        which case it is equal for all axes.\n    sigma_ratio : float, optional\n        The ratio between the standard deviation of Gaussian Kernels used for\n        computing the Difference of Gaussians\n    threshold : float or None, optional\n        The absolute lower bound for scale space maxima. Local maxima smaller\n        than `threshold` are ignored. Reduce this to detect blobs with lower\n        intensities. If `threshold_rel` is also specified, whichever threshold\n        is larger will be used. If None, `threshold_rel` is used instead.\n    overlap : float, optional\n        A value between 0 and 1. If the area of two blobs overlaps by a\n        fraction greater than `threshold`, the smaller blob is eliminated.\n    threshold_rel : float or None, optional\n        Minimum intensity of peaks, calculated as\n        ``max(dog_space) * threshold_rel``. Where ``dog_space`` refers to the\n        stack of difference-of-Gaussian (DoG) images computed internally. This\n        should have a value between 0 and 1. If None, `threshold_abs` is used\n        instead.\n    exclude_border : tuple of ints, int, or False, optional\n        If tuple of ints, the length of the tuple must match the input array's\n        dimensionality.  Each element of the tuple will exclude peaks from\n        within `exclude_border`-pixels of the border of the image along that\n        dimension.\n        If nonzero int, `exclude_border` excludes peaks from within\n        `exclude_border`-pixels of the border of the image.\n        If zero or False, peaks are identified regardless of their\n        distance from the border.\n\n    Returns\n    -------\n    A : (n, image.ndim + sigma) ndarray\n        A 2d array with each row representing 2 coordinate values for a 2D\n        image, and 3 coordinate values for a 3D image, plus the sigma(s) used.\n        When a single sigma is passed, outputs are:\n        ``(r, c, sigma)`` or ``(p, r, c, sigma)`` where ``(r, c)`` or\n        ``(p, r, c)`` are coordinates of the blob and ``sigma`` is the standard\n        deviation of the Gaussian kernel which detected the blob. When an\n        anisotropic gaussian is used (sigmas per dimension), the detected sigma\n        is returned for each dimension.\n\n    See also\n    --------\n    skimage.filters.difference_of_gaussians\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach\n    .. [2] Lowe, D. G. \"Distinctive Image Features from Scale-Invariant\n        Keypoints.\" International Journal of Computer Vision 60, 91\u2013110 (2004).\n        https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n        :DOI:`10.1023/B:VISI.0000029664.99615.94`\n\n    Examples\n    --------\n    >>> from skimage import data, feature\n    >>> coins = data.coins()\n    >>> feature.blob_dog(coins, threshold=.05, min_sigma=10, max_sigma=40)\n    array([[128., 155.,  10.],\n           [198., 155.,  10.],\n           [124., 338.,  10.],\n           [127., 102.,  10.],\n           [193., 281.,  10.],\n           [126., 208.,  10.],\n           [267., 115.,  10.],\n           [197., 102.,  10.],\n           [198., 215.,  10.],\n           [123., 279.,  10.],\n           [126.,  46.,  10.],\n           [259., 247.,  10.],\n           [196.,  43.,  10.],\n           [ 54., 276.,  10.],\n           [267., 358.,  10.],\n           [ 58., 100.,  10.],\n           [259., 305.,  10.],\n           [185., 347.,  16.],\n           [261., 174.,  16.],\n           [ 46., 336.,  16.],\n           [ 54., 217.,  10.],\n           [ 55., 157.,  10.],\n           [ 57.,  41.,  10.],\n           [260.,  47.,  16.]])\n\n    Notes\n    -----\n    The radius of each blob is approximately :math:`\\sqrt{2}\\sigma` for\n    a 2-D image and :math:`\\sqrt{3}\\sigma` for a 3-D image.", "file": "skimage_feature_blob.py", "function": "blob_dog", "project": "scikit-image", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ldelete(document: Document, layers) -> Document:\n    \n    \n\n    lids = set(multiple_to_layer_ids(layers, document))\n\n    new_doc = document.empty_copy()\n    for lid in document.ids():\n        if lid not in lids:\n            new_doc[lid] = document[lid]\n\n    return new_doc```\n\nUpdated code:\n```python\ndef ldelete(document: Document, layers, prob: Optional[float]) -> Document:\n    \n    \n\n    lids = set(multiple_to_layer_ids(layers, document))\n\n    for lid in lids:\n        if prob is not None:\n            lc = LineCollection()\n            for line in document[lid]:\n                if not random.random() < prob:\n                    lc.append(line)\n\n            if len(lc) == 0:\n                document.pop(lid)\n            else:\n                document[lid] = lc\n        else:\n            document.pop(lid)\n\n    return document\n```\n\nOriginal documentation:\n```python\nDelete one or more layers.\n\n    LAYERS can be a single layer ID, the string 'all' (to delete all layers), or a\n    coma-separated, whitespace-free list of layer IDs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Delete one or more layers.\n\n    LAYERS can be a single layer ID, the string 'all' (to delete all layers), or a\n    coma-separated, whitespace-free list of layer IDs.\n\n    The `--prob` option controls the probability with which each path is deleted. With a value\n    lower than 1.0, some paths will not be deleted.", "file": "vpype_cli_layerops.py", "function": "ldelete", "project": "vpype", "change_stats": {"code_changes": 23, "doc_changes": 5, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sync_value(value):\n\t'''\n\t\n\t'''\n\tfrappe.db.sql('''\n\t\tinsert into __global_search\n\t\t\t(doctype, name, content, published, title, route)\n\t\tvalues\n\t\t\t(%(doctype)s, %(name)s, %(content)s, %(published)s, %(title)s, %(route)s)\n\t\ton duplicate key update\n\t\t\tcontent = %(content)s''', value)```\n\nUpdated code:\n```python\ndef sync_value(value):\n\t'''\n\t\n\t'''\n\n\tfrappe.db.multisql({\n\t\t'mariadb': '''INSERT INTO `__global_search`\n\t\t\t(`doctype`, `name`, `content`, `published`, `title`, `route`)\n\t\t\tVALUES (%(doctype)s, %(name)s, %(content)s, %(published)s, %(title)s, %(route)s)\n\t\t\tON DUPLICATE key UPDATE\n\t\t\t\t`content`=%(content)s,\n\t\t\t\t`published`=%(published)s,\n\t\t\t\t`title`=%(title)s,\n\t\t\t\t`route`=%(route)s\n\t\t''',\n\t\t'postgres': '''INSERT INTO `__global_search`\n\t\t\t(`doctype`, `name`, `content`, `published`, `title`, `route`)\n\t\t\tVALUES (%(doctype)s, %(name)s, %(content)s, %(published)s, %(title)s, %(route)s)\n\t\t\tON CONFLICT(\"doctype\", \"name\") DO UPDATE SET\n\t\t\t\t`content`=%(content)s,\n\t\t\t\t`published`=%(published)s,\n\t\t\t\t`title`=%(title)s,\n\t\t\t\t`route`=%(route)s\n\t\t'''\n\t}, value)\n```\n\nOriginal documentation:\n```python\nSync a given document to global search\n\n\t:param value: dict of { doctype, name, content, published, title, route }\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sync a given document to global search\n\t:param value: dict of { doctype, name, content, published, title, route }", "file": "frappe_utils_global_search.py", "function": "sync_value", "project": "frappe", "change_stats": {"code_changes": 30, "doc_changes": 3, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef estimate_blur(image):\n        \n        \n        \n        if image.ndim == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        blur_map = cv2.Laplacian(image, cv2.CV_32F)\n        score = np.var(blur_map) / np.sqrt(image.shape[0] * image.shape[1])\n        return score```\n\nUpdated code:\n```python\ndef estimate_blur(cls, image, metadata=None):\n         \n        \n        if metadata is not None:\n            alignments = metadata[\"alignments\"]\n            det_face = DetectedFace()\n            det_face.from_png_meta(alignments)\n            aln_face = AlignedFace(np.array(alignments[\"landmarks_xy\"], dtype=\"float32\"),\n                                   image=image,\n                                   centering=\"legacy\",\n                                   size=256,\n                                   is_aligned=True)\n            mask = det_face.mask[\"components\"]\n            mask.set_sub_crop(aln_face.pose.offset[\"face\"] * -1)\n            mask = cv2.resize(mask.mask, (256, 256), interpolation=cv2.INTER_CUBIC)[..., None]\n            image = np.minimum(aln_face.face, mask)\n        if image.ndim == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        blur_map = cv2.Laplacian(image, cv2.CV_32F)\n        score = np.var(blur_map) / np.sqrt(image.shape[0] * image.shape[1])\n        return score\n```\n\nOriginal documentation:\n```python\nEstimate the amount of blur an image has with the variance of the Laplacian.\n        Normalize by pixel number to offset the effect of image size on pixel gradients & variance\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Estimate the amount of blur an image has with the variance of the Laplacian.\n        Normalize by pixel number to offset the effect of image size on pixel gradients & variance.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The face image to calculate blur for\n        metadata: dict, optional\n            The metadata for the face image or ``None`` if no metadata is available. If metadata is\n            provided the face will be masked by the \"components\" mask prior to calculating blur.\n            Default:``None``\n\n        Returns\n        -------\n        float\n            The estimated blur score for the face", "file": "tools_sort_sort.py", "function": "Sort.estimate_blur", "project": "faceswap", "change_stats": {"code_changes": 20, "doc_changes": 18, "total_changes": 38}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _parse_einsum_input(operands):\n    \n    \n\n    if len(operands) == 0:\n        raise ValueError(\"No input operands\")\n\n    if isinstance(operands[0], str):\n        subscripts = operands[0].replace(\" \", \"\")\n        operands = operands[1:]\n\n        # Ensure all characters are valid\n        for s in subscripts:\n            if s in '.,->':\n                continue\n            if s not in einsum_symbols:\n                raise ValueError(\"Character %s is not a valid symbol.\" % s)\n\n    else:\n        tmp_operands = list(operands)\n        operand_list = []\n        subscript_list = []\n        for p in range(len(operands) // 2):\n            operand_list.append(tmp_operands.pop(0))\n            subscript_list.append(tmp_operands.pop(0))\n\n        output_list = tmp_operands[-1] if len(tmp_operands) else None\n        operands = operand_list\n        subscripts = \"\"\n        last = len(subscript_list) - 1\n        for num, sub in enumerate(subscript_list):\n            for s in sub:\n                if s is Ellipsis:\n                    subscripts += \"...\"\n                elif isinstance(s, int):\n                    subscripts += einsum_symbols[s]\n                else:\n                    raise TypeError(\"For this input type lists must contain \"\n                                    \"either int or Ellipsis\")\n            if num != last:\n                subscripts += \",\"\n\n        if output_list is not None:\n            subscripts += \"->\"\n            for s in output_list:\n                if s is Ellipsis:\n                    subscripts += \"...\"\n                elif isinstance(s, int):\n                    subscripts += einsum_symbols[s]\n                else:\n                    raise TypeError(\"For this input type lists must contain \"\n                                    \"either int or Ellipsis\")\n    # Check for proper \"->\"\n    if (\"-\" in subscripts) or (\">\" in subscripts):\n        invalid = (subscripts.count(\"-\") > 1) or (subscripts.count(\">\") > 1)\n        if invalid or (subscripts.count(\"->\") != 1):\n            raise ValueError(\"Subscripts can only contain one '->'.\")\n\n    # Parse ellipses\n    if \".\" in subscripts:\n        used = subscripts.replace(\".\", \"\").replace(\",\", \"\").replace(\"->\", \"\")\n        unused = list(einsum_symbols_set - set(used))\n        ellipse_inds = \"\".join(unused)\n        longest = 0\n\n        if \"->\" in subscripts:\n            input_tmp, output_sub = subscripts.split(\"->\")\n            split_subscripts = input_tmp.split(\",\")\n            out_sub = True\n        else:\n            split_subscripts = subscripts.split(',')\n            out_sub = False\n\n        for num, sub in enumerate(split_subscripts):\n            if \".\" in sub:\n                if (sub.count(\".\") != 3) or (sub.count(\"...\") != 1):\n                    raise ValueError(\"Invalid Ellipses.\")\n\n                # Take into account numerical values\n                if operands[num].shape == ():\n                    ellipse_count = 0\n                else:\n                    ellipse_count = max(operands[num].ndim, 1)\n                    ellipse_count -= (len(sub) - 3)\n\n                if ellipse_count > longest:\n                    longest = ellipse_count\n\n                if ellipse_count < 0:\n                    raise ValueError(\"Ellipses lengths do not match.\")\n                elif ellipse_count == 0:\n                    split_subscripts[num] = sub.replace('...', '')\n                else:\n                    rep_inds = ellipse_inds[-ellipse_count:]\n                    split_subscripts[num] = sub.replace('...', rep_inds)\n\n        subscripts = \",\".join(split_subscripts)\n        if longest == 0:\n            out_ellipse = \"\"\n        else:\n            out_ellipse = ellipse_inds[-longest:]\n\n        if out_sub:\n            subscripts += \"->\" + output_sub.replace(\"...\", out_ellipse)\n        else:\n            # Special care for outputless ellipses\n            output_subscript = \"\"\n            tmp_subscripts = subscripts.replace(\",\", \"\")\n            for s in sorted(set(tmp_subscripts)):\n                if s not in (einsum_symbols):\n                    raise ValueError(\"Character %s is not a valid symbol.\" % s)\n                if tmp_subscripts.count(s) == 1:\n                    output_subscript += s\n            normal_inds = ''.join(sorted(set(output_subscript) -\n                                         set(out_ellipse)))\n\n            subscripts += \"->\" + out_ellipse + normal_inds\n\n    # Build output string if does not exist\n    if \"->\" in subscripts:\n        input_subscripts, output_subscript = subscripts.split(\"->\")\n    else:\n        input_subscripts = subscripts\n        # Build output subscripts\n        tmp_subscripts = subscripts.replace(\",\", \"\")\n        output_subscript = \"\"\n        for s in sorted(set(tmp_subscripts)):\n            if s not in einsum_symbols:\n                raise ValueError(\"Character %s is not a valid symbol.\" % s)\n            if tmp_subscripts.count(s) == 1:\n                output_subscript += s\n\n    # Make sure output subscripts are in the input\n    for char in output_subscript:\n        if char not in input_subscripts:\n            raise ValueError(\"Output character %s did not appear in the input\"\n                             % char)\n\n    # Make sure number operands is equivalent to the number of terms\n    if len(input_subscripts.split(',')) != len(operands):\n        raise ValueError(\"Number of einsum subscripts must be equal to the \"\n                         \"number of operands.\")\n\n    return (input_subscripts, output_subscript, operands)```\n\nUpdated code:\n```python\ndef _parse_einsum_input(operands):\n    \n    \n\n    if len(operands) == 0:\n        raise ValueError(\"No input operands\")\n\n    if isinstance(operands[0], str):\n        subscripts = operands[0].replace(\" \", \"\")\n        operands = operands[1:]\n\n        # Ensure all characters are valid\n        for s in subscripts:\n            if s in '.,->':\n                continue\n            if s not in einsum_symbols:\n                raise ValueError(\"Character %s is not a valid symbol.\" % s)\n\n        # Check for proper \"->\"\n        if (\"-\" in subscripts) or (\">\" in subscripts):\n            invalid = (subscripts.count(\"-\") > 1) or (subscripts.count(\">\") > 1)\n            if invalid or (subscripts.count(\"->\") != 1):\n                raise ValueError(\"Subscripts can only contain one '->'.\")\n\n        # Parse \"...\"\n        subscripts = subscripts.replace(\"...\", \"@\")\n        if \".\" in subscripts:\n            raise ValueError(\"Invalid Ellipses.\")\n\n    else:\n        tmp_operands = list(operands)\n        operand_list = []\n        subscript_list = []\n        for p in range(len(operands) // 2):\n            operand_list.append(tmp_operands.pop(0))\n            subscript_list.append(tmp_operands.pop(0))\n\n        output_list = tmp_operands[-1] if len(tmp_operands) else None\n        operands = operand_list\n        subscripts = \"\"\n        last = len(subscript_list) - 1\n        for num, sub in enumerate(subscript_list):\n            for s in sub:\n                if s is Ellipsis:\n                    subscripts += \"@\"\n                elif isinstance(s, int):\n                    subscripts += einsum_symbols[s]\n                else:\n                    raise TypeError(\"For this input type lists must contain \"\n                                    \"either int or Ellipsis\")\n            if num != last:\n                subscripts += \",\"\n\n        if output_list is not None:\n            subscripts += \"->\"\n            for s in output_list:\n                if s is Ellipsis:\n                    subscripts += \"@\"\n                elif isinstance(s, int):\n                    subscripts += einsum_symbols[s]\n                else:\n                    raise TypeError(\"For this input type lists must contain \"\n                                    \"either int or Ellipsis\")\n\n    # Build output string if does not exist\n    if \"->\" in subscripts:\n        input_subscripts, output_subscript = subscripts.split(\"->\")\n\n        # Make sure output subscripts are in the input\n        for char in output_subscript:\n            if char not in input_subscripts:\n                raise ValueError(\"Output character %s did not appear in the input\"\n                                 % ('...' if char == '@' else char))\n\n    else:\n        input_subscripts = subscripts\n        # Build output subscripts\n        tmp_subscripts = subscripts.replace(\",\", \"\")\n        output_subscript = \"\"\n        for s in sorted(set(tmp_subscripts)):\n            if s == '@' or tmp_subscripts.count(s) == 1:\n                output_subscript += s\n\n    # Make sure number operands is equivalent to the number of terms\n    if len(input_subscripts.split(',')) != len(operands):\n        raise ValueError(\"Number of einsum subscripts must be equal to the \"\n                         \"number of operands.\")\n\n    return (input_subscripts, output_subscript, operands)\n```\n\nOriginal documentation:\n```python\nParses einsum operands.\n\n    This function is based on `numpy.core.einsumfunc._parse_einsum_input`\n    function in NumPy 1.14.\n\n    Returns\n    -------\n    input_strings : str\n        Parsed input strings\n    output_string : str\n        Parsed output string\n    operands : list of array_like\n        The operands to use in the numpy contraction\n\n    Examples\n    --------\n    The operand list is simplified to reduce printing:\n\n    >>> a = np.random.rand(4, 4)\n    >>> b = np.random.rand(4, 4, 4)\n    >>> _parse_einsum_input(('...a,...a->...', a, b))\n    ('za,xza', 'xz', [a, b])\n\n    >>> _parse_einsum_input((a, [Ellipsis, 0], b, [Ellipsis, 0]))\n    ('za,xza', 'xz', [a, b])\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parses einsum operands.\n\n    This function is based on `numpy.core.einsumfunc._parse_einsum_input`\n    function in NumPy 1.14.\n\n    Returns\n    -------\n    input_strings : str\n        Parsed input strings\n    output_string : str\n        Parsed output string\n    operands : list of array_like\n        The operands to use in the numpy contraction\n\n    Examples\n    --------\n    The operand list is simplified to reduce printing:\n\n    >>> a = np.random.rand(4, 4)\n    >>> b = np.random.rand(4, 4, 4)\n    >>> _parse_einsum_input(('...a,...a->...', a, b))\n    ('@a,@a', '@', [a, b])\n\n    >>> _parse_einsum_input((a, [Ellipsis, 0], b, [Ellipsis, 0]))\n    ('@a,@a', '@', [a, b])", "file": "chainer_", "function": "_parse_einsum_input", "project": "chainer", "change_stats": {"code_changes": 99, "doc_changes": 6, "total_changes": 105}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef determine_observer_insert_points(self, model):\n        \n        \n        raise NotImplementedError(\"No observers are inserted in the PerChannelDetector.\")```\n\nUpdated code:\n```python\ndef determine_observer_insert_points(self, model: nn.Module) -> Dict:\n        \n        \n        return {}\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        There is no observers inserted for the PerChannelDetector\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        There is no observers inserted for the PerChannelDetector.\n\n        Returns an empty dictionary since no observers are added or needed", "file": "torch_ao_quantization_fx__model_report_detector.py", "function": "PerChannelDetector.determine_observer_insert_points", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef async_schedule_install_addon(\n        self, usb_path: str, network_key: str\n    ) -> asyncio.Task:\n        \n        \n        if not self._install_task or self._install_task.done():\n            LOGGER.info(\"Z-Wave JS add-on is not installed. Installing add-on\")\n            self._install_task = self._async_schedule_addon_operation(\n                self.async_install_addon,\n                partial(self.async_setup_addon, usb_path, network_key),\n            )\n        return self._install_task```\n\nUpdated code:\n```python\ndef async_schedule_install_addon(self, catch_error: bool = False) -> asyncio.Task:\n        \n        \n        if not self._install_task or self._install_task.done():\n            LOGGER.info(\"Z-Wave JS add-on is not installed. Installing add-on\")\n            self._install_task = self._async_schedule_addon_operation(\n                self.async_install_addon, catch_error=catch_error\n            )\n        return self._install_task\n```\n\nOriginal documentation:\n```python\nSchedule a task that installs and sets up the Z-Wave JS add-on.\n\n        Only schedule a new install task if the there's no running task.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Schedule a task that installs the Z-Wave JS add-on.\n\n        Only schedule a new install task if the there's no running task.", "file": "homeassistant_components_zwave_js_addon.py", "function": "AddonManager.async_schedule_install_addon", "project": "core", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_dataset_config_info(\n    path: str,\n    config_name: Optional[str] = None,\n    data_files: Optional[Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]]] = None,\n    download_config: Optional[DownloadConfig] = None,\n    download_mode: Optional[Union[DownloadMode, str]] = None,\n    revision: Optional[Union[str, Version]] = None,\n    use_auth_token: Optional[Union[bool, str]] = None,\n    **config_kwargs,\n) -> DatasetInfo:\n    \n\n    \n    builder = load_dataset_builder(\n        path,\n        name=config_name,\n        data_files=data_files,\n        download_config=download_config,\n        download_mode=download_mode,\n        revision=revision,\n        use_auth_token=use_auth_token,\n        **config_kwargs,\n    )\n    info = builder.info\n    if info.splits is None:\n        download_config = download_config.copy() if download_config else DownloadConfig()\n        if use_auth_token is not None:\n            download_config.use_auth_token = use_auth_token\n        builder._check_manual_download(\n            StreamingDownloadManager(base_path=builder.base_path, download_config=download_config)\n        )\n        try:\n            info.splits = {\n                split_generator.name: {\"name\": split_generator.name, \"dataset_name\": path}\n                for split_generator in builder._split_generators(\n                    StreamingDownloadManager(base_path=builder.base_path, download_config=download_config)\n                )\n            }\n        except Exception as err:\n            raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\n    return info```\n\nUpdated code:\n```python\ndef get_dataset_config_info(\n    path: str,\n    config_name: Optional[str] = None,\n    data_files: Optional[Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]]] = None,\n    download_config: Optional[DownloadConfig] = None,\n    download_mode: Optional[Union[DownloadMode, str]] = None,\n    revision: Optional[Union[str, Version]] = None,\n    token: Optional[Union[bool, str]] = None,\n    use_auth_token=\"deprecated\",\n    **config_kwargs,\n) -> DatasetInfo:\n    \n\n    \n    if use_auth_token != \"deprecated\":\n        warnings.warn(\n            \"'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\\n\"\n            f\"You can remove this warning by passing 'token={use_auth_token}' instead.\",\n            FutureWarning,\n        )\n        token = use_auth_token\n\n    builder = load_dataset_builder(\n        path,\n        name=config_name,\n        data_files=data_files,\n        download_config=download_config,\n        download_mode=download_mode,\n        revision=revision,\n        token=token,\n        **config_kwargs,\n    )\n    info = builder.info\n    if info.splits is None:\n        download_config = download_config.copy() if download_config else DownloadConfig()\n        if token is not None:\n            download_config.token = token\n        builder._check_manual_download(\n            StreamingDownloadManager(base_path=builder.base_path, download_config=download_config)\n        )\n        try:\n            info.splits = {\n                split_generator.name: {\"name\": split_generator.name, \"dataset_name\": path}\n                for split_generator in builder._split_generators(\n                    StreamingDownloadManager(base_path=builder.base_path, download_config=download_config)\n                )\n            }\n        except Exception as err:\n            raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\n    return info\n```\n\nOriginal documentation:\n```python\nGet the meta information (DatasetInfo) about a dataset for a particular config\n\n    Args:\n        path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n\n            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n                e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``\n            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with ``datasets.list_datasets()``)\n                e.g. ``'squad'``, ``'glue'`` or ``'openai/webtext'``\n        config_name (:obj:`str`, optional): Defining the name of the dataset configuration.\n        data_files (:obj:`str` or :obj:`Sequence` or :obj:`Mapping`, optional): Path(s) to source data file(s).\n        download_config (:class:`~download.DownloadConfig`, optional): Specific download configuration parameters.\n        download_mode (:class:`DownloadMode` or :obj:`str`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n        revision (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load.\n            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n        use_auth_token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n            If True, or not specified, will get token from `\"~/.huggingface\"`.\n        **config_kwargs (additional keyword arguments): optional attributes for builder class which will override the attributes if supplied.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the meta information (DatasetInfo) about a dataset for a particular config\n\n    Args:\n        path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n\n            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n                e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``\n            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with ``datasets.list_datasets()``)\n                e.g. ``'squad'``, ``'glue'`` or ``'openai/webtext'``\n        config_name (:obj:`str`, optional): Defining the name of the dataset configuration.\n        data_files (:obj:`str` or :obj:`Sequence` or :obj:`Mapping`, optional): Path(s) to source data file(s).\n        download_config (:class:`~download.DownloadConfig`, optional): Specific download configuration parameters.\n        download_mode (:class:`DownloadMode` or :obj:`str`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n        revision (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load.\n            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n        token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n            If True, or not specified, will get token from `\"~/.huggingface\"`.\n        use_auth_token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n            If True, or not specified, will get token from `\"~/.huggingface\"`.\n\n            <Deprecated version=\"2.14.0\">\n\n            `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n\n            </Deprecated>\n\n        **config_kwargs (additional keyword arguments): optional attributes for builder class which will override the attributes if supplied.", "file": "src_datasets_inspect.py", "function": "get_dataset_config_info", "project": "datasets", "change_stats": {"code_changes": 19, "doc_changes": 11, "total_changes": 30}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _set_axis(self, axis, labels, fastpath: bool = False) -> None:\n        \n        \n        \n        if not fastpath:\n            labels = ensure_index(labels)\n\n        is_all_dates = labels.is_all_dates\n        if is_all_dates:\n            if not isinstance(labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._data.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            self._data.set_axis(axis, labels)```\n\nUpdated code:\n```python\ndef _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \n        \n        \n        if not fastpath:\n            labels = ensure_index(labels)\n\n        is_all_dates = labels.is_all_dates\n        if is_all_dates:\n            if not isinstance(labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._data.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        object.__setattr__(self, \"_index\", labels)\n        if not fastpath:\n            # The ensure_index call aabove ensures we have an Index object\n            self._data.set_axis(axis, labels)\n```\n\nOriginal documentation:\n```python\nOverride generic, we want to set the _typ here.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.", "file": "pandas_core_series.py", "function": "Series._set_axis", "project": "pandas", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef score_deps(\n        examples: Iterable[Example],\n        attr: str,\n        *,\n        getter: Callable[[Token, str], Any] = getattr,\n        head_attr: str = \"head\",\n        head_getter: Callable[[Token, str], Any] = getattr,\n        ignore_labels: Tuple[str] = tuple(),\n        **cfg,\n    ) -> Dict[str, Any]:\n        \n        \n        unlabelled = PRFScore()\n        labelled = PRFScore()\n        labelled_per_dep = dict()\n        for example in examples:\n            gold_doc = example.reference\n            pred_doc = example.predicted\n            align = example.alignment\n            gold_deps = set()\n            gold_deps_per_dep = {}\n            for gold_i, token in enumerate(gold_doc):\n                dep = getter(token, attr)\n                head = head_getter(token, head_attr)\n                if dep not in ignore_labels:\n                    gold_deps.add((gold_i, head.i, dep))\n                    if dep not in labelled_per_dep:\n                        labelled_per_dep[dep] = PRFScore()\n                    if dep not in gold_deps_per_dep:\n                        gold_deps_per_dep[dep] = set()\n                    gold_deps_per_dep[dep].add((gold_i, head.i, dep))\n            pred_deps = set()\n            pred_deps_per_dep = {}\n            for token in pred_doc:\n                if token.orth_.isspace():\n                    continue\n                if align.x2y.lengths[token.i] != 1:\n                    gold_i = None\n                else:\n                    gold_i = align.x2y[token.i].dataXd[0, 0]\n                dep = getter(token, attr)\n                head = head_getter(token, head_attr)\n                if dep not in ignore_labels and token.orth_.strip():\n                    if align.x2y.lengths[head.i] == 1:\n                        gold_head = align.x2y[head.i].dataXd[0, 0]\n                    else:\n                        gold_head = None\n                    # None is indistinct, so we can't just add it to the set\n                    # Multiple (None, None) deps are possible\n                    if gold_i is None or gold_head is None:\n                        unlabelled.fp += 1\n                        labelled.fp += 1\n                    else:\n                        pred_deps.add((gold_i, gold_head, dep))\n                        if dep not in labelled_per_dep:\n                            labelled_per_dep[dep] = PRFScore()\n                        if dep not in pred_deps_per_dep:\n                            pred_deps_per_dep[dep] = set()\n                        pred_deps_per_dep[dep].add((gold_i, gold_head, dep))\n            labelled.score_set(pred_deps, gold_deps)\n            for dep in labelled_per_dep:\n                labelled_per_dep[dep].score_set(\n                    pred_deps_per_dep.get(dep, set()), gold_deps_per_dep.get(dep, set())\n                )\n            unlabelled.score_set(\n                set(item[:2] for item in pred_deps), set(item[:2] for item in gold_deps)\n            )\n        return {\n            f\"{attr}_uas\": unlabelled.fscore,\n            f\"{attr}_las\": labelled.fscore,\n            f\"{attr}_las_per_type\": {\n                k: v.to_dict() for k, v in labelled_per_dep.items()\n            },\n        }```\n\nUpdated code:\n```python\ndef score_deps(\n        examples: Iterable[Example],\n        attr: str,\n        *,\n        getter: Callable[[Token, str], Any] = getattr,\n        head_attr: str = \"head\",\n        head_getter: Callable[[Token, str], Token] = getattr,\n        ignore_labels: Tuple[str] = tuple(),\n        **cfg,\n    ) -> Dict[str, Any]:\n        \n        \n        unlabelled = PRFScore()\n        labelled = PRFScore()\n        labelled_per_dep = dict()\n        for example in examples:\n            gold_doc = example.reference\n            pred_doc = example.predicted\n            align = example.alignment\n            gold_deps = set()\n            gold_deps_per_dep = {}\n            for gold_i, token in enumerate(gold_doc):\n                dep = getter(token, attr)\n                head = head_getter(token, head_attr)\n                if dep not in ignore_labels:\n                    gold_deps.add((gold_i, head.i, dep))\n                    if dep not in labelled_per_dep:\n                        labelled_per_dep[dep] = PRFScore()\n                    if dep not in gold_deps_per_dep:\n                        gold_deps_per_dep[dep] = set()\n                    gold_deps_per_dep[dep].add((gold_i, head.i, dep))\n            pred_deps = set()\n            pred_deps_per_dep = {}\n            for token in pred_doc:\n                if token.orth_.isspace():\n                    continue\n                if align.x2y.lengths[token.i] != 1:\n                    gold_i = None\n                else:\n                    gold_i = align.x2y[token.i].dataXd[0, 0]\n                dep = getter(token, attr)\n                head = head_getter(token, head_attr)\n                if dep not in ignore_labels and token.orth_.strip():\n                    if align.x2y.lengths[head.i] == 1:\n                        gold_head = align.x2y[head.i].dataXd[0, 0]\n                    else:\n                        gold_head = None\n                    # None is indistinct, so we can't just add it to the set\n                    # Multiple (None, None) deps are possible\n                    if gold_i is None or gold_head is None:\n                        unlabelled.fp += 1\n                        labelled.fp += 1\n                    else:\n                        pred_deps.add((gold_i, gold_head, dep))\n                        if dep not in labelled_per_dep:\n                            labelled_per_dep[dep] = PRFScore()\n                        if dep not in pred_deps_per_dep:\n                            pred_deps_per_dep[dep] = set()\n                        pred_deps_per_dep[dep].add((gold_i, gold_head, dep))\n            labelled.score_set(pred_deps, gold_deps)\n            for dep in labelled_per_dep:\n                labelled_per_dep[dep].score_set(\n                    pred_deps_per_dep.get(dep, set()), gold_deps_per_dep.get(dep, set())\n                )\n            unlabelled.score_set(\n                set(item[:2] for item in pred_deps), set(item[:2] for item in gold_deps)\n            )\n        return {\n            f\"{attr}_uas\": unlabelled.fscore,\n            f\"{attr}_las\": labelled.fscore,\n            f\"{attr}_las_per_type\": {\n                k: v.to_dict() for k, v in labelled_per_dep.items()\n            },\n        }\n```\n\nOriginal documentation:\n```python\nReturns the UAS, LAS, and LAS per type scores for dependency\n        parses.\n\n        examples (Iterable[Example]): Examples to score\n        attr (str): The attribute containing the dependency label.\n        getter (Callable[[Token, str], Any]): Defaults to getattr. If provided,\n            getter(token, attr) should return the value of the attribute for an\n            individual token.\n        head_attr (str): The attribute containing the head token. Defaults to\n            'head'.\n        head_getter (Callable[[Token, str], Any]): Defaults to getattr. If provided,\n            head_getter(token, attr) should return the value of the head for an\n            individual token.\n        ignore_labels (Tuple): Labels to ignore while scoring (e.g., punct).\n        RETURNS (Dict[str, Any]): A dictionary containing the scores:\n            attr_uas, attr_las, and attr_las_per_type.\n\n        DOCS: https://spacy.io/api/scorer#score_deps\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the UAS, LAS, and LAS per type scores for dependency\n        parses.\n\n        examples (Iterable[Example]): Examples to score\n        attr (str): The attribute containing the dependency label.\n        getter (Callable[[Token, str], Any]): Defaults to getattr. If provided,\n            getter(token, attr) should return the value of the attribute for an\n            individual token.\n        head_attr (str): The attribute containing the head token. Defaults to\n            'head'.\n        head_getter (Callable[[Token, str], Token]): Defaults to getattr. If provided,\n            head_getter(token, attr) should return the value of the head for an\n            individual token.\n        ignore_labels (Tuple): Labels to ignore while scoring (e.g., punct).\n        RETURNS (Dict[str, Any]): A dictionary containing the scores:\n            attr_uas, attr_las, and attr_las_per_type.\n\n        DOCS: https://spacy.io/api/scorer#score_deps", "file": "spacy_scorer.py", "function": "Scorer.score_deps", "project": "spaCy", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef inception_resnet_v2_arg_scope(\n    weight_decay=0.00004,\n    batch_norm_decay=0.9997,\n    batch_norm_epsilon=0.001,\n    activation_fn=tf.nn.relu,\n    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):\n  \n  \n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        'decay': batch_norm_decay,\n        'epsilon': batch_norm_epsilon,\n        'updates_collections': batch_norm_updates_collections,\n        'fused': None,  # Use fused batch norm if possible.\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=activation_fn,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope```\n\nUpdated code:\n```python\ndef inception_resnet_v2_arg_scope(\n    weight_decay=0.00004,\n    batch_norm_decay=0.9997,\n    batch_norm_epsilon=0.001,\n    activation_fn=tf.nn.relu,\n    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,\n    batch_norm_scale=False):\n  \n  \n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        'decay': batch_norm_decay,\n        'epsilon': batch_norm_epsilon,\n        'updates_collections': batch_norm_updates_collections,\n        'fused': None,  # Use fused batch norm if possible.\n        'scale': batch_norm_scale,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=activation_fn,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n```\n\nOriginal documentation:\n```python\nReturns the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n    activation_fn: Activation function for conv2d.\n    batch_norm_updates_collections: Collection for the update ops for\n      batch norm.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n    activation_fn: Activation function for conv2d.\n    batch_norm_updates_collections: Collection for the update ops for\n      batch norm.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.", "file": "research_slim_nets_inception_resnet_v2.py", "function": "inception_resnet_v2_arg_scope", "project": "models", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef back_latex_name_matches(text:str):\n    \n    \n    if len(text)<2:\n        return u'', ()\n    maybe_slash = text[-2]\n    if maybe_slash != '\\\\':\n        return u'', ()\n\n\n    char = text[-1]\n    # no expand on quote for completion in strings.\n    # nor backcomplete standard ascii keys\n    if char in string.ascii_letters or char in ['\"',\"'\"]:\n        return u'', ()\n    try :\n        latex = reverse_latex_symbol[char]\n        # '\\\\' replace the \\ as well\n        return '\\\\'+char,[latex]\n    except KeyError:\n        pass\n    return u'', ()```\n\nUpdated code:\n```python\ndef back_latex_name_matches(text:str) -> Tuple[str, Sequence[str]] :\n    \n\n    \n    if len(text)<2:\n        return '', ()\n    maybe_slash = text[-2]\n    if maybe_slash != '\\\\':\n        return '', ()\n\n\n    char = text[-1]\n    # no expand on quote for completion in strings.\n    # nor backcomplete standard ascii keys\n    if char in string.ascii_letters or char in ('\"',\"'\"):\n        return '', ()\n    try :\n        latex = reverse_latex_symbol[char]\n        # '\\\\' replace the \\ as well\n        return '\\\\'+char,[latex]\n    except KeyError:\n        pass\n    return '', ()\n```\n\nOriginal documentation:\n```python\nMatch latex characters back to unicode name\n\n    This does ``\\\\\u2135`` -> ``\\\\aleph``\n\n    Used on Python 3 only.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Match latex characters back to unicode name\n\n    This does ``\\\\\u2135`` -> ``\\\\aleph``", "file": "IPython_core_completer.py", "function": "back_latex_name_matches", "project": "ipython", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lookup_generic(self,\n                       asset_convertible_or_iterable,\n                       as_of_date):\n        \n        \n        \n        matches = []\n        missing = []\n\n        # Interpret input as scalar.\n        if isinstance(asset_convertible_or_iterable, AssetConvertible):\n            self._lookup_generic_scalar(\n                asset_convertible=asset_convertible_or_iterable,\n                as_of_date=as_of_date,\n                matches=matches,\n                missing=missing,\n            )\n            try:\n                return matches[0], missing\n            except IndexError:\n                if hasattr(asset_convertible_or_iterable, '__int__'):\n                    raise SidsNotFound(sids=[asset_convertible_or_iterable])\n                else:\n                    raise SymbolNotFound(symbol=asset_convertible_or_iterable)\n\n        # If the input is a ContinuousFuture just return it as-is.\n        elif isinstance(asset_convertible_or_iterable, ContinuousFuture):\n            return asset_convertible_or_iterable, missing\n\n        # Interpret input as iterable.\n        try:\n            iterator = iter(asset_convertible_or_iterable)\n        except TypeError:\n            raise NotAssetConvertible(\n                \"Input was not a AssetConvertible \"\n                \"or iterable of AssetConvertible.\"\n            )\n\n        for obj in iterator:\n            if isinstance(obj, ContinuousFuture):\n                matches.append(obj)\n            else:\n                self._lookup_generic_scalar(obj, as_of_date, matches, missing)\n        return matches, missing```\n\nUpdated code:\n```python\ndef lookup_generic(self, obj, as_of_date, country_code):\n        \n        \n        \n        matches = []\n        missing = []\n\n        # Interpret input as scalar.\n        if isinstance(obj, (AssetConvertible, ContinuousFuture)):\n            self._lookup_generic_scalar(\n                obj=obj,\n                as_of_date=as_of_date,\n                country_code=country_code,\n                matches=matches,\n                missing=missing,\n            )\n            try:\n                return matches[0], missing\n            except IndexError:\n                if hasattr(obj, '__int__'):\n                    raise SidsNotFound(sids=[obj])\n                else:\n                    raise SymbolNotFound(symbol=obj)\n\n        # Interpret input as iterable.\n        try:\n            iterator = iter(obj)\n        except TypeError:\n            raise NotAssetConvertible(\n                \"Input was not a AssetConvertible \"\n                \"or iterable of AssetConvertible.\"\n            )\n\n        for obj in iterator:\n            self._lookup_generic_scalar(\n                obj=obj,\n                as_of_date=as_of_date,\n                country_code=country_code,\n                matches=matches,\n                missing=missing,\n            )\n\n        return matches, missing\n```\n\nOriginal documentation:\n```python\nConvert a AssetConvertible or iterable of AssetConvertibles into\n        a list of Asset objects.\n\n        This method exists primarily as a convenience for implementing\n        user-facing APIs that can handle multiple kinds of input.  It should\n        not be used for internal code where we already know the expected types\n        of our inputs.\n\n        Returns a pair of objects, the first of which is the result of the\n        conversion, and the second of which is a list containing any values\n        that couldn't be resolved.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert an object into an Asset or sequence of Assets.\n\n        This method exists primarily as a convenience for implementing\n        user-facing APIs that can handle multiple kinds of input.  It should\n        not be used for internal code where we already know the expected types\n        of our inputs.\n\n        Parameters\n        ----------\n        obj : int, str, Asset, ContinuousFuture, or iterable\n            The object to be converted into one or more Assets.\n            Integers are interpreted as sids. Strings are interpreted as\n            tickers. Assets and ContinuousFutures are returned unchanged.\n        as_of_date : pd.Timestamp or None\n            Timestamp to use to disambiguate ticker lookups. Has the same\n            semantics as in `lookup_symbol`.\n        country_code : str or None\n            ISO-3166 country code to use to disambiguate ticker lookups. Has\n            the same semantics as in `lookup_symbol`.\n\n        Returns\n        -------\n        matches, missing : tuple\n            ``matches`` is the result of the conversion. ``missing`` is a list\n             containing any values that couldn't be resolved. If ``obj`` is not\n             an iterable, ``missing`` will be an empty list.", "file": "zipline_assets_assets.py", "function": "AssetFinder.lookup_generic", "project": "zipline", "change_stats": {"code_changes": 35, "doc_changes": 27, "total_changes": 62}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, client: XboxLiveClient, console: SmartglassConsole) -> None:\n        \n        self.client: XboxLiveClient = client\n        self._console: SmartglassConsole = console\n\n        self._console_status: SmartglassConsoleStatus = None\n        self._app_details: Optional[Product] = None```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        client: XboxLiveClient,\n        console: SmartglassConsole,\n        coordinator: XboxUpdateCoordinator,\n    ) -> None:\n        \n        super().__init__(coordinator)\n        self.client: XboxLiveClient = client\n        self._console: SmartglassConsole = console\n```\n\nOriginal documentation:\n```python\nInitialize the Plex device.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the Xbox Media Player.", "file": "homeassistant_components_xbox_media_player.py", "function": "XboxMediaPlayer.__init__", "project": "core", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _send_wakeword_info(self, emitter):\n        \n        SessionManager.touch()\n        payload = {'utterance': self.wake_word_name,\n                   'session': SessionManager.get().session_id}\n        emitter.emit(\"recognizer_loop:wakeword\", payload)```\n\nUpdated code:\n```python\ndef _send_wakeword_info(self, emitter):\n        \n        \n        SessionManager.touch()\n        payload = {'utterance': self.wake_word_name,\n                   'session': SessionManager.get().session_id}\n        emitter.emit(\"recognizer_loop:wakeword\", payload)\n```\n\nOriginal documentation:\n```python\nSend messagebus message indicating that a wakeword was received.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Send messagebus message indicating that a wakeword was received.\n\n        Arguments:\n            emitter: bus emitter to send information on.", "file": "mycroft_client_speech_mic.py", "function": "ResponsiveRecognizer._send_wakeword_info", "project": "mycroft-core", "change_stats": {"code_changes": 3, "doc_changes": 5, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sync_device_config(self):\n        \n        desired_sensor_configuration = self.sensor_configuration()\n        current_sensor_configuration = [\n            {'pin': s[CONF_PIN]} for s in self.status.get('sensors')]\n        _LOGGER.debug('%s: desired sensor config: %s', self.device_id,\n                      desired_sensor_configuration)\n        _LOGGER.debug('%s: current sensor config: %s', self.device_id,\n                      current_sensor_configuration)\n\n        desired_actuator_config = self.actuator_configuration()\n        current_actuator_config = self.status.get('actuators')\n        _LOGGER.debug('%s: desired actuator config: %s', self.device_id,\n                      desired_actuator_config)\n        _LOGGER.debug('%s: current actuator config: %s', self.device_id,\n                      current_actuator_config)\n\n        desired_api_host = \\\n            self.hass.data[DOMAIN].get(CONF_API_HOST) or \\\n            self.hass.config.api.base_url\n        desired_api_endpoint = desired_api_host + ENDPOINT_ROOT\n        current_api_endpoint = self.status.get('endpoint')\n\n        _LOGGER.debug('%s: desired api endpoint: %s', self.device_id,\n                      desired_api_endpoint)\n        _LOGGER.debug('%s: current api endpoint: %s', self.device_id,\n                      current_api_endpoint)\n\n        if (desired_sensor_configuration != current_sensor_configuration) or \\\n                (current_actuator_config != desired_actuator_config) or \\\n                (current_api_endpoint != desired_api_endpoint) or \\\n                (self.status.get(CONF_BLINK) !=\n                 self.stored_configuration.get(CONF_BLINK)) or \\\n                (self.status.get(CONF_DISCOVERY) !=\n                 self.stored_configuration.get(CONF_DISCOVERY)):\n            _LOGGER.info('pushing settings to device %s', self.device_id)\n            self.client.put_settings(\n                desired_sensor_configuration,\n                desired_actuator_config,\n                self.hass.data[DOMAIN].get(CONF_ACCESS_TOKEN),\n                desired_api_endpoint,\n                blink=self.stored_configuration.get(CONF_BLINK),\n                discovery=self.stored_configuration.get(CONF_DISCOVERY)\n            )```\n\nUpdated code:\n```python\ndef sync_device_config(self):\n        \n        _LOGGER.debug('Device %s settings payload: %s', self.device_id,\n                      self.desired_settings_payload())\n        if self.desired_settings_payload() != self.current_settings_payload():\n            _LOGGER.info('pushing settings to device %s', self.device_id)\n            self.client.put_settings(**self.desired_settings_payload())\n```\n\nOriginal documentation:\n```python\nSync the new pin configuration to the Konnected device.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sync the new pin configuration to the Konnected device if needed.", "file": "homeassistant_components_konnected___init__.py", "function": "DiscoveredDevice.sync_device_config", "project": "core", "change_stats": {"code_changes": 47, "doc_changes": 4, "total_changes": 51}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef component_translation_path(\n    component: str, language: str, integration: Integration\n) -> str | None:\n    \n    \n    parts = component.split(\".\")\n    domain = parts[0]\n    is_platform = len(parts) == 2\n\n    # If it's a component that is just one file, we don't support translations\n    # Example custom_components/my_component.py\n    if integration.file_path.name != domain:\n        return None\n\n    if is_platform:\n        filename = f\"{parts[1]}.{language}.json\"\n    else:\n        filename = f\"{language}.json\"\n\n    translation_path = integration.file_path / \"translations\"\n\n    return str(translation_path / filename)```\n\nUpdated code:\n```python\ndef component_translation_path(language: str, integration: Integration) -> pathlib.Path:\n    \n\n    \n    return integration.file_path / \"translations\" / f\"{language}.json\"\n```\n\nOriginal documentation:\n```python\nReturn the translation json file location for a component.\n\n    For component:\n     - components/hue/translations/nl.json\n\n    For platform:\n     - components/hue/translations/light.nl.json\n\n    If component is just a single file, will return None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the translation json file location for a component.\n\n    For component:\n     - components/hue/translations/nl.json", "file": "homeassistant_helpers_translation.py", "function": "component_translation_path", "project": "core", "change_stats": {"code_changes": 25, "doc_changes": 7, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef multinomial(random_state, size=None, n=1, pvals=[0.5, 0.5],\n        ndim=None, dtype='int64'):\n    \n    \n\n    \n    n = tensor.as_tensor_variable(n)\n    pvals = tensor.as_tensor_variable(pvals)\n    # until ellipsis is implemented (argh)\n    tmp = pvals.T[0].T\n    ndim, size, bcast = _infer_ndim_bcast(ndim, size, n, tmp)\n    bcast = bcast+(pvals.type.broadcastable[-1],)\n    op = RandomFunction(multinomial_helper,\n            tensor.TensorType(dtype = dtype, broadcastable = bcast),\n            ndim_added=1)\n    return op(random_state, size, n, pvals)```\n\nUpdated code:\n```python\ndef multinomial(random_state, size=None, n=1, pvals=[0.5, 0.5],\n                ndim=None, dtype='int64'):\n    \n\n    \n    n = tensor.as_tensor_variable(n)\n    pvals = tensor.as_tensor_variable(pvals)\n    # until ellipsis is implemented (argh)\n    tmp = pvals.T[0].T\n    ndim, size, bcast = _infer_ndim_bcast(ndim, size, n, tmp)\n    bcast = bcast + (pvals.type.broadcastable[-1],)\n    op = RandomFunction(multinomial_helper,\n            tensor.TensorType(dtype=dtype, broadcastable=bcast),\n            ndim_added=1)\n    return op(random_state, size, n, pvals)\n```\n\nOriginal documentation:\n```python\nSample from one or more multinomial distributions defined by\n    one-dimensional slices in pvals.\n\n    :param pvals: a tensor of shape \"nmulti+(L,)\" describing each multinomial\n        distribution.  This tensor must have the property that\n        numpy.allclose(pvals.sum(axis=-1), 1) is true.\n\n    :param size: a vector of shape information for the output; this can also\n        specify the \"nmulti\" part of pvals' shape.  A -1 in the k'th position\n        from the right means to borrow the k'th position from the\n        right in nmulti. (See examples below.)\n        Default ``None`` means size=nmulti.\n\n    :param n: the number of experiments to simulate for each multinomial. This\n        can be a scalar, or tensor, it will be broadcasted to have shape \"nmulti\".\n\n    :param dtype: the dtype of the return value (which will represent counts)\n\n    :returns: tensor of len(size)+1 dimensions, and shape[-1]==L, with the specified ``dtype``,\n        with the experiment counts.  See examples to understand the shape of the\n        return value, which is derived from both size and pvals.shape.\n        In return value rval, \"numpy.allclose(rval.sum(axis=-1), n)\" will be true.\n\n    For example, to simulate n experiments from each multinomial in a batch of\n    size B:\n\n        size=None, pvals.shape=(B,L) --> rval.shape=[B,L]\n\n        rval[i,j] is the count of possibility j in the i'th distribution (row)\n        in pvals.\n\n    Using size:\n\n        size=(1,-1), pvals.shape=(A,B,L)\n        --> rval.shape=[1,B,L], and requires that A==1.\n\n        rval[k,i,j] is the count of possibility j in the distribution specified\n        by pvals[k,i].\n\n    Using size for broadcasting of pvals:\n\n        size=(10,1,-1), pvals.shape=(A,B,L)\n        --> rval.shape=[10,1,B,L], and requires that A==1.\n\n        rval[l,k,i,j] is the count of possibility j in the distribution specified\n        by pvals[k,i], in the l'th of 10 draws.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sample from one or more multinomial distributions defined by\n    one-dimensional slices in pvals.\n\n    :param pvals: a tensor of shape \"nmulti+(L,)\" describing each multinomial\n        distribution.  This tensor must have the property that\n        numpy.allclose(pvals.sum(axis=-1), 1) is true.\n\n    :param size: a vector of shape information for the output; this can also\n        specify the \"nmulti\" part of pvals' shape.  A -1 in the k'th position\n        from the right means to borrow the k'th position from the\n        right in nmulti. (See examples below.)\n        Default ``None`` means size=nmulti.\n\n    :param n: the number of experiments to simulate for each\n        multinomial. This can be a scalar, or tensor, it will be\n        broadcasted to have shape \"nmulti\".\n\n    :param dtype: the dtype of the return value (which will represent counts)\n\n    :returns: tensor of len(size)+1 dimensions, and shape[-1]==L, with\n        the specified ``dtype``, with the experiment counts.  See\n        examples to understand the shape of the return value, which is\n        derived from both size and pvals.shape.  In return value rval,\n        \"numpy.allclose(rval.sum(axis=-1), n)\" will be true.\n\n    For example, to simulate n experiments from each multinomial in a batch of\n    size B:\n\n        size=None, pvals.shape=(B,L) --> rval.shape=[B,L]\n\n        rval[i,j] is the count of possibility j in the i'th distribution (row)\n        in pvals.\n\n    Using size:\n\n        size=(1,-1), pvals.shape=(A,B,L)\n        --> rval.shape=[1,B,L], and requires that A==1.\n\n        rval[k,i,j] is the count of possibility j in the distribution specified\n        by pvals[k,i].\n\n    Using size for broadcasting of pvals:\n\n        size=(10, 1, -1), pvals.shape=(A, B, L)\n        --> rval.shape=[10,1,B,L], and requires that A==1.\n\n        rval[l,k,i,j] is the count of possibility j in the\n        distribution specified by pvals[k,i], in the l'th of 10\n        draws.", "file": "theano_tensor_raw_random.py", "function": "multinomial", "project": "Theano", "change_stats": {"code_changes": 9, "doc_changes": 23, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_compress_streaming_response_unicode(self):\n        \n        \n        \n        r = GZipMiddleware().process_response(self.req, self.stream_resp_unicode)\n        self.assertEqual(self.decompress(b''.join(r)), b''.join(x.encode('utf-8') for x in self.sequence_unicode))\n        self.assertEqual(r.get('Content-Encoding'), 'gzip')\n        self.assertFalse(r.has_header('Content-Length'))```\n\nUpdated code:\n```python\ndef test_compress_streaming_response_unicode(self):\n        \n        \n        \n        r = GZipMiddleware().process_response(self.req, self.stream_resp_unicode)\n        self.assertEqual(\n            self.decompress(b''.join(r)),\n            b''.join(x.encode('utf-8') for x in self.sequence_unicode)\n        )\n        self.assertEqual(r.get('Content-Encoding'), 'gzip')\n        self.assertFalse(r.has_header('Content-Length'))\n```\n\nOriginal documentation:\n```python\nTests that compression is performed on responses with streaming Unicode content.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compression is performed on responses with streaming Unicode content.", "file": "tests_middleware_tests.py", "function": "GZipMiddlewareTest.test_compress_streaming_response_unicode", "project": "django", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef can_fast_delete(self, objs, from_field=None):\n        \n        \n        \n        if from_field and from_field.remote_field.on_delete is not CASCADE:\n            return False\n        if not (hasattr(objs, 'model') and hasattr(objs, '_raw_delete')):\n            return False\n        model = objs.model\n        if (signals.pre_delete.has_listeners(model) or\n                signals.post_delete.has_listeners(model) or\n                signals.m2m_changed.has_listeners(model)):\n            return False\n        # The use of from_field comes from the need to avoid cascade back to\n        # parent when parent delete is cascading to child.\n        opts = model._meta\n        return (\n            all(link == from_field for link in opts.concrete_model._meta.parents.values()) and\n            # Foreign keys pointing to this model.\n            all(\n                related.field.remote_field.on_delete is DO_NOTHING\n                for related in get_candidate_relations_to_delete(opts)\n            ) and (\n                # Something like generic foreign key.\n                not any(hasattr(field, 'bulk_related_objects') for field in model._meta.private_fields)\n            )\n        )```\n\nUpdated code:\n```python\ndef can_fast_delete(self, objs, from_field=None):\n        \n        \n        \n        if from_field and from_field.remote_field.on_delete is not CASCADE:\n            return False\n        if hasattr(objs, '_meta'):\n            model = type(objs)\n        elif hasattr(objs, 'model') and hasattr(objs, '_raw_delete'):\n            model = objs.model\n        else:\n            return False\n        if (signals.pre_delete.has_listeners(model) or\n                signals.post_delete.has_listeners(model) or\n                signals.m2m_changed.has_listeners(model)):\n            return False\n        # The use of from_field comes from the need to avoid cascade back to\n        # parent when parent delete is cascading to child.\n        opts = model._meta\n        return (\n            all(link == from_field for link in opts.concrete_model._meta.parents.values()) and\n            # Foreign keys pointing to this model.\n            all(\n                related.field.remote_field.on_delete is DO_NOTHING\n                for related in get_candidate_relations_to_delete(opts)\n            ) and (\n                # Something like generic foreign key.\n                not any(hasattr(field, 'bulk_related_objects') for field in opts.private_fields)\n            )\n        )\n```\n\nOriginal documentation:\n```python\nDetermine if the objects in the given queryset-like can be\n        fast-deleted. This can be done if there are no cascades, no\n        parents and no signal listeners for the object class.\n\n        The 'from_field' tells where we are coming from - we need this to\n        determine if the objects are in fact to be deleted. Allow also\n        skipping parent -> child -> parent chain preventing fast delete of\n        the child.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Determine if the objects in the given queryset-like or single object\n        can be fast-deleted. This can be done if there are no cascades, no\n        parents and no signal listeners for the object class.\n\n        The 'from_field' tells where we are coming from - we need this to\n        determine if the objects are in fact to be deleted. Allow also\n        skipping parent -> child -> parent chain preventing fast delete of\n        the child.", "file": "django_db_models_deletion.py", "function": "Collector.can_fast_delete", "project": "django", "change_stats": {"code_changes": 11, "doc_changes": 6, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef block(fn, hide=None, expose=None, hide_types=None, expose_types=None):\n    \n    \n    \n    return BlockPoutine(fn, hide=hide, expose=expose,\n                        hide_types=hide_types, expose_types=expose_types)```\n\nUpdated code:\n```python\ndef block(fn, hide=None, expose=None, hide_types=None, expose_types=None):\n    \n    \n    \n    return BlockMessenger(hide=hide, expose=expose,\n                          hide_types=hide_types, expose_types=expose_types)(fn)\n```\n\nOriginal documentation:\n```python\n:param fn: a stochastic function (callable containing pyro primitive calls)\n    :param hide: list of site names to hide\n    :param expose: list of site names to be exposed while all others hidden\n    :param hide_types: list of site types to be hidden\n    :param expose_types: list of site types to be exposed while all others hidden\n    :returns: stochastic function wrapped in a BlockPoutine\n    :rtype: pyro.poutine.BlockPoutine\n\n    Alias for BlockPoutine constructor.\n\n    Given a callable that contains Pyro primitive calls,\n    selectively hide some of those calls from poutines higher up the stack\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":param fn: a stochastic function (callable containing pyro primitive calls)\n    :param hide: list of site names to hide\n    :param expose: list of site names to be exposed while all others hidden\n    :param hide_types: list of site types to be hidden\n    :param expose_types: list of site types to be exposed while all others hidden\n    :returns: stochastic function wrapped in a BlockHandler\n    :rtype: pyro.poutine.BlockHandler\n\n    Alias for BlockHandler constructor.\n\n    Given a callable that contains Pyro primitive calls,\n    selectively hide some of those calls from poutines higher up the stack", "file": "pyro_poutine___init__.py", "function": "block", "project": "pyro", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_deposits(self, code: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n       \n        self.load_markets()\n        request = {\n            # 'coin': currency['id'],\n            # 'limit': 20,  # max 50\n            # 'cursor': '',\n        }\n        currency = None\n        if code is not None:\n            currency = self.currency(code)\n            request['coin'] = currency['id']\n        if since is not None:\n            request['startTime'] = since\n        if limit is not None:\n            request['limit'] = limit\n        response = self.privateGetV5AssetDepositQueryRecord(self.extend(request, params))\n        #\n        #     {\n        #         \"retCode\": 0,\n        #         \"retMsg\": \"success\",\n        #         \"result\": {\n        #             \"rows\": [\n        #                 {\n        #                     \"coin\": \"USDT\",\n        #                     \"chain\": \"ETH\",\n        #                     \"amount\": \"10000\",\n        #                     \"txID\": \"skip-notification-scene-test-amount-202212270944-533285-USDT\",\n        #                     \"status\": 3,\n        #                     \"toAddress\": \"test-amount-address\",\n        #                     \"tag\": \"\",\n        #                     \"depositFee\": \"\",\n        #                     \"successAt\": \"1672134274000\",\n        #                     \"confirmations\": \"10000\",\n        #                     \"txIndex\": \"\",\n        #                     \"blockHash\": \"\"\n        #                 }\n        #             ],\n        #             \"nextPageCursor\": \"eyJtaW5JRCI6MTA0NjA0MywibWF4SUQiOjEwNDYwNDN9\"\n        #         },\n        #         \"retExtInfo\": {},\n        #         \"time\": 1672191992512\n        #     }\n        #\n        data = self.add_pagination_cursor_to_result(response)\n        return self.parse_transactions(data, currency, since, limit)```\n\nUpdated code:\n```python\ndef fetch_deposits(self, code: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n       \n        self.load_markets()\n        paginate = False\n        paginate, params = self.handle_option_and_params(params, 'fetchDeposits', 'paginate')\n        if paginate:\n            return self.fetch_paginated_call_cursor('fetchDeposits', code, since, limit, params, 'nextPageCursor', 'nextPageCursor', None, 50)\n        request = {\n            # 'coin': currency['id'],\n            # 'limit': 20,  # max 50\n            # 'cursor': '',\n        }\n        currency = None\n        if code is not None:\n            currency = self.currency(code)\n            request['coin'] = currency['id']\n        if since is not None:\n            request['startTime'] = since\n        if limit is not None:\n            request['limit'] = limit\n        request, params = self.handle_until_option('endTime', request, params)\n        response = self.privateGetV5AssetDepositQueryRecord(self.extend(request, params))\n        #\n        #     {\n        #         \"retCode\": 0,\n        #         \"retMsg\": \"success\",\n        #         \"result\": {\n        #             \"rows\": [\n        #                 {\n        #                     \"coin\": \"USDT\",\n        #                     \"chain\": \"ETH\",\n        #                     \"amount\": \"10000\",\n        #                     \"txID\": \"skip-notification-scene-test-amount-202212270944-533285-USDT\",\n        #                     \"status\": 3,\n        #                     \"toAddress\": \"test-amount-address\",\n        #                     \"tag\": \"\",\n        #                     \"depositFee\": \"\",\n        #                     \"successAt\": \"1672134274000\",\n        #                     \"confirmations\": \"10000\",\n        #                     \"txIndex\": \"\",\n        #                     \"blockHash\": \"\"\n        #                 }\n        #             ],\n        #             \"nextPageCursor\": \"eyJtaW5JRCI6MTA0NjA0MywibWF4SUQiOjEwNDYwNDN9\"\n        #         },\n        #         \"retExtInfo\": {},\n        #         \"time\": 1672191992512\n        #     }\n        #\n        data = self.add_pagination_cursor_to_result(response)\n        return self.parse_transactions(data, currency, since, limit)\n```\n\nOriginal documentation:\n```python\nfetch all deposits made to an account\n        see https://bybit-exchange.github.io/docs/v5/asset/deposit-record\n        :param str code: unified currency code\n        :param int [since]: the earliest time in ms to fetch deposits for, default = 30 days before the current time\n        :param int [limit]: the maximum number of deposits structures to retrieve, default = 50, max = 50\n        :param dict [params]: extra parameters specific to the bybit api endpoint\n        :param int [params.until]: the latest time in ms to fetch deposits for, default = 30 days after since\n         *\n         * EXCHANGE SPECIFIC PARAMETERS\n        :param str [params.cursor]: used for pagination\n        :returns dict[]: a list of `transaction structures <https://github.com/ccxt/ccxt/wiki/Manual#transaction-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch all deposits made to an account\n        see https://bybit-exchange.github.io/docs/v5/asset/deposit-record\n        :param str code: unified currency code\n        :param int [since]: the earliest time in ms to fetch deposits for, default = 30 days before the current time\n        :param int [limit]: the maximum number of deposits structures to retrieve, default = 50, max = 50\n        :param dict [params]: extra parameters specific to the bybit api endpoint\n        :param int [params.until]: the latest time in ms to fetch deposits for, default = 30 days after since\n         *\n         * EXCHANGE SPECIFIC PARAMETERS\n        :param boolean [params.paginate]: default False, when True will automatically paginate by calling self endpoint multiple times. See in the docs all the [availble parameters]  (ttps://github.com/ccxt/ccxt/wiki/Manual#pagination-params)\n        :param str [params.cursor]: used for pagination\n        :returns dict[]: a list of `transaction structures <https://github.com/ccxt/ccxt/wiki/Manual#transaction-structure>`", "file": "python_ccxt_bybit.py", "function": "bybit.fetch_deposits", "project": "ccxt", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_market_leverage_tiers(self, info, market=None):\n        '''\n            \n       '''\n        marginRequirements = self.safe_value(info, 'marginRequirements')\n        id = self.safe_string(info, 'symbol')\n        market = self.safe_market(id, market)\n        tiers = []\n        for i in range(0, len(marginRequirements)):\n            tier = marginRequirements[i]\n            initialMarginRate = self.safe_string(tier, 'initialMarginRate')\n            tiers.append({\n                'tier': self.sum(i, 1),\n                'currency': market['quote'],\n                'notionalFloor': self.safe_number(tier, 'positionNotionalLowerBound'),\n                'notionalCap': self.safe_number(tier, 'positionNotionalUpperBound'),\n                'maintenanceMarginRate': self.safe_number(tier, 'maintenanceMarginRate'),\n                'maxLeverage': self.parse_number(Precise.string_div('1', initialMarginRate)),\n                'info': tier,\n            })\n        return tiers```\n\nUpdated code:\n```python\ndef parse_market_leverage_tiers(self, info, market=None):\n        \n        \n        \n        #\n        #    {\n        #        \"symbol\":\"BTC-PERP\",\n        #        \"status\":\"Normal\",\n        #        \"displayName\":\"BTCUSDT\",\n        #        \"settlementAsset\":\"USDT\",\n        #        \"underlying\":\"BTC/USDT\",\n        #        \"tradingStartTime\":1579701600000,\n        #        \"priceFilter\":{\"minPrice\":\"1\",\"maxPrice\":\"1000000\",\"tickSize\":\"1\"},\n        #        \"lotSizeFilter\":{\"minQty\":\"0.0001\",\"maxQty\":\"1000000000\",\"lotSize\":\"0.0001\"},\n        #        \"commissionType\":\"Quote\",\n        #        \"commissionReserveRate\":\"0.001\",\n        #        \"marketOrderPriceMarkup\":\"0.03\",\n        #        \"marginRequirements\":[\n        #            {\"positionNotionalLowerBound\":\"0\",\"positionNotionalUpperBound\":\"50000\",\"initialMarginRate\":\"0.01\",\"maintenanceMarginRate\":\"0.006\"},\n        #            {\"positionNotionalLowerBound\":\"50000\",\"positionNotionalUpperBound\":\"200000\",\"initialMarginRate\":\"0.02\",\"maintenanceMarginRate\":\"0.012\"},\n        #            {\"positionNotionalLowerBound\":\"200000\",\"positionNotionalUpperBound\":\"2000000\",\"initialMarginRate\":\"0.04\",\"maintenanceMarginRate\":\"0.024\"},\n        #            {\"positionNotionalLowerBound\":\"2000000\",\"positionNotionalUpperBound\":\"20000000\",\"initialMarginRate\":\"0.1\",\"maintenanceMarginRate\":\"0.06\"},\n        #            {\"positionNotionalLowerBound\":\"20000000\",\"positionNotionalUpperBound\":\"40000000\",\"initialMarginRate\":\"0.2\",\"maintenanceMarginRate\":\"0.12\"},\n        #            {\"positionNotionalLowerBound\":\"40000000\",\"positionNotionalUpperBound\":\"1000000000\",\"initialMarginRate\":\"0.333333\",\"maintenanceMarginRate\":\"0.2\"}\n        #        ]\n        #    }\n        #\n        marginRequirements = self.safe_value(info, 'marginRequirements')\n        id = self.safe_string(info, 'symbol')\n        market = self.safe_market(id, market)\n        tiers = []\n        for i in range(0, len(marginRequirements)):\n            tier = marginRequirements[i]\n            initialMarginRate = self.safe_string(tier, 'initialMarginRate')\n            tiers.append({\n                'tier': self.sum(i, 1),\n                'currency': market['quote'],\n                'notionalFloor': self.safe_number(tier, 'positionNotionalLowerBound'),\n                'notionalCap': self.safe_number(tier, 'positionNotionalUpperBound'),\n                'maintenanceMarginRate': self.safe_number(tier, 'maintenanceMarginRate'),\n                'maxLeverage': self.parse_number(Precise.string_div('1', initialMarginRate)),\n                'info': tier,\n            })\n        return tiers\n```\n\nOriginal documentation:\n```python\n@param info: Exchange market response for 1 market\n            {\n                \"symbol\":\"BTC-PERP\",\n                \"status\":\"Normal\",\n                \"displayName\":\"BTCUSDT\",\n                \"settlementAsset\":\"USDT\",\n                \"underlying\":\"BTC/USDT\",\n                \"tradingStartTime\":1579701600000,\n                \"priceFilter\":{\"minPrice\":\"1\",\"maxPrice\":\"1000000\",\"tickSize\":\"1\"},\n                \"lotSizeFilter\":{\"minQty\":\"0.0001\",\"maxQty\":\"1000000000\",\"lotSize\":\"0.0001\"},\n                \"commissionType\":\"Quote\",\n                \"commissionReserveRate\":\"0.001\",\n                \"marketOrderPriceMarkup\":\"0.03\",\n                \"marginRequirements\":[\n                    {\"positionNotionalLowerBound\":\"0\",\"positionNotionalUpperBound\":\"50000\",\"initialMarginRate\":\"0.01\",\"maintenanceMarginRate\":\"0.006\"},\n                    {\"positionNotionalLowerBound\":\"50000\",\"positionNotionalUpperBound\":\"200000\",\"initialMarginRate\":\"0.02\",\"maintenanceMarginRate\":\"0.012\"},\n                    {\"positionNotionalLowerBound\":\"200000\",\"positionNotionalUpperBound\":\"2000000\",\"initialMarginRate\":\"0.04\",\"maintenanceMarginRate\":\"0.024\"},\n                    {\"positionNotionalLowerBound\":\"2000000\",\"positionNotionalUpperBound\":\"20000000\",\"initialMarginRate\":\"0.1\",\"maintenanceMarginRate\":\"0.06\"},\n                    {\"positionNotionalLowerBound\":\"20000000\",\"positionNotionalUpperBound\":\"40000000\",\"initialMarginRate\":\"0.2\",\"maintenanceMarginRate\":\"0.12\"},\n                    {\"positionNotionalLowerBound\":\"40000000\",\"positionNotionalUpperBound\":\"1000000000\",\"initialMarginRate\":\"0.333333\",\"maintenanceMarginRate\":\"0.2\"}\n                ]\n            @param market: CCXT market\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":param dict info: Exchange market response for 1 market\n        :param dict market: CCXT market", "file": "python_ccxt_ascendex.py", "function": "ascendex.parse_market_leverage_tiers", "project": "ccxt", "change_stats": {"code_changes": 31, "doc_changes": 26, "total_changes": 57}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef random_state(state=None):\n    \n    \n\n    \n    if (\n        is_integer(state)\n        or is_array_like(state)\n        or (not _np_version_under1p17 and isinstance(state, np.random.BitGenerator))\n    ):\n        return np.random.RandomState(state)\n    elif isinstance(state, np.random.RandomState):\n        return state\n    elif state is None:\n        return np.random\n    else:\n        raise ValueError(\n            (\n                \"random_state must be an integer, array-like, a BitGenerator, \"\n                \"a numpy RandomState, or None\"\n            )\n        )```\n\nUpdated code:\n```python\ndef random_state(state=None):\n    \n    \n\n    \n    if (\n        is_integer(state)\n        or is_array_like(state)\n        or (not _np_version_under1p18 and isinstance(state, np.random.BitGenerator))\n    ):\n        return np.random.RandomState(state)\n    elif isinstance(state, np.random.RandomState):\n        return state\n    elif state is None:\n        return np.random\n    else:\n        raise ValueError(\n            (\n                \"random_state must be an integer, array-like, a BitGenerator, \"\n                \"a numpy RandomState, or None\"\n            )\n        )\n```\n\nOriginal documentation:\n```python\nHelper function for processing random_state arguments.\n\n    Parameters\n    ----------\n    state : int, array-like, BitGenerator (NumPy>=1.17), np.random.RandomState, None.\n        If receives an int, array-like, or BitGenerator, passes to\n        np.random.RandomState() as seed.\n        If receives an np.random.RandomState object, just returns object.\n        If receives `None`, returns np.random.\n        If receives anything else, raises an informative ValueError.\n\n        ..versionchanged:: 1.1.0\n\n            array-like and BitGenerator (for NumPy>=1.17) object now passed to\n            np.random.RandomState() as seed\n\n        Default None.\n\n    Returns\n    -------\n    np.random.RandomState\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Helper function for processing random_state arguments.\n\n    Parameters\n    ----------\n    state : int, array-like, BitGenerator (NumPy>=1.17), np.random.RandomState, None.\n        If receives an int, array-like, or BitGenerator, passes to\n        np.random.RandomState() as seed.\n        If receives an np.random.RandomState object, just returns object.\n        If receives `None`, returns np.random.\n        If receives anything else, raises an informative ValueError.\n\n        ..versionchanged:: 1.1.0\n\n            array-like and BitGenerator (for NumPy>=1.18) object now passed to\n            np.random.RandomState() as seed\n\n        Default None.\n\n    Returns\n    -------\n    np.random.RandomState", "file": "pandas_core_common.py", "function": "random_state", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef brick():\n    \n\n    \n    return load(\"brick.png\")```\n\nUpdated code:\n```python\ndef brick():\n    \n\n    \n    return load(\"brick.png\", as_gray=True)\n```\n\nOriginal documentation:\n```python\nBrick wall.\n\n    The original image was downloaded from\n    `CC0Textures <https://cc0textures.com/view.php?tex=Bricks25>` and licensed\n    under the Creative Commons CC0 License.\n\n    The downloaded image was then imported into GIMP, rotated by 90 degrees,\n    and a perspective transform was also applied. Finally, the color image was\n    converted to grayscale in GIMP and cropped to include a region the size of\n    ``(1024, 1024)`` pixels prior to saving the result in PNG format.\n\n    Returns\n    -------\n    brick: (1024, 1024) uint8 image\n        A small section of a brick wall.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Brick wall.\n\n    The original image was downloaded from\n    `CC0Textures <https://cc0textures.com/view.php?tex=Bricks25>` and licensed\n    under the Creative Commons CC0 License.\n\n    The downloaded image was then imported into GIMP, rotated by 90 degrees,\n    and a perspective transform was also applied. Finally, the color image was\n    converted to grayscale in GIMP and cropped to include a region the size of\n    ``(1024, 1024)`` and rescaled to ``(512, 512)`` pixels prior to saving the\n    result in PNG format.\n\n    Returns\n    -------\n    brick: (512, 512) uint8 image\n        A small section of a brick wall.", "file": "skimage_data___init__.py", "function": "brick", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _output_processing(self, extract_media, size):\n         \n        \n        for face in extract_media.detected_faces:\n            face.load_aligned(extract_media.image, size=size)\n\n        self._post_process.do_actions(extract_media)\n        extract_media.remove_image()\n\n        faces_count = len(extract_media.detected_faces)\n        if faces_count == 0:\n            logger.verbose(\"No faces were detected in image: %s\",\n                           os.path.basename(extract_media.filename))\n\n        if not self._verify_output and faces_count > 1:\n            self._verify_output = True```\n\nUpdated code:\n```python\ndef _output_processing(self, extract_media, size):\n         \n        \n        for face in extract_media.detected_faces:\n            face.load_aligned(extract_media.image, size=size)\n            face.thumbnail = generate_thumbnail(face.aligned_face, size=80, quality=60)\n        self._post_process.do_actions(extract_media)\n        extract_media.remove_image()\n\n        faces_count = len(extract_media.detected_faces)\n        if faces_count == 0:\n            logger.verbose(\"No faces were detected in image: %s\",\n                           os.path.basename(extract_media.filename))\n\n        if not self._verify_output and faces_count > 1:\n            self._verify_output = True\n```\n\nOriginal documentation:\n```python\nPrepare faces for output\n\n        Loads the aligned face, perform any processing actions and verify the output.\n\n        Parameters\n        ----------\n        extract_media: :class:`plugins.extract.pipeline.ExtractMedia`\n            Output from :class:`plugins.extract.pipeline.Extractor`\n        size: int\n            The size that the aligned face should be created at\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Prepare faces for output\n\n        Loads the aligned face, generate the thumbnail, perform any processing actions and verify\n        the output.\n\n        Parameters\n        ----------\n        extract_media: :class:`plugins.extract.pipeline.ExtractMedia`\n            Output from :class:`plugins.extract.pipeline.Extractor`\n        size: int\n            The size that the aligned face should be created at", "file": "scripts_extract.py", "function": "Extract._output_processing", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef xticks(ticks=None, labels=None, *, minor=False, **kwargs):\n    \n    \n    \n    ax = gca()\n\n    if ticks is None:\n        locs = ax.get_xticks(minor=minor)\n        if labels is not None:\n            raise TypeError(\"xticks(): Parameter 'labels' can't be set \"\n                            \"without setting 'ticks'\")\n    else:\n        locs = ax.set_xticks(ticks, minor=minor)\n\n    if labels is None:\n        labels = ax.get_xticklabels(minor=minor)\n        for l in labels:\n            l._internal_update(kwargs)\n    else:\n        labels = ax.set_xticklabels(labels, minor=minor, **kwargs)\n\n    return locs, labels```\n\nUpdated code:\n```python\ndef xticks(ticks=None, labels=None, **kwargs):\n    \n    \n    \n    ax = gca()\n\n    if ticks is None:\n        locs = ax.get_xticks()\n        if labels is not None:\n            raise TypeError(\"xticks(): Parameter 'labels' can't be set \"\n                            \"without setting 'ticks'\")\n    else:\n        locs = ax.set_xticks(ticks)\n\n    if labels is None:\n        labels = ax.get_xticklabels()\n        for l in labels:\n            l._internal_update(kwargs)\n    else:\n        labels = ax.set_xticklabels(labels, **kwargs)\n\n    return locs, labels\n```\n\nOriginal documentation:\n```python\nGet or set the current tick locations and labels of the x-axis.\n\n    Pass no arguments to return the current values without modifying them.\n\n    Parameters\n    ----------\n    ticks : array-like, optional\n        The list of xtick locations.  Passing an empty list removes all xticks.\n    labels : array-like, optional\n        The labels to place at the given *ticks* locations.  This argument can\n        only be passed if *ticks* is passed as well.\n    minor : bool, default: False\n        If ``False``, get/set the major ticks/labels; if ``True``, the minor\n        ticks/labels.\n    **kwargs\n        `.Text` properties can be used to control the appearance of the labels.\n\n    Returns\n    -------\n    locs\n        The list of xtick locations.\n    labels\n        The list of xlabel `.Text` objects.\n\n    Notes\n    -----\n    Calling this function with no arguments (e.g. ``xticks()``) is the pyplot\n    equivalent of calling `~.Axes.get_xticks` and `~.Axes.get_xticklabels` on\n    the current axes.\n    Calling this function with arguments is the pyplot equivalent of calling\n    `~.Axes.set_xticks` and `~.Axes.set_xticklabels` on the current axes.\n\n    Examples\n    --------\n    >>> locs, labels = xticks()  # Get the current locations and labels.\n    >>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n    >>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n    >>> xticks([0, 1, 2], ['January', 'February', 'March'],\n    ...        rotation=20)  # Set text labels and properties.\n    >>> xticks([])  # Disable xticks.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get or set the current tick locations and labels of the x-axis.\n\n    Pass no arguments to return the current values without modifying them.\n\n    Parameters\n    ----------\n    ticks : array-like, optional\n        The list of xtick locations.  Passing an empty list removes all xticks.\n    labels : array-like, optional\n        The labels to place at the given *ticks* locations.  This argument can\n        only be passed if *ticks* is passed as well.\n    **kwargs\n        `.Text` properties can be used to control the appearance of the labels.\n\n    Returns\n    -------\n    locs\n        The list of xtick locations.\n    labels\n        The list of xlabel `.Text` objects.\n\n    Notes\n    -----\n    Calling this function with no arguments (e.g. ``xticks()``) is the pyplot\n    equivalent of calling `~.Axes.get_xticks` and `~.Axes.get_xticklabels` on\n    the current axes.\n    Calling this function with arguments is the pyplot equivalent of calling\n    `~.Axes.set_xticks` and `~.Axes.set_xticklabels` on the current axes.\n\n    Examples\n    --------\n    >>> locs, labels = xticks()  # Get the current locations and labels.\n    >>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n    >>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n    >>> xticks([0, 1, 2], ['January', 'February', 'March'],\n    ...        rotation=20)  # Set text labels and properties.\n    >>> xticks([])  # Disable xticks.", "file": "lib_matplotlib_pyplot.py", "function": "xticks", "project": "matplotlib", "change_stats": {"code_changes": 12, "doc_changes": 5, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef connect(self, _retry: int = 0) -> None:\n\t\t\n\t\ttry:\n\t\t\tif isinstance(self._conn, POP3):\n\t\t\t\tres = self._connect_pop()\n\n\t\t\t\tif not res.startswith(b\"+OK\"):\n\t\t\t\t\traise\n\n\t\t\telif isinstance(self._conn, IMAP4):\n\t\t\t\tself._connect_imap()\n\n\t\t\telse:\n\t\t\t\t# SMTP\n\t\t\t\tself._connect_smtp()\n\n\t\texcept Exception as e:\n\t\t\t# maybe the access token expired - refreshing\n\t\t\taccess_token = self._refresh_access_token()\n\n\t\t\tif not access_token or _retry > 0:\n\t\t\t\tfrappe.log_error(\n\t\t\t\t\t\"OAuth Error - Authentication Failed\", str(e), \"Email Account\", self.email_account\n\t\t\t\t)\n\t\t\t\t# raising a bare exception here as we have a lot of exception handling present\n\t\t\t\t# where the connect method is called from - hence just logging and raising.\n\t\t\t\traise\n\n\t\t\tself._access_token = access_token\n\t\t\tself.connect(_retry + 1)```\n\nUpdated code:\n```python\ndef connect(self, _retry: int = 0) -> None:\n\t\t\n\t\ttry:\n\t\t\tif isinstance(self._conn, POP3):\n\t\t\t\tres = self._connect_pop()\n\n\t\t\t\tif not res.startswith(b\"+OK\"):\n\t\t\t\t\traise\n\n\t\t\telif isinstance(self._conn, IMAP4):\n\t\t\t\tself._connect_imap()\n\n\t\t\telse:\n\t\t\t\t# SMTP\n\t\t\t\tself._connect_smtp()\n\n\t\texcept Exception as e:\n\t\t\tif _retry > 0:\n\t\t\t\tfrappe.log_error(\n\t\t\t\t\t\"SMTP Connection Error - Authentication Failed\", str(e), \"Email Account\", self.email_account\n\t\t\t\t)\n\t\t\t\t# raising a bare exception here as we have a lot of exception handling present\n\t\t\t\t# where the connect method is called from - hence just logging and raising.\n\t\t\t\traise\n\n\t\t\tself.connect(_retry + 1)\n```\n\nOriginal documentation:\n```python\nConnection method with retry on exception for Oauth\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Connection method with retry on exception for connection errors", "file": "frappe_email_oauth.py", "function": "Oauth.connect", "project": "frappe", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, nanoleaf: Nanoleaf) -> None:\n        \n        self._nanoleaf = nanoleaf\n        self._attr_unique_id = self._nanoleaf.serial_no\n        self._attr_name = self._nanoleaf.name\n        self._attr_device_info = DeviceInfo(\n            identifiers={(DOMAIN, self._nanoleaf.serial_no)},\n            manufacturer=self._nanoleaf.manufacturer,\n            model=self._nanoleaf.model,\n            name=self._nanoleaf.name,\n            sw_version=self._nanoleaf.firmware_version,\n        )\n        self._attr_min_mireds = math.ceil(\n            1000000 / self._nanoleaf.color_temperature_max\n        )\n        self._attr_max_mireds = kelvin_to_mired(self._nanoleaf.color_temperature_min)```\n\nUpdated code:\n```python\ndef __init__(self, nanoleaf: Nanoleaf) -> None:\n        \n        super().__init__(nanoleaf)\n        self._attr_unique_id = nanoleaf.serial_no\n        self._attr_name = nanoleaf.name\n        self._attr_min_mireds = math.ceil(1000000 / nanoleaf.color_temperature_max)\n        self._attr_max_mireds = kelvin_to_mired(nanoleaf.color_temperature_min)\n```\n\nOriginal documentation:\n```python\nInitialize an Nanoleaf light.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the Nanoleaf light.", "file": "homeassistant_components_nanoleaf_light.py", "function": "NanoleafLight.__init__", "project": "core", "change_stats": {"code_changes": 21, "doc_changes": 4, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef serialise_tracker(self, tracker: \"DialogueStateTracker\") -> Dict:\n        \n        d = tracker.as_dialogue().as_dict()\n        d.update({\"sender_id\": tracker.sender_id})\n        # DynamoDB cannot store `float`s, so we'll convert them to `Decimal`s\n        return core_utils.replace_floats_with_decimals(d)```\n\nUpdated code:\n```python\ndef serialise_tracker(\n        tracker: \"DialogueStateTracker\",\n    ) -> Dict:\n        \n        \n        return core_utils.replace_floats_with_decimals(\n            SerializedTrackerAsDict.serialise_tracker(tracker)\n        )\n```\n\nOriginal documentation:\n```python\nSerializes the tracker, returns object with decimal types.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Serializes the tracker, returns object with decimal types.\n\n        DynamoDB cannot store `float`s, so we'll convert them to `Decimal`s.", "file": "rasa_core_tracker_store.py", "function": "DynamoTrackerStore.serialise_tracker", "project": "rasa", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef elementwise_binary(alpha, A, desc_A, mode_A,\n                       gamma, C, desc_C, mode_C,\n                       out=None,\n                       op_AC=cutensor.OP_ADD,\n                       compute_dtype=None):\n    \n    \n    assert A.dtype == C.dtype\n    assert A.ndim == len(mode_A)\n    assert C.ndim == len(mode_C)\n    mode_A = numpy.array([ord(x) if isinstance(x, str) else x for x in mode_A],\n                         dtype=numpy.int32)\n    mode_C = numpy.array([ord(x) if isinstance(x, str) else x for x in mode_C],\n                         dtype=numpy.int32)\n    if out is None:\n        out = cupy.ndarray(C.shape, dtype=C.dtype)\n    else:\n        assert C.dtype == out.dtype\n        assert C.ndim == out.ndim\n        for i in range(C.ndim):\n            assert C.shape[i] == out.shape[i]\n    if compute_dtype is None:\n        compute_dtype = A.dtype\n    alpha = numpy.array(alpha, compute_dtype)\n    gamma = numpy.array(gamma, compute_dtype)\n    handle = get_handle()\n    compute_dtype = get_cuda_dtype(compute_dtype)\n    cutensor.elementwiseBinary(\n        handle,\n        alpha.ctypes.data,\n        A.data.ptr, desc_A.value, mode_A.ctypes.data,\n        gamma.ctypes.data,\n        C.data.ptr, desc_C.value, mode_C.ctypes.data,\n        out.data.ptr, desc_C.value, mode_C.ctypes.data,\n        op_AC, compute_dtype)\n    return out```\n\nUpdated code:\n```python\ndef elementwise_binary(alpha, A, desc_A, mode_A,\n                       gamma, C, desc_C, mode_C,\n                       out=None,\n                       op_AC=cutensor.OP_ADD,\n                       compute_dtype=None):\n    \n    \n    assert A.dtype == C.dtype\n    assert A.ndim == len(mode_A)\n    assert C.ndim == len(mode_C)\n    mode_A = _convert_mode(mode_A)\n    mode_C = _convert_mode(mode_C)\n    if out is None:\n        out = cupy.ndarray(C.shape, dtype=C.dtype)\n    else:\n        assert C.dtype == out.dtype\n        assert C.ndim == out.ndim\n        for i in range(C.ndim):\n            assert C.shape[i] == out.shape[i]\n    if compute_dtype is None:\n        compute_dtype = A.dtype\n    alpha = numpy.array(alpha, compute_dtype)\n    gamma = numpy.array(gamma, compute_dtype)\n    handle = get_handle()\n    compute_dtype = get_cuda_dtype(compute_dtype)\n    cutensor.elementwiseBinary(\n        handle,\n        alpha.ctypes.data,\n        A.data.ptr, desc_A.value, mode_A.ctypes.data,\n        gamma.ctypes.data,\n        C.data.ptr, desc_C.value, mode_C.ctypes.data,\n        out.data.ptr, desc_C.value, mode_C.ctypes.data,\n        op_AC, compute_dtype)\n    return out\n```\n\nOriginal documentation:\n```python\nElement-wise tensor operation for two input tensors\n\n    This function performs a element-wise tensor operation of the form:\n\n        D_{Pi^C(i_0,i_1,...,i_n)} =\n            op_AC(alpha * uop_A(A_{Pi^A(i_0,i_1,...,i_n)}),\n                  gamma * uop_C(C_{Pi^C(i_0,i_1,...,i_n)}))\n\n    See elementwise_trinary() for details.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Element-wise tensor operation for two input tensors\n\n    This function performs a element-wise tensor operation of the form:\n\n        D_{Pi^C(i_0,i_1,...,i_n)} =\n            op_AC(alpha * uop_A(A_{Pi^A(i_0,i_1,...,i_n)}),\n                  gamma * uop_C(C_{Pi^C(i_0,i_1,...,i_n)}))\n\n    See elementwise_trinary() for details.\n\n    Examples:\n        See examples/cutensor/elementwise_binary.py", "file": "cupy_cutensor.py", "function": "elementwise_binary", "project": "cupy", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_to_json_log_file(\n             self,\n             IDEA_dict: dict,\n             all_uids: list,\n             timewindow: str,\n             accumulated_threat_level: float =0\n        ):\n        \n        \n        \n        try:\n            # we add extra fields to alerts.json that are not in the IDEA format\n            IDEA_dict.update({\n                'uids': all_uids,\n                'accumulated_threat_level': accumulated_threat_level,\n                'timewindow': int(timewindow.replace('timewindow', '')),\n            })\n            json.dump(IDEA_dict, self.jsonfile)\n            self.jsonfile.write('\\n')\n        except KeyboardInterrupt:\n            return True\n        except Exception:\n            self.print('Error in addDataToJSONFile()')\n            self.print(traceback.print_exc(), 0, 1)```\n\nUpdated code:\n```python\ndef add_to_json_log_file(\n             self,\n             idea_dict: dict,\n             all_uids: list,\n             timewindow: str,\n             accumulated_threat_level: float =0\n        ):\n        \n        \n        \n        try:\n            # we add extra fields to alerts.json that are not in the IDEA format\n            idea_dict.update({\n                'uids': all_uids,\n                'accumulated_threat_level': accumulated_threat_level,\n                'timewindow': int(timewindow.replace('timewindow', '')),\n            })\n            json.dump(idea_dict, self.jsonfile)\n            self.jsonfile.write('\\n')\n        except KeyboardInterrupt:\n            return True\n        except Exception:\n            self.print('Error in add_to_json_log_file()')\n            self.print(traceback.print_exc(), 0, 1)\n```\n\nOriginal documentation:\n```python\nAdd a new evidence line to our alerts.json file in json format.\n        :param IDEA_dict: dict containing 1 alert\n        :param all_uids: the uids of the flows causing this evidence\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add a new evidence line to our alerts.json file in json format.\n        :param idea_dict: dict containing 1 alert\n        :param all_uids: the uids of the flows causing this evidence", "file": "slips_files_core_evidencehandler.py", "function": "EvidenceHandler.add_to_json_log_file", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_model_from_path(\n    model_path: Union[str, Path],\n    *,\n    meta: Optional[Dict[str, Any]] = None,\n    vocab: Union[\"Vocab\", bool] = True,\n    disable: Iterable[str] = SimpleFrozenList(),\n    exclude: Iterable[str] = SimpleFrozenList(),\n    config: Union[Dict[str, Any], Config] = SimpleFrozenDict(),\n) -> \"Language\":\n    \n    \n    if not model_path.exists():\n        raise IOError(Errors.E052.format(path=model_path))\n    if not meta:\n        meta = get_model_meta(model_path)\n    config_path = model_path / \"config.cfg\"\n    overrides = dict_to_dot(config)\n    config = load_config(config_path, overrides=overrides)\n    nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)\n    return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)```\n\nUpdated code:\n```python\ndef load_model_from_path(\n    model_path: Path,\n    *,\n    meta: Optional[Dict[str, Any]] = None,\n    vocab: Union[\"Vocab\", bool] = True,\n    disable: Iterable[str] = SimpleFrozenList(),\n    exclude: Iterable[str] = SimpleFrozenList(),\n    config: Union[Dict[str, Any], Config] = SimpleFrozenDict(),\n) -> \"Language\":\n    \n    \n    if not model_path.exists():\n        raise IOError(Errors.E052.format(path=model_path))\n    if not meta:\n        meta = get_model_meta(model_path)\n    config_path = model_path / \"config.cfg\"\n    overrides = dict_to_dot(config)\n    config = load_config(config_path, overrides=overrides)\n    nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)\n    return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n```\n\nOriginal documentation:\n```python\nLoad a model from a data directory path. Creates Language class with\n    pipeline from config.cfg and then calls from_disk() with path.\n\n    name (str): Package name or model path.\n    meta (Dict[str, Any]): Optional model meta.\n    vocab (Vocab / True): Optional vocab to pass in on initialization. If True,\n        a new Vocab object will be created.\n    disable (Iterable[str]): Names of pipeline components to disable. Disabled\n        pipes will be loaded but they won't be run unless you explicitly\n        enable them by calling nlp.enable_pipe.\n    exclude (Iterable[str]): Names of pipeline components to exclude. Excluded\n        components won't be loaded.\n    config (Dict[str, Any] / Config): Config overrides as nested dict or dict\n        keyed by section values in dot notation.\n    RETURNS (Language): The loaded nlp object.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a model from a data directory path. Creates Language class with\n    pipeline from config.cfg and then calls from_disk() with path.\n\n    model_path (Path): Mmodel path.\n    meta (Dict[str, Any]): Optional model meta.\n    vocab (Vocab / True): Optional vocab to pass in on initialization. If True,\n        a new Vocab object will be created.\n    disable (Iterable[str]): Names of pipeline components to disable. Disabled\n        pipes will be loaded but they won't be run unless you explicitly\n        enable them by calling nlp.enable_pipe.\n    exclude (Iterable[str]): Names of pipeline components to exclude. Excluded\n        components won't be loaded.\n    config (Dict[str, Any] / Config): Config overrides as nested dict or dict\n        keyed by section values in dot notation.\n    RETURNS (Language): The loaded nlp object.", "file": "spacy_util.py", "function": "load_model_from_path", "project": "spaCy", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef intersection(self, region: Region) -> Region:\n        \n        \n        x1, y1, x2, y2 = self.corners\n        cx1, cy1, cx2, cy2 = region.corners\n\n        _clamp = clamp\n        new_region = Region.from_corners(\n            _clamp(x1, cx1, cx2),\n            _clamp(y1, cy1, cy2),\n            _clamp(x2, cx1, cx2),\n            _clamp(y2, cy1, cy2),\n        )\n        return new_region```\n\nUpdated code:\n```python\ndef intersection(self, region: Region) -> Region:\n        \n        \n        # Unrolled because this method is used a lot\n        x1, y1, w1, h1 = self\n        cx1, cy1, w2, h2 = region\n        x2 = x1 + w1\n        y2 = y1 + h1\n        cx2 = cx1 + w2\n        cy2 = cy1 + h2\n\n        rx1 = cx2 if x1 > cx2 else (cx1 if x1 < cx1 else x1)\n        ry1 = cy2 if y1 > cy2 else (cy1 if y1 < cy1 else y1)\n        rx2 = cx2 if x2 > cx2 else (cx1 if x2 < cx1 else x2)\n        ry2 = cy2 if y2 > cy2 else (cy1 if y2 < cy1 else y2)\n\n        return Region(rx1, ry1, rx2 - rx1, ry2 - ry1)\n```\n\nOriginal documentation:\n```python\nGet that covers both regions.\n\n        Args:\n            region ([type]): A region that overlaps this region.\n\n        Returns:\n            Region: A new region that fits within ``region``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get that covers both regions.\n\n        Args:\n            region (Region): A region that overlaps this region.\n\n        Returns:\n            Region: A new region that fits within ``region``.", "file": "src_textual_geometry.py", "function": "Region.intersection", "project": "textual", "change_stats": {"code_changes": 25, "doc_changes": 4, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef notify(self, n=1):\n        \n        \n        if not self.locked():\n            raise RuntimeError('cannot notify on un-acquired lock')\n\n        idx = 0\n        for fut in self._waiters:\n            if idx >= n:\n                break\n\n            if not fut.done():\n                idx += 1\n                fut.set_result(False)```\n\nUpdated code:\n```python\ndef notify(self, n=1):\n        \n        \n        if not self.locked():\n            raise RuntimeError('cannot notify on un-acquired lock')\n        self._notify(n)\n```\n\nOriginal documentation:\n```python\nBy default, wake up one coroutine waiting on this condition, if any.\n        If the calling coroutine has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        This method wakes up at most n of the coroutines waiting for the\n        condition variable; it is a no-op if no coroutines are waiting.\n\n        Note: an awakened coroutine does not actually return from its\n        wait() call until it can reacquire the lock. Since notify() does\n        not release the lock, its caller should.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "By default, wake up one task waiting on this condition, if any.\n        If the calling task has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        This method wakes up n of the tasks waiting for the condition\n         variable; if fewer than n are waiting, they are all awoken.\n\n        Note: an awakened task does not actually return from its\n        wait() call until it can reacquire the lock. Since notify() does\n        not release the lock, its caller should.", "file": "Lib_asyncio_locks.py", "function": "Condition.notify", "project": "cpython", "change_stats": {"code_changes": 12, "doc_changes": 12, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef from_model_id_low_bit(\n        cls,\n        model_id: str,\n        model_kwargs: Optional[dict] = None,\n        **kwargs: Any,\n    ) -> LLM:\n        \n        \n        \n\n        logger.warning(\"BigdlLLM was deprecated. Please use IpexLLM instead.\")\n\n        try:\n            from bigdl.llm.transformers import (\n                AutoModel,\n                AutoModelForCausalLM,\n            )\n            from transformers import AutoTokenizer, LlamaTokenizer\n\n        except ImportError:\n            raise ValueError(\n                \"Could not import bigdl-llm or transformers. \"\n                \"Please install it with `pip install --pre --upgrade bigdl-llm[all]`.\"\n            )\n\n        _model_kwargs = model_kwargs or {}\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)\n        except Exception:\n            tokenizer = LlamaTokenizer.from_pretrained(model_id, **_model_kwargs)\n\n        try:\n            model = AutoModelForCausalLM.load_low_bit(model_id, **_model_kwargs)\n        except Exception:\n            model = AutoModel.load_low_bit(model_id, **_model_kwargs)\n\n        if \"trust_remote_code\" in _model_kwargs:\n            _model_kwargs = {\n                k: v for k, v in _model_kwargs.items() if k != \"trust_remote_code\"\n            }\n\n        return cls(\n            model_id=model_id,\n            model=model,\n            tokenizer=tokenizer,\n            model_kwargs=_model_kwargs,\n            **kwargs,\n        )```\n\nUpdated code:\n```python\ndef from_model_id_low_bit(\n        cls,\n        model_id: str,\n        model_kwargs: Optional[dict] = None,\n        *,\n        tokenizer_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> LLM:\n        \n        \n        \n\n        logger.warning(\"BigdlLLM was deprecated. Please use IpexLLM instead.\")\n\n        try:\n            from bigdl.llm.transformers import (\n                AutoModel,\n                AutoModelForCausalLM,\n            )\n            from transformers import AutoTokenizer, LlamaTokenizer\n\n        except ImportError:\n            raise ValueError(\n                \"Could not import bigdl-llm or transformers. \"\n                \"Please install it with `pip install --pre --upgrade bigdl-llm[all]`.\"\n            )\n\n        _model_kwargs = model_kwargs or {}\n        _tokenizer_id = tokenizer_id or model_id\n\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(_tokenizer_id, **_model_kwargs)\n        except Exception:\n            tokenizer = LlamaTokenizer.from_pretrained(_tokenizer_id, **_model_kwargs)\n\n        try:\n            model = AutoModelForCausalLM.load_low_bit(model_id, **_model_kwargs)\n        except Exception:\n            model = AutoModel.load_low_bit(model_id, **_model_kwargs)\n\n        if \"trust_remote_code\" in _model_kwargs:\n            _model_kwargs = {\n                k: v for k, v in _model_kwargs.items() if k != \"trust_remote_code\"\n            }\n\n        return cls(\n            model_id=model_id,\n            model=model,\n            tokenizer=tokenizer,\n            model_kwargs=_model_kwargs,\n            **kwargs,\n        )\n```\n\nOriginal documentation:\n```python\nConstruct low_bit object from model_id\n\n        Args:\n\n            model_id: Path for the bigdl-llm transformers low-bit model folder.\n            model_kwargs: Keyword arguments to pass to the model and tokenizer.\n            kwargs: Extra arguments to pass to the model and tokenizer.\n\n        Returns:\n            An object of BigdlLLM.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Construct low_bit object from model_id\n\n        Args:\n\n            model_id: Path for the bigdl-llm transformers low-bit model folder.\n            tokenizer_id: Path for the huggingface repo id or local model folder\n                      which contains the tokenizer.\n            model_kwargs: Keyword arguments to pass to the model and tokenizer.\n            kwargs: Extra arguments to pass to the model and tokenizer.\n\n        Returns:\n            An object of BigdlLLM.", "file": "libs_community_langchain_community_llms_bigdl_llm.py", "function": "BigdlLLM.from_model_id_low_bit", "project": "langchain", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef dialog_from_sentence(sentence, skill_path):\n    \n    dialog_paths = join(skill_path, 'dialog', 'en-us', '*.dialog')\n    best = (None, 0)\n    for path in glob(dialog_paths):\n        patterns = load_dialog_file(path)\n        match, _ = _match_dialog_patterns(patterns, sentence.lower())\n        if match is not False:\n            if len(patterns[match]) > best[1]:\n                best = (path, len(patterns[match]))\n    if best[0] is not None:\n        return basename(best[0])\n    else:\n        return None```\n\nUpdated code:\n```python\ndef dialog_from_sentence(sentence, skill_path, lang):\n    \n    \n    dialog_paths = join(skill_path, 'dialog', lang, '*.dialog')\n    best = (None, 0)\n    for path in glob(dialog_paths):\n        patterns = load_dialog_file(path)\n        match, _ = _match_dialog_patterns(patterns, sentence.lower())\n        if match is not False:\n            if len(patterns[match]) > best[1]:\n                best = (path, len(patterns[match]))\n    if best[0] is not None:\n        return basename(best[0])\n    else:\n        return None\n```\n\nOriginal documentation:\n```python\nFind dialog file from example sentence.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Find dialog file from example sentence.\n\n    Arguments:\n        sentence (str): Text to match\n        skill_path (str): path to skill directory\n        lang (str): language code to use\n\n    Returns (str): Dialog file best matching the sentence.", "file": "test_integrationtests_voight_kampff_features_steps_utterance_responses.py", "function": "dialog_from_sentence", "project": "mycroft-core", "change_stats": {"code_changes": 7, "doc_changes": 9, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ellipsoid_stats(a, b, c, sampling=(1., 1., 1.)):\n    \n    \n\n    \n    if (a <= 0) or (b <= 0) or (c <= 0):\n        raise ValueError('Parameters a, b, and c must all be > 0')\n\n    # Calculate volume & surface area\n    # Surface calculation requires a >= b >= c and a != c.\n    abc = [a, b, c]\n    abc.sort(reverse=True)\n    a = abc[0]\n    b = abc[1]\n    c = abc[2]\n\n    # Volume\n    vol = 4 / 3. * np.pi * a * b * c\n\n    # Analytical ellipsoid surface area\n    phi = np.arcsin((1. - (c ** 2 / (a ** 2.))) ** 0.5)\n    d = float((a ** 2 - c ** 2) ** 0.5)\n    m = (a ** 2 * (b ** 2 - c ** 2) /\n         float(b ** 2 * (a ** 2 - c ** 2)))\n    F = ellip_F(phi, m)\n    E = ellip_E(phi, m)\n\n    surf = 2 * np.pi * (c ** 2 +\n                        b * c ** 2 / d * F +\n                        b * d * E)\n\n    return vol, surf```\n\nUpdated code:\n```python\ndef ellipsoid_stats(a, b, c):\n    \n    \n\n    \n    if (a <= 0) or (b <= 0) or (c <= 0):\n        raise ValueError('Parameters a, b, and c must all be > 0')\n\n    # Calculate volume & surface area\n    # Surface calculation requires a >= b >= c and a != c.\n    abc = [a, b, c]\n    abc.sort(reverse=True)\n    a = abc[0]\n    b = abc[1]\n    c = abc[2]\n\n    # Volume\n    vol = 4 / 3. * np.pi * a * b * c\n\n    # Analytical ellipsoid surface area\n    phi = np.arcsin((1. - (c ** 2 / (a ** 2.))) ** 0.5)\n    d = float((a ** 2 - c ** 2) ** 0.5)\n    m = (a ** 2 * (b ** 2 - c ** 2) /\n         float(b ** 2 * (a ** 2 - c ** 2)))\n    F = ellip_F(phi, m)\n    E = ellip_E(phi, m)\n\n    surf = 2 * np.pi * (c ** 2 +\n                        b * c ** 2 / d * F +\n                        b * d * E)\n\n    return vol, surf\n```\n\nOriginal documentation:\n```python\nCalculates analytical surface area and volume for ellipsoid with\n    semimajor axes aligned with grid dimensions of specified `sampling`.\n\n    Parameters\n    ----------\n    a : float\n        Length of semimajor axis aligned with x-axis.\n    b : float\n        Length of semimajor axis aligned with y-axis.\n    c : float\n        Length of semimajor axis aligned with z-axis.\n    sampling : tuple of floats, length 3\n        Sampling in (x, y, z) spatial dimensions.\n\n    Returns\n    -------\n    vol : float\n        Calculated volume of ellipsoid.\n    surf : float\n        Calculated surface area of ellipsoid.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculates analytical surface area and volume for ellipsoid with\n    semimajor axes aligned with grid dimensions of specified `spacing`.\n\n    Parameters\n    ----------\n    a : float\n        Length of semimajor axis aligned with x-axis.\n    b : float\n        Length of semimajor axis aligned with y-axis.\n    c : float\n        Length of semimajor axis aligned with z-axis.\n\n    Returns\n    -------\n    vol : float\n        Calculated volume of ellipsoid.\n    surf : float\n        Calculated surface area of ellipsoid.", "file": "skimage_draw_draw3d.py", "function": "ellipsoid_stats", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_units(self):\n        ''\n        return self.units```\n\nUpdated code:\n```python\ndef get_units(self):\n        \n        return self.units\n```\n\nOriginal documentation:\n```python\nreturn the units for axis\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the units for axis.", "file": "lib_matplotlib_axis.py", "function": "Axis.get_units", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef notify(self, mappable):\n        \n        \n        \n        self.set_cmap(mappable.cmap)\n        self.set_norm(mappable.norm)```\n\nUpdated code:\n```python\ndef notify(self, mappable):\n        \n        \n        \n        self.set_cmap(mappable.cmap)\n        self.set_norm(mappable.norm)\n        try:\n            self.set_alpha(mappable.get_alpha())\n        except AttributeError:\n            pass\n```\n\nOriginal documentation:\n```python\nIf this is called then we are pegged to another mappable.\n        Update the cmap, norm accordingly\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "If this is called then we are pegged to another mappable.\n        Update our cmap, norm, alpha from the other mappable.", "file": "lib_matplotlib_cm.py", "function": "ScalarMappable.notify", "project": "matplotlib", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, query, key, value, mask):\n        \n        \n\n        # linear -> GLU -- -> lightconv -> linear \n        #               \\        /\n        #                 Lienar\n        x = query\n        B, T, C = x.size()\n        H = self.wshare\n        k = self.kernel_size\n        \n        # first liner layer\n        x = self.linear1(x)\n\n        # GLU activation\n        x = self.act(x)\n    \n        # get kernel of convolution\n        weight = self.linear_weight(x) # B x T x kH\n        weight = F.dropout(weight,self.dropout_rate,training=self.training)\n        weight = weight.view(B,T,H,k).transpose(1,2).contiguous() # B x H x T x k\n        weight_new = torch.zeros(B*H*T*(T+k-1)).view(B,H,T,T+k-1).fill_(float('-inf')) # B x H x T x T+k-1\n        weight_new = weight_new.to(x.device)\n        weight_new.as_strided((B,H,T,k),((T+k-1)*T*H, (T+k-1)*T, T+k, 1)).copy_(weight)\n        weight_new = weight_new.narrow(-1,int((k-1)/2),T) # B x H x T x T(k)\n        if self.use_kernel_mask:\n            kernel_mask = torch.tril(torch.ones(T,T,device=x.device)).unsqueeze(0)\n            weight_new = weight_new.masked_fill(kernel_mask==0.0, float('-inf'))\n        weight_new = F.softmax(weight_new,dim=-1)\n        self.attn = weight_new\n        weight_new = weight_new.view(B*H,T,T)\n        \n        # convolution\n        x = x.transpose(1,2).contiguous() # B x C x T\n        x = x.view(B*H,int(C/H),T).transpose(1,2)\n        x = torch.bmm(weight_new,x) # BH x T x C/H\n        x = x.transpose(1,2).contiguous().view(B, C, T)\n\n        if self.use_bias:\n            x = x + self.bias.view(1,-1,1)\n        x = x.transpose(1,2) # B x T x C\n\n        if mask is not None and not self.use_kernel_mask:\n            mask = mask.transpose(-1,-2)\n            x = x.masked_fill(mask == 0, 0.0)\n\n        # second linear layer\n        x = self.linear2(x)\n        return x```\n\nUpdated code:\n```python\ndef forward(self, query, key, value, mask):\n        \n\n        \n        # linear -> GLU -- -> lightconv -> linear\n        #               \\        /\n        #                 Lienar\n        x = query\n        B, T, C = x.size()\n        H = self.wshare\n        k = self.kernel_size\n\n        # first liner layer\n        x = self.linear1(x)\n\n        # GLU activation\n        x = self.act(x)\n\n        # get kernel of convolution\n        weight = self.linear_weight(x)  # B x T x kH\n        weight = F.dropout(weight, self.dropout_rate, training=self.training)\n        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k\n        weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))\n        weight_new = weight_new.to(x.device)  # B x H x T x T+k-1\n        weight_new.as_strided((B, H, T, k), ((T + k - 1) * T * H, (T + k - 1) * T, T + k, 1)).copy_(weight)\n        weight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)\n        if self.use_kernel_mask:\n            kernel_mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0)\n            weight_new = weight_new.masked_fill(kernel_mask == 0.0, float('-inf'))\n        weight_new = F.softmax(weight_new, dim=-1)\n        self.attn = weight_new\n        weight_new = weight_new.view(B * H, T, T)\n\n        # convolution\n        x = x.transpose(1, 2).contiguous()  # B x C x T\n        x = x.view(B * H, int(C / H), T).transpose(1, 2)\n        x = torch.bmm(weight_new, x)  # BH x T x C/H\n        x = x.transpose(1, 2).contiguous().view(B, C, T)\n\n        if self.use_bias:\n            x = x + self.bias.view(1, -1, 1)\n        x = x.transpose(1, 2)  # B x T x C\n\n        if mask is not None and not self.use_kernel_mask:\n            mask = mask.transpose(-1, -2)\n            x = x.masked_fill(mask == 0, 0.0)\n\n        # second linear layer\n        x = self.linear2(x)\n        return x\n```\n\nOriginal documentation:\n```python\nCompute 'Dynamic Convolution'\n        This function takes query, key and value but uses only value. This is just for compatibility with self-attention layer (attention.py)\n        :param torch.Tensor mask: (batch, time1)\n        :return torch.Tensor: convoluved `value` (batch, time1, d_model)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward of 'Dynamic Convolution'.\n\n        This function takes query, key and value but uses only quert.\n        This is just for compatibility with self-attention layer (attention.py)\n\n        Args:\n            query (torch.Tensor): (batch, time1, d_model) input tensor\n            key (torch.Tensor): (batch, time2, d_model) NOT USED\n            value (torch.Tensor): (batch, time2, d_model) NOT USED\n            mask (torch.Tensor): (batch, time1, time2) mask\n\n        Return:\n            x (torch.Tensor): (batch, time1, d_model) ouput", "file": "espnet_nets_pytorch_backend_transformer_dynamic_conv.py", "function": "DynamicConvolution.forward", "project": "espnet", "change_stats": {"code_changes": 48, "doc_changes": 19, "total_changes": 67}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef slo_request_to_idp(\n        cls, request: HttpRequest, return_to: Optional[str] = None\n    ) -> Optional[HttpResponse]:\n        \n        \n        \n\n        user_profile = request.user\n        assert isinstance(user_profile, UserProfile)\n\n        realm = user_profile.realm\n        assert saml_auth_enabled(realm)\n\n        complete_url = reverse(\"social:complete\", args=(\"saml\",))\n        saml_backend = load_backend(load_strategy(request), \"saml\", complete_url)\n\n        idp_name = cls.get_logged_in_user_idp(request)\n        if idp_name is None:\n            return None\n\n        idp = saml_backend.get_idp(idp_name)\n        auth = saml_backend._create_saml_auth(idp)\n        slo_url = auth.logout(name_id=user_profile.delivery_email, return_to=return_to)\n\n        return HttpResponseRedirect(slo_url)```\n\nUpdated code:\n```python\ndef slo_request_to_idp(\n        cls, request: HttpRequest, return_to: Optional[str] = None\n    ) -> HttpResponse:\n        \n        \n        \n\n        user_profile = request.user\n        assert isinstance(user_profile, UserProfile)\n\n        realm = user_profile.realm\n        assert saml_auth_enabled(realm)\n\n        complete_url = reverse(\"social:complete\", args=(\"saml\",))\n        saml_backend = load_backend(load_strategy(request), \"saml\", complete_url)\n\n        idp_name = cls.get_logged_in_user_idp(request)\n        if idp_name is None:\n            raise AssertionError(\"User not logged in via SAML\")\n\n        idp = saml_backend.get_idp(idp_name)\n        auth = saml_backend._create_saml_auth(idp)\n        slo_url = auth.logout(name_id=user_profile.delivery_email, return_to=return_to)\n\n        return HttpResponseRedirect(slo_url)\n```\n\nOriginal documentation:\n```python\nGenerates the redirect to the IdP's SLO endpoint with\n        the appropriately generated LogoutRequest or None if the session\n        wasn't authenticated via SAML.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates the redirect to the IdP's SLO endpoint with\n        the appropriately generated LogoutRequest. This should only be called\n        on requests with a session that was indeed obtained via SAML.", "file": "zproject_backends.py", "function": "SAMLSPInitiatedLogout.slo_request_to_idp", "project": "zulip", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    \n    \n    return _resnet(\"resnet101\", Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)```\n\nUpdated code:\n```python\ndef resnet101(*, weights: Optional[ResNet101_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n    \n    \n    weights = ResNet101_Weights.verify(weights)\n\n    return _resnet(Bottleneck, [3, 4, 23, 3], weights, progress, **kwargs)\n```\n\nOriginal documentation:\n```python\nr\"\"\"ResNet-101 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"ResNet-101 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n\n    Args:\n        weights (ResNet101_Weights, optional): The pretrained weights for the model\n        progress (bool): If True, displays a progress bar of the download to stderr", "file": "torchvision_models_resnet.py", "function": "resnet101", "project": "vision", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ravel(coord: Tuple[int, ...], shape: Tuple[int, ...]) -> int:\n        \n        \n        return np.ravel_multi_index(coord, shape)```\n\nUpdated code:\n```python\ndef ravel(coord: Tuple[int, ...], shape: Tuple[int, ...], mode: str = 'raise') -> int:\n        \n        \n\n        assert mode in [\"raise\", \"wrap\", \"clip\"]\n        return np.ravel_multi_index(coord, shape, mode)\n```\n\nOriginal documentation:\n```python\nConvert a coordinate to a rank.\n\n        Args:\n            coords (Tuple[int, ...]): Coordinate to be converted.\n            shape (Tuple[int, ...]): Shape of the process group mesh.\n\n        Returns:\n            int: Rank of the coordinate.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert a coordinate to a rank.\n           mode: ['raise', 'wrap', 'clip'], see https://numpy.org/doc/stable/reference/generated/numpy.ravel_multi_index.html.\n           with wrap, index out of range would be wrapped around.\n           For instance, ravel((0, i, 0), (1, 2, 1), 'wrap') returns (i % 2)\n\n        Args:\n            coords (Tuple[int, ...]): Coordinate to be converted.\n            shape (Tuple[int, ...]): Shape of the process group mesh.\n            mode (Optional[str]): The mode for numpy.ravel_multi_index.\n\n        Returns:\n            int: Rank of the coordinate.", "file": "colossalai_cluster_process_group_mesh.py", "function": "ProcessGroupMesh.ravel", "project": "ColossalAI", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _generate_checkpoint_name_for_or_statement(\n        self, messages: List[UserUttered]\n    ) -> str:\n        \n        \n        messages_texts_or_intents = sorted([m.text or m.intent_name for m in messages])\n\n        current_steps_event_counts = [str(len(s.events)) for s in self.current_steps]\n        # name: to identify the current story or rule\n        # current steps event counts: to identify the event position\n        #                             within the current story/rule\n        # messages texts or intents: identifying the members of the or statement\n        unique_id = (\n            f\"{self.name}_<>_{','.join(current_steps_event_counts)}\"\n            f\"_<>_{'@@@'.join(messages_texts_or_intents)}\"\n        )\n        hashed_id = rasa.shared.utils.io.get_text_hash(unique_id)[:8]\n        return f\"{GENERATED_CHECKPOINT_PREFIX}OR_{hashed_id}\"```\n\nUpdated code:\n```python\ndef _generate_checkpoint_name_for_or_statement(\n        self, messages: List[UserUttered]\n    ) -> str:\n        \n        \n        messages_texts_or_intents = sorted([m.text or m.intent_name for m in messages])\n        start_checkpoint_names = sorted(\n            list({chk.name for s in self.current_steps for chk in s.start_checkpoints})\n        )\n        current_steps_event_counts = [str(len(s.events)) for s in self.current_steps]\n        # name: to identify the current story or rule\n        # current steps event counts: to identify the event position\n        #                             within the current story/rule\n        # start checkpoint names: to identify the section\n        #                         within the current story/rule when there are\n        #                         multiple internal checkpoints\n        # messages texts or intents: identifying the members of the or statement\n        unique_id = (\n            f\"{self.name}_<>_{','.join(current_steps_event_counts)}\"\n            f\"_<>_{'@@@'.join(start_checkpoint_names)}\"\n            f\"_<>_{'@@@'.join(messages_texts_or_intents)}\"\n        )\n        hashed_id = rasa.shared.utils.io.get_text_hash(unique_id)[\n            :GENERATED_HASH_LENGTH\n        ]\n        return f\"{GENERATED_CHECKPOINT_PREFIX}OR_{hashed_id}\"\n```\n\nOriginal documentation:\n```python\nGenerates a unique checkpoint name for an or statement.\n\n        The name is based on the current story/rule name,\n        the current place in the story,\n        and the involved intents/e2e messages.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generates a unique checkpoint name for an or statement.\n\n        The name is based on the current story/rule name,\n        the current place in the story since the last checkpoint or start,\n        the name of the starting checkpoints,\n        and the involved intents/e2e messages.", "file": "rasa_shared_core_training_data_story_reader_story_step_builder.py", "function": "StoryStepBuilder._generate_checkpoint_name_for_or_statement", "project": "rasa", "change_stats": {"code_changes": 14, "doc_changes": 5, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef message_user(self, request, message):\n        \n        \n        \n        messages.info(request, message)```\n\nUpdated code:\n```python\ndef message_user(self, request, message, level=messages.INFO, extra_tags='',\n                     fail_silently=False):\n        \n        \n        \n\n        if not isinstance(level, int):\n            # attempt to get the level if passed a string\n            try:\n                level = getattr(messages.constants, level.upper())\n            except AttributeError:\n                levels = messages.constants.DEFAULT_TAGS.values()\n                levels_repr = ', '.join('`%s`' % l for l in levels)\n                raise ValueError('Bad message level string: `%s`. '\n                        'Possible values are: %s' % (level, levels_repr))\n\n        messages.add_message(request, level, message, extra_tags=extra_tags,\n                fail_silently=fail_silently)\n```\n\nOriginal documentation:\n```python\nSend a message to the user. The default implementation\n        posts a message using the django.contrib.messages backend.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Send a message to the user. The default implementation\n        posts a message using the django.contrib.messages backend.\n\n        Exposes almost the same API as messages.add_message(), but accepts the\n        positional arguments in a different order to maintain backwards\n        compatibility. For convenience, it accepts the `level` argument as\n        a string rather than the ususal level number.", "file": "django_contrib_admin_options.py", "function": "ModelAdmin.message_user", "project": "django", "change_stats": {"code_changes": 19, "doc_changes": 7, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_trade_stake_amount(self, pair) -> Optional[float]:\n        \n        \n        \n        if self.edge:\n            return self.edge.stake_amount(\n                pair,\n                self.wallets.get_free(self.config['stake_currency']),\n                self.wallets.get_total(self.config['stake_currency']),\n                Trade.total_open_trades_stakes()\n            )\n        else:\n            stake_amount = self.config['stake_amount']\n\n        available_amount = self.wallets.get_free(self.config['stake_currency'])\n\n        if stake_amount == constants.UNLIMITED_STAKE_AMOUNT:\n            open_trades = len(Trade.get_open_trades())\n            if open_trades >= self.config['max_open_trades']:\n                logger.warning(\"Can't open a new trade: max number of trades is reached\")\n                return None\n            return available_amount / (self.config['max_open_trades'] - open_trades)\n\n        # Check if stake_amount is fulfilled\n        if available_amount < stake_amount:\n            raise DependencyException(\n                f\"Available balance ({available_amount} {self.config['stake_currency']}) is \"\n                f\"lower than stake amount ({stake_amount} {self.config['stake_currency']})\"\n            )\n\n        return stake_amount```\n\nUpdated code:\n```python\ndef get_trade_stake_amount(self, pair) -> Optional[float]:\n        \n        \n        \n        if self.edge:\n            return self.edge.stake_amount(\n                pair,\n                self.wallets.get_free(self.config['stake_currency']),\n                self.wallets.get_total(self.config['stake_currency']),\n                Trade.total_open_trades_stakes()\n            )\n        else:\n            stake_amount = self.config['stake_amount']\n\n        if stake_amount == constants.UNLIMITED_STAKE_AMOUNT:\n            open_trades = len(Trade.get_open_trades())\n            if open_trades >= self.config['max_open_trades']:\n                logger.warning(\"Can't open a new trade: max number of trades is reached\")\n                return None\n            available_amount = self.wallets.get_free(self.config['stake_currency'])\n            return available_amount / (self.config['max_open_trades'] - open_trades)\n\n        return self._check_available_stake_amount(stake_amount)\n```\n\nOriginal documentation:\n```python\nCheck if stake amount can be fulfilled with the available balance\n        for the stake currency\n        :return: float: Stake Amount\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculate stake amount for the trade\n        :return: float: Stake amount", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.get_trade_stake_amount", "project": "freqtrade", "change_stats": {"code_changes": 14, "doc_changes": 7, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(self):\n        \n        \n        log.info(\"Running: '%s' [%s]\", self.get_command(), self.cwd)\n\n        self.start_time = datetime.utcnow()\n        stdout = subprocess.PIPE\n        stderr = subprocess.PIPE\n        stdin = None\n        if self.input_data is not None:\n            stdin = subprocess.PIPE\n        if self.combine_output:\n            stderr = subprocess.STDOUT\n\n        environment = {}\n        environment.update(self.environment)\n        environment['READTHEDOCS'] = 'True'\n        if self.build_env is not None:\n            environment['READTHEDOCS_VERSION'] = self.build_env.version.slug\n            environment['READTHEDOCS_PROJECT'] = self.build_env.project.slug\n        if 'DJANGO_SETTINGS_MODULE' in environment:\n            del environment['DJANGO_SETTINGS_MODULE']\n        if 'PYTHONPATH' in environment:\n            del environment['PYTHONPATH']\n        if self.bin_path is not None:\n            env_paths = environment.get('PATH', '').split(':')\n            env_paths.insert(0, self.bin_path)\n            environment['PATH'] = ':'.join(env_paths)\n\n        try:\n            proc = subprocess.Popen(\n                self.command,\n                shell=self.shell,\n                cwd=self.cwd,\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                env=environment,\n            )\n            cmd_input = None\n            if self.input_data is not None:\n                cmd_input = self.input_data\n\n            if isinstance(cmd_input, six.string_types):\n                cmd_input_bytes = cmd_input.encode('utf-8')\n            else:\n                cmd_input_bytes = cmd_input\n            cmd_output = proc.communicate(input=cmd_input_bytes)\n            (cmd_stdout, cmd_stderr) = cmd_output\n            try:\n                self.output = cmd_stdout.decode('utf-8', 'replace')\n            except (TypeError, AttributeError):\n                self.output = None\n            try:\n                self.error = cmd_stderr.decode('utf-8', 'replace')\n            except (TypeError, AttributeError):\n                self.error = None\n            self.exit_code = proc.returncode\n        except OSError:\n            self.error = traceback.format_exc()\n            self.output = self.error\n            self.exit_code = -1\n        finally:\n            self.end_time = datetime.utcnow()```\n\nUpdated code:\n```python\ndef run(self):\n        \n        \n        \n        log.info(\"Running: '%s' [%s]\", self.get_command(), self.cwd)\n\n        self.start_time = datetime.utcnow()\n        stdout = subprocess.PIPE\n        stderr = subprocess.PIPE\n        stdin = None\n        if self.input_data is not None:\n            stdin = subprocess.PIPE\n        if self.combine_output:\n            stderr = subprocess.STDOUT\n\n        environment = {}\n        environment.update(self.environment)\n        environment['READTHEDOCS'] = 'True'\n        if self.build_env is not None:\n            environment['READTHEDOCS_VERSION'] = self.build_env.version.slug\n            environment['READTHEDOCS_PROJECT'] = self.build_env.project.slug\n        if 'DJANGO_SETTINGS_MODULE' in environment:\n            del environment['DJANGO_SETTINGS_MODULE']\n        if 'PYTHONPATH' in environment:\n            del environment['PYTHONPATH']\n        if self.bin_path is not None:\n            env_paths = environment.get('PATH', '').split(':')\n            env_paths.insert(0, self.bin_path)\n            environment['PATH'] = ':'.join(env_paths)\n\n        try:\n            proc = subprocess.Popen(\n                self.command,\n                shell=self.shell,\n                cwd=self.cwd,\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                env=environment,\n            )\n            cmd_input = None\n            if self.input_data is not None:\n                cmd_input = self.input_data\n\n            if isinstance(cmd_input, six.string_types):\n                cmd_input_bytes = cmd_input.encode('utf-8')\n            else:\n                cmd_input_bytes = cmd_input\n            cmd_output = proc.communicate(input=cmd_input_bytes)\n            (cmd_stdout, cmd_stderr) = cmd_output\n            try:\n                self.output = cmd_stdout.decode('utf-8', 'replace')\n            except (TypeError, AttributeError):\n                self.output = None\n            try:\n                self.error = cmd_stderr.decode('utf-8', 'replace')\n            except (TypeError, AttributeError):\n                self.error = None\n            self.exit_code = proc.returncode\n        except OSError:\n            self.error = traceback.format_exc()\n            self.output = self.error\n            self.exit_code = -1\n        finally:\n            self.end_time = datetime.utcnow()\n```\n\nOriginal documentation:\n```python\nSet up subprocess and execute command\n\n        :param cmd_input: input to pass to command in STDIN\n        :type cmd_input: str\n        :param combine_output: combine STDERR into STDOUT\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set up subprocess and execute command.\n\n        :param cmd_input: input to pass to command in STDIN\n        :type cmd_input: str\n        :param combine_output: combine STDERR into STDOUT", "file": "readthedocs_doc_builder_environments.py", "function": "BuildCommand.run", "project": "readthedocs.org", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_schedule(self, in_vars, enable_double_backprop=False):\n        \n        \n        if self._end_forward:\n            self._end_forward = False\n        if self._minimize_cache_size:\n            if chainer.config.train != self._prev_train_config:\n                # Training config changed, so clear caches.\n                self._prev_train_config = chainer.config.train\n                if self._verbosity_level >= 2:\n                    print(\"Clearing schedule cache...\")\n                self.schedules.clear()\n                self.in_use_count.clear()\n            # todo (vogel): Also check if minibatch size has changed and clear schedules.\n\n        if chainer.config.train is False:\n            key_str = 'test:' + ''.join(str(x.shape) for x in in_vars)\n            # If the maximum number of in-use schedules in any iteration\n            # during training mode was exactly 1, assume it should also\n            # be 1 for test mode.\n\n            if key_str in self.schedules:\n                sched_list = self.schedules[key_str]\n                sched = sched_list[0]\n            else:\n                sched = StaticScheduleFunction(self,\n                                               verbosity_level=self._verbosity_level,\n                                               enable_double_backprop=enable_double_backprop)\n                self.schedules[key_str] = [sched]\n            return sched\n\n        else:\n            key_str = 'train:' + ''.join(str(x.shape) for x in in_vars)\n            #print(\"key: \\n\", key_str)\n            self._train_count += 1\n\n            if key_str in self.schedules:\n                sched_list = self.schedules[key_str]\n                available_index = self.in_use_count[key_str]\n                if available_index >= len(sched_list):\n                    sched = StaticScheduleFunction(self,\n                                                   verbosity_level=self._verbosity_level,\n                                                   enable_double_backprop=enable_double_backprop)\n                    sched_list.append(sched)\n\n                sched = sched_list[available_index]\n                self.in_use_count[key_str] = available_index + 1\n            else:\n                sched = StaticScheduleFunction(self,\n                                               enable_double_backprop=enable_double_backprop)\n                self.schedules[key_str] = [sched]\n                self.in_use_count[key_str] = 1\n\n        return sched```\n\nUpdated code:\n```python\ndef get_schedule(self, in_vars, enable_double_backprop=False):\n        \n        \n        if self._end_forward:\n            self._end_forward = False\n        if self._minimize_cache_size:\n            if chainer.config.train != self._prev_train_config:\n                # Training config changed, so clear caches.\n                self._prev_train_config = chainer.config.train\n                if self._verbosity_level >= 2:\n                    print(\"Clearing schedule cache...\")\n                self.schedules.clear()\n                self.in_use_count.clear()\n            # todo (vogel): Also check if minibatch size has changed and clear schedules.\n\n        if chainer.config.train is False or chainer.config.enable_backprop is False:\n            key_str = 'test:' + ''.join(str(x.shape) for x in in_vars)\n            # If the maximum number of in-use schedules in any iteration\n            # during training mode was exactly 1, assume it should also\n            # be 1 for test mode.\n\n            if key_str in self.schedules:\n                sched_list = self.schedules[key_str]\n                sched = sched_list[0]\n            else:\n                sched = StaticScheduleFunction(self,\n                                               verbosity_level=self._verbosity_level,\n                                               enable_double_backprop=enable_double_backprop)\n                self.schedules[key_str] = [sched]\n            return sched\n\n        else:\n            key_str = 'train:' + ''.join(str(x.shape) for x in in_vars)\n            #print(\"key: \\n\", key_str)\n            self._train_count += 1\n\n            if key_str in self.schedules:\n                sched_list = self.schedules[key_str]\n                available_index = self.in_use_count[key_str]\n                if available_index >= len(sched_list):\n                    sched = StaticScheduleFunction(self,\n                                                   verbosity_level=self._verbosity_level,\n                                                   enable_double_backprop=enable_double_backprop)\n                    sched_list.append(sched)\n\n                sched = sched_list[available_index]\n                self.in_use_count[key_str] = available_index + 1\n            else:\n                sched = StaticScheduleFunction(self,\n                                               enable_double_backprop=enable_double_backprop)\n                self.schedules[key_str] = [sched]\n                self.in_use_count[key_str] = 1\n\n        return sched\n```\n\nOriginal documentation:\n```python\nGet a static schedule.\n\n        Try to return an existing static schedule (instance of\n        ``StaticScheduleFunction``) that is compatible with\n        the current configuration and input variables to the supplied chain.\n        If there is no existing schedule available, return an empty schedule\n        object.\n\n        If `chainer.config.train` is `True`, then this function will return a\n        distinct schedule object on each call during the forward pass. That is,\n        that returned schedule will either be a distinct cached schedule or\n        a distinct empty schedule. These returned schedules cannot be reused\n        (that is, returned again) until the next iteration. Then end of the\n        forward pass is signified by calling ``loss.backward()`` on a variable\n        that contains the supplied chain in its computation graph. It is\n        therefore necessary to call ``loss.backward()`` each iteration after\n        the forward pass has completed. Otherwise, this method would always\n        return a distinct schedule object which would eventually cause an\n        out-of-memory error to occur.\n\n        If `chainer.config.train` is `False` and this function is called multiple\n        times during the forward pass, then the same schedule object can be\n        returned multiple times provided that it is compatible with the\n        types and shapes of the input variables.\n\n        Args:\n            in_vars (tuple of :class:`~chainer.Variable`): The input variables to the chain.\n\n        Returns:\n            An instance of ``StaticScheduleFunction``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get a static schedule.\n\n        Return a static schedule object (that is, an instance of\n        ``StaticScheduleFunction``) that is compatible with\n        the current configuration and input variables to the supplied chain.\n        If there is no existing schedule available, return an empty schedule\n        object.\n\n        During the usual \"training mode\" (that is, when both\n        `chainer.config.enable_backprop` and `chainer.config.train`\n        are `True`), this method will always return a distince static\n        schedule within the same iteration. It will also try to reuse\n        existing schedules across iterations. Therefore, any schedule that\n        is returned in a given iteration cannot be returned again until\n        the following iteration. However, if either of these flags is\n        'False', then this method may return the same schedule instance\n        multiple times within the same iteration, as long as it is\n        compatible with `in_vars`.\n\n        Note that in order to implement the above behavior, the schedule\n        manager must be informed when the current iteration has finished.\n        This is accomplished by calling `end_forward()` after the\n        iteration has finished.\n\n        Args:\n            in_vars (tuple of :class:`~chainer.Variable`): The input variables to the chain.\n\n        Returns:\n            An instance of ``StaticScheduleFunction``.", "file": "chainer_graph_optimizations_static_graph.py", "function": "ScheduleManager.get_schedule", "project": "chainer", "change_stats": {"code_changes": 4, "doc_changes": 33, "total_changes": 37}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get(self, field_):\n        \n        \n        \n        field = field_\n        field = field.encode('utf-8')\n\n        if field_ in self.gettable.keys():\n\n            # allocate array\n            dims = self.gettable[field_]\n            out = np.ascontiguousarray(np.zeros((dims,)), dtype=np.float64)\n            out_data = cast(out.ctypes.data, POINTER(c_double))\n\n            self.shared_lib.sim_out_get.argtypes = [c_void_p, c_void_p, c_void_p, c_char_p, c_void_p]\n            self.shared_lib.sim_out_get(self.sim_config, self.sim_dims, self.sim_out, field, out_data)\n\n            if field_ == 'S_forw':\n                nu = self.sim_struct.dims.nu\n                nx = self.sim_struct.dims.nx\n                out = out.reshape(nx, nx+nu, order='F')\n            elif field_ == 'Sx':\n                nx = self.sim_struct.dims.nx\n                out = out.reshape(nx, nx, order='F')\n            elif field_ == 'Su':\n                nx = self.sim_struct.dims.nx\n                nu = self.sim_struct.dims.nu\n                out = out.reshape(nx, nu, order='F')\n            elif field_ == 'S_hess':\n                nx = self.sim_struct.dims.nx\n                nu = self.sim_struct.dims.nu\n                out = out.reshape(nx+nu, nx+nu, order='F')\n            elif field_ == 'S_algebraic':\n                nx = self.sim_struct.dims.nx\n                nu = self.sim_struct.dims.nu\n                nz = self.sim_struct.dims.nz\n                out = out.reshape(nz, nx+nu, order='F')\n        else:\n            raise Exception(f'AcadosSimSolver.get(): Unknown field {field_},' \\\n                f' available fields are {\", \".join(self.gettable.keys())}')\n\n        return out```\n\nUpdated code:\n```python\ndef get(self, field_):\n        \n        \n        \n        field = field_.encode('utf-8')\n\n        if field_ in self.gettable_vectors:\n            # get dims\n            dims = np.ascontiguousarray(np.zeros((2,)), dtype=np.intc)\n            dims_data = cast(dims.ctypes.data, POINTER(c_int))\n\n            self.shared_lib.sim_dims_get_from_attr.argtypes = [c_void_p, c_void_p, c_char_p, POINTER(c_int)]\n            self.shared_lib.sim_dims_get_from_attr(self.sim_config, self.sim_dims, field, dims_data)\n\n            # allocate array\n            out = np.ascontiguousarray(np.zeros((dims[0],)), dtype=np.float64)\n            out_data = cast(out.ctypes.data, POINTER(c_double))\n\n            self.shared_lib.sim_out_get.argtypes = [c_void_p, c_void_p, c_void_p, c_char_p, c_void_p]\n            self.shared_lib.sim_out_get(self.sim_config, self.sim_dims, self.sim_out, field, out_data)\n\n        elif field_ in self.gettable_matrices:\n            # get dims\n            dims = np.ascontiguousarray(np.zeros((2,)), dtype=np.intc)\n            dims_data = cast(dims.ctypes.data, POINTER(c_int))\n\n            self.shared_lib.sim_dims_get_from_attr.argtypes = [c_void_p, c_void_p, c_char_p, POINTER(c_int)]\n            self.shared_lib.sim_dims_get_from_attr(self.sim_config, self.sim_dims, field, dims_data)\n\n            out = np.zeros((dims[0], dims[1]), order='F')\n            out_data = cast(out.ctypes.data, POINTER(c_double))\n\n            self.shared_lib.sim_out_get.argtypes = [c_void_p, c_void_p, c_void_p, c_char_p, c_void_p]\n            self.shared_lib.sim_out_get(self.sim_config, self.sim_dims, self.sim_out, field, out_data)\n\n        elif field_ in self.gettable_scalars:\n            scalar = c_double()\n            scalar_data = byref(scalar)\n            self.shared_lib.sim_out_get.argtypes = [c_void_p, c_void_p, c_void_p, c_char_p, c_void_p]\n            self.shared_lib.sim_out_get(self.sim_config, self.sim_dims, self.sim_out, field, scalar_data)\n\n            out = scalar.value\n        else:\n            raise Exception(f'AcadosSimSolver.get(): Unknown field {field_},' \\\n                f' available fields are {\", \".join(self.gettable_vectors+self.gettable_matrices)}, {\", \".join(self.gettable_scalars)}')\n\n        return out\n```\n\nOriginal documentation:\n```python\nGet the last solution of the solver.\n\n            :param str field: string in ['x', 'u', 'z', 'S_forw', 'Sx', 'Su', 'S_adj', 'S_hess', 'S_algebraic']\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the last solution of the solver.\n\n            :param str field: string in ['x', 'u', 'z', 'S_forw', 'Sx', 'Su', 'S_adj', 'S_hess', 'S_algebraic', 'CPUtime', 'time_tot', 'ADtime', 'time_ad', 'LAtime', 'time_la']", "file": "third_party_acados_acados_template_acados_sim_solver.py", "function": "AcadosSimSolver.get", "project": "openpilot", "change_stats": {"code_changes": 59, "doc_changes": 4, "total_changes": 63}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef notnull(obj):\n    '''\n    \n    '''\n    res = isnull(obj)\n    if np.isscalar(res):\n        return not res\n    return -res```\n\nUpdated code:\n```python\ndef notnull(obj):\n    \n    \n    res = isnull(obj)\n    if np.isscalar(res):\n        return not res\n    return -res\n```\n\nOriginal documentation:\n```python\nReplacement for numpy.isfinite / -numpy.isnan which is suitable\n    for use on object arrays.\n\n    Parameters\n    ----------\n    arr: ndarray or object value\n\n    Returns\n    -------\n    boolean ndarray or boolean\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replacement for numpy.isfinite / -numpy.isnan which is suitable for use\n    on object arrays.\n\n    Parameters\n    ----------\n    arr : ndarray or object value\n        Object to check for *not*-null-ness\n\n    Returns\n    -------\n    isnulled : array-like of bool or bool\n        Array or bool indicating whether an object is *not* null or if an array\n        is given which of the element is *not* null.", "file": "pandas_core_common.py", "function": "notnull", "project": "pandas", "change_stats": {"code_changes": 5, "doc_changes": 13, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef output_filename(self, filename: str) -> str:\n         \n        \n        filename = os.path.splitext(os.path.basename(filename))[0]\n        out_filename = f\"{filename}.{self.config['format']}\"\n        out_filename = os.path.join(self.output_folder, out_filename)\n        logger.trace(\"in filename: '%s', out filename: '%s'\", filename, out_filename)\n        return out_filename```\n\nUpdated code:\n```python\ndef output_filename(self, filename: str, separate_mask: bool = False) -> List[str]:\n         \n        \n        filename = os.path.splitext(os.path.basename(filename))[0]\n        out_filename = f\"{filename}.{self.config['format']}\"\n        retval = [os.path.join(self.output_folder, out_filename)]\n        if separate_mask:\n            retval.append(os.path.join(self.output_folder, \"masks\", out_filename))\n\n        if separate_mask and not self._subfolders_created:\n            locations = [os.path.dirname(loc) for loc in retval]\n            logger.debug(\"Creating sub-folders: %s\", locations)\n            for location in locations:\n                os.makedirs(location, exist_ok=True)\n\n        logger.trace(\"in filename: '%s', out filename: '%s'\", filename, retval)  # type:ignore\n        return retval\n```\n\nOriginal documentation:\n```python\nObtain the full path for the output file, including the correct extension, for the\n        given input filename.\n\n        NB: The plugin must have a config item 'format' that contains the file extension to use\n        this method.\n\n        Parameters\n        ----------\n        filename: str\n            The input frame filename to generate the output file name for\n\n        Returns\n        -------\n        str\n            The full path for the output converted frame to be saved to.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Obtain the full path for the output file, including the correct extension, for the\n        given input filename.\n\n        NB: The plugin must have a config item 'format' that contains the file extension to use\n        this method.\n\n        Parameters\n        ----------\n        filename: str\n            The input frame filename to generate the output file name for\n        separate_mask: bool, optional\n            ``True`` if the mask should be saved out to a sub-folder otherwise ``False``\n\n        Returns\n        -------\n        list\n            The full path for the output converted frame to be saved to in position 1. The full\n            path for the mask to be output to in position 2 (if requested)", "file": "plugins_convert_writer__base.py", "function": "Output.output_filename", "project": "faceswap", "change_stats": {"code_changes": 19, "doc_changes": 9, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef post(self):\n        \n        \n        \n        params = request.get_json(force=True)\n        parameter_values = collect_parameters_from_request(request.args)\n\n        query = params['query']\n        max_age = int(params.get('max_age', -1))\n        query_id = params.get('query_id', 'adhoc')\n\n        data_source = models.DataSource.get_by_id_and_org(params.get('data_source_id'), self.current_org)\n\n        if not has_access(data_source.groups, self.current_user, not_view_only):\n            return {'job': {'status': 4, 'error': 'You do not have permission to run queries with this data source.'}}, 403\n\n        self.record_event({\n            'action': 'execute_query',\n            'object_id': data_source.id,\n            'object_type': 'data_source',\n            'query': query\n        })\n        return run_query(data_source, parameter_values, query, query_id, max_age)```\n\nUpdated code:\n```python\ndef post(self):\n        \n        \n        \n        params = request.get_json(force=True)\n\n        query = params['query']\n        max_age = int(params.get('max_age', -1))\n        query_id = params.get('query_id', 'adhoc')\n        parameters = params.get('parameters', collect_parameters_from_request(request.args))\n\n        data_source = models.DataSource.get_by_id_and_org(params.get('data_source_id'), self.current_org)\n\n        if not has_access(data_source.groups, self.current_user, not_view_only):\n            return {'job': {'status': 4, 'error': 'You do not have permission to run queries with this data source.'}}, 403\n\n        self.record_event({\n            'action': 'execute_query',\n            'object_id': data_source.id,\n            'object_type': 'data_source',\n            'query': query,\n            'query_id': query_id,\n            'parameters': parameters\n        })\n        return run_query(data_source, parameters, query, query_id, max_age)\n```\n\nOriginal documentation:\n```python\nExecute a query (or retrieve recent results).\n\n        :qparam string query: The query text to execute\n        :qparam number query_id: The query object to update with the result (optional)\n        :qparam number max_age: If query results less than `max_age` seconds old are available,\n                                return them, otherwise execute the query; if omitted or -1, returns\n                                any cached result, or executes if not available. Set to zero to\n                                always execute.\n        :qparam number data_source_id: ID of data source to query\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Execute a query (or retrieve recent results).\n\n        :qparam string query: The query text to execute\n        :qparam number query_id: The query object to update with the result (optional)\n        :qparam number max_age: If query results less than `max_age` seconds old are available,\n                                return them, otherwise execute the query; if omitted or -1, returns\n                                any cached result, or executes if not available. Set to zero to\n                                always execute.\n        :qparam number data_source_id: ID of data source to query\n        :qparam object parameters: A set of parameter values to apply to the query.", "file": "redash_handlers_query_results.py", "function": "QueryResultListResource.post", "project": "redash", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _init_modules(self, db_url: Optional[str] = None) -> None:\n        \n        \n        \n        # Initialize all modules\n\n        persistence.init(self.config, db_url)\n        exchange.init(self.config)\n\n        # Set initial application state\n        initial_state = self.config.get('initial_state')\n\n        if initial_state:\n            self.state = State[initial_state.upper()]\n        else:\n            self.state = State.STOPPED```\n\nUpdated code:\n```python\ndef _init_modules(self) -> None:\n        \n        \n        \n        # Initialize all modules\n\n        persistence.init(self.config)\n        exchange.init(self.config)\n\n        # Set initial application state\n        initial_state = self.config.get('initial_state')\n\n        if initial_state:\n            self.state = State[initial_state.upper()]\n        else:\n            self.state = State.STOPPED\n```\n\nOriginal documentation:\n```python\nInitializes all modules and updates the config\n        :param db_url: database connector string for sqlalchemy (Optional)\n        :return: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initializes all modules and updates the config\n        :return: None", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot._init_modules", "project": "freqtrade", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _called_with_wrong_args(factory):\n    \n    \n    tb = sys.exc_info()[2]\n\n    try:\n        while tb is not None:\n            if tb.tb_frame.f_code is factory.__code__:\n                # in the factory, it was called successfully\n                return False\n\n            tb = tb.tb_next\n\n        # didn't reach the factory\n        return True\n    finally:\n        # explicitly delete tb as it is circular referenced\n        # https://docs.python.org/2/library/sys.html#sys.exc_info\n        del tb```\n\nUpdated code:\n```python\ndef _called_with_wrong_args(f):\n    \n    \n    tb = sys.exc_info()[2]\n\n    try:\n        while tb is not None:\n            if tb.tb_frame.f_code is f.__code__:\n                # In the function, it was called successfully.\n                return False\n\n            tb = tb.tb_next\n\n        # Didn't reach the function.\n        return True\n    finally:\n        # Delete tb to break a circular reference.\n        # https://docs.python.org/2/library/sys.html#sys.exc_info\n        del tb\n```\n\nOriginal documentation:\n```python\nCheck whether calling a function raised a ``TypeError`` because\n    the call failed or because something in the factory raised the\n    error.\n\n    :param factory: the factory function that was called\n    :return: true if the call failed\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check whether calling a function raised a ``TypeError`` because\n    the call failed or because something in the factory raised the\n    error.\n\n    :param f: The function that was called.\n    :return: ``True`` if the call failed.", "file": "src_flask_cli.py", "function": "_called_with_wrong_args", "project": "flask", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef dependencies(self):\n        \n        \n        \n        return self.inputs + (self.mask,)```\n\nUpdated code:\n```python\ndef dependencies(self):\n        \n        \n        \n        raise NotImplementedError('dependencies')\n```\n\nOriginal documentation:\n```python\nA tuple containing all terms that must be computed before this term can\n        be loaded or computed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A dictionary mapping terms that must be computed before `self` to the\n        number of extra rows needed for those terms.", "file": "zipline_pipeline_term.py", "function": "Term.dependencies", "project": "zipline", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_block_type(values, dtype: DtypeObj | None = None):\n    \n    \n    \n    # We use vtype and kind checks because they are much more performant\n    #  than is_foo_dtype\n    if dtype is None:\n        dtype = values.dtype\n\n    vtype = dtype.type\n    kind = dtype.kind\n\n    cls: type[Block]\n\n    if isinstance(dtype, SparseDtype):\n        # Need this first(ish) so that Sparse[datetime] is sparse\n        cls = ExtensionBlock\n    elif isinstance(dtype, CategoricalDtype):\n        cls = CategoricalBlock\n    elif vtype is Timestamp:\n        cls = DatetimeTZBlock\n    elif isinstance(dtype, ExtensionDtype):\n        # Note: need to be sure PandasArray is unwrapped before we get here\n        cls = ExtensionBlock\n\n    elif kind in [\"M\", \"m\"]:\n        cls = DatetimeLikeBlock\n    elif kind in [\"f\", \"c\", \"i\", \"u\", \"b\"]:\n        cls = NumericBlock\n    else:\n        cls = ObjectBlock\n    return cls```\n\nUpdated code:\n```python\ndef get_block_type(dtype: DtypeObj):\n    \n    \n    \n    # We use vtype and kind checks because they are much more performant\n    #  than is_foo_dtype\n    vtype = dtype.type\n    kind = dtype.kind\n\n    cls: type[Block]\n\n    if isinstance(dtype, SparseDtype):\n        # Need this first(ish) so that Sparse[datetime] is sparse\n        cls = ExtensionBlock\n    elif isinstance(dtype, CategoricalDtype):\n        cls = CategoricalBlock\n    elif vtype is Timestamp:\n        cls = DatetimeTZBlock\n    elif isinstance(dtype, ExtensionDtype):\n        # Note: need to be sure PandasArray is unwrapped before we get here\n        cls = ExtensionBlock\n\n    elif kind in [\"M\", \"m\"]:\n        cls = DatetimeLikeBlock\n    elif kind in [\"f\", \"c\", \"i\", \"u\", \"b\"]:\n        cls = NumericBlock\n    else:\n        cls = ObjectBlock\n    return cls\n```\n\nOriginal documentation:\n```python\nFind the appropriate Block subclass to use for the given values and dtype.\n\n    Parameters\n    ----------\n    values : ndarray-like\n    dtype : numpy or pandas dtype\n\n    Returns\n    -------\n    cls : class, subclass of Block\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Find the appropriate Block subclass to use for the given values and dtype.\n\n    Parameters\n    ----------\n    dtype : numpy or pandas dtype\n\n    Returns\n    -------\n    cls : class, subclass of Block", "file": "pandas_core_internals_blocks.py", "function": "get_block_type", "project": "pandas", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef click(\n        self,\n        fn: Callable,\n        inputs: List[Component],\n        outputs: List[Component],\n        status_tracker: Optional[StatusTracker] = None,\n        api_name: AnyStr = None,\n        queue=None,\n        _js: Optional[str] = None,\n        _preprocess: bool = True,\n        _postprocess: bool = True,\n    ):\n        \n        \n        \n        self.set_event_trigger(\n            \"click\",\n            fn,\n            inputs,\n            outputs,\n            status_tracker=status_tracker,\n            api_name=api_name,\n            queue=queue,\n            js=_js,\n            preprocess=_preprocess,\n            postprocess=_postprocess,\n        )```\n\nUpdated code:\n```python\ndef click(\n        self,\n        fn: Callable,\n        inputs: List[Component],\n        outputs: List[Component],\n        api_name: AnyStr = None,\n        status_tracker: Optional[StatusTracker] = None,\n        scroll_to_output: bool = False,\n        show_progress: bool = True,\n        queue=None,\n        _js: Optional[str] = None,\n        _preprocess: bool = True,\n        _postprocess: bool = True,\n    ):\n        \n        \n        \n        self.set_event_trigger(\n            \"click\",\n            fn,\n            inputs,\n            outputs,\n            api_name=api_name,\n            status_tracker=status_tracker,\n            scroll_to_output=scroll_to_output,\n            show_progress=show_progress,\n            queue=queue,\n            js=_js,\n            preprocess=_preprocess,\n            postprocess=_postprocess,\n        )\n```\n\nOriginal documentation:\n```python\nThis event is triggered when the component (e.g. a button) is clicked.\n\n        Parameters:\n            fn: Callable function\n            inputs: List of inputs\n            outputs: List of outputs\n            status_tracker: StatusTracker to visualize function progress\n            api_name: Defining this parameter exposes the endpoint in the api docs\n            _js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components.\n            _preprocess: If False, will not run preprocessing of component data before running 'fn'.\n            _postprocess: If False, will not run postprocessing of component data before returning 'fn' output.\n        Returns: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This event is triggered when the component (e.g. a button) is clicked.\n\n        Parameters:\n            fn: Callable function\n            inputs: List of inputs\n            outputs: List of outputs\n            api_name: Defining this parameter exposes the endpoint in the api docs\n            status_tracker: StatusTracker to visualize function progress\n            scroll_to_output: If True, will scroll to output component on completion\n            show_progress: If True, will show progress animation while pending\n            _js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components.\n            _preprocess: If False, will not run preprocessing of component data before running 'fn'.\n            _postprocess: If False, will not run postprocessing of component data before returning 'fn' output.\n        Returns: None", "file": "gradio_events.py", "function": "Clickable.click", "project": "gradio", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef transpose_sequence(xs):\n    \n    \n    if len(xs) == 0:\n        return ()\n    return TransposeSequence(len(xs[0])).apply(xs)```\n\nUpdated code:\n```python\ndef transpose_sequence(xs):\n    \n    \n    ys = TransposeSequence()(*xs)\n    if not isinstance(ys, tuple):\n        ys = (ys,)\n    return ys\n```\n\nOriginal documentation:\n```python\nTranspose a list of Variables.\n\n    This function transposes a list of :class:`~chainer.Variable` s and returns\n    a list of :class:`Variable` s.\n    For example a user gives ``[(0, 1, 2, 3), (4, 5), (6)]``, the function\n    returns ``[(0, 4, 6), (1, 5), (2), (3)]``.\n    Note that a given list needs to be sorted by each length of\n    :class:`~chainer.Variable`.\n\n    Args:\n        xs (list of ~chainer.Variable): Variables to transpose.\n\n    Returns:\n        tuple or Variable: Transposed list.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Transpose a list of Variables.\n\n    This function transposes a list of :class:`~chainer.Variable`\\\\ s and\n    returns a list of :class:`Variable`\\\\ s.\n    For example a user gives ``[(0, 1, 2, 3), (4, 5), (6)]``, the function\n    returns ``[(0, 4, 6), (1, 5), (2), (3)]``.\n    Note that a given list needs to be sorted by each length of\n    :class:`~chainer.Variable`.\n\n    Args:\n        xs (list of ~chainer.Variable): Variables to transpose.\n\n    Returns:\n        tuple or Variable: Transposed list.", "file": "chainer_", "function": "transpose_sequence", "project": "chainer", "change_stats": {"code_changes": 9, "doc_changes": 6, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef deserialize(cls, info: TypeInfo, data: JsonDict) -> PydanticModelField:\n        \n        return cls(**data)```\n\nUpdated code:\n```python\ndef deserialize(cls, info: TypeInfo, data: JsonDict, api: SemanticAnalyzerPluginInterface) -> PydanticModelField:\n        \n        data = data.copy()\n        typ = deserialize_and_fixup_type(data.pop('type'), api)\n        return cls(type=typ, info=info, **data)\n```\n\nOriginal documentation:\n```python\nCreates a Pydantic model field from a dict.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Based on mypy.plugins.dataclasses.DataclassAttribute.deserialize.", "file": "pydantic_mypy.py", "function": "PydanticModelField.deserialize", "project": "pydantic", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        embedding: Embeddings,\n        index_uri: str,\n        metric: str,\n        *,\n        vector_index_uri: str = \"\",\n        docs_array_uri: str = \"\",\n        config: Optional[Mapping[str, Any]] = None,\n        timestamp: Any = None,\n        **kwargs: Any,\n    ):\n        \n        self.embedding = embedding\n        self.embedding_function = embedding.embed_query\n        self.index_uri = index_uri\n        self.metric = metric\n        self.config = config\n\n        tiledb_vs, tiledb = dependable_tiledb_import()\n        with tiledb.scope_ctx(ctx_or_config=config):\n            index_group = tiledb.Group(self.index_uri, \"r\")\n            self.vector_index_uri = (\n                vector_index_uri\n                if vector_index_uri != \"\"\n                else get_vector_index_uri_from_group(index_group)\n            )\n            self.docs_array_uri = (\n                docs_array_uri\n                if docs_array_uri != \"\"\n                else get_documents_array_uri_from_group(index_group)\n            )\n            index_group.close()\n            group = tiledb.Group(self.vector_index_uri, \"r\")\n            self.index_type = group.meta.get(\"index_type\")\n            group.close()\n            self.timestamp = timestamp\n            if self.index_type == \"FLAT\":\n                self.vector_index = tiledb_vs.flat_index.FlatIndex(\n                    uri=self.vector_index_uri,\n                    config=self.config,\n                    timestamp=self.timestamp,\n                    **kwargs,\n                )\n            elif self.index_type == \"IVF_FLAT\":\n                self.vector_index = tiledb_vs.ivf_flat_index.IVFFlatIndex(\n                    uri=self.vector_index_uri,\n                    config=self.config,\n                    timestamp=self.timestamp,\n                    **kwargs,\n                )```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        embedding: Embeddings,\n        index_uri: str,\n        metric: str,\n        *,\n        vector_index_uri: str = \"\",\n        docs_array_uri: str = \"\",\n        config: Optional[Mapping[str, Any]] = None,\n        timestamp: Any = None,\n        allow_dangerous_deserialization: bool = False,\n        **kwargs: Any,\n    ):\n        \n        \n        if not allow_dangerous_deserialization:\n            raise ValueError(\n                \"TileDB relies on pickle for serialization and deserialization. \"\n                \"This can be dangerous if the data is intercepted and/or modified \"\n                \"by malicious actors prior to being de-serialized. \"\n                \"If you are sure that the data is safe from modification, you can \"\n                \" set allow_dangerous_deserialization=True to proceed. \"\n                \"Loading of compromised data using pickle can result in execution of \"\n                \"arbitrary code on your machine.\"\n            )\n        self.embedding = embedding\n        self.embedding_function = embedding.embed_query\n        self.index_uri = index_uri\n        self.metric = metric\n        self.config = config\n\n        tiledb_vs, tiledb = dependable_tiledb_import()\n        with tiledb.scope_ctx(ctx_or_config=config):\n            index_group = tiledb.Group(self.index_uri, \"r\")\n            self.vector_index_uri = (\n                vector_index_uri\n                if vector_index_uri != \"\"\n                else get_vector_index_uri_from_group(index_group)\n            )\n            self.docs_array_uri = (\n                docs_array_uri\n                if docs_array_uri != \"\"\n                else get_documents_array_uri_from_group(index_group)\n            )\n            index_group.close()\n            group = tiledb.Group(self.vector_index_uri, \"r\")\n            self.index_type = group.meta.get(\"index_type\")\n            group.close()\n            self.timestamp = timestamp\n            if self.index_type == \"FLAT\":\n                self.vector_index = tiledb_vs.flat_index.FlatIndex(\n                    uri=self.vector_index_uri,\n                    config=self.config,\n                    timestamp=self.timestamp,\n                    **kwargs,\n                )\n            elif self.index_type == \"IVF_FLAT\":\n                self.vector_index = tiledb_vs.ivf_flat_index.IVFFlatIndex(\n                    uri=self.vector_index_uri,\n                    config=self.config,\n                    timestamp=self.timestamp,\n                    **kwargs,\n                )\n```\n\nOriginal documentation:\n```python\nInitialize with necessary components.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize with necessary components.\n\n        Args:\n            allow_dangerous_deserialization: whether to allow deserialization\n                of the data which involves loading data using pickle.\n                data can be modified by malicious actors to deliver a\n                malicious payload that results in execution of\n                arbitrary code on your machine.", "file": "libs_community_langchain_community_vectorstores_tiledb.py", "function": "TileDB.__init__", "project": "langchain", "change_stats": {"code_changes": 14, "doc_changes": 9, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_input_tensors(dataset, config):\n  \n  batch_size = config.hparams.batch_size\n  iterator = dataset.make_one_shot_iterator()\n  (input_sequence, output_sequence, control_sequence,\n   sequence_length) = iterator.get_next()\n  input_sequence.set_shape(\n      [batch_size, None, config.data_converter.input_depth])\n  output_sequence.set_shape(\n      [batch_size, None, config.data_converter.output_depth])\n  control_sequence.set_shape(\n      [batch_size, None, config.data_converter.control_depth])\n  sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n  return {\n      'input_sequence': input_sequence,\n      'output_sequence': output_sequence,\n      'control_sequence': control_sequence,\n      'sequence_length': sequence_length\n  }```\n\nUpdated code:\n```python\ndef _get_input_tensors(dataset, config):\n  \n  batch_size = config.hparams.batch_size\n  iterator = dataset.make_one_shot_iterator()\n  (input_sequence, output_sequence, control_sequence,\n   sequence_length) = iterator.get_next()\n  input_sequence.set_shape(\n      [batch_size, None, config.data_converter.input_depth])\n  output_sequence.set_shape(\n      [batch_size, None, config.data_converter.output_depth])\n  if not config.data_converter.control_depth:\n    control_sequence = None\n  else:\n    control_sequence.set_shape(\n        [batch_size, None, config.data_converter.control_depth])\n  sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n\n  return {\n      'input_sequence': input_sequence,\n      'output_sequence': output_sequence,\n      'control_sequence': control_sequence,\n      'sequence_length': sequence_length\n  }\n```\n\nOriginal documentation:\n```python\nGet input tensors from dataset for training or evaluation.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get input tensors from dataset.", "file": "magenta_models_music_vae_music_vae_train.py", "function": "_get_input_tensors", "project": "magenta", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_ndim(self, values, ndim):\n        \n        \n        \n        if ndim is None:\n            ndim = values.ndim\n\n        if self._validate_ndim:\n            if values.ndim != ndim:\n                raise ValueError(\n                    \"Wrong number of dimensions. \"\n                    f\"values.ndim != ndim [{values.ndim} != {ndim}]\"\n                )\n        elif values.ndim > ndim:\n            # ExtensionBlock\n            raise ValueError(\n                \"Wrong number of dimensions. \"\n                f\"values.ndim > ndim [{values.ndim} > {ndim}]\"\n            )\n        return ndim```\n\nUpdated code:\n```python\ndef _check_ndim(self, values, ndim: int):\n        \n        \n        \n        assert isinstance(ndim, int)  # GH#38134 enforce this\n\n        if self._validate_ndim:\n            if values.ndim != ndim:\n                raise ValueError(\n                    \"Wrong number of dimensions. \"\n                    f\"values.ndim != ndim [{values.ndim} != {ndim}]\"\n                )\n        elif values.ndim > ndim:\n            # ExtensionBlock\n            raise ValueError(\n                \"Wrong number of dimensions. \"\n                f\"values.ndim > ndim [{values.ndim} > {ndim}]\"\n            )\n        return ndim\n```\n\nOriginal documentation:\n```python\nndim inference and validation.\n\n        Infers ndim from 'values' if not provided to __init__.\n        Validates that values.ndim and ndim are consistent if and only if\n        the class variable '_validate_ndim' is True.\n\n        Parameters\n        ----------\n        values : array-like\n        ndim : int or None\n\n        Returns\n        -------\n        ndim : int\n\n        Raises\n        ------\n        ValueError : the number of dimensions do not match\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "ndim inference and validation.\n\n        Infers ndim from 'values' if not provided to __init__.\n        Validates that values.ndim and ndim are consistent if and only if\n        the class variable '_validate_ndim' is True.\n\n        Parameters\n        ----------\n        values : array-like\n        ndim : int\n\n        Returns\n        -------\n        ndim : int\n\n        Raises\n        ------\n        ValueError : the number of dimensions do not match", "file": "pandas_core_internals_blocks.py", "function": "Block._check_ndim", "project": "pandas", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _update_faces(self, extractor_output):\n         \n        \n        for face in extractor_output.detected_faces:\n            frame_name = extractor_output.mask_tool_face_info[\"source_filename\"]\n            face_index = extractor_output.mask_tool_face_info[\"face_index\"]\n            logger.trace(\"Saving face: (frame: %s, face index: %s)\", frame_name, face_index)\n\n            self._alignments.update_face(frame_name, face_index, face.to_alignment())\n            metadata = dict(alignments=face.to_png_meta(),\n                            source=extractor_output.mask_tool_face_info)\n            self._faces_saver.save(extractor_output.filename,\n                                   encode_image(extractor_output.image, \".png\", metadata=metadata))\n\n            if self._saver is not None:\n                face.image = extractor_output.image\n                self._save(frame_name, face_index, face)```\n\nUpdated code:\n```python\ndef _update_faces(self, extractor_output: ExtractMedia) -> None:\n         \n        \n        assert self._faces_saver is not None\n        for face in extractor_output.detected_faces:\n            frame_name = extractor_output.frame_metadata[\"source_filename\"]\n            face_index = extractor_output.frame_metadata[\"face_index\"]\n            logger.trace(\"Saving face: (frame: %s, face index: %s)\",  # type: ignore\n                         frame_name, face_index)\n\n            self._alignments.update_face(frame_name, face_index, face.to_alignment())\n            metadata = dict(alignments=face.to_png_meta(),\n                            source=extractor_output.frame_metadata)\n            self._faces_saver.save(extractor_output.filename,\n                                   encode_image(extractor_output.image, \".png\", metadata=metadata))\n\n            if self._saver is not None:\n                face.image = extractor_output.image\n                self._save(frame_name, face_index, face)\n```\n\nOriginal documentation:\n```python\nUpdate alignments for the mask if the input type is a faces folder\n\n        If an output location has been indicated, then puts the mask preview to the save queue\n\n        Parameters\n        ----------\n        extractor_output: dict\n            The output from the :class:`plugins.extract.pipeline.Extractor` object\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update alignments for the mask if the input type is a faces folder\n\n        If an output location has been indicated, then puts the mask preview to the save queue\n\n        Parameters\n        ----------\n        extractor_output: :class:`plugins.extract.pipeline.ExtractMedia`\n            The output from the :class:`plugins.extract.pipeline.Extractor` object", "file": "tools_mask_mask.py", "function": "Mask._update_faces", "project": "faceswap", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_rgba(self, x, alpha=None, bytes=False, norm=True):\n        \n        \n\n        \n        # First check for special case, image input:\n        try:\n            if x.ndim == 3:\n                if x.shape[2] == 3:\n                    if alpha is None:\n                        alpha = 1\n                    if x.dtype == np.uint8:\n                        alpha = np.uint8(alpha * 255)\n                    m, n = x.shape[:2]\n                    xx = np.empty(shape=(m, n, 4), dtype=x.dtype)\n                    xx[:, :, :3] = x\n                    xx[:, :, 3] = alpha\n                elif x.shape[2] == 4:\n                    xx = x\n                else:\n                    raise ValueError(\"Third dimension must be 3 or 4\")\n                if xx.dtype.kind == 'f':\n                    if norm and (xx.max() > 1 or xx.min() < 0):\n                        raise ValueError(\"Floating point image RGB values \"\n                                         \"must be in the 0..1 range.\")\n                    if bytes:\n                        xx = (xx * 255).astype(np.uint8)\n                elif xx.dtype == np.uint8:\n                    if not bytes:\n                        xx = xx.astype(np.float32) / 255\n                else:\n                    raise ValueError(\"Image RGB array must be uint8 or \"\n                                     \"floating point; found %s\" % xx.dtype)\n                # Account for any masked entries in the original array\n                # If any of R, G, B, or A are masked for an entry, we set alpha to 0\n                if np.ma.is_masked(x):\n                    xx[np.any(np.ma.getmaskarray(x), axis=2), 3] = 0\n                return xx\n        except AttributeError:\n            # e.g., x is not an ndarray; so try mapping it\n            pass\n\n        # This is the normal case, mapping a scalar array:\n        x = ma.asarray(x)\n        if norm:\n            x = self.norm(x)\n        rgba = self.cmap(x, alpha=alpha, bytes=bytes)\n        return rgba```\n\nUpdated code:\n```python\ndef to_rgba(self, x, alpha=None, bytes=False, norm=True):\n        \n        \n\n        \n        # First check for special case, image input:\n        try:\n            if x.ndim == 3:\n                if x.shape[2] == 3:\n                    if alpha is None:\n                        alpha = 1\n                    if x.dtype == np.uint8:\n                        alpha = np.uint8(alpha * 255)\n                    m, n = x.shape[:2]\n                    xx = np.empty(shape=(m, n, 4), dtype=x.dtype)\n                    xx[:, :, :3] = x\n                    xx[:, :, 3] = alpha\n                elif x.shape[2] == 4:\n                    xx = x\n                else:\n                    raise ValueError(\"Third dimension must be 3 or 4\")\n                if xx.dtype.kind == 'f':\n                    # If any of R, G, B, or A is nan, set to 0\n                    if np.any(nans := np.isnan(x)):\n                        if xx.shape[2] == 4:\n                            xx = xx.copy()\n                        xx[np.any(nans, axis=2), :] = 0\n\n                    if norm and (xx.max() > 1 or xx.min() < 0):\n                        raise ValueError(\"Floating point image RGB values \"\n                                         \"must be in the 0..1 range.\")\n                    if bytes:\n                        xx = (xx * 255).astype(np.uint8)\n                elif xx.dtype == np.uint8:\n                    if not bytes:\n                        xx = xx.astype(np.float32) / 255\n                else:\n                    raise ValueError(\"Image RGB array must be uint8 or \"\n                                     \"floating point; found %s\" % xx.dtype)\n                # Account for any masked entries in the original array\n                # If any of R, G, B, or A are masked for an entry, we set alpha to 0\n                if np.ma.is_masked(x):\n                    xx[np.any(np.ma.getmaskarray(x), axis=2), 3] = 0\n                return xx\n        except AttributeError:\n            # e.g., x is not an ndarray; so try mapping it\n            pass\n\n        # This is the normal case, mapping a scalar array:\n        x = ma.asarray(x)\n        if norm:\n            x = self.norm(x)\n        rgba = self.cmap(x, alpha=alpha, bytes=bytes)\n        return rgba\n```\n\nOriginal documentation:\n```python\nReturn a normalized RGBA array corresponding to *x*.\n\n        In the normal case, *x* is a 1D or 2D sequence of scalars, and\n        the corresponding `~numpy.ndarray` of RGBA values will be returned,\n        based on the norm and colormap set for this ScalarMappable.\n\n        There is one special case, for handling images that are already\n        RGB or RGBA, such as might have been read from an image file.\n        If *x* is an `~numpy.ndarray` with 3 dimensions,\n        and the last dimension is either 3 or 4, then it will be\n        treated as an RGB or RGBA array, and no mapping will be done.\n        The array can be `~numpy.uint8`, or it can be floats with\n        values in the 0-1 range; otherwise a ValueError will be raised.\n        If it is a masked array, any masked elements will be set to 0 alpha.\n        If the last dimension is 3, the *alpha* kwarg (defaulting to 1)\n        will be used to fill in the transparency.  If the last dimension\n        is 4, the *alpha* kwarg is ignored; it does not\n        replace the preexisting alpha.  A ValueError will be raised\n        if the third dimension is other than 3 or 4.\n\n        In either case, if *bytes* is *False* (default), the RGBA\n        array will be floats in the 0-1 range; if it is *True*,\n        the returned RGBA array will be `~numpy.uint8` in the 0 to 255 range.\n\n        If norm is False, no normalization of the input data is\n        performed, and it is assumed to be in the range (0-1).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a normalized RGBA array corresponding to *x*.\n\n        In the normal case, *x* is a 1D or 2D sequence of scalars, and\n        the corresponding `~numpy.ndarray` of RGBA values will be returned,\n        based on the norm and colormap set for this ScalarMappable.\n\n        There is one special case, for handling images that are already\n        RGB or RGBA, such as might have been read from an image file.\n        If *x* is an `~numpy.ndarray` with 3 dimensions,\n        and the last dimension is either 3 or 4, then it will be\n        treated as an RGB or RGBA array, and no mapping will be done.\n        The array can be `~numpy.uint8`, or it can be floats with\n        values in the 0-1 range; otherwise a ValueError will be raised.\n        Any NaNs or masked elements will be set to 0 alpha.\n        If the last dimension is 3, the *alpha* kwarg (defaulting to 1)\n        will be used to fill in the transparency.  If the last dimension\n        is 4, the *alpha* kwarg is ignored; it does not\n        replace the preexisting alpha.  A ValueError will be raised\n        if the third dimension is other than 3 or 4.\n\n        In either case, if *bytes* is *False* (default), the RGBA\n        array will be floats in the 0-1 range; if it is *True*,\n        the returned RGBA array will be `~numpy.uint8` in the 0 to 255 range.\n\n        If norm is False, no normalization of the input data is\n        performed, and it is assumed to be in the range (0-1).", "file": "lib_matplotlib_cm.py", "function": "ScalarMappable.to_rgba", "project": "matplotlib", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef polygon_perimeter(cy, cx, shape=None, clip=False):\n    \n\n    \n    if clip:\n        if shape is None:\n            raise ValueError(\"Must specify clipping shape\")\n        clip_box = np.array([0, 0, shape[0] - 1, shape[1] - 1])\n    else:\n        clip_box = np.array([np.min(cy), np.min(cx),\n                             np.max(cy), np.max(cx)])\n\n    # Do the clipping irrespective of whether clip is set.  This\n    # ensures that the returned polygon is closed and is an array.\n    cy, cx = polygon_clip(cy, cx, *clip_box)\n\n    cy = np.round(cy).astype(int)\n    cx = np.round(cx).astype(int)\n\n    # Construct line segments\n    rr, cc = [], []\n    for i in range(len(cy) - 1):\n        line_r, line_c = line(cy[i], cx[i], cy[i + 1], cx[i + 1])\n        rr.extend(line_r)\n        cc.extend(line_c)\n\n    rr = np.asarray(rr)\n    cc = np.asarray(cc)\n\n    if shape is None:\n        return rr, cc\n    else:\n        return _coords_inside_image(rr, cc, shape)```\n\nUpdated code:\n```python\ndef polygon_perimeter(cr, cc, shape=None, clip=False):\n    \n\n    \n    if clip:\n        if shape is None:\n            raise ValueError(\"Must specify clipping shape\")\n        clip_box = np.array([0, 0, shape[0] - 1, shape[1] - 1])\n    else:\n        clip_box = np.array([np.min(cr), np.min(cc),\n                             np.max(cr), np.max(cc)])\n\n    # Do the clipping irrespective of whether clip is set.  This\n    # ensures that the returned polygon is closed and is an array.\n    cr, cc = polygon_clip(cr, cc, *clip_box)\n\n    cr = np.round(cr).astype(int)\n    cc = np.round(cc).astype(int)\n\n    # Construct line segments\n    pr, pc = [], []\n    for i in range(len(cr) - 1):\n        line_r, line_c = line(cr[i], cc[i], cr[i + 1], cc[i + 1])\n        pr.extend(line_r)\n        pc.extend(line_c)\n\n    pr = np.asarray(pr)\n    pc = np.asarray(pc)\n\n    if shape is None:\n        return pr, pc\n    else:\n        return _coords_inside_image(pr, pc, shape)\n```\n\nOriginal documentation:\n```python\nGenerate polygon perimeter coordinates.\n\n    Parameters\n    ----------\n    cy : (N,) ndarray\n        Y-coordinates of vertices of polygon.\n    cx : (N,) ndarray\n        X-coordinates of vertices of polygon.\n    shape : tuple, optional\n        Image shape which is used to determine maximum extents of output pixel\n        coordinates. This is useful for polygons which exceed the image size.\n        By default the full extents of the polygon are used.\n    clip : bool, optional\n        Whether to clip the polygon to the provided shape.  If this is set\n        to True, the drawn figure will always be a closed polygon with all\n        edges visible.\n\n    Returns\n    -------\n    rr, cc : ndarray of int\n        Pixel coordinates of polygon.\n        May be used to directly index into an array, e.g.\n        ``img[rr, cc] = 1``.\n\n    Examples\n    --------\n    >>> from skimage.draw import polygon_perimeter\n    >>> img = np.zeros((10, 10), dtype=np.uint8)\n    >>> rr, cc = polygon_perimeter([5, -1, 5, 10],\n    ...                            [-1, 5, 11, 5],\n    ...                            shape=img.shape, clip=True)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n           [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n           [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n           [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n           [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n           [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n           [0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n           [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate polygon perimeter coordinates.\n\n    Parameters\n    ----------\n    cr : (N,) ndarray\n        Row (Y) coordinates of vertices of polygon.\n    cc : (N,) ndarray\n        Column (X) coordinates of vertices of polygon.\n    shape : tuple, optional\n        Image shape which is used to determine maximum extents of output pixel\n        coordinates. This is useful for polygons which exceed the image size.\n        By default the full extents of the polygon are used.\n    clip : bool, optional\n        Whether to clip the polygon to the provided shape.  If this is set\n        to True, the drawn figure will always be a closed polygon with all\n        edges visible.\n\n    Returns\n    -------\n    pr, pc : ndarray of int\n        Pixel coordinates of polygon.\n        May be used to directly index into an array, e.g.\n        ``img[pr, pc] = 1``.\n\n    Examples\n    --------\n    >>> from skimage.draw import polygon_perimeter\n    >>> img = np.zeros((10, 10), dtype=np.uint8)\n    >>> rr, cc = polygon_perimeter([5, -1, 5, 10],\n    ...                            [-1, 5, 11, 5],\n    ...                            shape=img.shape, clip=True)\n    >>> img[rr, cc] = 1\n    >>> img\n    array([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n           [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n           [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n           [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n           [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n           [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n           [0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n           [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)", "file": "skimage_draw_draw.py", "function": "polygon_perimeter", "project": "scikit-image", "change_stats": {"code_changes": 32, "doc_changes": 14, "total_changes": 46}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _update_label_postion(self, bboxes):\n        \n        \n        \n\n        x,y = self.label.get_position()\n        if not len(bboxes):\n            left = self.axes.bbox.xmin()\n        else:\n            bbox = bbox_all(bboxes)\n            left = bbox.xmin()\n        self.label.set_position((left - self.figure.dpi.get()*self.LABELPAD/72.0,y))```\n\nUpdated code:\n```python\ndef _update_label_postion(self, bboxes, bboxes2):\n        \n         \n        \n\n        x,y = self.label.get_position()\n        if self.label_position == 'left':\n            if not len(bboxes):\n                left = self.axes.bbox.xmin()\n            else:\n\n                bbox = bbox_all(bboxes)\n                left = bbox.xmin()\n            \n            self.label.set_position( (left-self.LABELPAD*self.figure.dpi.get()/72.0, y))\n\n        else:\n            if not len(bboxes2):\n                right = self.axes.bbox.xmax()\n            else:\n\n                bbox = bbox_all(bboxes2)\n                right = bbox.xmax()\n            \n            self.label.set_position( (right+self.LABELPAD*self.figure.dpi.get()/72.0, y))\n```\n\nOriginal documentation:\n```python\nUpdate the label position based on the sequence of bounding\n        boxes overlaps of all the ticklabels that overlap the current\n        ticklabel.  overlaps are the bounding boxes of the ticklabels\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update the label position based on the sequence of bounding\n        boxes of all the ticklabels", "file": "lib_matplotlib_axis.py", "function": "YAxis._update_label_postion", "project": "matplotlib", "change_stats": {"code_changes": 29, "doc_changes": 5, "total_changes": 34}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef backward(self, gradient=None, retain_graph=None, create_graph=None, retain_variables=None):\n        \n        \n        torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)```\n\nUpdated code:\n```python\ndef backward(self, gradient=None, retain_graph=None, create_graph=False):\n        \n        \n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n```\n\nOriginal documentation:\n```python\nComputes the gradient of current variable w.r.t. graph leaves.\n\n        The graph is differentiated using the chain rule. If the variable is\n        non-scalar (i.e. its data has more than one element) and requires\n        gradient, the function additionally requires specifying ``gradient``.\n        It should be a tensor of matching type and location, that contains\n        the gradient of the differentiated function w.r.t. ``self``.\n\n        This function accumulates gradients in the leaves - you might need to\n        zero them before calling it.\n\n        Arguments:\n            gradient (Tensor, Variable or None): Gradient w.r.t. the\n                variable. If it is a tensor, it will be automatically converted\n                to a Variable that is volatile unless ``create_graph`` is True.\n                None values can be specified for scalar Variables or ones that\n                don't require grad. If a None value would be acceptable then\n                this argument is optional.\n            retain_graph (bool, optional): If ``False``, the graph used to compute\n                the grads will be freed. Note that in nearly all cases setting\n                this option to True is not needed and often can be worked around\n                in a much more efficient way. Defaults to the value of\n                ``create_graph``.\n            create_graph (bool, optional): If ``True``, graph of the derivative will\n                be constructed, allowing to compute higher order derivative\n                products. Defaults to ``False``, unless ``gradient`` is a volatile\n                Variable.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes the gradient of current variable w.r.t. graph leaves.\n\n        The graph is differentiated using the chain rule. If the variable is\n        non-scalar (i.e. its data has more than one element) and requires\n        gradient, the function additionally requires specifying ``gradient``.\n        It should be a tensor of matching type and location, that contains\n        the gradient of the differentiated function w.r.t. ``self``.\n\n        This function accumulates gradients in the leaves - you might need to\n        zero them before calling it.\n\n        Arguments:\n            gradient (Tensor, Variable or None): Gradient w.r.t. the\n                variable. If it is a tensor, it will be automatically converted\n                to a Variable that does not require grad unless ``create_graph`` is True.\n                None values can be specified for scalar Variables or ones that\n                don't require grad. If a None value would be acceptable then\n                this argument is optional.\n            retain_graph (bool, optional): If ``False``, the graph used to compute\n                the grads will be freed. Note that in nearly all cases setting\n                this option to True is not needed and often can be worked around\n                in a much more efficient way. Defaults to the value of\n                ``create_graph``.\n            create_graph (bool, optional): If ``True``, graph of the derivative will\n                be constructed, allowing to compute higher order derivative\n                products. Defaults to ``False``.", "file": "torch_autograd_variable.py", "function": "Variable.backward", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 7, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef back_propagation(self) -> None:\n        \n        \n        \n\n        updated_second_hidden_layer_and_output_layer_weights = numpy.dot(\n            self.layer_between_first_hidden_layer_and_second_hidden_layer.T,\n            2\n            * (self.output_array - self.predicted_output)\n            * sigmoid_derivative(self.predicted_output),\n        )\n        updated_first_hidden_layer_and_second_hidden_layer_weights = numpy.dot(\n            self.layer_between_input_and_first_hidden_layer.T,\n            numpy.dot(\n                2\n                * (self.output_array - self.predicted_output)\n                * sigmoid_derivative(self.predicted_output),\n                self.second_hidden_layer_and_output_layer_weights.T,\n            )\n            * sigmoid_derivative(\n                self.layer_between_first_hidden_layer_and_second_hidden_layer\n            ),\n        )\n        updated_input_layer_and_first_hidden_layer_weights = numpy.dot(\n            self.input_array.T,\n            numpy.dot(\n                numpy.dot(\n                    2\n                    * (self.output_array - self.predicted_output)\n                    * sigmoid_derivative(self.predicted_output),\n                    self.second_hidden_layer_and_output_layer_weights.T,\n                )\n                * sigmoid_derivative(\n                    self.layer_between_first_hidden_layer_and_second_hidden_layer\n                ),\n                self.first_hidden_layer_and_second_hidden_layer_weights.T,\n            )\n            * sigmoid_derivative(self.layer_between_input_and_first_hidden_layer),\n        )\n\n        self.input_layer_and_first_hidden_layer_weights += (\n            updated_input_layer_and_first_hidden_layer_weights\n        )\n        self.first_hidden_layer_and_second_hidden_layer_weights += (\n            updated_first_hidden_layer_and_second_hidden_layer_weights\n        )\n        self.second_hidden_layer_and_output_layer_weights += (\n            updated_second_hidden_layer_and_output_layer_weights\n        )```\n\nUpdated code:\n```python\ndef back_propagation(self) -> None:\n        \n        \n        \n\n        updated_second_hidden_layer_and_output_layer_weights = np.dot(\n            self.layer_between_first_hidden_layer_and_second_hidden_layer.T,\n            2\n            * (self.output_array - self.predicted_output)\n            * sigmoid_derivative(self.predicted_output),\n        )\n        updated_first_hidden_layer_and_second_hidden_layer_weights = np.dot(\n            self.layer_between_input_and_first_hidden_layer.T,\n            np.dot(\n                2\n                * (self.output_array - self.predicted_output)\n                * sigmoid_derivative(self.predicted_output),\n                self.second_hidden_layer_and_output_layer_weights.T,\n            )\n            * sigmoid_derivative(\n                self.layer_between_first_hidden_layer_and_second_hidden_layer\n            ),\n        )\n        updated_input_layer_and_first_hidden_layer_weights = np.dot(\n            self.input_array.T,\n            np.dot(\n                np.dot(\n                    2\n                    * (self.output_array - self.predicted_output)\n                    * sigmoid_derivative(self.predicted_output),\n                    self.second_hidden_layer_and_output_layer_weights.T,\n                )\n                * sigmoid_derivative(\n                    self.layer_between_first_hidden_layer_and_second_hidden_layer\n                ),\n                self.first_hidden_layer_and_second_hidden_layer_weights.T,\n            )\n            * sigmoid_derivative(self.layer_between_input_and_first_hidden_layer),\n        )\n\n        self.input_layer_and_first_hidden_layer_weights += (\n            updated_input_layer_and_first_hidden_layer_weights\n        )\n        self.first_hidden_layer_and_second_hidden_layer_weights += (\n            updated_first_hidden_layer_and_second_hidden_layer_weights\n        )\n        self.second_hidden_layer_and_output_layer_weights += (\n            updated_second_hidden_layer_and_output_layer_weights\n        )\n```\n\nOriginal documentation:\n```python\nFunction for fine-tuning the weights of the neural net based on the\n        error rate obtained in the previous epoch (i.e., iteration).\n        Updation is done using derivative of sogmoid activation function.\n\n        >>> input_val = numpy.array(([0, 0, 0], [0, 0, 0], [0, 0, 0]), dtype=float)\n        >>> output_val = numpy.array(([0], [0], [0]), dtype=float)\n        >>> nn = TwoHiddenLayerNeuralNetwork(input_val, output_val)\n        >>> res = nn.feedforward()\n        >>> nn.back_propagation()\n        >>> updated_weights = nn.second_hidden_layer_and_output_layer_weights\n        >>> (res == updated_weights).all()\n        False\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Function for fine-tuning the weights of the neural net based on the\n        error rate obtained in the previous epoch (i.e., iteration).\n        Updation is done using derivative of sogmoid activation function.\n\n        >>> input_val = np.array(([0, 0, 0], [0, 0, 0], [0, 0, 0]), dtype=float)\n        >>> output_val = np.array(([0], [0], [0]), dtype=float)\n        >>> nn = TwoHiddenLayerNeuralNetwork(input_val, output_val)\n        >>> res = nn.feedforward()\n        >>> nn.back_propagation()\n        >>> updated_weights = nn.second_hidden_layer_and_output_layer_weights\n        >>> (res == updated_weights).all()\n        False", "file": "neural_network_two_hidden_layers_neural_network.py", "function": "TwoHiddenLayerNeuralNetwork.back_propagation", "project": "Python", "change_stats": {"code_changes": 14, "doc_changes": 6, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(\n        self,\n        x: Union[Tensor, Dict[NodeType, Tensor]],\n        edge_index: Union[Tensor, Dict[EdgeType, Tensor]],\n        *,\n        target: Optional[Tensor] = None,\n        index: Optional[Union[int, Tensor]] = None,\n        **kwargs,\n    ) -> Union[Explanation, HeteroExplanation]:\n        \n        \n        # Choose the `target` depending on the explanation type:\n        prediction: Optional[Tensor] = None\n        if self.explanation_type == ExplanationType.phenomenon:\n            if target is None:\n                raise ValueError(\n                    f\"The 'target' has to be provided for the explanation \"\n                    f\"type '{self.explanation_type.value}'\")\n        elif self.explanation_type == ExplanationType.model:\n            if target is not None:\n                warnings.warn(\n                    f\"The 'target' should not be provided for the explanation \"\n                    f\"type '{self.explanation_type.value}'\")\n            prediction = self.get_prediction(x, edge_index, **kwargs)\n            target = self.get_target(prediction)\n\n        training = self.model.training\n        self.model.eval()\n\n        explanation = self.algorithm(\n            self.model,\n            x,\n            edge_index,\n            target=target,\n            index=index,\n            **kwargs,\n        )\n\n        self.model.train(training)\n\n        # Add explainer objectives to the `Explanation` object:\n        explanation._model_config = self.model_config\n        explanation.prediction = prediction\n        explanation.target = target\n        explanation.index = index\n\n        # Add model inputs to the `Explanation` object:\n        if isinstance(explanation, Explanation):\n            explanation._model_args = list(kwargs.keys())\n            explanation.x = x\n            explanation.edge_index = edge_index\n\n            for key, arg in kwargs.items():  # Add remaining `kwargs`:\n                explanation[key] = arg\n\n        elif isinstance(explanation, HeteroExplanation):\n            # TODO Add `explanation._model_args`\n\n            assert isinstance(x, dict)\n            explanation.set_value_dict('x', x)\n\n            assert isinstance(edge_index, dict)\n            explanation.set_value_dict('edge_index', edge_index)\n\n            for key, arg in kwargs.items():  # Add remaining `kwargs`:\n                if isinstance(arg, dict):\n                    # Keyword arguments are likely named `{attr_name}_dict`\n                    # while we only want to assign the `{attr_name}` to the\n                    # `HeteroExplanation` object:\n                    key = key[:-5] if key.endswith('_dict') else key\n                    explanation.set_value_dict(key, arg)\n                else:\n                    explanation[key] = arg\n\n        explanation.validate_masks()\n        return explanation.threshold(self.threshold_config)```\n\nUpdated code:\n```python\ndef __call__(\n        self,\n        x: Union[Tensor, Dict[NodeType, Tensor]],\n        edge_index: Union[Tensor, Dict[EdgeType, Tensor]],\n        *,\n        target: Optional[Tensor] = None,\n        index: Optional[Union[int, Tensor]] = None,\n        **kwargs,\n    ) -> Union[Explanation, HeteroExplanation]:\n        \n        \n        # Choose the `target` depending on the explanation type:\n        prediction: Optional[Tensor] = None\n        if self.explanation_type == ExplanationType.phenomenon:\n            if target is None:\n                raise ValueError(\n                    f\"The 'target' has to be provided for the explanation \"\n                    f\"type '{self.explanation_type.value}'\")\n        elif self.explanation_type == ExplanationType.model:\n            if target is not None:\n                warnings.warn(\n                    f\"The 'target' should not be provided for the explanation \"\n                    f\"type '{self.explanation_type.value}'\")\n            prediction = self.get_prediction(x, edge_index, **kwargs)\n            target = self.get_target(prediction)\n\n        if isinstance(index, int):\n            index = torch.tensor([index])\n\n        training = self.model.training\n        self.model.eval()\n\n        explanation = self.algorithm(\n            self.model,\n            x,\n            edge_index,\n            target=target,\n            index=index,\n            **kwargs,\n        )\n\n        self.model.train(training)\n\n        # Add explainer objectives to the `Explanation` object:\n        explanation._model_config = self.model_config\n        explanation.prediction = prediction\n        explanation.target = target\n        explanation.index = index\n\n        # Add model inputs to the `Explanation` object:\n        if isinstance(explanation, Explanation):\n            explanation._model_args = list(kwargs.keys())\n            explanation.x = x\n            explanation.edge_index = edge_index\n\n            for key, arg in kwargs.items():  # Add remaining `kwargs`:\n                explanation[key] = arg\n\n        elif isinstance(explanation, HeteroExplanation):\n            # TODO Add `explanation._model_args`\n\n            assert isinstance(x, dict)\n            explanation.set_value_dict('x', x)\n\n            assert isinstance(edge_index, dict)\n            explanation.set_value_dict('edge_index', edge_index)\n\n            for key, arg in kwargs.items():  # Add remaining `kwargs`:\n                if isinstance(arg, dict):\n                    # Keyword arguments are likely named `{attr_name}_dict`\n                    # while we only want to assign the `{attr_name}` to the\n                    # `HeteroExplanation` object:\n                    key = key[:-5] if key.endswith('_dict') else key\n                    explanation.set_value_dict(key, arg)\n                else:\n                    explanation[key] = arg\n\n        explanation.validate_masks()\n        return explanation.threshold(self.threshold_config)\n```\n\nOriginal documentation:\n```python\nr\"\"\"Computes the explanation of the GNN for the given inputs and\n        target.\n\n        .. note::\n\n            If you get an error message like \"Trying to backward through the\n            graph a second time\", make sure that the target you provided\n            was computed with :meth:`torch.no_grad`.\n\n        Args:\n            x (Union[torch.Tensor, Dict[NodeType, torch.Tensor]]): The input\n                node features of a homogeneous or heterogeneous graph.\n            edge_index (Union[torch.Tensor, Dict[NodeType, torch.Tensor]]): The\n                input edge indices of a homogeneous or heterogeneous graph.\n            target (torch.Tensor): The target of the model.\n                If the explanation type is :obj:`\"phenomenon\"`, the target has\n                to be provided.\n                If the explanation type is :obj:`\"model\"`, the target should be\n                set to :obj:`None` and will get automatically inferred.\n                (default: :obj:`None`)\n            index (Union[int, Tensor], optional): The index of the model\n                output to explain. Can be a single index or a tensor of\n                indices. (default: :obj:`None`)\n            **kwargs: additional arguments to pass to the GNN.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Computes the explanation of the GNN for the given inputs and\n        target.\n\n        .. note::\n\n            If you get an error message like \"Trying to backward through the\n            graph a second time\", make sure that the target you provided\n            was computed with :meth:`torch.no_grad`.\n\n        Args:\n            x (Union[torch.Tensor, Dict[NodeType, torch.Tensor]]): The input\n                node features of a homogeneous or heterogeneous graph.\n            edge_index (Union[torch.Tensor, Dict[NodeType, torch.Tensor]]): The\n                input edge indices of a homogeneous or heterogeneous graph.\n            target (torch.Tensor): The target of the model.\n                If the explanation type is :obj:`\"phenomenon\"`, the target has\n                to be provided.\n                If the explanation type is :obj:`\"model\"`, the target should be\n                set to :obj:`None` and will get automatically inferred. For\n                classification tasks, the target needs to contain the class\n                labels. (default: :obj:`None`)\n            index (Union[int, Tensor], optional): The indices in the\n                first-dimension of the model output to explain.\n                Can be a single index or a tensor of indices.\n                If set to :obj:`None`, all model outputs will be explained.\n                (default: :obj:`None`)\n            **kwargs: additional arguments to pass to the GNN.", "file": "torch_geometric_explain_explainer.py", "function": "Explainer.__call__", "project": "pytorch_geometric", "change_stats": {"code_changes": 5, "doc_changes": 13, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    \n\n    X_train_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_categories = 3\n    n_samples = X_train_array.shape[0]\n\n    if categories == \"auto\":\n        X_train = X_train_array\n    else:\n        X_train = categories[0][X_train_array]\n\n    if categories == \"auto\":\n        X_test = X_test_array\n    else:\n        X_test = categories[0][X_test_array]\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n\n    rng = np.random.RandomState(global_random_seed)\n\n    if target_type == \"binary\":\n        y_int = rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array([\"cat\", \"dog\"], dtype=object)\n        y_train = target_names[y_int]\n        cv = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)\n    else:  # target_type == continuous\n        y_int = rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_int\n        cv = KFold(n_splits=3, random_state=0, shuffle=True)\n\n    shuffled_idx = rng.permutation(n_samples)\n    X_train_array = X_train_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_int = y_int[shuffled_idx]\n\n    # Get encodings for cv splits to validate `fit_transform`\n    expected_X_fit_transform = np.empty_like(X_train_array, dtype=np.float64)\n\n    for train_idx, test_idx in cv.split(X_train_array, y_train):\n        X_, y_ = X_train_array[train_idx, 0], y_int[train_idx]\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[\n            X_train_array[test_idx, 0]\n        ]\n\n    target_encoder = TargetEncoder(\n        smooth=smooth, categories=categories, cv=3, random_state=0\n    )\n\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n\n    # compute encodings for all data to validate `transform`\n    y_mean = np.mean(y_int)\n    expected_encodings = _encode_target(\n        X_train_array[:, 0], y_int, n_categories, smooth\n    )\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n\n    # Transform on test data, the last value is unknown is it is encoded as the target\n    # mean\n    expected_X_test_transform = np.concatenate(\n        (expected_encodings, np.array([y_mean]))\n    ).reshape(-1, 1)\n\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)```\n\nUpdated code:\n```python\ndef test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):\n    \n    \n\n    n_categories = 3\n    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T\n    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T\n    n_samples = X_train_int_array.shape[0]\n\n    if categories == \"auto\":\n        X_train = X_train_int_array\n        X_test = X_test_int_array\n    else:\n        X_train = categories[0][X_train_int_array]\n        X_test = categories[0][X_test_int_array]\n\n    X_test = np.concatenate((X_test, [[unknown_value]]))\n\n    data_rng = np.random.RandomState(global_random_seed)\n    n_splits = 3\n    if target_type == \"binary\":\n        y_int = data_rng.randint(low=0, high=2, size=n_samples)\n        target_names = np.array([\"cat\", \"dog\"], dtype=object)\n        y_train = target_names[y_int]\n\n    else:  # target_type == continuous\n        y_int = data_rng.uniform(low=-10, high=20, size=n_samples)\n        y_train = y_int\n\n    shuffled_idx = data_rng.permutation(n_samples)\n    X_train_int_array = X_train_int_array[shuffled_idx]\n    X_train = X_train[shuffled_idx]\n    y_train = y_train[shuffled_idx]\n    y_int = y_int[shuffled_idx]\n\n    # Define our CV splitting strategy\n    if target_type == \"binary\":\n        cv = StratifiedKFold(\n            n_splits=n_splits, random_state=global_random_seed, shuffle=True\n        )\n    else:\n        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)\n\n    # Compute the expected values using our reference Python implementation of\n    # target encoding:\n    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)\n\n    for train_idx, test_idx in cv.split(X_train_int_array, y_train):\n        X_, y_ = X_train_int_array[train_idx, 0], y_int[train_idx]\n        cur_encodings = _encode_target(X_, y_, n_categories, smooth)\n        expected_X_fit_transform[test_idx, 0] = cur_encodings[\n            X_train_int_array[test_idx, 0]\n        ]\n\n    # Check that we can obtain the same encodings by calling `fit_transform` on\n    # the estimator with the same CV parameters:\n    target_encoder = TargetEncoder(\n        smooth=smooth,\n        categories=categories,\n        cv=n_splits,\n        random_state=global_random_seed,\n    )\n\n    X_fit_transform = target_encoder.fit_transform(X_train, y_train)\n\n    assert target_encoder.target_type_ == target_type\n    assert_allclose(X_fit_transform, expected_X_fit_transform)\n    assert len(target_encoder.encodings_) == 1\n\n    # compute encodings for all data to validate `transform`\n    y_mean = np.mean(y_int)\n    expected_encodings = _encode_target(\n        X_train_int_array[:, 0], y_int, n_categories, smooth\n    )\n    assert_allclose(target_encoder.encodings_[0], expected_encodings)\n    assert target_encoder.target_mean_ == pytest.approx(y_mean)\n\n    # Transform on test data, the last value is unknown so it is encoded as the target\n    # mean\n    expected_X_test_transform = np.concatenate(\n        (expected_encodings, np.array([y_mean]))\n    ).reshape(-1, 1)\n\n    X_test_transform = target_encoder.transform(X_test)\n    assert_allclose(X_test_transform, expected_X_test_transform)\n```\n\nOriginal documentation:\n```python\nCheck encoding for binary and continuous targets.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check encoding for binary and continuous targets.\n\n    Compare the values returned by `TargetEncoder.fit_transform` against the\n    expected encodings for cv splits from a naive reference Python\n    implementation in _encode_target.", "file": "sklearn_preprocessing_tests_test_target_encoder.py", "function": "test_encoding", "project": "scikit-learn", "change_stats": {"code_changes": 64, "doc_changes": 6, "total_changes": 70}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add(self, lines: LineCollectionLike, layer_id: Union[None, int] = None) -> None:\n        \n        \n        if layer_id is None:\n            layer_id = 1\n            while layer_id in self._layers:\n                layer_id += 1\n\n        if layer_id not in self._layers:\n            self._layers[layer_id] = LineCollection()\n\n        self._layers[layer_id].extend(lines)```\n\nUpdated code:\n```python\ndef add(\n        self,\n        lines: LineCollectionLike,\n        layer_id: Union[None, int] = None,\n        with_metadata: bool = False,\n    ) -> None:\n        \n        \n        if layer_id is None:\n            layer_id = 1\n            while layer_id in self._layers:\n                layer_id += 1\n\n        if layer_id not in self._layers:\n            self._layers[layer_id] = LineCollection()\n\n        self._layers[layer_id].extend(lines)\n\n        if with_metadata and isinstance(lines, LineCollection):\n            self._layers[layer_id].metadata.update(lines.metadata)\n```\n\nOriginal documentation:\n```python\nAdd a the content of a :py:class:`LineCollection` to a given layer.\n\n        If the given layer is None, a new layer with the lowest available layer ID is created\n        (with emtpy metadata) and initialized with the content of ``lc``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add a the content of a :py:class:`LineCollection` to a given layer.\n\n        If the given layer ID is :data:`None`, a new layer with the lowest available layer ID\n        is created and initialized with the content of ``lc``.\n\n        The destination layer's metadata is unchanged, unless ``with_metadata`` is\n        :data:`True`.\n\n        Args:\n            lines: lines to add to the layer\n            layer_id: the destination layer ID, or :data:`None` for a new layer\n            with_metadata: copies/update the metadata to the destination layer (if any)", "file": "vpype_model.py", "function": "Document.add", "project": "vpype", "change_stats": {"code_changes": 12, "doc_changes": 14, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _decrypt_signature(self, s):\n        \n\n        if len(s) == 88:\n            return s[48] + s[81:67:-1] + s[82] + s[66:62:-1] + s[85] + s[61:48:-1] + s[67] + s[47:12:-1] + s[3] + s[11:3:-1] + s[2] + s[12]\n        elif len(s) == 87:\n            return s[62] + s[82:62:-1] + s[83] + s[61:52:-1] + s[0] + s[51:2:-1]\n        elif len(s) == 86:\n            return s[2:63] + s[82] + s[64:82] + s[63]\n        elif len(s) == 85:\n            return s[76] + s[82:76:-1] + s[83] + s[75:60:-1] + s[0] + s[59:50:-1] + s[1] + s[49:2:-1]\n        elif len(s) == 84:\n            return s[83:36:-1] + s[2] + s[35:26:-1] + s[3] + s[25:3:-1] + s[26]\n        elif len(s) == 83:\n            return s[52] + s[81:55:-1] + s[2] + s[54:52:-1] + s[82] + s[51:36:-1] + s[55] + s[35:2:-1] + s[36]\n        elif len(s) == 82:\n            return s[36] + s[79:67:-1] + s[81] + s[66:40:-1] + s[33] + s[39:36:-1] + s[40] + s[35] + s[0] + s[67] + s[32:0:-1] + s[34]\n\n        else:\n            raise ExtractorError(u'Unable to decrypt signature, subkeys length %d not supported; retrying might work' % (len(s)))```\n\nUpdated code:\n```python\ndef _decrypt_signature(self, s):\n        \n\n        if len(s) == 88:\n            return s[48] + s[81:67:-1] + s[82] + s[66:62:-1] + s[85] + s[61:48:-1] + s[67] + s[47:12:-1] + s[3] + s[11:3:-1] + s[2] + s[12]\n        elif len(s) == 87:\n            return s[62] + s[82:62:-1] + s[83] + s[61:52:-1] + s[0] + s[51:2:-1]\n        elif len(s) == 86:\n            return s[2:63] + s[82] + s[64:82] + s[63]\n        elif len(s) == 85:\n            return s[76] + s[82:76:-1] + s[83] + s[75:60:-1] + s[0] + s[59:50:-1] + s[1] + s[49:2:-1]\n        elif len(s) == 84:\n            return s[83:36:-1] + s[2] + s[35:26:-1] + s[3] + s[25:3:-1] + s[26]\n        elif len(s) == 83:\n            return s[52] + s[81:55:-1] + s[2] + s[54:52:-1] + s[82] + s[51:36:-1] + s[55] + s[35:2:-1] + s[36]\n        elif len(s) == 82:\n            return s[36] + s[79:67:-1] + s[81] + s[66:40:-1] + s[33] + s[39:36:-1] + s[40] + s[35] + s[0] + s[67] + s[32:0:-1] + s[34]\n\n        else:\n            raise ExtractorError(u'Unable to decrypt signature, key length %d not supported; retrying might work' % (len(s)))\n```\n\nOriginal documentation:\n```python\nDecrypt the key the two subkeys must have a length of 43\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Decrypt the key", "file": "youtube_dl_extractor_youtube.py", "function": "YoutubeIE._decrypt_signature", "project": "youtube-dl", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_comment(reference_doctype, reference_name, content, comment_email, comment_by):\n\t\n\tdoc = frappe.get_doc(\n\t\tdict(\n\t\t\tdoctype=\"Comment\",\n\t\t\treference_doctype=reference_doctype,\n\t\t\treference_name=reference_name,\n\t\t\tcomment_email=comment_email,\n\t\t\tcomment_type=\"Comment\",\n\t\t\tcomment_by=comment_by,\n\t\t)\n\t)\n\treference_doc = frappe.get_doc(reference_doctype, reference_name)\n\tdoc.content = extract_images_from_html(reference_doc, content, is_private=True)\n\tdoc.insert(ignore_permissions=True)\n\tif frappe.get_cached_value(\"User\", frappe.session.user, \"follow_commented_documents\"):\n\t\tfollow_document(doc.reference_doctype, doc.reference_name, frappe.session.user)\n\treturn doc.as_dict()```\n\nUpdated code:\n```python\ndef add_comment(\n\treference_doctype: str, reference_name: str, content: str, comment_email: str, comment_by: str\n) -> \"Comment\":\n\t\n\treference_doc = frappe.get_doc(reference_doctype, reference_name)\n\treference_doc.check_permission()\n\n\tcomment = frappe.new_doc(\"Comment\")\n\tcomment.update(\n\t\t{\n\t\t\t\"comment_type\": \"Comment\",\n\t\t\t\"reference_doctype\": reference_doctype,\n\t\t\t\"reference_name\": reference_name,\n\t\t\t\"comment_email\": comment_email,\n\t\t\t\"comment_by\": comment_by,\n\t\t\t\"content\": extract_images_from_html(reference_doc, content, is_private=True),\n\t\t}\n\t)\n\tcomment.insert(ignore_permissions=True)\n\n\tif frappe.get_cached_value(\"User\", frappe.session.user, \"follow_commented_documents\"):\n\t\tfollow_document(comment.reference_doctype, comment.reference_name, frappe.session.user)\n\n\treturn comment\n```\n\nOriginal documentation:\n```python\nallow any logged user to post a comment\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Allow logged user with permission to read document to add a comment", "file": "frappe_desk_form_utils.py", "function": "add_comment", "project": "frappe", "change_stats": {"code_changes": 38, "doc_changes": 4, "total_changes": 42}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run_job(self, project_id, body, dataset=None):\n        \n        \n\n        if dataset and not self.dataset_exists(dataset):\n            self.make_dataset(dataset)\n\n        new_job = self.client.jobs().insert(projectId=project_id, body=body).execute()\n        job_id = new_job['jobReference']['jobId']\n        logger.info('Started import job %s:%s', project_id, job_id)\n        while True:\n            status = self.client.jobs().get(projectId=project_id, jobId=job_id).execute(num_retries=10)\n            if status['status']['state'] == 'DONE':\n                if status['status'].get('errorResult'):\n                    raise Exception('BigQuery job failed: {}'.format(status['status']['errorResult']))\n                return\n\n            logger.info('Waiting for job %s:%s to complete...', project_id, job_id)\n            time.sleep(5)```\n\nUpdated code:\n```python\ndef run_job(self, project_id, body, dataset=None):\n        \n        \n\n        if dataset and not self.dataset_exists(dataset):\n            self.make_dataset(dataset)\n\n        new_job = self.client.jobs().insert(projectId=project_id, body=body).execute()\n        job_id = new_job['jobReference']['jobId']\n        logger.info('Started import job %s:%s', project_id, job_id)\n        while True:\n            status = self.client.jobs().get(projectId=project_id, jobId=job_id).execute(num_retries=10)\n            if status['status']['state'] == 'DONE':\n                if status['status'].get('errorResult'):\n                    raise BigQueryExecutionError(job_id, status['status']['errorResult'])\n                return job_id\n\n            logger.info('Waiting for job %s:%s to complete...', project_id, job_id)\n            time.sleep(5)\n```\n\nOriginal documentation:\n```python\nRuns a BigQuery \"job\". See the documentation for the format of body.\n\n           .. note::\n               You probably don't need to use this directly. Use the tasks defined below.\n\n           :param dataset:\n           :type dataset: BQDataset\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Runs a BigQuery \"job\". See the documentation for the format of body.\n\n           .. note::\n               You probably don't need to use this directly. Use the tasks defined below.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :return: the job id of the job.\n           :rtype: str\n           :raises luigi.contrib.BigQueryExecutionError: if the job fails.", "file": "luigi_contrib_bigquery.py", "function": "BigQueryClient.run_job", "project": "luigi", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef from_column(\n        cls, button_column: List[InlineKeyboardButton], **kwargs: object\n    ) -> \"InlineKeyboardMarkup\":\n        \n\n        \n        button_grid = [[button] for button in button_column]\n        return cls(button_grid, **kwargs)```\n\nUpdated code:\n```python\ndef from_column(\n        cls, button_column: Sequence[InlineKeyboardButton], **kwargs: object\n    ) -> \"InlineKeyboardMarkup\":\n        \n\n        \n        button_grid = [[button] for button in button_column]\n        return cls(button_grid, **kwargs)\n```\n\nOriginal documentation:\n```python\nShortcut for::\n\n            InlineKeyboardMarkup([[button] for button in button_column], **kwargs)\n\n        Return an InlineKeyboardMarkup from a single column of InlineKeyboardButtons\n\n        Args:\n            button_column (List[:class:`telegram.InlineKeyboardButton`]): The button to use in the\n                markup\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Shortcut for::\n\n            InlineKeyboardMarkup([[button] for button in button_column], **kwargs)\n\n        Return an InlineKeyboardMarkup from a single column of InlineKeyboardButtons\n\n        Args:\n            button_column (Sequence[:class:`telegram.InlineKeyboardButton`]): The button to use\n                in the markup\n\n                 .. versionchanged:: 20.0\n                    |sequenceargs|", "file": "telegram__inline_inlinekeyboardmarkup.py", "function": "InlineKeyboardMarkup.from_column", "project": "python-telegram-bot", "change_stats": {"code_changes": 4, "doc_changes": 9, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_partial_workers(self):\n        \n        for pin_memory in (True, False):\n            loader = iter(DataLoader(self.dataset, batch_size=2, num_workers=4, pin_memory=pin_memory))\n            workers = loader.workers\n            if pin_memory:\n                pin_memory_thread = loader.pin_memory_thread\n            for i, sample in enumerate(loader):\n                if i == 10:\n                    break\n            del loader\n            for w in workers:\n                w.join(JOIN_TIMEOUT)\n                self.assertFalse(w.is_alive(), 'subprocess not terminated')\n            if pin_memory:\n                pin_memory_thread.join(JOIN_TIMEOUT)\n                self.assertFalse(pin_memory_thread.is_alive())```\n\nUpdated code:\n```python\ndef test_partial_workers(self):\n        \"\"\n        loader = iter(DataLoader(self.dataset, batch_size=2, num_workers=4, pin_memory=True))\n        workers = loader.workers\n        worker_manager_thread = loader.worker_manager_thread\n        for i, sample in enumerate(loader):\n            if i == 3:\n                break\n        del loader\n        for w in workers:\n            w.join(JOIN_TIMEOUT)\n            self.assertFalse(w.is_alive(), 'subprocess not terminated')\n            self.assertEqual(w.exitcode, 0)\n        worker_manager_thread.join(JOIN_TIMEOUT)\n        self.assertFalse(worker_manager_thread.is_alive())\n```\n\nOriginal documentation:\n```python\nr\"\"\"Check that workers exit even if the iterator is not exhausted.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "check that workers exit even if the iterator is not exhausted", "file": "test_test_dataloader.py", "function": "TestDataLoader.test_partial_workers", "project": "pytorch", "change_stats": {"code_changes": 32, "doc_changes": 4, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_template_loader(self, template_path):\n        \n        \n        settings = self.application.settings\n        if \"template_loader\" in settings:\n            return settings[\"template_loader\"]\n        kwargs = {}\n        if \"autoescape\" in settings:\n            # autoescape=None means \"no escaping\", so we have to be sure\n            # to only pass this kwarg if the user asked for it.\n            kwargs[\"autoescape\"] = settings[\"autoescape\"]\n        return template.Loader(template_path, **kwargs)```\n\nUpdated code:\n```python\ndef create_template_loader(self, template_path):\n        \n        \n        settings = self.application.settings\n        if \"template_loader\" in settings:\n            return settings[\"template_loader\"]\n        kwargs = {}\n        if \"autoescape\" in settings:\n            # autoescape=None means \"no escaping\", so we have to be sure\n            # to only pass this kwarg if the user asked for it.\n            kwargs[\"autoescape\"] = settings[\"autoescape\"]\n        if \"template_whitespace\" in settings:\n            kwargs[\"whitespace\"] = settings[\"template_whitespace\"]\n        return template.Loader(template_path, **kwargs)\n```\n\nOriginal documentation:\n```python\nReturns a new template loader for the given path.\n\n        May be overridden by subclasses.  By default returns a\n        directory-based loader on the given path, using the\n        ``autoescape`` application setting.  If a ``template_loader``\n        application setting is supplied, uses that instead.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a new template loader for the given path.\n\n        May be overridden by subclasses.  By default returns a\n        directory-based loader on the given path, using the\n        ``autoescape`` and ``template_whitespace`` application\n        settings.  If a ``template_loader`` application setting is\n        supplied, uses that instead.", "file": "tornado_web.py", "function": "RequestHandler.create_template_loader", "project": "tornado", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef os_port_daily(self, mission=True, supply=True):\n        \n        \n        \n        logger.hr('OS port daily', level=1)\n        if np.random.uniform() > 0.5:\n            # St. Petersburg => Liverpool => Gibraltar => NY City\n            ports = [3, 1, 2, 0]\n        else:\n            # NY City => Gibraltar => Liverpool => St. Petersburg\n            ports = [0, 2, 1, 3]\n\n        mission_success = True\n        supply_success = True\n        for port in ports:\n            port = self.name_to_zone(port)\n            logger.hr(f'OS port daily in {port}', level=2)\n            self.globe_goto(port)\n            self.port_goto2()\n            self.port_enter()\n            if mission and mission_success:\n                mission_success &= self.port_mission_accept()\n            if supply and supply_success:\n                supply_success &= self.port_supply_buy()\n            self.port_quit()\n            if not ((mission and mission_success) or (supply and supply_success)):\n                return False\n\n        return True```\n\nUpdated code:\n```python\ndef os_port_daily(self, mission=True, supply=True):\n        \n        \n        \n        logger.hr('OS port daily', level=1)\n        if np.random.uniform() > 0.5:\n            # St. Petersburg => Liverpool => Gibraltar => NY City\n            ports = [3, 1, 2, 0]\n        else:\n            # NY City => Gibraltar => Liverpool => St. Petersburg\n            ports = [0, 2, 1, 3]\n\n        mission_success = True\n        supply_success = True\n        for port in ports:\n            port = self.name_to_zone(port)\n            logger.hr(f'OS port daily in {port}', level=2)\n            self.globe_goto(port)\n            self.port_goto2()\n            self.port_enter()\n            if mission and mission_success:\n                mission_success &= self.port_mission_accept()\n            if supply and supply_success:\n                supply_success &= self.port_supply_buy()\n            self.port_quit()\n\n        return mission_success, supply_success\n```\n\nOriginal documentation:\n```python\nAccept all missions and buy all supplies in all ports.\n        If reach the maximum number of missions, skip accept missions in next port.\n        If not having enough yellow coins or purple coins, skip buying supplies in next port.\n\n        Args:\n            mission (bool): If needs to accept missions.\n            supply (bool): If needs to buy supplies.\n\n        Returns:\n            bool: True if all finished.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Accept all missions and buy all supplies in all ports.\n        If reach the maximum number of missions, skip accept missions in next port.\n        If not having enough yellow coins or purple coins, skip buying supplies in next port.\n\n        Args:\n            mission (bool): If needs to accept missions.\n            supply (bool): If needs to buy supplies.\n\n        Returns:\n            bool: True if all mission received.\n            bool: True if all supplies bought.", "file": "module_os_operation_siren.py", "function": "OperationSiren.os_port_daily", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_batch_indices(self, dataloader: object) -> List[List[int]]:  # batches x samples\n        \n        batch_sampler = getattr(dataloader, \"batch_sampler\", None)\n        if not isinstance(batch_sampler, IndexBatchSamplerWrapper):\n            self._warning_cache.warn(\n                f\"Couldn't infer the batch indices fetched from your dataloader: `{type(dataloader).__name__}`\"\n            )\n            return []\n        seen_batch_indices = batch_sampler.seen_batch_indices\n        # TODO(carmocca): this could be avoided\n        # we need to truncate the list because `IndexBatchSamplerWrapper` computes all indices on `__iter__`\n        seen_batch_indices = seen_batch_indices[: (self.batch_progress.current.completed + 1)]\n        return seen_batch_indices```\n\nUpdated code:\n```python\ndef _get_batch_indices(self, dataloader: object) -> List[List[int]]:  # batches x samples\n        \n        batch_sampler = getattr(dataloader, \"batch_sampler\", None)\n        if not isinstance(batch_sampler, _IndexBatchSamplerWrapper):\n            self._warning_cache.warn(\n                f\"Couldn't infer the batch indices fetched from your dataloader: `{type(dataloader).__name__}`\"\n            )\n            return []\n        return batch_sampler.seen_batch_indices\n```\n\nOriginal documentation:\n```python\nReturns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our\n        :class:`~lightning.pytorch.overrides.distributed.IndexBatchSamplerWrapper`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our\n        :class:`~lightning.pytorch.overrides.distributed._IndexBatchSamplerWrapper`.", "file": "src_lightning_pytorch_loops_prediction_loop.py", "function": "_PredictionLoop._get_batch_indices", "project": "pytorch-lightning", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __contains__(self, word):\n        \n\n        \n        if word in self.vocab:\n            return True\n        else:\n            hashes = ft_ngram_hashes(word, self.min_n, self.max_n, self.bucket, self.compatible_hash)\n            return any(h in self.hash2index for h in hashes)```\n\nUpdated code:\n```python\ndef __contains__(self, word):\n        \n\n        \n        return True\n```\n\nOriginal documentation:\n```python\nCheck if `word` or any character ngrams in `word` are present in the vocabulary.\n        A vector for the word is guaranteed to exist if current method returns True.\n\n        Parameters\n        ----------\n        word : str\n            Input word.\n\n        Returns\n        -------\n        bool\n            True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if `word` or any character ngrams in `word` are present in the vocabulary.\n        A vector for the word is guaranteed to exist if current method returns True.\n\n        Parameters\n        ----------\n        word : str\n            Input word.\n\n        Returns\n        -------\n        bool\n            True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n\n        Note\n        ----\n        This method **always** returns True, because of the way FastText works.\n\n        If you want to check if a word is an in-vocabulary term, use this instead:\n\n        .. pycon:\n\n            >>> from gensim.test.utils import datapath\n            >>> from gensim.models import FastText\n            >>> cap_path = datapath(\"crime-and-punishment.bin\")\n            >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n            >>> 'steamtrain' in model.wv.vocab  # If False, is an OOV term\n            False", "file": "gensim_models_keyedvectors.py", "function": "FastTextKeyedVectors.__contains__", "project": "gensim", "change_stats": {"code_changes": 8, "doc_changes": 17, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef follow_user(browser, track, login, user_name, button, blacklist, logger, logfolder):\n      \n    # check action availability\n    if quota_supervisor(\"follows\") == \"jump\":\n        return False, \"jumped\"\n\n    # available tracks are to follow in `profile`, `post` and `dialog`\n    if track in [\"profile\", \"post\"]:\n        if track == \"profile\":\n            # check URL of the webpage, if it already is user's profile page, then do not navigate to it again\n            user_link = \"https://www.instagram.com/{}/\".format(user_name)\n            web_address_navigator(browser, user_link)\n\n        try:\n            sleep(2)\n            follow_xpath = \"//button[text()='Follow' or text()='Follow Back']\"\n            follow_button = browser.find_element_by_xpath(follow_xpath)\n\n            # click to follow\n            if follow_button.is_displayed():\n                click_element(browser, follow_button)\n\n            else:\n                browser.execute_script(\"arguments[0].style.visibility = 'visible'; \"\n                                       \"arguments[0].style.height = '10px'; \"\n                                       \"arguments[0].style.width = '10px'; \"\n                                       \"arguments[0].style.opacity = 1\",\n                                            follow_button)\n                # update server calls\n                update_activity()\n\n                click_element(browser, follow_button)\n\n            # wait until the follow action succeed\n            post_follow_element = \"//button[text()='Following' or text()='Requested']\"\n            button_change = explicit_wait(browser, \"VOEL\", [post_follow_element, \"XPath\"], logger, 9, False)\n\n            # verify the last follow\n            if not button_change:\n                browser.execute_script(\"location.reload()\")\n                update_activity()\n                sleep(2)\n\n                following, follow_button = get_following_status(browser,\n                                                                 track,\n                                                                  user_name,\n                                                                   logger)\n                if following is None:\n                    valid_page = is_page_available(browser, logger)\n                    if valid_page:\n                        sirens_wailing, emergency_state = emergency_exit(browser,\n                                                                          user_name,\n                                                                           logger)\n                        if sirens_wailing == True:\n                            logger.warning(\"There is a serious issue: '{}'!\\n\".format(emergency_state))\n                            return False, emergency_state\n\n                    if not valid_page or sirens_wailing == False:\n                        logger.error(\"Unexpected failure happened after last follow!\\n\")\n                        return False, \"unexpected failure\"\n\n                if following == False:\n                    logger.warning(\"Last follow is not verified!\\t~smells of a temporary block\\n\")\n                    sleep(600)\n                    return False, \"temporary block\"\n\n                elif following in [True, \"Requested\"]:\n                    logger.info(\"Last follow is verified after reloading the page!\\n\")\n\n        except NoSuchElementException:\n            logger.info(\"--> '{}' is already followed\".format(user_name))\n            sleep(1)\n\n            return False, \"already followed\"\n\n        except StaleElementReferenceException:\n            # https://stackoverflow.com/questions/16166261/selenium-webdriver-how-to-resolve-stale-element-reference-exception\n            # 1. An element that is found on a web page referenced as a WebElement in WebDriver then the DOM changes\n            # (probably due to JavaScript functions) that WebElement goes stale.\n            # 2. The element has been deleted entirely.\n            logger.error('--> element that is found on a web page referenced while the DOM changes')\n            sleep(1)\n\n            return False, \"stale element\"\n\n\n    elif track == \"dialog\":\n        click_element(browser, button)\n        sleep(3)\n\n\n    ## general tasks after a successful follow\n\n    logger.info(\"--> Followed '{}'!\".format(user_name.encode(\"utf-8\")))\n    update_activity('follows')\n\n    # get user ID to record alongside username\n    user_id = get_user_id(browser, track, user_name, logger)\n\n    logtime = datetime.now().strftime('%Y-%m-%d %H:%M')\n    log_followed_pool(login, user_name, logger, logfolder, logtime, user_id)\n\n    follow_restriction(\"write\", user_name, None, logger)\n\n    if blacklist['enabled'] == True:\n        action = 'followed'\n        add_user_to_blacklist(user_name,\n                               blacklist['campaign'],\n                                action,\n                                 logger,\n                                  logfolder)\n\n    # get the post-follow delay time to sleep\n    naply = get_action_delay(\"follow\")\n    sleep(naply)\n\n    return True, \"success\"```\n\nUpdated code:\n```python\ndef follow_user(browser, track, login, user_name, button, blacklist, logger, logfolder):\n      \n    # list of available tracks to follow in: [\"profile\", \"post\" \"dialog\"]\n\n    # check action availability\n    if quota_supervisor(\"follows\") == \"jump\":\n        return False, \"jumped\"\n\n    if track in [\"profile\", \"post\"]:\n        if track == \"profile\":\n            # check URL of the webpage, if it already is user's profile page, then do not navigate to it again\n            user_link = \"https://www.instagram.com/{}/\".format(user_name)\n            web_address_navigator(browser, user_link)\n\n        # find out CURRENT following status\n        following_status, follow_button = get_following_status(browser,\n                                                         track,\n                                                          login,\n                                                          user_name,\n                                                           None,\n                                                            logger,\n                                                            logfolder)\n        if following_status in [\"Follow\", \"Follow Back\"]:\n            click_visibly(browser, follow_button)   # click to follow\n            follow_state, msg = verify_action(browser, \"follow\", track, login,\n                                               user_name, None, logger, logfolder)\n            if follow_state != True:\n                return False, msg\n\n        elif following_status in [\"Following\", \"Requested\"]:\n            if following_status == \"Following\":\n                logger.info(\"--> Already following '{}'!\\n\".format(user_name))\n\n            elif following_status == \"Requested\":\n                logger.info(\"--> Already requested '{}' to follow!\\n\".format(user_name))\n\n            sleep(1)\n            return False, \"already followed\"\n\n        elif following_status in [\"Unblock\", \"UNAVAILABLE\"]:\n            if following_status == \"Unblock\":\n                failure_msg = \"user is in block\"\n\n            elif following_status == \"UNAVAILABLE\":\n                failure_msg = \"user is inaccessible\"\n\n            logger.warning(\"--> Couldn't follow '{}'!\\t~{}\".format(user_name, failure_msg))\n            return False, following_status\n\n        elif following_status is None:\n            sirens_wailing, emergency_state = emergency_exit(browser, login, logger)\n            if sirens_wailing == True:\n                return False, emergency_state\n\n            else:\n                logger.warning(\"--> Couldn't unfollow '{}'!\\t~unexpected failure\".format(user_name))\n                return False, \"unexpected failure\"\n\n\n    elif track == \"dialog\":\n        click_element(browser, button)\n        sleep(3)\n\n\n    ## general tasks after a successful follow\n\n    logger.info(\"--> Followed '{}'!\".format(user_name.encode(\"utf-8\")))\n    update_activity('follows')\n\n    # get user ID to record alongside username\n    user_id = get_user_id(browser, track, user_name, logger)\n\n    logtime = datetime.now().strftime('%Y-%m-%d %H:%M')\n    log_followed_pool(login, user_name, logger, logfolder, logtime, user_id)\n\n    follow_restriction(\"write\", user_name, None, logger)\n\n    if blacklist['enabled'] == True:\n        action = 'followed'\n        add_user_to_blacklist(user_name,\n                               blacklist['campaign'],\n                                action,\n                                 logger,\n                                  logfolder)\n\n    # get the post-follow delay time to sleep\n    naply = get_action_delay(\"follow\")\n    sleep(naply)\n\n    return True, \"success\"\n```\n\nOriginal documentation:\n```python\nFollows the user from either its 'profile' page, a 'post' page or the users 'dialog' box\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Follow a user either from the profile page or post page or dialog box", "file": "instapy_unfollow_util.py", "function": "follow_user", "project": "InstaPy", "change_stats": {"code_changes": 113, "doc_changes": 4, "total_changes": 117}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(self, declarations_str: str) -> dict[str, dict[str, str]]:\n        \n        \n        \n        # TODO: memoize?\n        properties = self.compute_css(declarations_str, self.inherited)\n        return self.build_xlstyle(properties)```\n\nUpdated code:\n```python\ndef __call__(\n        self, declarations: str | frozenset[tuple[str, str]]\n    ) -> dict[str, dict[str, str]]:\n        \n        \n        \n        properties = self.compute_css(declarations, self.inherited)\n        return self.build_xlstyle(properties)\n```\n\nOriginal documentation:\n```python\nConvert CSS declarations to ExcelWriter style.\n\n        Parameters\n        ----------\n        declarations_str : str\n            List of CSS declarations.\n            e.g. \"font-weight: bold; background: blue\"\n\n        Returns\n        -------\n        xlstyle : dict\n            A style as interpreted by ExcelWriter when found in\n            ExcelCell.style.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert CSS declarations to ExcelWriter style.\n\n        Parameters\n        ----------\n        declarations : str | frozenset[tuple[str, str]]\n            CSS string or set of CSS declaration tuples.\n            e.g. \"font-weight: bold; background: blue\" or\n            {(\"font-weight\", \"bold\"), (\"background\", \"blue\")}\n\n        Returns\n        -------\n        xlstyle : dict\n            A style as interpreted by ExcelWriter when found in\n            ExcelCell.style.", "file": "pandas_io_formats_excel.py", "function": "CSSToExcelConverter.__call__", "project": "pandas", "change_stats": {"code_changes": 9, "doc_changes": 9, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef sync_global_search(flags=None):\n\t\n\t\n\t\n\n\tif not flags:\n\t\tflags = frappe.flags.update_global_search\n\n\t# Can pass flags manually as frappe.flags.update_global_search isn't reliable at a later time,\n\t# when syncing is enqueued\n\tfor value in flags:\n\t\tfrappe.db.multisql({\n\t\t\t'mariadb': '''INSERT INTO `__global_search`\n\t\t\t\t(`doctype`, `name`, `content`, `published`, `title`, `route`)\n\t\t\t\tVALUES (%(doctype)s, %(name)s, %(content)s, %(published)s, %(title)s, %(route)s)\n\t\t\t\tON DUPLICATE key UPDATE `content`=%(content)s''',\n\t\t\t'postgres': '''INSERT INTO `__global_search`\n\t\t\t\t(`doctype`, `name`, `content`, `published`, `title`, `route`)\n\t\t\t\tVALUES (%(doctype)s, %(name)s, %(content)s, %(published)s, %(title)s, %(route)s)\n\t\t\t\tON CONFLICT(\"doctype\", \"name\") DO UPDATE SET `content`=%(content)s'''\n\t\t}, value)\n\n\tfrappe.flags.update_global_search = []```\n\nUpdated code:\n```python\ndef sync_global_search():\n\t\n\t\n\t\n\twhile frappe.cache().llen('global_search_queue') > 0:\n\t\tvalue = json.loads(frappe.cache().lpop('global_search_queue'))\n\n\t\tfrappe.db.sql('''\n\t\t\tinsert into __global_search\n\t\t\t\t(doctype, name, content, published, title, route)\n\t\t\tvalues\n\t\t\t\t(%(doctype)s, %(name)s, %(content)s, %(published)s, %(title)s, %(route)s)\n\t\t\ton duplicate key update\n\t\t\t\tcontent = %(content)s''', value)\n```\n\nOriginal documentation:\n```python\nAdd values from `flags` (frappe.flags.update_global_search) to __global_search.\n\tThis is called internally at the end of the request.\n\t:param flags:\n\t:return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inserts / updates values from `global_search_queue` to __global_search.\n\tThis is called via job scheduler\n\t:param flags:\n\t:return:", "file": "frappe_utils_global_search.py", "function": "sync_global_search", "project": "frappe", "change_stats": {"code_changes": 31, "doc_changes": 6, "total_changes": 37}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        vocab: Vocab,\n        model: Optional[Model],\n        name: str = \"lemmatizer\",\n        *,\n        mode: str = \"lookup\",\n        lookups: Optional[Lookups] = None,\n        overwrite: bool = False,\n    ) -> None:\n        \n        \n        self.vocab = vocab\n        self.model = model\n        self.name = name\n        self._mode = mode\n        self.lookups = lookups if lookups is not None else Lookups()\n        self.overwrite = overwrite\n        if self.mode == \"lookup\":\n            self.lemmatize = self.lookup_lemmatize\n        elif self.mode == \"rule\":\n            self.lemmatize = self.rule_lemmatize\n        else:\n            mode_attr = f\"{self.mode}_lemmatize\"\n            if not hasattr(self, mode_attr):\n                raise ValueError(Errors.E1003.format(mode=mode))\n            self.lemmatize = getattr(self, mode_attr)\n        self.cache = {}```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        vocab: Vocab,\n        model: Optional[Model],\n        name: str = \"lemmatizer\",\n        *,\n        mode: str = \"lookup\",\n        overwrite: bool = False,\n    ) -> None:\n        \n        \n        self.vocab = vocab\n        self.model = model\n        self.name = name\n        self._mode = mode\n        self.lookups = Lookups()\n        self.overwrite = overwrite\n        self._validated = False\n        if self.mode == \"lookup\":\n            self.lemmatize = self.lookup_lemmatize\n        elif self.mode == \"rule\":\n            self.lemmatize = self.rule_lemmatize\n        else:\n            mode_attr = f\"{self.mode}_lemmatize\"\n            if not hasattr(self, mode_attr):\n                raise ValueError(Errors.E1003.format(mode=mode))\n            self.lemmatize = getattr(self, mode_attr)\n        self.cache = {}\n```\n\nOriginal documentation:\n```python\nInitialize a Lemmatizer.\n\n        vocab (Vocab): The vocab.\n        model (Model): A model (not yet implemented).\n        name (str): The component name. Defaults to \"lemmatizer\".\n        mode (str): The lemmatizer mode: \"lookup\", \"rule\". Defaults to \"lookup\".\n        lookups (Lookups): The lookups object containing the (optional) tables\n            such as \"lemma_rules\", \"lemma_index\", \"lemma_exc\" and\n            \"lemma_lookup\". Defaults to None\n        overwrite (bool): Whether to overwrite existing lemmas. Defaults to\n            `False`.\n\n        DOCS: https://nightly.spacy.io/api/lemmatizer#init\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize a Lemmatizer.\n\n        vocab (Vocab): The vocab.\n        model (Model): A model (not yet implemented).\n        name (str): The component name. Defaults to \"lemmatizer\".\n        mode (str): The lemmatizer mode: \"lookup\", \"rule\". Defaults to \"lookup\".\n        overwrite (bool): Whether to overwrite existing lemmas. Defaults to\n            `False`.\n\n        DOCS: https://nightly.spacy.io/api/lemmatizer#init", "file": "spacy_pipeline_lemmatizer.py", "function": "Lemmatizer.__init__", "project": "spaCy", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef deny(self, include: \"GlobPattern\", *, exclude: \"GlobPattern\" = ()):\n        \n        \n        self.patterns.append(\n            (GlobGroup(include, exclude=exclude), self._reject_denied_module, True)\n        )```\n\nUpdated code:\n```python\ndef deny(self, include: \"GlobPattern\", *, exclude: \"GlobPattern\" = ()):\n        \n        \n        self.patterns[GlobGroup(include, exclude=exclude)] = _PatternInfo(\n            _ModuleProviderAction.DENY, allow_empty=True\n        )\n```\n\nOriginal documentation:\n```python\nBlocklist modules who names match the given glob patterns from the list of modules the package can import.\n        If a dependency on any matching packages is found, a :class:`DeniedModuleError` is thrown.\n\n        Args:\n            include (Union[List[str], str]): A string e.g. \"my_package.my_subpackage\", or list of strings\n                for the names of the modules to be externed. This can also be a glob-style pattern, as described in :meth:`mock`\n\n            exclude (Union[List[str], str]): An optional pattern that excludes some patterns that match the include string.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\n        If a dependency on any matching packages is found, a :class:`PackagingError` is raised.\n\n        Args:\n            include (Union[List[str], str]): A string e.g. \"my_package.my_subpackage\", or list of strings\n                for the names of the modules to be externed. This can also be a glob-style pattern, as described in :meth:`mock`\n\n            exclude (Union[List[str], str]): An optional pattern that excludes some patterns that match the include string.", "file": "torch_package_package_exporter.py", "function": "PackageExporter.deny", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef async_alarm_arm_away(self, code=None):\n        \n        yield from self.hass.loop.run_in_executor(\n            None, self.alarm_arm_away, code)```\n\nUpdated code:\n```python\ndef async_alarm_arm_away(self, code=None):\n        \n        \n        return self.hass.loop.run_in_executor(\n            None, self.alarm_arm_away, code)\n```\n\nOriginal documentation:\n```python\nSend arm away command.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Send arm away command.\n\n        This method must be run in the event loop and returns a coroutine.", "file": "homeassistant_components_alarm_control_panel___init__.py", "function": "AlarmControlPanel.async_alarm_arm_away", "project": "core", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cer(self, output, target):\n    \n    \n    return distance.edit_distance(output, target)```\n\nUpdated code:\n```python\ndef cer(self, decode, target):\n    \n    \n    return distance.edit_distance(decode, target)\n```\n\nOriginal documentation:\n```python\nComputes the Character Error Rate (CER).\n\n    CER is  defined as the edit distance between the given strings.\n\n    Args:\n      output: a string of the decoded output.\n      target: a string for the ground truth transcript.\n\n    Returns:\n      A float number denoting the CER for the current sentence pair.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes the Character Error Rate (CER).\n\n    CER is defined as the edit distance between the two given strings.\n\n    Args:\n      decode: a string of the decoded output.\n      target: a string for the ground truth label.\n\n    Returns:\n      A float number denoting the CER for the current sentence pair.", "file": "research_deep_speech_decoder.py", "function": "DeepSpeechDecoder.cer", "project": "models", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef stats_history_csv(stats, stats_history_enabled=False, csv_for_web_ui=False):\n    \n    # csv_for_web_ui boolean returns the header along with the stats history row so that\n    # it can be returned as a csv for download on the web ui. Otherwise when run with\n    # the '--headless' option we write the header first and then append the file with stats\n    # entries every interval.\n    if csv_for_web_ui:\n        rows = [stats_history_csv_header()]\n    else:\n        rows = []\n\n    timestamp = int(time.time())\n    stats_entries_per_iteration = []\n\n    if stats_history_enabled:\n        stats_entries_per_iteration = sort_stats(stats.entries)\n\n    for s in chain(stats_entries_per_iteration, [stats.total]):\n        if s.num_requests:\n            percentile_str = ','.join([\n                str(int(s.get_current_response_time_percentile(x) or 0)) for x in PERCENTILES_TO_REPORT])\n        else:\n            percentile_str = ','.join(['\"N/A\"'] * len(PERCENTILES_TO_REPORT))\n\n        rows.append('\"%s\",\"%s\",\"%s\",%i,%i,%.2f,%.2f,%i,%i,%i,%.2f,%.2f,%s' % (\n            s.method,\n            s.name,\n            timestamp,\n            s.num_requests,\n            s.num_failures,\n            s.current_rps,\n            s.current_fail_per_sec,\n            s.median_response_time,\n            s.avg_response_time,\n            s.min_response_time or 0,\n            s.max_response_time,\n            s.avg_content_length,\n            percentile_str\n        ))\n\n    return \"\\n\".join(rows)```\n\nUpdated code:\n```python\ndef stats_history_csv(stats, all_entries=False):\n    \n    \n    \n    timestamp = int(time.time())\n    stats_entries = []\n    if all_entries:\n        stats_entries = sort_stats(stats.entries)\n    \n    rows = []\n    for s in chain(stats_entries, [stats.total]):\n        if s.num_requests:\n            percentile_str = ','.join([\n                str(int(s.get_current_response_time_percentile(x) or 0)) for x in PERCENTILES_TO_REPORT])\n        else:\n            percentile_str = ','.join(['\"N/A\"'] * len(PERCENTILES_TO_REPORT))\n\n        rows.append('\"%s\",\"%s\",\"%s\",%i,%i,%.2f,%.2f,%i,%i,%i,%.2f,%.2f,%s' % (\n            s.method,\n            s.name,\n            timestamp,\n            s.num_requests,\n            s.num_failures,\n            s.current_rps,\n            s.current_fail_per_sec,\n            s.median_response_time,\n            s.avg_response_time,\n            s.min_response_time or 0,\n            s.max_response_time,\n            s.avg_content_length,\n            percentile_str\n        ))\n\n    return \"\\n\".join(rows)\n```\n\nOriginal documentation:\n```python\nReturns the Aggregated stats entry every interval\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a string of CSV rows with the *current* stats. By default only includes the \n    Aggregated stats entry, but if all_entries is set to True, a row for each entry will \n    will be included.", "file": "locust_stats.py", "function": "stats_history_csv", "project": "locust", "change_stats": {"code_changes": 27, "doc_changes": 6, "total_changes": 33}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, hf_foken, dataset_name, organization=None, private=False):\n        \n        \n\n        \n        self.hf_foken = hf_foken\n        self.dataset_name = dataset_name\n        self.organization_name = organization\n        self.dataset_private = private```\n\nUpdated code:\n```python\ndef __init__(self, hf_foken, dataset_name, organization=None, \n                 private=False, verbose=True):\n        \n        \n        \n        self.hf_foken = hf_foken\n        self.dataset_name = dataset_name\n        self.organization_name = organization\n        self.dataset_private = private\n        self.verbose = verbose\n```\n\nOriginal documentation:\n```python\nParams:\n        hf_token (str): The token to use to access the huggingface API.\n        dataset_name (str): The name of the dataset to save the data to, e.g. \"abidlabs/image-classifier-1\"\n        organization (str): The name of the organization to which to attach the datasets\n        private (optional): If the dataset does not already exist, whether it should be created as a private dataset or public. \n            Private datasets may require paid huggingface.co accounts\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Params:\n        hf_token (str): The token to use to access the huggingface API.\n        dataset_name (str): The name of the dataset to save the data to, e.g. \n            \"image-classifier-1\"\n        organization (str): The name of the organization to which to attach \n            the datasets. If None, the dataset attaches to the user only.\n        private (bool): If the dataset does not already exist, whether it \n            should be created as a private dataset or public. Private datasets \n            may require paid huggingface.co accounts\n        verbose (bool): Whether to print out the status of the dataset \n            creation.", "file": "gradio_flagging.py", "function": "HuggingFaceDatasetSaver.__init__", "project": "gradio", "change_stats": {"code_changes": 7, "doc_changes": 15, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef user_cache_dir():\n    \n    \n    if WINDOWS and os.environ.get('LOCALAPPDATA') is not None:\n        path = os.path.join(os.environ.get('LOCALAPPDATA'), 'glances', 'cache')\n    elif MACOS:\n        path = os.path.expanduser('~/Library/Caches/glances')\n    else:\n        path = os.path.join(os.environ.get('XDG_CACHE_HOME') or os.path.expanduser('~/.cache'),\n                            'glances')\n\n    return path```\n\nUpdated code:\n```python\ndef user_cache_dir():\n    \n    \n    if WINDOWS:\n        path = os.path.join(os.environ.get('LOCALAPPDATA') or os.environ.get('APPDATA'),\n                            'glances', 'cache')\n    elif MACOS:\n        path = os.path.expanduser('~/Library/Caches/glances')\n    else:\n        path = os.path.join(os.environ.get('XDG_CACHE_HOME') or os.path.expanduser('~/.cache'),\n                            'glances')\n\n    return path\n```\n\nOriginal documentation:\n```python\nr\"\"\"Return the per-user cache dir (full path).\n\n    - Linux, *BSD, SunOS: ~/.cache/glances\n    - macOS: ~/Library/Caches/glances\n    - Windows: %LOCALAPPDATA%\\glances\\cache\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Return the per-user cache dir (full path).\n\n    - Linux, *BSD, SunOS: ~/.cache/glances\n    - macOS: ~/Library/Caches/glances\n    - Windows: {%LOCALAPPDATA%,%APPDATA%}\\glances\\cache", "file": "glances_config.py", "function": "user_cache_dir", "project": "glances", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        # Multi-scale discriminator related\n        scales: int = 1,\n        scale_downsample_pooling: str = \"AvgPool1d\",\n        scale_downsample_pooling_params: Dict[str, Any] = {\n            \"kernel_size\": 4,\n            \"stride\": 2,\n            \"padding\": 2,\n        },\n        scale_discriminator_params: Dict[str, Any] = {\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [15, 41, 5, 3],\n            \"channels\": 128,\n            \"max_downsample_channels\": 1024,\n            \"max_groups\": 16,\n            \"bias\": True,\n            \"downsample_scales\": [2, 2, 4, 4, 1],\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n        },\n        follow_official_norm: bool = True,\n        # Multi-period discriminator related\n        periods: List[int] = [2, 3, 5, 7, 11],\n        period_discriminator_params: Dict[str, Any] = {\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [5, 3],\n            \"channels\": 32,\n            \"downsample_scales\": [3, 3, 3, 3, 1],\n            \"max_downsample_channels\": 1024,\n            \"bias\": True,\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n            \"use_weight_norm\": True,\n            \"use_spectral_norm\": False,\n        },\n        # Multi-frequency discriminator related\n        sample_rate: int = 22050,\n        multi_freq_disc_params: Dict[str, Any] = {\n            \"hop_length_factors\": [4, 8, 16],\n            \"hidden_channels\": [256, 512, 512],\n            \"domain\": \"double\",\n            \"mel_scale\": True,\n            \"divisors\": [32, 16, 8, 4, 2, 1, 1],\n            \"strides\": [1, 2, 1, 2, 1, 2, 1],\n        },\n    ):\n        \n        \n        \n        super().__init__()\n\n        # Multi-scale discriminator related\n        self.msd = HiFiGANMultiScaleDiscriminator(\n            scales=scales,\n            downsample_pooling=scale_downsample_pooling,\n            downsample_pooling_params=scale_downsample_pooling_params,\n            discriminator_params=scale_discriminator_params,\n            follow_official_norm=follow_official_norm,\n        )\n\n        # Multi-period discriminator related\n        self.mpd = HiFiGANMultiPeriodDiscriminator(\n            periods=periods,\n            discriminator_params=period_discriminator_params,\n        )\n\n        # Multi-frequency discriminator related\n        if \"hop_lengths\" not in multi_freq_disc_params:\n            # Transfer hop lengths factors to hop lengths\n            multi_freq_disc_params[\"hop_lengths\"] = []\n\n            for i in range(len(multi_freq_disc_params[\"hop_length_factors\"])):\n                multi_freq_disc_params[\"hop_lengths\"].append(\n                    int(\n                        sample_rate\n                        * multi_freq_disc_params[\"hop_length_factors\"][i]\n                        / 1000\n                    )\n                )\n\n            del multi_freq_disc_params[\"hop_length_factors\"]\n\n        self.mfd = MultiFrequencyDiscriminator(\n            **multi_freq_disc_params,\n        )```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        # Multi-scale discriminator related\n        scales: int = 1,\n        scale_downsample_pooling: str = \"AvgPool1d\",\n        scale_downsample_pooling_params: Dict[str, Any] = {\n            \"kernel_size\": 4,\n            \"stride\": 2,\n            \"padding\": 2,\n        },\n        scale_discriminator_params: Dict[str, Any] = {\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [15, 41, 5, 3],\n            \"channels\": 128,\n            \"max_downsample_channels\": 1024,\n            \"max_groups\": 16,\n            \"bias\": True,\n            \"downsample_scales\": [2, 2, 4, 4, 1],\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n        },\n        follow_official_norm: bool = True,\n        # Multi-period discriminator related\n        periods: List[int] = [2, 3, 5, 7, 11],\n        period_discriminator_params: Dict[str, Any] = {\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [5, 3],\n            \"channels\": 32,\n            \"downsample_scales\": [3, 3, 3, 3, 1],\n            \"max_downsample_channels\": 1024,\n            \"bias\": True,\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n            \"use_weight_norm\": True,\n            \"use_spectral_norm\": False,\n        },\n        # Multi-frequency discriminator related\n        multi_freq_disc_params: Dict[str, Any] = {\n            \"sample_rate\": 22050,\n            \"hop_length_factors\": [4, 8, 16],\n            \"hidden_channels\": [256, 512, 512],\n            \"domain\": \"double\",\n            \"mel_scale\": True,\n            \"divisors\": [32, 16, 8, 4, 2, 1, 1],\n            \"strides\": [1, 2, 1, 2, 1, 2, 1],\n        },\n    ):\n        \n        \n        \n        super().__init__()\n\n        # Multi-scale discriminator related\n        self.msd = HiFiGANMultiScaleDiscriminator(\n            scales=scales,\n            downsample_pooling=scale_downsample_pooling,\n            downsample_pooling_params=scale_downsample_pooling_params,\n            discriminator_params=scale_discriminator_params,\n            follow_official_norm=follow_official_norm,\n        )\n\n        # Multi-period discriminator related\n        self.mpd = HiFiGANMultiPeriodDiscriminator(\n            periods=periods,\n            discriminator_params=period_discriminator_params,\n        )\n\n        # Multi-frequency discriminator related\n        if \"hop_lengths\" not in multi_freq_disc_params:\n            # Transfer hop lengths factors to hop lengths\n            multi_freq_disc_params[\"hop_lengths\"] = []\n\n            for i in range(len(multi_freq_disc_params[\"hop_length_factors\"])):\n                multi_freq_disc_params[\"hop_lengths\"].append(\n                    int(\n                        multi_freq_disc_params[\"sample_rate\"]\n                        * multi_freq_disc_params[\"hop_length_factors\"][i]\n                        / 1000\n                    )\n                )\n\n            del multi_freq_disc_params[\"hop_length_factors\"]\n\n        self.mfd = MultiFrequencyDiscriminator(\n            **multi_freq_disc_params,\n        )\n```\n\nOriginal documentation:\n```python\nDiscriminator module for VISinger2, including MSD, MPD, and MFD.\n\n        Args:\n            sample_rate (int): Sample rate of the audio.\n            scales (int): Number of scales to be used in the multi-scale discriminator.\n            scale_downsample_pooling (str): Type of pooling used for downsampling.\n            scale_downsample_pooling_params (Dict[str, Any]): Parameters for the downsampling pooling layer.\n            scale_discriminator_params (Dict[str, Any]): Parameters for the scale discriminator.\n            follow_official_norm (bool): Whether to follow the official normalization or not.\n            periods (List[int]): List of periods to be used in the multi-period discriminator.\n            period_discriminator_params (Dict[str, Any]): Parameters for the period discriminator.\n            multi_freq_disc_params (Dict[str, Any]): Parameters for the multi-frequency discriminator.\n            use_spectral_norm (bool): Whether to use spectral normalization or not.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Discriminator module for VISinger2, including MSD, MPD, and MFD.\n\n        Args:\n            scales (int): Number of scales to be used in the multi-scale discriminator.\n            scale_downsample_pooling (str): Type of pooling used for downsampling.\n            scale_downsample_pooling_params (Dict[str, Any]): Parameters for the downsampling pooling layer.\n            scale_discriminator_params (Dict[str, Any]): Parameters for the scale discriminator.\n            follow_official_norm (bool): Whether to follow the official normalization or not.\n            periods (List[int]): List of periods to be used in the multi-period discriminator.\n            period_discriminator_params (Dict[str, Any]): Parameters for the period discriminator.\n            multi_freq_disc_params (Dict[str, Any]): Parameters for the multi-frequency discriminator.\n            use_spectral_norm (bool): Whether to use spectral normalization or not.", "file": "espnet2_gan_svs_visinger2_visinger2_vocoder.py", "function": "VISinger2Discriminator.__init__", "project": "espnet", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef match(self, path_pattern, *, case_sensitive=None):\n        \n        \n        \n        if not isinstance(path_pattern, PurePathBase):\n            path_pattern = self.with_segments(path_pattern)\n        if case_sensitive is None:\n            case_sensitive = _is_case_sensitive(self.pathmod)\n        sep = path_pattern.pathmod.sep\n        if path_pattern.anchor:\n            pattern_str = str(path_pattern)\n        elif path_pattern.parts:\n            pattern_str = str('**' / path_pattern)\n        else:\n            raise ValueError(\"empty pattern\")\n        match = _compile_pattern(pattern_str, sep, case_sensitive)\n        return match(str(self)) is not None```\n\nUpdated code:\n```python\ndef match(self, path_pattern, *, case_sensitive=None):\n        \n        \n        \n        if not isinstance(path_pattern, PurePathBase):\n            path_pattern = self.with_segments(path_pattern)\n        if case_sensitive is None:\n            case_sensitive = _is_case_sensitive(self.pathmod)\n        sep = path_pattern.pathmod.sep\n        path_parts = self.parts[::-1]\n        pattern_parts = path_pattern.parts[::-1]\n        if not pattern_parts:\n            raise ValueError(\"empty pattern\")\n        if len(path_parts) < len(pattern_parts):\n            return False\n        if len(path_parts) > len(pattern_parts) and path_pattern.anchor:\n            return False\n        for path_part, pattern_part in zip(path_parts, pattern_parts):\n            match = _compile_pattern(pattern_part, sep, case_sensitive, recursive=False)\n            if match(path_part) is None:\n                return False\n        return True\n```\n\nOriginal documentation:\n```python\nReturn True if this path matches the given pattern.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return True if this path matches the given pattern. If the pattern is\n        relative, matching is done from the right; otherwise, the entire path\n        is matched. The recursive wildcard '**' is *not* supported by this\n        method.", "file": "Lib_pathlib__abc.py", "function": "PurePathBase.match", "project": "cpython", "change_stats": {"code_changes": 21, "doc_changes": 7, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_set_stream_color(self) -> None:\n        \n        \n        \n        test_user = self.example_user('hamlet')\n        test_email = test_user.email\n        test_realm = test_user.realm\n        self.login(test_email)\n\n        old_subs, _ = gather_subscriptions(test_user)\n        sub = old_subs[0]\n        stream_id = sub['stream_id']\n        new_color = \"#ffffff\"  # TODO: ensure that this is different from old_color\n        result = self.api_post(test_email, \"/api/v1/users/me/subscriptions/properties\",\n                               {\"subscription_data\": ujson.dumps([{\"property\": \"color\",\n                                                                   \"stream_id\": stream_id,\n                                                                   \"value\": \"#ffffff\"}])})\n        self.assert_json_success(result)\n\n        new_subs = gather_subscriptions(get_user(test_email, test_realm))[0]\n        found_sub = None\n        for sub in new_subs:\n            if sub['stream_id'] == stream_id:\n                found_sub = sub\n                break\n\n        assert(found_sub is not None)\n        self.assertEqual(found_sub['color'], new_color)\n\n        new_subs.remove(found_sub)\n        for sub in old_subs:\n            if sub['stream_id'] == stream_id:\n                found_sub = sub\n                break\n        old_subs.remove(found_sub)\n        self.assertEqual(old_subs, new_subs)```\n\nUpdated code:\n```python\ndef test_set_stream_color(self) -> None:\n        \n        \n        \n        test_user = self.example_user('hamlet')\n        test_email = test_user.email\n        test_realm = test_user.realm\n        self.login(test_email)\n\n        old_subs, _ = gather_subscriptions(test_user)\n        sub = old_subs[0]\n        stream_id = sub['stream_id']\n        new_color = \"#ffffff\"  # TODO: ensure that this is different from old_color\n        result = self.api_post(test_email, \"/api/v1/users/me/subscriptions/properties\",\n                               {\"subscription_data\": ujson.dumps([{\"property\": \"color\",\n                                                                   \"stream_id\": stream_id,\n                                                                   \"value\": \"#ffffff\"}])})\n        self.assert_json_success(result)\n\n        new_subs = gather_subscriptions(get_user(test_email, test_realm))[0]\n        found_sub = None\n        for sub in new_subs:\n            if sub['stream_id'] == stream_id:\n                found_sub = sub\n                break\n\n        assert(found_sub is not None)\n        self.assertEqual(found_sub['color'], new_color)\n\n        new_subs.remove(found_sub)\n        for sub in old_subs:\n            if sub['stream_id'] == stream_id:\n                found_sub = sub\n                break\n        old_subs.remove(found_sub)\n        self.assertEqual(old_subs, new_subs)\n\n        invalid_color = \"3ffrff\"\n        result = self.api_post(test_email, \"/api/v1/users/me/subscriptions/properties\",\n                               {\"subscription_data\": ujson.dumps([{\"property\": \"color\",\n                                                                   \"stream_id\": stream_id,\n                                                                   \"value\": invalid_color}])})\n        self.assert_json_error(result, \"color is not a valid hex color code\")\n```\n\nOriginal documentation:\n```python\nA POST request to /api/v1/users/me/subscriptions/properties with stream_id and\n        color data sets the stream color, and for that stream only.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A POST request to /api/v1/users/me/subscriptions/properties with stream_id and\n        color data sets the stream color, and for that stream only. Also, make sure that\n        any invalid hex color codes are bounced.", "file": "zerver_tests_test_subs.py", "function": "SubscriptionPropertiesTest.test_set_stream_color", "project": "zulip", "change_stats": {"code_changes": 9, "doc_changes": 5, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(\n        self,\n        hidden_states,\n        attention_mask,\n        return_att=False,\n        query_states=None,\n        relative_pos=None,\n        rel_embeddings=None,\n    ):\n        \n        \n\n\n        \n        if query_states is None:\n            query_states = hidden_states\n        query_layer = self.transpose_for_scores(self.query_proj(query_states), self.num_attention_heads)\n        key_layer = self.transpose_for_scores(self.key_proj(hidden_states), self.num_attention_heads)\n        value_layer = self.transpose_for_scores(self.value_proj(hidden_states), self.num_attention_heads)\n\n        rel_att = None\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        scale_factor = 1\n        if \"c2p\" in self.pos_att_type:\n            scale_factor += 1\n        if \"p2c\" in self.pos_att_type:\n            scale_factor += 1\n        if \"p2p\" in self.pos_att_type:\n            scale_factor += 1\n        scale = math.sqrt(query_layer.size(-1) * scale_factor)\n        attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / scale\n        if self.relative_attention:\n            rel_embeddings = self.pos_dropout(rel_embeddings)\n            rel_att = self.disentangled_attention_bias(\n                query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n            )\n\n        if rel_att is not None:\n            attention_scores = attention_scores + rel_att\n        attention_scores = attention_scores\n        attention_scores = attention_scores.view(\n            -1, self.num_attention_heads, attention_scores.size(-2), attention_scores.size(-1)\n        )\n\n        # bsz x height x length x dimension\n        attention_probs = XSoftmax.apply(attention_scores, attention_mask, -1)\n        attention_probs = self.dropout(attention_probs)\n        context_layer = torch.bmm(\n            attention_probs.view(-1, attention_probs.size(-2), attention_probs.size(-1)), value_layer\n        )\n        context_layer = (\n            context_layer.view(-1, self.num_attention_heads, context_layer.size(-2), context_layer.size(-1))\n            .permute(0, 2, 1, 3)\n            .contiguous()\n        )\n        new_context_layer_shape = context_layer.size()[:-2] + (-1,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        if return_att:\n            return (context_layer, attention_probs)\n        else:\n            return context_layer```\n\nUpdated code:\n```python\ndef forward(\n        self,\n        hidden_states,\n        attention_mask,\n        output_attentions=False,\n        query_states=None,\n        relative_pos=None,\n        rel_embeddings=None,\n    ):\n        \n        \n\n\n        \n        if query_states is None:\n            query_states = hidden_states\n        query_layer = self.transpose_for_scores(self.query_proj(query_states), self.num_attention_heads)\n        key_layer = self.transpose_for_scores(self.key_proj(hidden_states), self.num_attention_heads)\n        value_layer = self.transpose_for_scores(self.value_proj(hidden_states), self.num_attention_heads)\n\n        rel_att = None\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        scale_factor = 1\n        if \"c2p\" in self.pos_att_type:\n            scale_factor += 1\n        if \"p2c\" in self.pos_att_type:\n            scale_factor += 1\n        if \"p2p\" in self.pos_att_type:\n            scale_factor += 1\n        scale = math.sqrt(query_layer.size(-1) * scale_factor)\n        attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / scale\n        if self.relative_attention:\n            rel_embeddings = self.pos_dropout(rel_embeddings)\n            rel_att = self.disentangled_attention_bias(\n                query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n            )\n\n        if rel_att is not None:\n            attention_scores = attention_scores + rel_att\n        attention_scores = attention_scores\n        attention_scores = attention_scores.view(\n            -1, self.num_attention_heads, attention_scores.size(-2), attention_scores.size(-1)\n        )\n\n        # bsz x height x length x dimension\n        attention_probs = XSoftmax.apply(attention_scores, attention_mask, -1)\n        attention_probs = self.dropout(attention_probs)\n        context_layer = torch.bmm(\n            attention_probs.view(-1, attention_probs.size(-2), attention_probs.size(-1)), value_layer\n        )\n        context_layer = (\n            context_layer.view(-1, self.num_attention_heads, context_layer.size(-2), context_layer.size(-1))\n            .permute(0, 2, 1, 3)\n            .contiguous()\n        )\n        new_context_layer_shape = context_layer.size()[:-2] + (-1,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        if output_attentions:\n            return (context_layer, attention_probs)\n        else:\n            return context_layer\n```\n\nOriginal documentation:\n```python\nCall the module\n\n        Args:\n            hidden_states (:obj:`torch.FloatTensor`):\n                Input states to the module usually the output from previous layer, it will be the Q,K and V in\n                `Attention(Q,K,V)`\n\n            attention_mask (:obj:`torch.ByteTensor`):\n                An attention mask matrix of shape [`B`, `N`, `N`] where `B` is the batch size, `N` is the maximum\n                sequence length in which element [i,j] = `1` means the `i` th token in the input can attend to the `j`\n                th token.\n\n            return_att (:obj:`bool`, optional):\n                Whether return the attention matrix.\n\n            query_states (:obj:`torch.FloatTensor`, optional):\n                The `Q` state in `Attention(Q,K,V)`.\n\n            relative_pos (:obj:`torch.LongTensor`):\n                The relative position encoding between the tokens in the sequence. It's of shape [`B`, `N`, `N`] with\n                values ranging in [`-max_relative_positions`, `max_relative_positions`].\n\n            rel_embeddings (:obj:`torch.FloatTensor`):\n                The embedding of relative distances. It's a tensor of shape [:math:`2 \\\\times\n                \\\\text{max_relative_positions}`, `hidden_size`].\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Call the module\n\n        Args:\n            hidden_states (:obj:`torch.FloatTensor`):\n                Input states to the module usually the output from previous layer, it will be the Q,K and V in\n                `Attention(Q,K,V)`\n\n            attention_mask (:obj:`torch.ByteTensor`):\n                An attention mask matrix of shape [`B`, `N`, `N`] where `B` is the batch size, `N` is the maximum\n                sequence length in which element [i,j] = `1` means the `i` th token in the input can attend to the `j`\n                th token.\n\n            output_attentions (:obj:`bool`, optional):\n                Whether return the attention matrix.\n\n            query_states (:obj:`torch.FloatTensor`, optional):\n                The `Q` state in `Attention(Q,K,V)`.\n\n            relative_pos (:obj:`torch.LongTensor`):\n                The relative position encoding between the tokens in the sequence. It's of shape [`B`, `N`, `N`] with\n                values ranging in [`-max_relative_positions`, `max_relative_positions`].\n\n            rel_embeddings (:obj:`torch.FloatTensor`):\n                The embedding of relative distances. It's a tensor of shape [:math:`2 \\\\times\n                \\\\text{max_relative_positions}`, `hidden_size`].", "file": "src_transformers_models_deberta_v2_modeling_deberta_v2.py", "function": "DisentangledSelfAttention.forward", "project": "transformers", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _convert_key(self, key, is_setter: bool = False):\n         \n        \n\n        # allow arbitrary setting\n        if is_setter:\n            return list(key)\n\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not ax.holds_integer():\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key```\n\nUpdated code:\n```python\ndef _convert_key(self, key, is_setter: bool = False):\n        \n        \n        \n        # allow arbitrary setting\n        if is_setter:\n            return list(key)\n\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not ax.holds_integer():\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n```\n\nOriginal documentation:\n```python\nrequire they keys to be the same type as the index (so we don't\n        fallback)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Require they keys to be the same type as the index. (so we don't\n        fallback)", "file": "pandas_core_indexing.py", "function": "_AtIndexer._convert_key", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_pipelines(\n        self, metadata_schema: Optional[Dict] = None, **kwargs: str\n    ) -> dict:\n        \n        \n        if len(self.pipeline_ids) > 0:\n            raise RuntimeError(\n                f\"Pipelines already exist for collection {self.collection_name}: {self.pipeline_ids}\"\n            )\n\n        params_dict = {}\n        index_doc_func = {\n            \"name\": \"index_my_doc\",\n            \"action\": \"INDEX_DOC\",\n            \"inputField\": \"doc_url\",\n            \"language\": \"ENGLISH\",\n        }\n        index_doc_func.update(kwargs)\n        functions = [index_doc_func]\n        if metadata_schema:\n            for k, v in metadata_schema.items():\n                preserve_func = {\n                    \"name\": f\"keep_{k}\",\n                    \"action\": \"PRESERVE\",\n                    \"inputField\": k,\n                    \"outputField\": k,\n                    \"fieldType\": v,\n                }\n                functions.append(preserve_func)\n        params_dict[\"INGESTION\"] = {\n            \"name\": f\"{self.collection_name}_ingestion\",\n            \"projectId\": self.project_id,\n            \"clusterId\": self.cluster_id,\n            \"newCollectionName\": self.collection_name,\n            \"type\": \"INGESTION\",\n            \"functions\": functions,\n        }\n\n        params_dict[\"SEARCH\"] = {\n            \"name\": f\"{self.collection_name}_search\",\n            \"projectId\": self.project_id,\n            \"type\": \"SEARCH\",\n            \"functions\": [\n                {\n                    \"name\": \"search_chunk_text\",\n                    \"action\": \"SEARCH_DOC_CHUNK\",\n                    \"inputField\": \"query_text\",\n                    \"clusterId\": self.cluster_id,\n                    \"collectionName\": self.collection_name,\n                }\n            ],\n        }\n\n        params_dict[\"DELETION\"] = {\n            \"name\": f\"{self.collection_name}_deletion\",\n            \"type\": \"DELETION\",\n            \"functions\": [\n                {\n                    \"name\": \"purge_chunks_by_doc_name\",\n                    \"action\": \"PURGE_DOC_INDEX\",\n                    \"inputField\": \"doc_name\",\n                }\n            ],\n            \"projectId\": self.project_id,\n            \"clusterId\": self.cluster_id,\n            \"collectionName\": self.collection_name,\n        }\n\n        for k, v in params_dict.items():\n            response = requests.post(self.domain, headers=self.headers, json=v)\n            if response.status_code != 200:\n                raise RuntimeError(response.text)\n            response_dict = response.json()\n            if response_dict[\"code\"] != 200:\n                raise RuntimeError(response_dict)\n            self.pipeline_ids[k] = response_dict[\"data\"][\"pipelineId\"]\n\n        return self.pipeline_ids```\n\nUpdated code:\n```python\ndef create_pipelines(\n        project_id: str,\n        cluster_id: str,\n        cloud_region: str = \"gcp-us-west1\",\n        api_key: str = None,\n        collection_name: str = \"zcp_llamalection\",\n        data_type: str = \"text\",\n        metadata_schema: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> dict:\n        \n        \n        if data_type == \"text\":\n            ingest_action = \"INDEX_TEXT\"\n            search_action = \"SEARCH_TEXT\"\n        elif data_type == \"doc\":\n            ingest_action = \"INDEX_DOC\"\n            search_action = \"SEARCH_DOC_CHUNK\"\n        else:\n            raise Exception(\"Only text or doc is supported as the data type.\")\n\n        params_dict = {}\n        additional_params = kwargs or {}\n\n        language = additional_params.pop(\"language\", \"ENGLISH\")\n        embedding = additional_params.pop(\"embedding\", \"zilliz/bge-base-en-v1.5\")\n        reranker = additional_params.pop(\"reranker\", None)\n        index_func = {\n            \"name\": \"llamaindex_index\",\n            \"action\": ingest_action,\n            \"language\": language,\n            \"embedding\": embedding,\n        }\n        index_func.update(additional_params)\n        ingest_functions = [index_func]\n        if metadata_schema:\n            for k, v in metadata_schema.items():\n                preserve_func = {\n                    \"name\": f\"keep_{k}\",\n                    \"action\": \"PRESERVE\",\n                    \"inputField\": k,\n                    \"outputField\": k,\n                    \"fieldType\": v,\n                }\n                ingest_functions.append(preserve_func)\n        params_dict[\"INGESTION\"] = {\n            \"name\": f\"{collection_name}_ingestion\",\n            \"projectId\": project_id,\n            \"clusterId\": cluster_id,\n            \"collectionName\": collection_name,\n            \"type\": \"INGESTION\",\n            \"functions\": ingest_functions,\n        }\n\n        search_function = {\n            \"name\": \"llamaindex_search\",\n            \"action\": search_action,\n            \"clusterId\": cluster_id,\n            \"collectionName\": collection_name,\n            \"embedding\": embedding,\n        }\n        if reranker:\n            search_function[\"reranker\"] = reranker\n        params_dict[\"SEARCH\"] = {\n            \"name\": f\"{collection_name}_search\",\n            \"projectId\": project_id,\n            \"type\": \"SEARCH\",\n            \"functions\": [search_function],\n        }\n\n        params_dict[\"DELETION\"] = {\n            \"name\": f\"{collection_name}_deletion\",\n            \"type\": \"DELETION\",\n            \"functions\": [\n                {\n                    \"name\": \"purge_by_expression\",\n                    \"action\": \"PURGE_BY_EXPRESSION\",\n                }\n            ],\n            \"projectId\": project_id,\n            \"clusterId\": cluster_id,\n            \"collectionName\": collection_name,\n        }\n\n        domain = f\"https://controller.api.{cloud_region}.zillizcloud.com/v1/pipelines\"\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n        }\n        pipeline_ids = {}\n\n        for k, v in params_dict.items():\n            response = requests.post(domain, headers=headers, json=v)\n            if response.status_code != 200:\n                raise RuntimeError(response.text)\n            response_dict = response.json()\n            if response_dict[\"code\"] != 200:\n                raise RuntimeError(response_dict)\n            pipeline_ids[k] = response_dict[\"data\"][\"pipelineId\"]\n\n        return pipeline_ids\n```\n\nOriginal documentation:\n```python\nCreate INGESTION, SEARCH, DELETION pipelines using self.collection_name.\n\n        Args:\n            metadata_schema (Dict=None): A dictionary of metadata schema, defaults to None. Use metadata name as key and the corresponding data type as value: {'field_name': 'field_type'}.\n                Only support the following values as the field type: 'Bool', 'Int8', 'Int16', 'Int32', 'Int64', 'Float', 'Double', 'VarChar'.\n            kwargs: optional parameters to create ingestion pipeline\n                - chunkSize: An integer within range [20, 500] to customize chunk size.\n                - language: The language of documents. Available options: \"ENGLISH\", \"CHINESE\".\n\n        Returns:\n            A dictionary of pipeline ids for INGESTION, SEARCH, and DELETION pipelines.\n\n        Example:\n            >>> from llama_index.indices import ZillizCloudPipelineIndex\n            >>> index = ZillizCloudPipelineIndex(\n            >>>     project_id='YOUR_ZILLIZ_CLOUD_PROJECT_ID',\n            >>>     cluster_id='YOUR_ZILLIZ_CLOUD_CLUSTER_ID',\n            >>>     token='YOUR_ZILLIZ_CLOUD_API_KEY',\n            >>>     collection_name='your_new_collection_name'\n            >>> )\n            >>> pipeline_ids = index.create_pipelines(\n            >>>     metadata_schema={'year': 'Int32', 'author': 'VarChar'}  # optional, defaults to None\n            >>> )\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create INGESTION, SEARCH, DELETION pipelines using self.collection_name.\n\n        Args:\n            project_id (str): Zilliz Cloud's project ID.\n            cluster_id (str): Zilliz Cloud's cluster ID.\n            api_key (str=None): Zilliz Cloud's API key. Defaults to None.\n            cloud_region (str='gcp-us-west1'): The region of Zilliz Cloud's cluster. Defaults to 'gcp-us-west1'.\n            collection_name (str=\"zcp_llamalection\"): A collection name, defaults to 'zcp_llamalection'.\n            data_type (str=\"text\"): The data type of pipelines, defaults to \"text\". Currently only \"text\" or \"doc\" are supported.\n            metadata_schema (Dict=None): A dictionary of metadata schema, defaults to None. Use metadata name as key and the corresponding data type as value: {'field_name': 'field_type'}.\n                Only support the following values as the field type: 'Bool', 'Int8', 'Int16', 'Int32', 'Int64', 'Float', 'Double', 'VarChar'.\n            kwargs: optional function parameters to create ingestion & search pipelines.\n                - language: The language of documents. Available options: \"ENGLISH\", \"CHINESE\".\n                - embedding: The embedding service used in both ingestion & search pipeline.\n                - reranker: The reranker service used in search function.\n                - chunkSize: The chunk size to split a document. Only for doc data.\n                - splitBy: The separators to chunking a document. Only for doc data.\n\n        Returns:\n            The pipeline ids of created pipelines.\n\n        Example:\n            >>> from llama_index.indices import ZillizCloudPipelineIndex\n            >>> index = ZillizCloudPipelineIndex(\n            >>>     project_id='YOUR_ZILLIZ_CLOUD_PROJECT_ID',\n            >>>     cluster_id='YOUR_ZILLIZ_CLOUD_CLUSTER_ID',\n            >>>     token='YOUR_ZILLIZ_CLOUD_API_KEY',\n            >>>     collection_name='your_new_collection_name'\n            >>> )\n            >>> pipeline_ids = index.create_pipelines(\n            >>>     metadata_schema={'year': 'Int32', 'author': 'VarChar'}  # optional, defaults to None\n            >>> )", "file": "llama-index-integrations_indices_llama-index-indices-managed-zilliz_llama_index_indices_managed_zilliz_base.py", "function": "ZillizCloudPipelineIndex.create_pipelines", "project": "llama_index", "change_stats": {"code_changes": 104, "doc_changes": 17, "total_changes": 121}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef make_tokens_by_line(lines):\n    \n    \n    # NL tokens are used inside multiline expressions, but also after blank\n    # lines or comments. This is intentional - see https://bugs.python.org/issue17061\n    # We want to group the former case together but split the latter, so we\n    # track parentheses level, similar to the internals of tokenize.\n    NEWLINE, NL = tokenize.NEWLINE, tokenize.NL\n    tokens_by_line = [[]]\n    parenlev = 0\n    try:\n        for token in tokenize.generate_tokens(iter(lines).__next__):\n            tokens_by_line[-1].append(token)\n            if (token.type == NEWLINE) \\\n                    or ((token.type == NL) and (parenlev <= 0)):\n                tokens_by_line.append([])\n            elif token.string in {'(', '[', '{'}:\n                parenlev += 1\n            elif token.string in {')', ']', '}'}:\n                if parenlev > 0:\n                    parenlev -= 1\n    except tokenize.TokenError:\n        # Input ended in a multiline string or expression. That's OK for us.\n        pass\n\n\n    if not tokens_by_line[-1]:\n        tokens_by_line.pop()\n\n\n    return tokens_by_line```\n\nUpdated code:\n```python\ndef make_tokens_by_line(lines:List[str]):\n    \n\n    \n    # NL tokens are used inside multiline expressions, but also after blank\n    # lines or comments. This is intentional - see https://bugs.python.org/issue17061\n    # We want to group the former case together but split the latter, so we\n    # track parentheses level, similar to the internals of tokenize.\n    NEWLINE, NL = tokenize.NEWLINE, tokenize.NL\n    tokens_by_line = [[]]\n    if len(lines) > 1 and not lines[0].endswith(('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c')):\n        warnings.warn(\"`make_tokens_by_line` received a list of lines which do not have lineending markers ('\\\\n', '\\\\r', '\\\\r\\\\n', '\\\\x0b', '\\\\x0c'), behavior will be unspecified\")\n    parenlev = 0\n    try:\n        for token in tokenize.generate_tokens(iter(lines).__next__):\n            tokens_by_line[-1].append(token)\n            if (token.type == NEWLINE) \\\n                    or ((token.type == NL) and (parenlev <= 0)):\n                tokens_by_line.append([])\n            elif token.string in {'(', '[', '{'}:\n                parenlev += 1\n            elif token.string in {')', ']', '}'}:\n                if parenlev > 0:\n                    parenlev -= 1\n    except tokenize.TokenError:\n        # Input ended in a multiline string or expression. That's OK for us.\n        pass\n\n\n    if not tokens_by_line[-1]:\n        tokens_by_line.pop()\n\n\n    return tokens_by_line\n```\n\nOriginal documentation:\n```python\nTokenize a series of lines and group tokens by line.\n\n    The tokens for a multiline Python string or expression are\n    grouped as one line.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tokenize a series of lines and group tokens by line.\n\n    The tokens for a multiline Python string or expression are grouped as one\n    line. All lines except the last lines should keep their line ending ('\\\\n',\n    '\\\\r\\\\n') for this to properly work. Use `.splitlines(keeplineending=True)`\n    for example when passing block of text to this function.", "file": "IPython_core_inputtransformer2.py", "function": "make_tokens_by_line", "project": "ipython", "change_stats": {"code_changes": 7, "doc_changes": 8, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_node_syncer(local_dir, remote_dir=None, sync_function=None):\n    \n    \n    key = (local_dir, remote_dir)\n    if key in _syncers:\n        return _syncers[key]\n    elif not remote_dir:\n        sync_client = NOOP\n    elif sync_function:\n        sync_client = get_sync_client(sync_function)\n    else:\n        sync_up = log_sync_template()\n        sync_down = log_sync_template(options=\"--remove-source-files\")\n        if sync_up and sync_down:\n            sync_client = CommandBasedClient(sync_up, sync_down)\n            sync_client.set_logdir(local_dir)\n        else:\n            sync_client = NOOP\n\n    _syncers[key] = NodeSyncer(local_dir, remote_dir, sync_client)\n    return _syncers[key]```\n\nUpdated code:\n```python\ndef get_node_syncer(local_dir, remote_dir=None, sync_function=None):\n    \n    \n    key = (local_dir, remote_dir)\n    if key in _syncers:\n        return _syncers[key]\n    elif not remote_dir or sync_function is False:\n        sync_client = NOOP\n    elif sync_function and sync_function is not True:\n        sync_client = get_sync_client(sync_function)\n    else:\n        sync = log_sync_template()\n        if sync:\n            sync_client = CommandBasedClient(sync, sync)\n            sync_client.set_logdir(local_dir)\n        else:\n            sync_client = NOOP\n\n    _syncers[key] = NodeSyncer(local_dir, remote_dir, sync_client)\n    return _syncers[key]\n```\n\nOriginal documentation:\n```python\nReturns a NodeSyncer.\n\n    Args:\n        local_dir (str): Source directory for syncing.\n        remote_dir (str): Target directory for syncing. If not provided, a\n            no-op Syncer is returned.\n        sync_function (func|str): Function for syncing the local_dir to\n            remote_dir. If string, then it must be a string template for\n            syncer to run. If not provided, it defaults rsync.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a NodeSyncer.\n\n    Args:\n        local_dir (str): Source directory for syncing.\n        remote_dir (str): Target directory for syncing. If not provided, a\n            noop Syncer is returned.\n        sync_function (func|str|bool): Function for syncing the local_dir to\n            remote_dir. If string, then it must be a string template for\n            syncer to run. If True or not provided, it defaults rsync. If\n            False, a noop Syncer is returned.", "file": "python_ray_tune_syncer.py", "function": "get_node_syncer", "project": "ray", "change_stats": {"code_changes": 13, "doc_changes": 9, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate(self,\n                 im,\n                 crop_n_layers=0,\n                 crop_overlap_ratio=512 / 1500,\n                 crop_downscale_factor=1,\n                 point_grids=None,\n                 points_stride=32,\n                 points_batch_size=64,\n                 conf_thres=0.88,\n                 stability_score_thresh=0.95,\n                 stability_score_offset=0.95,\n                 crop_nms_thresh=0.7):\n        \n        \n        \n        self.segment_all = True\n        ih, iw = im.shape[2:]\n        crop_regions, layer_idxs = generate_crop_boxes((ih, iw), crop_n_layers, crop_overlap_ratio)\n        if point_grids is None:\n            point_grids = build_all_layer_point_grids(\n                points_stride,\n                crop_n_layers,\n                crop_downscale_factor,\n            )\n        pred_masks, pred_scores, pred_bboxes, region_areas = [], [], [], []\n        for crop_region, layer_idx in zip(crop_regions, layer_idxs):\n            x1, y1, x2, y2 = crop_region\n            w, h = x2 - x1, y2 - y1\n            area = torch.tensor(w * h, device=im.device)\n            points_scale = np.array([[w, h]])  # w, h\n            # Crop image and interpolate to input size\n            crop_im = F.interpolate(im[..., y1:y2, x1:x2], (ih, iw), mode='bilinear', align_corners=False)\n            # (num_points, 2)\n            points_for_image = point_grids[layer_idx] * points_scale\n            crop_masks, crop_scores, crop_bboxes = [], [], []\n            for (points, ) in batch_iterator(points_batch_size, points_for_image):\n                pred_mask, pred_score = self.prompt_inference(crop_im, points=points, multimask_output=True)\n                # Interpolate predicted masks to input size\n                pred_mask = F.interpolate(pred_mask[None], (h, w), mode='bilinear', align_corners=False)[0]\n                idx = pred_score > conf_thres\n                pred_mask, pred_score = pred_mask[idx], pred_score[idx]\n\n                stability_score = calculate_stability_score(pred_mask, self.model.mask_threshold,\n                                                            stability_score_offset)\n                idx = stability_score > stability_score_thresh\n                pred_mask, pred_score = pred_mask[idx], pred_score[idx]\n                # Bool type is much more memory-efficient.\n                pred_mask = pred_mask > self.model.mask_threshold\n                # (N, 4)\n                pred_bbox = batched_mask_to_box(pred_mask).float()\n                keep_mask = ~is_box_near_crop_edge(pred_bbox, crop_region, [0, 0, iw, ih])\n                if not torch.all(keep_mask):\n                    pred_bbox, pred_mask, pred_score = pred_bbox[keep_mask], pred_mask[keep_mask], pred_score[keep_mask]\n\n                crop_masks.append(pred_mask)\n                crop_bboxes.append(pred_bbox)\n                crop_scores.append(pred_score)\n\n            # Do nms within this crop\n            crop_masks = torch.cat(crop_masks)\n            crop_bboxes = torch.cat(crop_bboxes)\n            crop_scores = torch.cat(crop_scores)\n            keep = torchvision.ops.nms(crop_bboxes, crop_scores, self.args.iou)  # NMS\n            crop_bboxes = uncrop_boxes_xyxy(crop_bboxes[keep], crop_region)\n            crop_masks = uncrop_masks(crop_masks[keep], crop_region, ih, iw)\n            crop_scores = crop_scores[keep]\n\n            pred_masks.append(crop_masks)\n            pred_bboxes.append(crop_bboxes)\n            pred_scores.append(crop_scores)\n            region_areas.append(area.expand(len(crop_masks)))\n\n        pred_masks = torch.cat(pred_masks)\n        pred_bboxes = torch.cat(pred_bboxes)\n        pred_scores = torch.cat(pred_scores)\n        region_areas = torch.cat(region_areas)\n\n        # Remove duplicate masks between crops\n        if len(crop_regions) > 1:\n            scores = 1 / region_areas\n            keep = torchvision.ops.nms(pred_bboxes, scores, crop_nms_thresh)\n            pred_masks, pred_bboxes, pred_scores = pred_masks[keep], pred_bboxes[keep], pred_scores[keep]\n\n        return pred_masks, pred_scores, pred_bboxes```\n\nUpdated code:\n```python\ndef generate(self,\n                 im,\n                 crop_n_layers=0,\n                 crop_overlap_ratio=512 / 1500,\n                 crop_downscale_factor=1,\n                 point_grids=None,\n                 points_stride=32,\n                 points_batch_size=64,\n                 conf_thres=0.88,\n                 stability_score_thresh=0.95,\n                 stability_score_offset=0.95,\n                 crop_nms_thresh=0.7):\n        \n        \n        \n        self.segment_all = True\n        ih, iw = im.shape[2:]\n        crop_regions, layer_idxs = generate_crop_boxes((ih, iw), crop_n_layers, crop_overlap_ratio)\n        if point_grids is None:\n            point_grids = build_all_layer_point_grids(points_stride, crop_n_layers, crop_downscale_factor)\n        pred_masks, pred_scores, pred_bboxes, region_areas = [], [], [], []\n        for crop_region, layer_idx in zip(crop_regions, layer_idxs):\n            x1, y1, x2, y2 = crop_region\n            w, h = x2 - x1, y2 - y1\n            area = torch.tensor(w * h, device=im.device)\n            points_scale = np.array([[w, h]])  # w, h\n            # Crop image and interpolate to input size\n            crop_im = F.interpolate(im[..., y1:y2, x1:x2], (ih, iw), mode='bilinear', align_corners=False)\n            # (num_points, 2)\n            points_for_image = point_grids[layer_idx] * points_scale\n            crop_masks, crop_scores, crop_bboxes = [], [], []\n            for (points, ) in batch_iterator(points_batch_size, points_for_image):\n                pred_mask, pred_score = self.prompt_inference(crop_im, points=points, multimask_output=True)\n                # Interpolate predicted masks to input size\n                pred_mask = F.interpolate(pred_mask[None], (h, w), mode='bilinear', align_corners=False)[0]\n                idx = pred_score > conf_thres\n                pred_mask, pred_score = pred_mask[idx], pred_score[idx]\n\n                stability_score = calculate_stability_score(pred_mask, self.model.mask_threshold,\n                                                            stability_score_offset)\n                idx = stability_score > stability_score_thresh\n                pred_mask, pred_score = pred_mask[idx], pred_score[idx]\n                # Bool type is much more memory-efficient.\n                pred_mask = pred_mask > self.model.mask_threshold\n                # (N, 4)\n                pred_bbox = batched_mask_to_box(pred_mask).float()\n                keep_mask = ~is_box_near_crop_edge(pred_bbox, crop_region, [0, 0, iw, ih])\n                if not torch.all(keep_mask):\n                    pred_bbox, pred_mask, pred_score = pred_bbox[keep_mask], pred_mask[keep_mask], pred_score[keep_mask]\n\n                crop_masks.append(pred_mask)\n                crop_bboxes.append(pred_bbox)\n                crop_scores.append(pred_score)\n\n            # Do nms within this crop\n            crop_masks = torch.cat(crop_masks)\n            crop_bboxes = torch.cat(crop_bboxes)\n            crop_scores = torch.cat(crop_scores)\n            keep = torchvision.ops.nms(crop_bboxes, crop_scores, self.args.iou)  # NMS\n            crop_bboxes = uncrop_boxes_xyxy(crop_bboxes[keep], crop_region)\n            crop_masks = uncrop_masks(crop_masks[keep], crop_region, ih, iw)\n            crop_scores = crop_scores[keep]\n\n            pred_masks.append(crop_masks)\n            pred_bboxes.append(crop_bboxes)\n            pred_scores.append(crop_scores)\n            region_areas.append(area.expand(len(crop_masks)))\n\n        pred_masks = torch.cat(pred_masks)\n        pred_bboxes = torch.cat(pred_bboxes)\n        pred_scores = torch.cat(pred_scores)\n        region_areas = torch.cat(region_areas)\n\n        # Remove duplicate masks between crops\n        if len(crop_regions) > 1:\n            scores = 1 / region_areas\n            keep = torchvision.ops.nms(pred_bboxes, scores, crop_nms_thresh)\n            pred_masks, pred_bboxes, pred_scores = pred_masks[keep], pred_bboxes[keep], pred_scores[keep]\n\n        return pred_masks, pred_scores, pred_bboxes\n```\n\nOriginal documentation:\n```python\nSegment the whole image.\n\n        Args:\n            im (torch.Tensor): The preprocessed image, (N, C, H, W).\n            crop_n_layers (int): If >0, mask prediction will be run again on\n                crops of the image. Sets the number of layers to run, where each\n                layer has 2**i_layer number of image crops.\n            crop_overlap_ratio (float): Sets the degree to which crops overlap.\n                In the first crop layer, crops will overlap by this fraction of\n                the image length. Later layers with more crops scale down this overlap.\n            crop_downscale_factor (int): The number of points-per-side\n                sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n            point_grids (list(np.ndarray), None): A list over explicit grids\n                of points used for sampling, normalized to [0,1]. The nth grid in the\n                list is used in the nth crop layer. Exclusive with points_per_side.\n            points_stride (int, None): The number of points to be sampled\n                along one side of the image. The total number of points is\n                points_per_side**2. If None, 'point_grids' must provide explicit\n                point sampling.\n            points_batch_size (int): Sets the number of points run simultaneously\n                by the model. Higher numbers may be faster but use more GPU memory.\n            conf_thres (float): A filtering threshold in [0,1], using the\n                model's predicted mask quality.\n            stability_score_thresh (float): A filtering threshold in [0,1], using\n                the stability of the mask under changes to the cutoff used to binarize\n                the model's mask predictions.\n            stability_score_offset (float): The amount to shift the cutoff when\n                calculated the stability score.\n            crop_nms_thresh (float): The box IoU cutoff used by non-maximal\n                suppression to filter duplicate masks between different crops.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform image segmentation using the Segment Anything Model (SAM).\n\n        This function segments an entire image into constituent parts by leveraging SAM's advanced architecture\n        and real-time performance capabilities. It can optionally work on image crops for finer segmentation.\n\n        Args:\n            im (torch.Tensor): Input tensor representing the preprocessed image with dimensions (N, C, H, W).\n            crop_n_layers (int): Specifies the number of layers for additional mask predictions on image crops.\n                                 Each layer produces 2**i_layer number of image crops.\n            crop_overlap_ratio (float): Determines the extent of overlap between crops. Scaled down in subsequent layers.\n            crop_downscale_factor (int): Scaling factor for the number of sampled points-per-side in each layer.\n            point_grids (list[np.ndarray], optional): Custom grids for point sampling normalized to [0,1].\n                                                      Used in the nth crop layer.\n            points_stride (int, optional): Number of points to sample along each side of the image.\n                                           Exclusive with 'point_grids'.\n            points_batch_size (int): Batch size for the number of points processed simultaneously.\n            conf_thres (float): Confidence threshold [0,1] for filtering based on the model's mask quality prediction.\n            stability_score_thresh (float): Stability threshold [0,1] for mask filtering based on mask stability.\n            stability_score_offset (float): Offset value for calculating stability score.\n            crop_nms_thresh (float): IoU cutoff for Non-Maximum Suppression (NMS) to remove duplicate masks between crops.\n\n        Returns:\n            tuple: A tuple containing segmented masks, confidence scores, and bounding boxes.", "file": "ultralytics_models_sam_predict.py", "function": "Predictor.generate", "project": "ultralytics", "change_stats": {"code_changes": 8, "doc_changes": 51, "total_changes": 59}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef import_and_test_deprecated_constant(\n    caplog: pytest.LogCaptureFixture,\n    module: ModuleType,\n    constant_name: str,\n    replacement_name: str,\n    replacement: Any,\n    breaks_in_ha_version: str,\n) -> None:\n    \n    \n    value = import_deprecated_costant(module, constant_name)\n    assert value == replacement\n    assert (\n        module.__name__,\n        logging.WARNING,\n        (\n            f\"{constant_name} was used from test_constant_deprecation,\"\n            f\" this is a deprecated constant which will be removed in HA Core {breaks_in_ha_version}. \"\n            f\"Use {replacement_name} instead, please report \"\n            \"it to the author of the 'test_constant_deprecation' custom integration\"\n        ),\n    ) in caplog.record_tuples\n\n    # verify deprecated constant is included in dir()\n    assert constant_name in dir(module)```\n\nUpdated code:\n```python\ndef import_and_test_deprecated_constant(\n    caplog: pytest.LogCaptureFixture,\n    module: ModuleType,\n    constant_name: str,\n    replacement_name: str,\n    replacement: Any,\n    breaks_in_ha_version: str,\n) -> None:\n    \n    \n    value = import_deprecated_constant(module, constant_name)\n    assert value == replacement\n    assert (\n        module.__name__,\n        logging.WARNING,\n        (\n            f\"{constant_name} was used from test_constant_deprecation,\"\n            f\" this is a deprecated constant which will be removed in HA Core {breaks_in_ha_version}. \"\n            f\"Use {replacement_name} instead, please report \"\n            \"it to the author of the 'test_constant_deprecation' custom integration\"\n        ),\n    ) in caplog.record_tuples\n\n    # verify deprecated constant is included in dir()\n    assert constant_name in dir(module)\n    assert constant_name in module.__all__\n```\n\nOriginal documentation:\n```python\nImport and test deprecated constant replaced by a value.\n\n    - Import deprecated constant\n    - Assert value is the same as the replacement\n    - Assert a warning is logged\n    - Assert the deprecated constant is included in the modules.__dir__()\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Import and test deprecated constant replaced by a value.\n\n    - Import deprecated constant\n    - Assert value is the same as the replacement\n    - Assert a warning is logged\n    - Assert the deprecated constant is included in the modules.__dir__()\n    - Assert the deprecated constant is included in the modules.__all__()", "file": "tests_common.py", "function": "import_and_test_deprecated_constant", "project": "core", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef apply(self, segments: list[Segment]) -> list[Segment]:\n        \n        \n        _Segment = Segment\n        _dim_style = dim_style\n        factor = self.dim_factor\n\n        return [\n            (\n                _Segment(\n                    segment.text,\n                    _dim_style(segment.style, factor),\n                    None,\n                )\n                if segment.style is not None and segment.style.dim\n                else segment\n            )\n            for segment in segments\n        ]```\n\nUpdated code:\n```python\ndef apply(self, segments: list[Segment], background: Color) -> list[Segment]:\n        \n        \n        _Segment = Segment\n        _dim_style = dim_style\n        factor = self.dim_factor\n\n        return [\n            (\n                _Segment(\n                    segment.text,\n                    _dim_style(segment.style, background, factor),\n                    None,\n                )\n                if segment.style is not None and segment.style.dim\n                else segment\n            )\n            for segment in segments\n        ]\n```\n\nOriginal documentation:\n```python\nTransform a list of segments.\n\n        Args:\n            segments: A list of segments.\n\n        Returns:\n            A new list of segments.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Transform a list of segments.\n\n        Args:\n            segments: A list of segments.\n            background: The background color.\n\n        Returns:\n            A new list of segments.", "file": "src_textual_filter.py", "function": "DimFilter.apply", "project": "textual", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef train(input_ids: List[str],\n          label_id: str,\n          model_id: str,\n          learner: str,\n          task: Optional[TaskType] = Task.CLASSIFICATION,\n          generic_hparms: Optional[\n              abstract_learner_pb2.GenericHyperParameters] = None,\n          ranking_group: Optional[str] = None,\n          training_config: Optional[abstract_learner_pb2.TrainingConfig] = None,\n          deployment_config: Optional[\n              abstract_learner_pb2.DeploymentConfig] = None,\n          guide: Optional[data_spec_pb2.DataSpecificationGuide] = None,\n          model_dir: Optional[str] = None,\n          keep_model_in_resource: Optional[bool] = True) -> tf.Operation:\n  \n  \n\n  if generic_hparms is None:\n    generic_hparms = abstract_learner_pb2.GenericHyperParameters()\n\n  if training_config is None:\n    training_config = abstract_learner_pb2.TrainingConfig()\n  else:\n    training_config = copy.deepcopy(training_config)\n\n  if deployment_config is None:\n    deployment_config = abstract_learner_pb2.DeploymentConfig()\n  else:\n    deployment_config = copy.deepcopy(deployment_config)\n\n  if guide is None:\n    guide = data_spec_pb2.DataSpecificationGuide()\n\n  feature_ids = [_input_key_to_id(model_id, x) for x in sorted(input_ids)]\n\n  if ranking_group is not None:\n    training_config.ranking_group = ranking_group\n    feature_ids.append(_input_key_to_id(model_id, ranking_group))\n\n  return training_op.SimpleMLModelTrainer(\n      feature_ids=\",\".join(feature_ids),\n      label_id=_input_key_to_id(model_id, label_id),\n      weight_id=\"\",\n      model_id=model_id if keep_model_in_resource else \"\",\n      model_dir=model_dir or \"\",\n      learner=learner,\n      hparams=generic_hparms.SerializeToString(),\n      task=task,\n      training_config=training_config.SerializeToString(),\n      deployment_config=deployment_config.SerializeToString(),\n      guide=guide.SerializeToString())```\n\nUpdated code:\n```python\ndef train(input_ids: List[str],\n          label_id: str,\n          weight_id: Optional[str],\n          model_id: str,\n          learner: str,\n          task: Optional[TaskType] = Task.CLASSIFICATION,\n          generic_hparms: Optional[\n              abstract_learner_pb2.GenericHyperParameters] = None,\n          ranking_group: Optional[str] = None,\n          training_config: Optional[abstract_learner_pb2.TrainingConfig] = None,\n          deployment_config: Optional[\n              abstract_learner_pb2.DeploymentConfig] = None,\n          guide: Optional[data_spec_pb2.DataSpecificationGuide] = None,\n          model_dir: Optional[str] = None,\n          keep_model_in_resource: Optional[bool] = True) -> tf.Operation:\n  \n  \n\n  if generic_hparms is None:\n    generic_hparms = abstract_learner_pb2.GenericHyperParameters()\n\n  if training_config is None:\n    training_config = abstract_learner_pb2.TrainingConfig()\n  else:\n    training_config = copy.deepcopy(training_config)\n\n  if deployment_config is None:\n    deployment_config = abstract_learner_pb2.DeploymentConfig()\n  else:\n    deployment_config = copy.deepcopy(deployment_config)\n\n  if guide is None:\n    guide = data_spec_pb2.DataSpecificationGuide()\n\n  feature_ids = [_input_key_to_id(model_id, x) for x in sorted(input_ids)]\n\n  if ranking_group is not None:\n    training_config.ranking_group = ranking_group\n    feature_ids.append(_input_key_to_id(model_id, ranking_group))\n\n  return training_op.SimpleMLModelTrainer(\n      feature_ids=\",\".join(feature_ids),\n      label_id=_input_key_to_id(model_id, label_id),\n      weight_id=\"\" if weight_id is None else _input_key_to_id(\n          model_id, weight_id),\n      model_id=model_id if keep_model_in_resource else \"\",\n      model_dir=model_dir or \"\",\n      learner=learner,\n      hparams=generic_hparms.SerializeToString(),\n      task=task,\n      training_config=training_config.SerializeToString(),\n      deployment_config=deployment_config.SerializeToString(),\n      guide=guide.SerializeToString())\n```\n\nOriginal documentation:\n```python\nTrains a model on the dataset accumulated by collect_training_examples.\n\n  Args:\n    input_ids: Ids/names of the input features.\n    label_id: Id/name of the label feature.\n    model_id: Id of the model.\n    learner: Key of the learner.\n    task: Task to solve.\n    generic_hparms: Hyper-parameter of the learner.\n    ranking_group: Id of the ranking group feature. Only for ranking.\n    training_config: Training configuration.\n    deployment_config: Deployment configuration (e.g. where to train the model).\n    guide: Dataset specification guide.\n    model_dir: If specified, export the trained model into this directory.\n    keep_model_in_resource: If true, keep the model as a training model\n      resource.\n\n  Returns:\n    The OP that trigger the training.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Trains a model on the dataset accumulated by collect_training_examples.\n\n  Args:\n    input_ids: Ids/names of the input features.\n    label_id: Id/name of the label feature.\n    weight_id: Id/name of the weight feature.\n    model_id: Id of the model.\n    learner: Key of the learner.\n    task: Task to solve.\n    generic_hparms: Hyper-parameter of the learner.\n    ranking_group: Id of the ranking group feature. Only for ranking.\n    training_config: Training configuration.\n    deployment_config: Deployment configuration (e.g. where to train the model).\n    guide: Dataset specification guide.\n    model_dir: If specified, export the trained model into this directory.\n    keep_model_in_resource: If true, keep the model as a training model\n      resource.\n\n  Returns:\n    The OP that trigger the training.", "file": "tensorflow_decision_forests_tensorflow_core.py", "function": "train", "project": "decision-forests", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef WriteMakeRule(self, outputs, inputs, actions=None, comment=None,\n                    order_only=False, force=False, phony=False):\n    \n    \n    outputs = map(QuoteSpaces, outputs)\n    inputs = map(QuoteSpaces, inputs)\n\n    if comment:\n      self.WriteLn('# ' + comment)\n    if phony:\n      self.WriteLn('.PHONY: ' + ' '.join(outputs))\n    # TODO(evanm): just make order_only a list of deps instead of these hacks.\n    if order_only:\n      order_insert = '| '\n      pick_output = ' '.join(outputs)\n    else:\n      order_insert = ''\n      pick_output = outputs[0]\n    if force:\n      force_append = ' FORCE_DO_CMD'\n    else:\n      force_append = ''\n    if actions:\n      self.WriteLn(\"%s: TOOLSET := $(TOOLSET)\" % outputs[0])\n    self.WriteLn('%s: %s%s%s' % (pick_output, order_insert, ' '.join(inputs),\n                                 force_append))\n    if actions:\n      for action in actions:\n        self.WriteLn('\\t%s' % action)\n    if not order_only and len(outputs) > 1:\n      # If we have more than one output, a rule like\n      #   foo bar: baz\n      # that for *each* output we must run the action, potentially\n      # in parallel.  That is not what we're trying to write -- what\n      # we want is that we run the action once and it generates all\n      # the files.\n      # http://www.gnu.org/software/hello/manual/automake/Multiple-Outputs.html\n      # discusses this problem and has this solution:\n      # 1) Write the naive rule that would produce parallel runs of\n      # the action.\n      # 2) Make the outputs seralized on each other, so we won't start\n      # a parallel run until the first run finishes, at which point\n      # we'll have generated all the outputs and we're done.\n      self.WriteLn('%s: %s' % (' '.join(outputs[1:]), outputs[0]))\n      # Add a dummy command to the \"extra outputs\" rule, otherwise make seems to\n      # think these outputs haven't (couldn't have?) changed, and thus doesn't\n      # flag them as changed (i.e. include in '$?') when evaluating dependent\n      # rules, which in turn causes do_cmd() to skip running dependent commands.\n      self.WriteLn('%s: ;' % (' '.join(outputs[1:])))\n    self.WriteLn()```\n\nUpdated code:\n```python\ndef WriteMakeRule(self, outputs, inputs, actions=None, comment=None,\n                    order_only=False, force=False, phony=False, command=None):\n    \n    \n    outputs = map(QuoteSpaces, outputs)\n    inputs = map(QuoteSpaces, inputs)\n\n    if comment:\n      self.WriteLn('# ' + comment)\n    if phony:\n      self.WriteLn('.PHONY: ' + ' '.join(outputs))\n    if actions:\n      self.WriteLn(\"%s: TOOLSET := $(TOOLSET)\" % outputs[0])\n    force_append = ' FORCE_DO_CMD' if force else ''\n\n    if order_only:\n      # Order only rule: Just write a simple rule.\n      # TODO(evanm): just make order_only a list of deps instead of this hack.\n      self.WriteLn('%s: | %s%s' %\n                   (' '.join(outputs), ' '.join(inputs), force_append))\n    elif len(outputs) == 1:\n      # Regular rule, one output: Just write a simple rule.\n      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))\n    else:\n      # Regular rule, more than one output: Multiple outputs are tricky in\n      # make. We will write three rules:\n      # - All outputs depend on an intermediate file.\n      # - Make .INTERMEDIATE depend on the intermediate.\n      # - The intermediate file depends on the inputs and executes the\n      #   actual command.\n      # - The intermediate recipe will 'touch' the intermediate file.\n      # - The multi-output rule will have an do-nothing recipe.\n      intermediate = \"%s.intermediate\" % (command if command else self.target)\n      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))\n      self.WriteLn('\\t%s' % '@:');\n      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))\n      self.WriteLn('%s: %s%s' %\n                   (intermediate, ' '.join(inputs), force_append))\n      actions.insert(0, '$(call do_cmd,touch)')\n\n    if actions:\n      for action in actions:\n        self.WriteLn('\\t%s' % action)\n    self.WriteLn()\n```\n\nOriginal documentation:\n```python\nWrite a Makefile rule, with some extra tricks.\n\n    outputs: a list of outputs for the rule (note: this is not directly\n             supported by make; see comments below)\n    inputs: a list of inputs for the rule\n    actions: a list of shell commands to run for the rule\n    comment: a comment to put in the Makefile above the rule (also useful\n             for making this Python script's code self-documenting)\n    order_only: if true, makes the dependency order-only\n    force: if true, include FORCE_DO_CMD as an order-only dep\n    phony: if true, the rule does not actually generate the named output, the\n           output is just a name to run the rule\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write a Makefile rule, with some extra tricks.\n\n    outputs: a list of outputs for the rule (note: this is not directly\n             supported by make; see comments below)\n    inputs: a list of inputs for the rule\n    actions: a list of shell commands to run for the rule\n    comment: a comment to put in the Makefile above the rule (also useful\n             for making this Python script's code self-documenting)\n    order_only: if true, makes the dependency order-only\n    force: if true, include FORCE_DO_CMD as an order-only dep\n    phony: if true, the rule does not actually generate the named output, the\n           output is just a name to run the rule\n    command: (optional) command name to generate unambiguous labels", "file": "gyp_pylib_gyp_generator_make.py", "function": "MakefileWriter.WriteMakeRule", "project": "node-gyp", "change_stats": {"code_changes": 64, "doc_changes": 3, "total_changes": 67}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _extract_lookup_tables(\n        entity_js: Dict[Text, Any], examples_js: List[Dict[Text, Any]]\n    ) -> Optional[List[Dict[Text, Any]]]:\n        \n        synonyms = [e[\"synonyms\"] for e in examples_js if \"synonyms\" in e]\n        synonyms = DialogflowReader._flatten(synonyms)\n        elements = [synonym for synonym in synonyms if \"@\" not in synonym]\n\n        if len(elements) == 0:\n            return None\n        return [{\"name\": entity_js.get(\"name\"), \"elements\": elements}]```\n\nUpdated code:\n```python\ndef _extract_lookup_tables(\n        entity: Dict[Text, Any], examples: List[Dict[Text, Any]]\n    ) -> Optional[List[Dict[Text, Any]]]:\n        \n        synonyms = [e[\"synonyms\"] for e in examples if \"synonyms\" in e]\n        synonyms = DialogflowReader._flatten(synonyms)\n        elements = [synonym for synonym in synonyms if \"@\" not in synonym]\n\n        if len(elements) == 0:\n            return None\n        return [{\"name\": entity.get(\"name\"), \"elements\": elements}]\n```\n\nOriginal documentation:\n```python\nExtract the lookup table from the entity synonyms.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extracts the lookup table from the entity synonyms.", "file": "rasa_shared_nlu_training_data_formats_dialogflow.py", "function": "DialogflowReader._extract_lookup_tables", "project": "rasa", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_tracker(\n        self, sender_id: Text, append_action_listen: bool = True,\n    ) -> DialogueStateTracker:\n        \n\n        \n        tracker = self.init_tracker(sender_id)\n\n        if tracker:\n            if append_action_listen:\n                tracker.update(ActionExecuted(ACTION_LISTEN_NAME))\n\n            self.save(tracker)\n\n        return tracker```\n\nUpdated code:\n```python\ndef create_tracker(\n        self, sender_id: Text, append_action_listen: bool = True\n    ) -> DialogueStateTracker:\n        \n\n        tracker = self.init_tracker(sender_id)\n        if tracker:\n            if append_action_listen:\n                tracker.update(ActionExecuted(ACTION_LISTEN_NAME))\n            self.save(tracker)\n        return tracker\n```\n\nOriginal documentation:\n```python\nCreates a new tracker for `sender_id`.\n\n        The tracker begins with a `SessionStarted` event and is initially listening.\n\n        Args:\n            sender_id: Conversation ID associated with the tracker.\n            append_action_listen: Whether or not to append an initial `action_listen`.\n\n        Returns:\n            The newly created tracker for `sender_id`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates a new tracker for the sender_id.\n\n        The tracker is initially listening.", "file": "rasa_core_tracker_store.py", "function": "TrackerStore.create_tracker", "project": "rasa", "change_stats": {"code_changes": 8, "doc_changes": 13, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(\n        self,\n        prompt: Optional[Union[List[str], str]] = None,\n        image: PipelineImageInput = None,\n        strength: float = 0.5,\n        num_inference_steps: int = 12,\n        guidance_scale: float = 10.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        generator: Optional[torch.Generator] = None,\n        prompt_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        negative_prompt_embeds: Optional[torch.Tensor] = None,\n        negative_encoder_hidden_states: Optional[torch.Tensor] = None,\n        output_type=\"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        micro_conditioning_aesthetic_score: int = 6,\n        micro_conditioning_crop_coord: Tuple[int, int] = (0, 0),\n        temperature: Union[int, Tuple[int, int], List[int]] = (2, 0),\n    ):\n        \n        \n        \n\n        if (prompt_embeds is not None and encoder_hidden_states is None) or (\n            prompt_embeds is None and encoder_hidden_states is not None\n        ):\n            raise ValueError(\"pass either both `prompt_embeds` and `encoder_hidden_states` or neither\")\n\n        if (negative_prompt_embeds is not None and negative_encoder_hidden_states is None) or (\n            negative_prompt_embeds is None and negative_encoder_hidden_states is not None\n        ):\n            raise ValueError(\n                \"pass either both `negatve_prompt_embeds` and `negative_encoder_hidden_states` or neither\"\n            )\n\n        if (prompt is None and prompt_embeds is None) or (prompt is not None and prompt_embeds is not None):\n            raise ValueError(\"pass only one of `prompt` or `prompt_embeds`\")\n\n        if isinstance(prompt, str):\n            prompt = [prompt]\n\n        if prompt is not None:\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        batch_size = batch_size * num_images_per_prompt\n\n        if prompt_embeds is None:\n            input_ids = self.tokenizer(\n                prompt,\n                return_tensors=\"pt\",\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids.to(self._execution_device)\n\n            outputs = self.text_encoder(input_ids, return_dict=True, output_hidden_states=True)\n            prompt_embeds = outputs.text_embeds\n            encoder_hidden_states = outputs.hidden_states[-2]\n\n        prompt_embeds = prompt_embeds.repeat(num_images_per_prompt, 1)\n        encoder_hidden_states = encoder_hidden_states.repeat(num_images_per_prompt, 1, 1)\n\n        if guidance_scale > 1.0:\n            if negative_prompt_embeds is None:\n                if negative_prompt is None:\n                    negative_prompt = [\"\"] * len(prompt)\n\n                if isinstance(negative_prompt, str):\n                    negative_prompt = [negative_prompt]\n\n                input_ids = self.tokenizer(\n                    negative_prompt,\n                    return_tensors=\"pt\",\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.tokenizer.model_max_length,\n                ).input_ids.to(self._execution_device)\n\n                outputs = self.text_encoder(input_ids, return_dict=True, output_hidden_states=True)\n                negative_prompt_embeds = outputs.text_embeds\n                negative_encoder_hidden_states = outputs.hidden_states[-2]\n\n            negative_prompt_embeds = negative_prompt_embeds.repeat(num_images_per_prompt, 1)\n            negative_encoder_hidden_states = negative_encoder_hidden_states.repeat(num_images_per_prompt, 1, 1)\n\n            prompt_embeds = torch.concat([negative_prompt_embeds, prompt_embeds])\n            encoder_hidden_states = torch.concat([negative_encoder_hidden_states, encoder_hidden_states])\n\n        image = self.image_processor.preprocess(image)\n\n        height, width = image.shape[-2:]\n\n        # Note that the micro conditionings _do_ flip the order of width, height for the original size\n        # and the crop coordinates. This is how it was done in the original code base\n        micro_conds = torch.tensor(\n            [\n                width,\n                height,\n                micro_conditioning_crop_coord[0],\n                micro_conditioning_crop_coord[1],\n                micro_conditioning_aesthetic_score,\n            ],\n            device=self._execution_device,\n            dtype=encoder_hidden_states.dtype,\n        )\n\n        micro_conds = micro_conds.unsqueeze(0)\n        micro_conds = micro_conds.expand(2 * batch_size if guidance_scale > 1.0 else batch_size, -1)\n\n        self.scheduler.set_timesteps(num_inference_steps, temperature, self._execution_device)\n        num_inference_steps = int(len(self.scheduler.timesteps) * strength)\n        start_timestep_idx = len(self.scheduler.timesteps) - num_inference_steps\n\n        needs_upcasting = self.vqvae.dtype == torch.float16 and self.vqvae.config.force_upcast\n\n        if needs_upcasting:\n            self.vqvae.float()\n\n        latents = self.vqvae.encode(image.to(dtype=self.vqvae.dtype, device=self._execution_device)).latents\n        latents_bsz, channels, latents_height, latents_width = latents.shape\n        latents = self.vqvae.quantize(latents)[2][2].reshape(latents_bsz, latents_height, latents_width)\n        latents = self.scheduler.add_noise(\n            latents, self.scheduler.timesteps[start_timestep_idx - 1], generator=generator\n        )\n        latents = latents.repeat(num_images_per_prompt, 1, 1)\n\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i in range(start_timestep_idx, len(self.scheduler.timesteps)):\n                timestep = self.scheduler.timesteps[i]\n\n                if guidance_scale > 1.0:\n                    model_input = torch.cat([latents] * 2)\n                else:\n                    model_input = latents\n\n                model_output = self.transformer(\n                    model_input,\n                    micro_conds=micro_conds,\n                    pooled_text_emb=prompt_embeds,\n                    encoder_hidden_states=encoder_hidden_states,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                )\n\n                if guidance_scale > 1.0:\n                    uncond_logits, cond_logits = model_output.chunk(2)\n                    model_output = uncond_logits + guidance_scale * (cond_logits - uncond_logits)\n\n                latents = self.scheduler.step(\n                    model_output=model_output,\n                    timestep=timestep,\n                    sample=latents,\n                    generator=generator,\n                ).prev_sample\n\n                if i == len(self.scheduler.timesteps) - 1 or ((i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, timestep, latents)\n\n        if output_type == \"latent\":\n            output = latents\n        else:\n            output = self.vqvae.decode(\n                latents,\n                force_not_quantize=True,\n                shape=(\n                    batch_size,\n                    height // self.vae_scale_factor,\n                    width // self.vae_scale_factor,\n                    self.vqvae.config.latent_channels,\n                ),\n            ).sample.clip(0, 1)\n            output = self.image_processor.postprocess(output, output_type)\n\n            if needs_upcasting:\n                self.vqvae.half()\n\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (output,)\n\n        return ImagePipelineOutput(output)```\n\nUpdated code:\n```python\ndef __call__(\n        self,\n        prompt: Optional[Union[List[str], str]] = None,\n        image: PipelineImageInput = None,\n        strength: float = 0.5,\n        num_inference_steps: int = 12,\n        guidance_scale: float = 10.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        generator: Optional[torch.Generator] = None,\n        prompt_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        negative_prompt_embeds: Optional[torch.Tensor] = None,\n        negative_encoder_hidden_states: Optional[torch.Tensor] = None,\n        output_type=\"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        micro_conditioning_aesthetic_score: int = 6,\n        micro_conditioning_crop_coord: Tuple[int, int] = (0, 0),\n        temperature: Union[int, Tuple[int, int], List[int]] = (2, 0),\n    ):\n        \n        \n        \n\n        if (prompt_embeds is not None and encoder_hidden_states is None) or (\n            prompt_embeds is None and encoder_hidden_states is not None\n        ):\n            raise ValueError(\"pass either both `prompt_embeds` and `encoder_hidden_states` or neither\")\n\n        if (negative_prompt_embeds is not None and negative_encoder_hidden_states is None) or (\n            negative_prompt_embeds is None and negative_encoder_hidden_states is not None\n        ):\n            raise ValueError(\n                \"pass either both `negative_prompt_embeds` and `negative_encoder_hidden_states` or neither\"\n            )\n\n        if (prompt is None and prompt_embeds is None) or (prompt is not None and prompt_embeds is not None):\n            raise ValueError(\"pass only one of `prompt` or `prompt_embeds`\")\n\n        if isinstance(prompt, str):\n            prompt = [prompt]\n\n        if prompt is not None:\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        batch_size = batch_size * num_images_per_prompt\n\n        if prompt_embeds is None:\n            input_ids = self.tokenizer(\n                prompt,\n                return_tensors=\"pt\",\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids.to(self._execution_device)\n\n            outputs = self.text_encoder(input_ids, return_dict=True, output_hidden_states=True)\n            prompt_embeds = outputs.text_embeds\n            encoder_hidden_states = outputs.hidden_states[-2]\n\n        prompt_embeds = prompt_embeds.repeat(num_images_per_prompt, 1)\n        encoder_hidden_states = encoder_hidden_states.repeat(num_images_per_prompt, 1, 1)\n\n        if guidance_scale > 1.0:\n            if negative_prompt_embeds is None:\n                if negative_prompt is None:\n                    negative_prompt = [\"\"] * len(prompt)\n\n                if isinstance(negative_prompt, str):\n                    negative_prompt = [negative_prompt]\n\n                input_ids = self.tokenizer(\n                    negative_prompt,\n                    return_tensors=\"pt\",\n                    padding=\"max_length\",\n                    truncation=True,\n                    max_length=self.tokenizer.model_max_length,\n                ).input_ids.to(self._execution_device)\n\n                outputs = self.text_encoder(input_ids, return_dict=True, output_hidden_states=True)\n                negative_prompt_embeds = outputs.text_embeds\n                negative_encoder_hidden_states = outputs.hidden_states[-2]\n\n            negative_prompt_embeds = negative_prompt_embeds.repeat(num_images_per_prompt, 1)\n            negative_encoder_hidden_states = negative_encoder_hidden_states.repeat(num_images_per_prompt, 1, 1)\n\n            prompt_embeds = torch.concat([negative_prompt_embeds, prompt_embeds])\n            encoder_hidden_states = torch.concat([negative_encoder_hidden_states, encoder_hidden_states])\n\n        image = self.image_processor.preprocess(image)\n\n        height, width = image.shape[-2:]\n\n        # Note that the micro conditionings _do_ flip the order of width, height for the original size\n        # and the crop coordinates. This is how it was done in the original code base\n        micro_conds = torch.tensor(\n            [\n                width,\n                height,\n                micro_conditioning_crop_coord[0],\n                micro_conditioning_crop_coord[1],\n                micro_conditioning_aesthetic_score,\n            ],\n            device=self._execution_device,\n            dtype=encoder_hidden_states.dtype,\n        )\n\n        micro_conds = micro_conds.unsqueeze(0)\n        micro_conds = micro_conds.expand(2 * batch_size if guidance_scale > 1.0 else batch_size, -1)\n\n        self.scheduler.set_timesteps(num_inference_steps, temperature, self._execution_device)\n        num_inference_steps = int(len(self.scheduler.timesteps) * strength)\n        start_timestep_idx = len(self.scheduler.timesteps) - num_inference_steps\n\n        needs_upcasting = self.vqvae.dtype == torch.float16 and self.vqvae.config.force_upcast\n\n        if needs_upcasting:\n            self.vqvae.float()\n\n        latents = self.vqvae.encode(image.to(dtype=self.vqvae.dtype, device=self._execution_device)).latents\n        latents_bsz, channels, latents_height, latents_width = latents.shape\n        latents = self.vqvae.quantize(latents)[2][2].reshape(latents_bsz, latents_height, latents_width)\n        latents = self.scheduler.add_noise(\n            latents, self.scheduler.timesteps[start_timestep_idx - 1], generator=generator\n        )\n        latents = latents.repeat(num_images_per_prompt, 1, 1)\n\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i in range(start_timestep_idx, len(self.scheduler.timesteps)):\n                timestep = self.scheduler.timesteps[i]\n\n                if guidance_scale > 1.0:\n                    model_input = torch.cat([latents] * 2)\n                else:\n                    model_input = latents\n\n                model_output = self.transformer(\n                    model_input,\n                    micro_conds=micro_conds,\n                    pooled_text_emb=prompt_embeds,\n                    encoder_hidden_states=encoder_hidden_states,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                )\n\n                if guidance_scale > 1.0:\n                    uncond_logits, cond_logits = model_output.chunk(2)\n                    model_output = uncond_logits + guidance_scale * (cond_logits - uncond_logits)\n\n                latents = self.scheduler.step(\n                    model_output=model_output,\n                    timestep=timestep,\n                    sample=latents,\n                    generator=generator,\n                ).prev_sample\n\n                if i == len(self.scheduler.timesteps) - 1 or ((i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, timestep, latents)\n\n        if output_type == \"latent\":\n            output = latents\n        else:\n            output = self.vqvae.decode(\n                latents,\n                force_not_quantize=True,\n                shape=(\n                    batch_size,\n                    height // self.vae_scale_factor,\n                    width // self.vae_scale_factor,\n                    self.vqvae.config.latent_channels,\n                ),\n            ).sample.clip(0, 1)\n            output = self.image_processor.postprocess(output, output_type)\n\n            if needs_upcasting:\n                self.vqvae.half()\n\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (output,)\n\n        return ImagePipelineOutput(output)\n```\n\nOriginal documentation:\n```python\nThe call function to the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n                `Image`, numpy array or tensor representing an image batch to be used as the starting point. For both\n                numpy array and pytorch tensor, the expected value range is between `[0, 1]` If it's a tensor or a list\n                or tensors, the expected shape should be `(B, C, H, W)` or `(C, H, W)`. If it is a numpy array or a\n                list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n                latents as `image`, but if passing latents directly it is not encoded again.\n            strength (`float`, *optional*, defaults to 0.5):\n                Indicates extent to transform the reference `image`. Must be between 0 and 1. `image` is used as a\n                starting point and more noise is added the higher the `strength`. The number of denoising steps depends\n                on the amount of noise initially added. When `strength` is 1, added noise is maximum and the denoising\n                process runs for the full number of iterations specified in `num_inference_steps`. A value of 1\n                essentially ignores `image`.\n            num_inference_steps (`int`, *optional*, defaults to 16):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 10.0):\n                A higher guidance scale value encourages the model to generate images closely linked to the text\n                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            generator (`torch.Generator`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n                provided, text embeddings are generated from the `prompt` input argument. A single vector from the\n                pooled and projected final hidden states.\n            encoder_hidden_states (`torch.FloatTensor`, *optional*):\n                Pre-generated penultimate hidden states from the text encoder providing additional text conditioning.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n            negative_encoder_hidden_states (`torch.FloatTensor`, *optional*):\n                Analogous to `encoder_hidden_states` for the positive prompt.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that calls every `callback_steps` steps during inference. The function is called with the\n                following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function is called. If not specified, the callback is called at\n                every step.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n            micro_conditioning_aesthetic_score (`int`, *optional*, defaults to 6):\n                The targeted aesthetic score according to the laion aesthetic classifier. See https://laion.ai/blog/laion-aesthetics/\n                and the micro-conditioning section of https://arxiv.org/abs/2307.01952.\n            micro_conditioning_crop_coord (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                The targeted height, width crop coordinates. See the micro-conditioning section of https://arxiv.org/abs/2307.01952.\n            temperature (`Union[int, Tuple[int, int], List[int]]`, *optional*, defaults to (2, 0)):\n                Configures the temperature scheduler on `self.scheduler` see `AmusedScheduler#set_timesteps`.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.pipeline_utils.ImagePipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`~pipelines.pipeline_utils.ImagePipelineOutput`] is returned, otherwise a\n                `tuple` is returned where the first element is a list with the generated images.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "The call function to the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n            image (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n                `Image`, numpy array or tensor representing an image batch to be used as the starting point. For both\n                numpy array and pytorch tensor, the expected value range is between `[0, 1]` If it's a tensor or a list\n                or tensors, the expected shape should be `(B, C, H, W)` or `(C, H, W)`. If it is a numpy array or a\n                list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n                latents as `image`, but if passing latents directly it is not encoded again.\n            strength (`float`, *optional*, defaults to 0.5):\n                Indicates extent to transform the reference `image`. Must be between 0 and 1. `image` is used as a\n                starting point and more noise is added the higher the `strength`. The number of denoising steps depends\n                on the amount of noise initially added. When `strength` is 1, added noise is maximum and the denoising\n                process runs for the full number of iterations specified in `num_inference_steps`. A value of 1\n                essentially ignores `image`.\n            num_inference_steps (`int`, *optional*, defaults to 12):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 10.0):\n                A higher guidance scale value encourages the model to generate images closely linked to the text\n                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            generator (`torch.Generator`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n                provided, text embeddings are generated from the `prompt` input argument. A single vector from the\n                pooled and projected final hidden states.\n            encoder_hidden_states (`torch.FloatTensor`, *optional*):\n                Pre-generated penultimate hidden states from the text encoder providing additional text conditioning.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n            negative_encoder_hidden_states (`torch.FloatTensor`, *optional*):\n                Analogous to `encoder_hidden_states` for the positive prompt.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that calls every `callback_steps` steps during inference. The function is called with the\n                following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function is called. If not specified, the callback is called at\n                every step.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n            micro_conditioning_aesthetic_score (`int`, *optional*, defaults to 6):\n                The targeted aesthetic score according to the laion aesthetic classifier. See https://laion.ai/blog/laion-aesthetics/\n                and the micro-conditioning section of https://arxiv.org/abs/2307.01952.\n            micro_conditioning_crop_coord (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                The targeted height, width crop coordinates. See the micro-conditioning section of https://arxiv.org/abs/2307.01952.\n            temperature (`Union[int, Tuple[int, int], List[int]]`, *optional*, defaults to (2, 0)):\n                Configures the temperature scheduler on `self.scheduler` see `AmusedScheduler#set_timesteps`.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.pipeline_utils.ImagePipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`~pipelines.pipeline_utils.ImagePipelineOutput`] is returned, otherwise a\n                `tuple` is returned where the first element is a list with the generated images.", "file": "src_diffusers_pipelines_amused_pipeline_amused_img2img.py", "function": "AmusedImg2ImgPipeline.__call__", "project": "diffusers", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, emitter, schedule_file='schedule.json'):\n        \n            \n        \n        super(EventScheduler, self).__init__()\n        data_dir = expanduser(Configuration.get()['data_dir'])\n\n        self.events = {}\n        self.event_lock = Lock()\n\n        self.emitter = emitter\n        self.isRunning = True\n        self.schedule_file = join(data_dir, schedule_file)\n        if self.schedule_file:\n            self.load()\n\n        self.emitter.on('mycroft.scheduler.schedule_event',\n                        self.schedule_event_handler)\n        self.emitter.on('mycroft.scheduler.remove_event',\n                        self.remove_event_handler)\n        self.emitter.on('mycroft.scheduler.update_event',\n                        self.update_event_handler)\n        self.emitter.on('mycroft.scheduler.get_event',\n                        self.get_event_handler)\n        self.start()```\n\nUpdated code:\n```python\ndef __init__(self, bus, schedule_file='schedule.json'):\n        \n            \n        \n        super(EventScheduler, self).__init__()\n        data_dir = expanduser(Configuration.get()['data_dir'])\n\n        self.events = {}\n        self.event_lock = Lock()\n\n        self.bus = bus\n        self.isRunning = True\n        self.schedule_file = join(data_dir, schedule_file)\n        if self.schedule_file:\n            self.load()\n\n        self.bus.on('mycroft.scheduler.schedule_event',\n                    self.schedule_event_handler)\n        self.bus.on('mycroft.scheduler.remove_event',\n                    self.remove_event_handler)\n        self.bus.on('mycroft.scheduler.update_event',\n                    self.update_event_handler)\n        self.bus.on('mycroft.scheduler.get_event',\n                    self.get_event_handler)\n        self.start()\n```\n\nOriginal documentation:\n```python\nCreate an event scheduler thread. Will send messages at a\n            predetermined time to the registered targets.\n\n            Args:\n                emitter:        event emitter to use to send messages\n                schedule_file:  File to store pending events to on shutdown\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create an event scheduler thread. Will send messages at a\n            predetermined time to the registered targets.\n\n            Args:\n                bus:            Mycroft messagebus (mycroft.messagebus)\n                schedule_file:  File to store pending events to on shutdown", "file": "mycroft_skills_event_scheduler.py", "function": "EventScheduler.__init__", "project": "mycroft-core", "change_stats": {"code_changes": 22, "doc_changes": 4, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_nameid(self):\n        \n        \n        \n        nameid_data = self.get_nameid_data()\n        return nameid_data['Value']```\n\nUpdated code:\n```python\ndef get_nameid(self):\n        \n        \n        \n        nameid_value = None\n        nameid_data = self.get_nameid_data()\n        if nameid_data and 'Value' in nameid_data.keys():\n            nameid_value = nameid_data['Value']\n        return nameid_value\n```\n\nOriginal documentation:\n```python\nGets the NameID provided by the SAML Response from the IdP\n\n        :returns: NameID (value)\n        :rtype: string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets the NameID provided by the SAML Response from the IdP\n\n        :returns: NameID (value)\n        :rtype: string|None", "file": "src_onelogin_saml2_response.py", "function": "OneLogin_Saml2_Response.get_nameid", "project": "python3-saml", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef enhance_ships_order(self, favourite=None):\n        \n        \n        \n        if favourite is None:\n            favourite = self.config.ENHANCE_FAVOURITE\n\n        logger.hr('Enhancement by type')\n        total = 0\n        exhaust_count = 0\n\n        ship_types = [s.strip().lower() for s in self.config.ENHANCE_ORDER_STRING.split('>')]\n        if ship_types == ['']:\n            ship_types = [None]\n        logger.attr('Enhance Order', ship_types)\n\n        for ship_type in ship_types:\n            logger.info(f'Favourite={favourite}, Ship Type={ship_type}')\n\n            if not self._enhance_enter(favourite=favourite, ship_type=ship_type):\n                logger.hr(f'Dock Empty by ship type {ship_type}')\n                continue\n\n            # Keep track whether a successful enhancement went by at least once\n            at_least_once = False\n\n            while 1:\n                if not self._enhance_choose():\n                    if not at_least_once:\n                        exhaust_count += 1\n                    break\n                else:\n                    at_least_once = True\n                self._enhance_confirm()\n                total += 10\n                if total >= self._retire_amount:\n                    break\n            self.ui_back(DOCK_FILTER)\n\n            if exhaust_count > 2:\n                logger.info(f'Mats exhausted, threshold exceeded, terminate early. Last filter was {ship_type}')\n                break\n\n        self._enhance_quit()\n        return total```\n\nUpdated code:\n```python\ndef enhance_ships_order(self, favourite=None):\n        \n        \n        \n        if favourite is None:\n            favourite = self.config.ENHANCE_FAVOURITE\n\n        logger.hr('Enhancement by type')\n        total = 0\n        exhaust_count = 0\n\n        ship_types = [s.strip().lower() for s in self.config.ENHANCE_ORDER_STRING.split('>')]\n        if ship_types == ['']:\n            ship_types = [None]\n        logger.attr('Enhance Order', ship_types)\n\n        for ship_type in ship_types:\n            logger.info(f'Favourite={favourite}, Ship Type={ship_type}')\n\n            if not self._enhance_enter(favourite=favourite, ship_type=ship_type):\n                logger.hr(f'Dock Empty by ship type {ship_type}')\n                continue\n\n            # Keep track whether a successful enhancement went by at least once\n            at_least_once = False\n\n            while 1:\n                if not self._enhance_choose():\n                    if not at_least_once:\n                        exhaust_count += 1\n                    break\n                at_least_once = True\n                self._enhance_confirm()\n                total += 10\n                if total >= self._retire_amount:\n                    break\n            self.ui_back(DOCK_FILTER)\n\n            if exhaust_count > 2:\n                logger.info(f'Mats exhausted, threshold exceeded, terminate early. Last filter was {ship_type}')\n                break\n\n        self._enhance_quit()\n        return total\n```\n\nOriginal documentation:\n```python\nPages:\n            in: page_dock\n            out: page_dock\n\n        Args:\n            favourite (bool):\n\n        Returns:\n            int: total enhanced\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Info:\n            Target ships in order of specified\n            type listing by ENHANCE_ORDER_STRING\n\n        Pages:\n            in: page_dock\n            out: page_dock\n\n        Args:\n            favourite (bool):\n\n        Returns:\n            int: total enhanced", "file": "module_retire_enhancement.py", "function": "Enhancement.enhance_ships_order", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 5, "doc_changes": 8, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef execute(self, *args, **kwargs):\n        \n        return self.engine.execute(*args, **kwargs)```\n\nUpdated code:\n```python\ndef execute(self, *args, **kwargs):\n        \n        return self.connectable.execute(*args, **kwargs)\n```\n\nOriginal documentation:\n```python\nSimple passthrough to SQLAlchemy engine\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Simple passthrough to SQLAlchemy connectable", "file": "pandas_io_sql.py", "function": "SQLDatabase.execute", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_range_header(range_header: str, object_size: int) -> ObjectRange:\n    \n    \n    \n    last = object_size - 1\n    _, rspec = range_header.split(\"=\")\n    # TODO: check with AWS\n    if \",\" in rspec:\n        raise NotImplementedError(\"Multiple range specifiers not supported\")\n\n    begin, end = [int(i) if i else None for i in rspec.split(\"-\")]\n    if begin is not None:  # byte range\n        end = last if end is None else min(end, last)\n    elif end is not None:  # suffix byte range\n        begin = object_size - min(end, object_size)\n        end = last\n    else:\n        # TODO, find exception here\n        raise Exception(\"TODO\")\n    if begin < 0 or end > last or begin > min(end, last):\n        raise InvalidRange(\n            \"The requested range is not satisfiable\",\n            ActualObjectSize=str(object_size),\n            RangeRequested=range_header,\n        )\n\n    return ObjectRange(\n        content_range=f\"bytes {begin}-{end}/{object_size}\",\n        content_length=end - begin + 1,\n        begin=begin,\n        end=end,\n    )```\n\nUpdated code:\n```python\ndef parse_range_header(range_header: str, object_size: int) -> ObjectRange | None:\n    \n    \n    \n    last = object_size - 1\n    try:\n        _, rspec = range_header.split(\"=\")\n    except ValueError:\n        return None\n    if \",\" in rspec:\n        return None\n\n    try:\n        begin, end = [int(i) if i else None for i in rspec.split(\"-\")]\n    except ValueError:\n        # if we can't parse the Range header, S3 just treat the request as a non-range request\n        return None\n\n    if (begin is None and end == 0) or (begin is not None and begin > last):\n        raise InvalidRange(\n            \"The requested range is not satisfiable\",\n            ActualObjectSize=str(object_size),\n            RangeRequested=range_header,\n        )\n\n    if begin is not None:  # byte range\n        end = last if end is None else min(end, last)\n    elif end is not None:  # suffix byte range\n        begin = object_size - min(end, object_size)\n        end = last\n    else:\n        # Treat as non-range request\n        return None\n\n    if begin > min(end, last):\n        # Treat as non-range request if after the logic is applied\n        return None\n\n    return ObjectRange(\n        content_range=f\"bytes {begin}-{end}/{object_size}\",\n        content_length=end - begin + 1,\n        begin=begin,\n        end=end,\n    )\n```\n\nOriginal documentation:\n```python\nTakes a Range header, and returns a dataclass containing the necessary information to return only a slice of an\n    S3 object\n    :param range_header: a Range header\n    :param object_size: the requested S3 object total size\n    :return: ObjectRange\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Takes a Range header, and returns a dataclass containing the necessary information to return only a slice of an\n    S3 object. If the range header is invalid, we return None so that the request is treated as a regular request.\n    :param range_header: a Range header\n    :param object_size: the requested S3 object total size\n    :return: ObjectRange or None if the Range header is invalid", "file": "localstack_services_s3_utils.py", "function": "parse_range_header", "project": "localstack", "change_stats": {"code_changes": 40, "doc_changes": 6, "total_changes": 46}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef warp_coords(coord_map, shape, dtype=np.float64):\n    \n\n    \n    rows, cols = shape[0], shape[1]\n    coords_shape = [len(shape), rows, cols]\n    if len(shape) == 3:\n        coords_shape.append(shape[2])\n    coords = np.empty(coords_shape, dtype=dtype)\n\n    # Reshape grid coordinates into a (P, 2) array of (x, y) pairs\n    tf_coords = np.indices((cols, rows), dtype=dtype).reshape(2, -1).T\n\n    # Map each (x, y) pair to the source image according to\n    # the user-provided mapping\n    tf_coords = coord_map(tf_coords)\n\n    # Reshape back to a (2, M, N) coordinate grid\n    tf_coords = tf_coords.T.reshape((-1, cols, rows)).swapaxes(1, 2)\n\n    # Place the y-coordinate mapping\n    _stackcopy(coords[1, ...], tf_coords[0, ...])\n\n    # Place the x-coordinate mapping\n    _stackcopy(coords[0, ...], tf_coords[1, ...])\n\n    if len(shape) == 3:\n        coords[2, ...] = range(shape[2])\n\n    return coords```\n\nUpdated code:\n```python\ndef warp_coords(coord_map, shape, dtype=np.float64):\n    \n\n    \n    shape = safe_as_int(shape)\n    rows, cols = shape[0], shape[1]\n    coords_shape = [len(shape), rows, cols]\n    if len(shape) == 3:\n        coords_shape.append(shape[2])\n    coords = np.empty(coords_shape, dtype=dtype)\n\n    # Reshape grid coordinates into a (P, 2) array of (row, col) pairs\n    tf_coords = np.indices((cols, rows), dtype=dtype).reshape(2, -1).T\n\n    # Map each (row, col) pair to the source image according to\n    # the user-provided mapping\n    tf_coords = coord_map(tf_coords)\n\n    # Reshape back to a (2, M, N) coordinate grid\n    tf_coords = tf_coords.T.reshape((-1, cols, rows)).swapaxes(1, 2)\n\n    # Place the y-coordinate mapping\n    _stackcopy(coords[1, ...], tf_coords[0, ...])\n\n    # Place the x-coordinate mapping\n    _stackcopy(coords[0, ...], tf_coords[1, ...])\n\n    if len(shape) == 3:\n        coords[2, ...] = range(shape[2])\n\n    return coords\n```\n\nOriginal documentation:\n```python\nBuild the source coordinates for the output pixels of an image warp.\n\n    Parameters\n    ----------\n    coord_map : callable like GeometricTransform.inverse\n        Return input coordinates for given output coordinates.\n        Coordinates are in the shape (P, 2), where P is the number\n        of coordinates and each element is a ``(x, y)`` pair.\n    shape : tuple\n        Shape of output image ``(rows, cols[, bands])``.\n    dtype : np.dtype or string\n        dtype for return value (sane choices: float32 or float64).\n\n    Returns\n    -------\n    coords : (ndim, rows, cols[, bands]) array of dtype `dtype`\n            Coordinates for `scipy.ndimage.map_coordinates`, that will yield\n            an image of shape (orows, ocols, bands) by drawing from source\n            points according to the `coord_transform_fn`.\n\n    Notes\n    -----\n    This is a lower-level routine that produces the source coordinates used by\n    `warp()`.\n\n    It is provided separately from `warp` to give additional flexibility to\n    users who would like, for example, to re-use a particular coordinate\n    mapping, to use specific dtypes at various points along the the\n    image-warping process, or to implement different post-processing logic\n    than `warp` performs after the call to `ndimage.map_coordinates`.\n\n\n    Examples\n    --------\n    Produce a coordinate map that Shifts an image up and to the right:\n\n    >>> from skimage import data\n    >>> from scipy.ndimage import map_coordinates\n    >>>\n    >>> def shift_up10_left20(xy):\n    ...     return xy - np.array([-20, 10])[None, :]\n    >>>\n    >>> image = data.lena().astype(np.float32)\n    >>> coords = warp_coords(shift_up10_left20, image.shape)\n    >>> warped_image = map_coordinates(image, coords)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Build the source coordinates for the output pixels of an image warp.\n\n    Parameters\n    ----------\n    coord_map : callable like GeometricTransform.inverse\n        Return input coordinates for given output coordinates.\n        Coordinates are in the shape (P, 2), where P is the number\n        of coordinates and each element is a ``(row, col)`` pair.\n    shape : tuple\n        Shape of output image ``(rows, cols[, bands])``.\n    dtype : np.dtype or string\n        dtype for return value (sane choices: float32 or float64).\n\n    Returns\n    -------\n    coords : (ndim, rows, cols[, bands]) array of dtype `dtype`\n            Coordinates for `scipy.ndimage.map_coordinates`, that will yield\n            an image of shape (orows, ocols, bands) by drawing from source\n            points according to the `coord_transform_fn`.\n\n    Notes\n    -----\n    This is a lower-level routine that produces the source coordinates used by\n    `warp()`.\n\n    It is provided separately from `warp` to give additional flexibility to\n    users who would like, for example, to re-use a particular coordinate\n    mapping, to use specific dtypes at various points along the the\n    image-warping process, or to implement different post-processing logic\n    than `warp` performs after the call to `ndimage.map_coordinates`.\n\n\n    Examples\n    --------\n    Produce a coordinate map that Shifts an image up and to the right:\n\n    >>> from skimage import data\n    >>> from scipy.ndimage import map_coordinates\n    >>>\n    >>> def shift_up10_left20(xy):\n    ...     return xy - np.array([-20, 10])[None, :]\n    >>>\n    >>> image = data.lena().astype(np.float32)\n    >>> coords = warp_coords(shift_up10_left20, image.shape)\n    >>> warped_image = map_coordinates(image, coords)", "file": "skimage_transform__geometric.py", "function": "warp_coords", "project": "scikit-image", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _forward_discrminator(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        feats: torch.Tensor,\n        feats_lengths: torch.Tensor,\n        singing: torch.Tensor,\n        singing_lengths: torch.Tensor,\n        duration: torch.Tensor,\n        label: Optional[Dict[str, torch.Tensor]] = None,\n        label_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        melody: Optional[Dict[str, torch.Tensor]] = None,\n        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        beat: Optional[Dict[str, torch.Tensor]] = None,\n        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        pitch: Optional[torch.Tensor] = None,\n        pitch_lengths: Optional[torch.Tensor] = None,\n        sids: Optional[torch.Tensor] = None,\n        spembs: Optional[torch.Tensor] = None,\n        lids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, Any]:\n        \n\n        \n        # setup\n        batch_size = text.size(0)\n        singing = singing.unsqueeze(1)\n\n        # calculate generator outputs\n        reuse_cache = True\n        if not self.cache_generator_outputs or self._cache is None:\n            reuse_cache = False\n            # calculate score2mel outputs\n            score2mel_loss, stats, feats_gen = self.generator[\"score2mel\"](\n                text=text,\n                text_lengths=text_lengths,\n                feats=feats,\n                feats_lengths=feats_lengths,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                melody_lengths=melody_lengths,\n                duration=beat,\n                duration_lengths=beat_lengths,\n                pitch=pitch,\n                pitch_lengths=pitch_lengths,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n                joint_training=True,\n            )\n            # get random segments\n            feats_gen_, start_idxs = get_random_segments(\n                x=feats_gen.transpose(1, 2),\n                x_lengths=feats_lengths,\n                segment_size=self.segment_size,\n            )\n            # calculate vocoder outputs\n            singing_hat_ = self.generator[\"vocoder\"](feats_gen_)\n            if self.use_pqmf:\n                singing_hat_ = self.pqmf.synthesis(singing_hat_)\n        else:\n            _, _, singing_hat_, start_idxs = self._cache\n\n        # store cache\n        if self.cache_generator_outputs and not reuse_cache:\n            self._cache = (score2mel_loss, stats, singing_hat_, start_idxs)\n\n        # parse outputs\n        singing_ = get_segments(\n            x=singing,\n            start_idxs=start_idxs * self.generator[\"vocoder\"].upsample_factor,\n            segment_size=self.segment_size * self.generator[\"vocoder\"].upsample_factor,\n        )\n\n        # calculate discriminator outputs\n        p_hat = self.discriminator(singing_hat_.detach())\n        p = self.discriminator(singing_)\n\n        # calculate losses\n        real_loss, fake_loss = self.discriminator_adv_loss(p_hat, p)\n        loss = real_loss + fake_loss\n\n        stats = dict(\n            discriminator_loss=loss.item(),\n            real_loss=real_loss.item(),\n            fake_loss=fake_loss.item(),\n        )\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n\n        # reset cache\n        if reuse_cache or not self.training:\n            self._cache = None\n\n        return {\n            \"loss\": loss,\n            \"stats\": stats,\n            \"weight\": weight,\n            \"optim_idx\": 1,  # needed for trainer\n        }```\n\nUpdated code:\n```python\ndef _forward_discrminator(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        feats: torch.Tensor,\n        feats_lengths: torch.Tensor,\n        singing: torch.Tensor,\n        singing_lengths: torch.Tensor,\n        label: Optional[Dict[str, torch.Tensor]] = None,\n        label_lengths: Optional[Dict[str, torch.Tensor]] = None,\n        melody: Optional[Dict[str, torch.Tensor]] = None,\n        duration: Optional[Dict[str, torch.Tensor]] = None,\n        slur: Optional[torch.Tensor] = None,\n        pitch: Optional[torch.Tensor] = None,\n        sids: Optional[torch.Tensor] = None,\n        spembs: Optional[torch.Tensor] = None,\n        lids: Optional[torch.Tensor] = None,\n    ) -> Dict[str, Any]:\n        \n\n        \n        # setup\n        batch_size = text.size(0)\n        singing = singing.unsqueeze(1)\n\n        # calculate generator outputs\n        reuse_cache = True\n        if not self.cache_generator_outputs or self._cache is None:\n            reuse_cache = False\n            # calculate score2mel outputs\n            score2mel_loss, stats, feats_gen = self.generator[\"score2mel\"](\n                text=text,\n                text_lengths=text_lengths,\n                feats=feats,\n                feats_lengths=feats_lengths,\n                label=label,\n                label_lengths=label_lengths,\n                melody=melody,\n                melody_lengths=label_lengths,\n                duration=duration,\n                duration_lengths=label_lengths,\n                pitch=pitch,\n                pitch_lengths=feats_lengths,\n                sids=sids,\n                spembs=spembs,\n                lids=lids,\n                joint_training=True,\n            )\n            # get random segments\n            feats_gen_, start_idxs = get_random_segments(\n                x=feats_gen.transpose(1, 2),\n                x_lengths=feats_lengths,\n                segment_size=self.segment_size,\n            )\n            # calculate vocoder outputs\n            singing_hat_ = self.generator[\"vocoder\"](feats_gen_)\n            if self.use_pqmf:\n                singing_hat_ = self.pqmf.synthesis(singing_hat_)\n        else:\n            _, _, singing_hat_, start_idxs = self._cache\n\n        # store cache\n        if self.cache_generator_outputs and not reuse_cache:\n            self._cache = (score2mel_loss, stats, singing_hat_, start_idxs)\n\n        # parse outputs\n        singing_ = get_segments(\n            x=singing,\n            start_idxs=start_idxs * self.generator[\"vocoder\"].upsample_factor,\n            segment_size=self.segment_size * self.generator[\"vocoder\"].upsample_factor,\n        )\n\n        # calculate discriminator outputs\n        p_hat = self.discriminator(singing_hat_.detach())\n        p = self.discriminator(singing_)\n\n        # calculate losses\n        real_loss, fake_loss = self.discriminator_adv_loss(p_hat, p)\n        loss = real_loss + fake_loss\n\n        stats = dict(\n            discriminator_loss=loss.item(),\n            real_loss=real_loss.item(),\n            fake_loss=fake_loss.item(),\n        )\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n\n        # reset cache\n        if reuse_cache or not self.training:\n            self._cache = None\n\n        return {\n            \"loss\": loss,\n            \"stats\": stats,\n            \"weight\": weight,\n            \"optim_idx\": 1,  # needed for trainer\n        }\n```\n\nOriginal documentation:\n```python\nPerform discriminator forward.\n\n        Args:\n            text (Tensor): Text index tensor (B, T_text).\n            text_lengths (Tensor): Text length tensor (B,).\n            feats (Tensor): Feature tensor (B, T_feats, aux_channels).\n            feats_lengths (Tensor): Feature length tensor (B,).\n            singing (Tensor): Singing waveform tensor (B, T_wav).\n            singing_lengths (Tensor): Singing length tensor (B,).\n            duration (Optional[Dict]): key is \"phn\", \"syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            label (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded label ids (B, Tmax).\n            label_lengths (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of the lengths of padded label ids (B, ).\n            melody (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded melody (B, Tmax).\n            melody_lengths (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of the lengths of padded melody (B, ).\n            tempo (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded tempo (B, Tmax).\n            tempo_lengths (Optional[Dict]):  key is \"lab\" or \"score\";\n                value (LongTensor): Batch of the lengths of padded tempo (B, ).\n            beat (Optional[Dict]): key is \"lab\", \"score_phn\" or \"score_syb\";\n                value (LongTensor): Batch of padded beat (B, Tmax).\n            beat_length (Optional[Dict]): key is \"lab\", \"score_phn\" or \"score_syb\";\n                value (LongTensor): Batch of the lengths of padded beat (B, ).\n            pitch (FloatTensor): Batch of padded f0 (B, Tmax).\n            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).\n            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).\n            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).\n            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).\n\n        Returns:\n            Dict[str, Any]:\n                * loss (Tensor): Loss scalar tensor.\n                * stats (Dict[str, float]): Statistics to be monitored.\n                * weight (Tensor): Weight tensor to summarize losses.\n                * optim_idx (int): Optimizer index (0 for G and 1 for D).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform discriminator forward.\n\n        Args:\n            text (Tensor): Text index tensor (B, T_text).\n            text_lengths (Tensor): Text length tensor (B,).\n            feats (Tensor): Feature tensor (B, T_feats, aux_channels).\n            feats_lengths (Tensor): Feature length tensor (B,).\n            singing (Tensor): Singing waveform tensor (B, T_wav).\n            singing_lengths (Tensor): Singing length tensor (B,).\n            label (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded label ids (B, Tmax).\n            label_lengths (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of the lengths of padded label ids (B, ).\n            melody (Optional[Dict]): key is \"lab\" or \"score\";\n                value (LongTensor): Batch of padded melody (B, Tmax).\n            duration (Optional[Dict]): key is \"lab\", \"score_phn\" or \"score_syb\";\n                value (LongTensor): Batch of padded duration (B, Tmax).\n            slur (FloatTensor): Batch of padded slur (B, T_max).\n            pitch (FloatTensor): Batch of padded f0 (B, Tmax).\n            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).\n            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).\n            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).\n\n        Returns:\n            Dict[str, Any]:\n                * loss (Tensor): Loss scalar tensor.\n                * stats (Dict[str, float]): Statistics to be monitored.\n                * weight (Tensor): Weight tensor to summarize losses.\n                * optim_idx (int): Optimizer index (0 for G and 1 for D).", "file": "espnet2_gan_svs_joint_joint_score2wav.py", "function": "JointScore2Wav._forward_discrminator", "project": "espnet", "change_stats": {"code_changes": 17, "doc_changes": 18, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_tensor(\n        self,\n        *args,\n        convert_type: bool = False,\n        **kwargs,\n    ) -> FeatureTensorType:\n        \n        \n        attr = self._tensor_attr_cls.cast(*args, **kwargs)\n        if not attr.is_fully_specified():\n            raise ValueError(f\"The input TensorAttr '{attr}' is not fully \"\n                             f\"specified. Please fully-specify the input by \"\n                             f\"specifying all 'UNSET' fields.\")\n\n        tensor = self._get_tensor(attr)\n        if tensor is None:\n            raise KeyError(f\"A tensor corresponding to '{attr}' was not found\")\n        return self._to_type(attr, tensor) if convert_type else tensor```\n\nUpdated code:\n```python\ndef get_tensor(\n        self,\n        *args,\n        convert_type: bool = False,\n        **kwargs,\n    ) -> FeatureTensorType:\n        \n        \n        attr = self._tensor_attr_cls.cast(*args, **kwargs)\n        if not attr.is_fully_specified():\n            raise ValueError(f\"The input TensorAttr '{attr}' is not fully \"\n                             f\"specified. Please fully-specify the input by \"\n                             f\"specifying all 'UNSET' fields.\")\n\n        tensor = self._get_tensor(attr)\n        if convert_type:\n            tensor = self._to_type(attr, tensor)\n        return tensor\n```\n\nOriginal documentation:\n```python\nr\"\"\"Synchronously obtains a :class:`tensor` from the\n        :class:`FeatureStore`.\n\n        Args:\n            *args: Arguments passed to :class:`TensorAttr`.\n            convert_type (bool, optional): Whether to convert the type of the\n                output tensor to the type of the attribute index.\n                (default: :obj:`False`)\n            **kwargs: Keyword arguments passed to :class:`TensorAttr`.\n\n        Raises:\n            ValueError: If the input :class:`TensorAttr` is not fully\n                specified.\n            KeyError: If the tensor corresponding to the input\n                :class:`TensorAttr` was not found.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Synchronously obtains a :class:`tensor` from the\n        :class:`FeatureStore`.\n\n        Args:\n            *args: Arguments passed to :class:`TensorAttr`.\n            convert_type (bool, optional): Whether to convert the type of the\n                output tensor to the type of the attribute index.\n                (default: :obj:`False`)\n            **kwargs: Keyword arguments passed to :class:`TensorAttr`.\n\n        Raises:\n            ValueError: If the input :class:`TensorAttr` is not fully\n                specified.", "file": "torch_geometric_data_feature_store.py", "function": "FeatureStore.get_tensor", "project": "pytorch_geometric", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef register_from_pack(self, pack_dir):\n        \n        \n        \n        pack_dir = pack_dir[:-1] if pack_dir.endswith(\"/\") else pack_dir\n        _, pack = os.path.split(pack_dir)\n        actions_dir = self._pack_loader.get_content_from_pack(\n            pack_dir=pack_dir, content_type=\"actions\"\n        )\n\n        # Register pack first\n        self.register_pack(pack_name=pack, pack_dir=pack_dir)\n\n        registered_count = 0\n        if not actions_dir:\n            return registered_count\n\n        LOG.debug(\"Registering actions from pack %s:, dir: %s\", pack, actions_dir)\n\n        try:\n            actions = self._get_actions_from_pack(actions_dir=actions_dir)\n            registered_count = self._register_actions_from_pack(\n                pack=pack, actions=actions\n            )\n        except Exception as e:\n            if self._fail_on_failure:\n                raise e\n\n            LOG.exception(\"Failed registering all actions from pack: %s\", actions_dir)\n\n        return registered_count```\n\nUpdated code:\n```python\ndef register_from_pack(self, pack_dir):\n        \n        \n        \n        pack_dir = pack_dir[:-1] if pack_dir.endswith(\"/\") else pack_dir\n        _, pack = os.path.split(pack_dir)\n        actions_dir = self._pack_loader.get_content_from_pack(\n            pack_dir=pack_dir, content_type=\"actions\"\n        )\n\n        # Register pack first\n        self.register_pack(pack_name=pack, pack_dir=pack_dir)\n\n        registered_count = 0\n        overridden_count = 0\n        if not actions_dir:\n            return registered_count\n\n        LOG.debug(\"Registering actions from pack %s:, dir: %s\", pack, actions_dir)\n\n        try:\n            actions = self._get_actions_from_pack(actions_dir=actions_dir)\n            registered_count, overridden_count = self._register_actions_from_pack(\n                pack=pack, actions=actions\n            )\n        except Exception as e:\n            if self._fail_on_failure:\n                raise e\n\n            LOG.exception(\"Failed registering all actions from pack: %s\", actions_dir)\n\n        return registered_count, overridden_count\n```\n\nOriginal documentation:\n```python\nRegister all the actions from the provided pack.\n\n        :return: Number of actions registered.\n        :rtype: ``int``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Register all the actions from the provided pack.\n\n        :return: Number of actions registered, Number of actions overridden\n        :rtype: ``tuple``", "file": "st2common_st2common_bootstrap_actionsregistrar.py", "function": "ActionsRegistrar.register_from_pack", "project": "st2", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef magic_gui(self, parameter_s=''):\n        \n        \n        from IPython.lib.inputhook import enable_gui\n        opts, arg = self.parse_options(parameter_s, '')\n        if arg=='': arg = None\n        return enable_gui(arg)```\n\nUpdated code:\n```python\ndef magic_gui(self, parameter_s=''):\n        \n        \n        opts, arg = self.parse_options(parameter_s, '')\n        if arg=='': arg = None\n        try:\n            return self.enable_gui(arg)\n        except Exception as e:\n            # print simple error message, rather than traceback if we can't\n            # hook up the GUI\n            error(str(e))\n```\n\nOriginal documentation:\n```python\nEnable or disable IPython GUI event loop integration.\n\n        %gui [GUINAME]\n\n        This magic replaces IPython's threaded shells that were activated\n        using the (pylab/wthread/etc.) command line flags.  GUI toolkits\n        can now be enabled, disabled and changed at runtime and keyboard\n        interrupts should work without any problems.  The following toolkits\n        are supported:  wxPython, PyQt4, PyGTK, and Tk::\n\n            %gui wx      # enable wxPython event loop integration\n            %gui qt4|qt  # enable PyQt4 event loop integration\n            %gui gtk     # enable PyGTK event loop integration\n            %gui tk      # enable Tk event loop integration\n            %gui         # disable all event loop integration\n\n        WARNING:  after any of these has been called you can simply create\n        an application object, but DO NOT start the event loop yourself, as\n        we have already handled that.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Enable or disable IPython GUI event loop integration.\n\n        %gui [GUINAME]\n\n        This magic replaces IPython's threaded shells that were activated\n        using the (pylab/wthread/etc.) command line flags.  GUI toolkits\n        can now be enabled at runtime and keyboard\n        interrupts should work without any problems.  The following toolkits\n        are supported:  wxPython, PyQt4, PyGTK, Tk and Cocoa (OSX)::\n\n            %gui wx      # enable wxPython event loop integration\n            %gui qt4|qt  # enable PyQt4 event loop integration\n            %gui gtk     # enable PyGTK event loop integration\n            %gui tk      # enable Tk event loop integration\n            %gui OSX     # enable Cocoa event loop integration\n                         # (requires %matplotlib 1.1) \n            %gui         # disable all event loop integration\n\n        WARNING:  after any of these has been called you can simply create\n        an application object, but DO NOT start the event loop yourself, as\n        we have already handled that.", "file": "IPython_core_magic.py", "function": "Magic.magic_gui", "project": "ipython", "change_stats": {"code_changes": 10, "doc_changes": 8, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _reset_account_id(data):\n          \n        return aws_stack.fix_account_id_in_arns(\n            data, colon_delimiter='%3A', existing=TEST_AWS_ACCOUNT_ID, replace=MOTO_ACCOUNT_ID)```\n\nUpdated code:\n```python\ndef _reset_account_id(data):\n        \n        return aws_stack.fix_account_id_in_arns(\n            data,\n            colon_delimiter=\"%3A\",\n            existing=TEST_AWS_ACCOUNT_ID,\n            replace=MOTO_ACCOUNT_ID,\n        )\n```\n\nOriginal documentation:\n```python\nFix account ID in request payload. All external-facing responses contain our\n            predefined account ID (defaults to 000000000000), whereas the backend endpoint\n            from moto expects a different hardcoded account ID (123456789012).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fix account ID in request payload. All external-facing responses contain our\n        predefined account ID (defaults to 000000000000), whereas the backend endpoint\n        from moto expects a different hardcoded account ID (123456789012).", "file": "localstack_utils_aws_aws_responses.py", "function": "MessageConversion._reset_account_id", "project": "localstack", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef partition(self, model_names: set[NormalizedModelName]) -> PrimaryKeyMap:\n        \n        \n        \n\n        building = PrimaryKeyMap()\n        for model_name_str, mappings in self.mapping.items():\n            model_name = NormalizedModelName(model_name_str)\n            if model_name not in model_names:\n                continue\n\n            for old_pk, new_entry in mappings.items():\n                building.mapping[model_name_str][old_pk] = new_entry\n\n        return building```\n\nUpdated code:\n```python\ndef partition(\n        self, model_names: set[NormalizedModelName], kinds: set[ImportKind] | None = None\n    ) -> PrimaryKeyMap:\n        \n        \n        \n\n        building = PrimaryKeyMap()\n        import_kinds = {k for k in ImportKind} if kinds is None else kinds\n        for model_name_str, mappings in self.mapping.items():\n            model_name = NormalizedModelName(model_name_str)\n            if model_name not in model_names:\n                continue\n\n            for old_pk, new_entry in mappings.items():\n                (_, import_kind, _) = new_entry\n                if import_kind not in import_kinds:\n                    continue\n\n                building.mapping[model_name_str][old_pk] = new_entry\n\n        return building\n```\n\nOriginal documentation:\n```python\nCreate a new map with only the specified model kinds retained.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a new map with only the specified models and kinds retained.", "file": "src_sentry_backup_dependencies.py", "function": "PrimaryKeyMap.partition", "project": "sentry", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef run(self, optimizer: Optimizer, kwargs: OrderedDict) -> _OUTPUTS_TYPE:\n        \n\n        \n        closure = self._make_closure(kwargs, optimizer)\n\n        if (\n            # when the strategy handles accumulation, we want to always call the optimizer step\n            not self.trainer.strategy.handles_gradient_accumulation\n            and self.trainer.fit_loop._should_accumulate()\n        ):\n            # For gradient accumulation\n\n            # -------------------\n            # calculate loss (train step + train step end)\n            # -------------------\n            # automatic_optimization=True: perform ddp sync only when performing optimizer_step\n            with _block_parallel_sync_behavior(self.trainer.strategy, block=True):\n                closure()\n\n        # ------------------------------\n        # BACKWARD PASS\n        # ------------------------------\n        # gradient update with accumulated gradients\n        else:\n            self._optimizer_step(kwargs.get(\"batch_idx\", 0), closure)\n\n        result = closure.consume_result()\n        if result.loss is None:\n            return {}\n        return result.asdict()```\n\nUpdated code:\n```python\ndef run(self, optimizer: Optimizer, batch_idx: int, kwargs: OrderedDict) -> _OUTPUTS_TYPE:\n        \n\n        \n        closure = self._make_closure(kwargs, optimizer, batch_idx)\n\n        if (\n            # when the strategy handles accumulation, we want to always call the optimizer step\n            not self.trainer.strategy.handles_gradient_accumulation\n            and self.trainer.fit_loop._should_accumulate()\n        ):\n            # For gradient accumulation\n\n            # -------------------\n            # calculate loss (train step + train step end)\n            # -------------------\n            # automatic_optimization=True: perform ddp sync only when performing optimizer_step\n            with _block_parallel_sync_behavior(self.trainer.strategy, block=True):\n                closure()\n\n        # ------------------------------\n        # BACKWARD PASS\n        # ------------------------------\n        # gradient update with accumulated gradients\n        else:\n            self._optimizer_step(batch_idx, closure)\n\n        result = closure.consume_result()\n        if result.loss is None:\n            return {}\n        return result.asdict()\n```\n\nOriginal documentation:\n```python\nRuns closure (train step + backward) together with optimization if necessary.\n\n        Args:\n            kwargs: the kwargs passed down to the hooks\n            optimizer: the optimizer\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Runs closure (train step + backward) together with optimization if necessary.\n\n        Args:\n            kwargs: the kwargs passed down to the hooks\n            batch_idx: the current batch index.\n            optimizer: the optimizer", "file": "src_lightning_pytorch_loops_optimization_automatic.py", "function": "_AutomaticOptimization.run", "project": "pytorch-lightning", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_get_ipython_dir_3():\n    \n    path.get_home_dir = lambda : \"someplace\"\n    path._writable_dir = lambda path: True\n    os.name = \"posix\"\n    env.pop('IPYTHON_DIR', None)\n    env.pop('IPYTHONDIR', None)\n    env['XDG_CONFIG_HOME'] = XDG_TEST_DIR\n    ipdir = path.get_ipython_dir()\n    if sys.platform == \"darwin\":\n        expected = os.path.join(\"someplace\", \".ipython\")\n    else:\n        expected = os.path.join(XDG_TEST_DIR, \"ipython\")\n    nt.assert_equal(ipdir, expected)```\n\nUpdated code:\n```python\ndef test_get_ipython_dir_3():\n    \n    tmphome = TemporaryDirectory()\n    try:\n        with patch_get_home_dir(tmphome.name):\n            os.name = \"posix\"\n            env.pop('IPYTHON_DIR', None)\n            env.pop('IPYTHONDIR', None)\n            env['XDG_CONFIG_HOME'] = XDG_TEST_DIR\n\n            with warnings.catch_warnings(record=True) as w:\n                ipdir = path.get_ipython_dir()\n\n            nt.assert_equal(ipdir, os.path.join(tmphome.name, \".ipython\"))\n            if sys.platform != 'darwin':\n                nt.assert_equal(len(w), 1)\n                nt.assert_in('Moving', str(w[0]))\n    finally:\n        tmphome.cleanup()\n```\n\nOriginal documentation:\n```python\ntest_get_ipython_dir_3, use XDG if defined, and .ipython doesn't exist.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "test_get_ipython_dir_3, move XDG if defined, and .ipython doesn't exist.", "file": "IPython_utils_tests_test_path.py", "function": "test_get_ipython_dir_3", "project": "ipython", "change_stats": {"code_changes": 31, "doc_changes": 4, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_display_recipient_remote_cache(\n    recipient_id: int, recipient_type: int, recipient_type_id: Optional[int]\n) -> DisplayRecipientT:\n    \n    \n    \n    if recipient_type == Recipient.STREAM:  # nocoverage\n        assert recipient_type_id is not None\n        stream = Stream.objects.values(\"name\").get(id=recipient_type_id)\n        return stream[\"name\"]\n\n    # The main priority for ordering here is being deterministic.\n    # Right now, we order by ID, which matches the ordering of user\n    # names in the left sidebar.\n    user_profile_list = (\n        UserProfile.objects.filter(\n            subscription__recipient_id=recipient_id,\n        )\n        .order_by(\"id\")\n        .values(*display_recipient_fields)\n    )\n    return list(user_profile_list)```\n\nUpdated code:\n```python\ndef get_display_recipient_remote_cache(\n    recipient_id: int, recipient_type: int, recipient_type_id: Optional[int]\n) -> List[UserDisplayRecipient]:\n    \n    \n    \n\n    assert recipient_type != Recipient.STREAM\n\n    # The main priority for ordering here is being deterministic.\n    # Right now, we order by ID, which matches the ordering of user\n    # names in the left sidebar.\n    user_profile_list = (\n        UserProfile.objects.filter(\n            subscription__recipient_id=recipient_id,\n        )\n        .order_by(\"id\")\n        .values(*display_recipient_fields)\n    )\n    return list(user_profile_list)\n```\n\nOriginal documentation:\n```python\nreturns: an appropriate object describing the recipient.  For a\n    stream this will be the stream name as a string.  For a huddle or\n    personal, it will be an array of dicts about each recipient.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This returns an appropriate object describing the recipient of a\n    direct message (whether individual or group).\n\n    It will be an array of dicts for each recipient.\n\n    Do not use this for streams.", "file": "zerver_lib_display_recipient.py", "function": "get_display_recipient_remote_cache", "project": "zulip", "change_stats": {"code_changes": 10, "doc_changes": 11, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef pager(self, styles: bool = False, links: bool = False) -> PagerContext:\n        \n        \n        return PagerContext(self, styles=styles, links=links)```\n\nUpdated code:\n```python\ndef pager(\n        self, pager: Pager = None, styles: bool = False, links: bool = False\n    ) -> PagerContext:\n        \n        \n        return PagerContext(self, pager=pager, styles=styles, links=links)\n```\n\nOriginal documentation:\n```python\nA context manager to display anything printed within a \"pager\". The pager used\n        is defined by the system and will typically support at less pressing a key to scroll.\n\n        Args:\n            styles (bool, optional): Show styles in pager. Defaults to False.\n            links (bool, optional): Show links in pager. Defaults to False.\n\n        Example:\n            >>> from rich.console import Console\n            >>> from rich.__main__ import make_test_card\n            >>> console = Console()\n            >>> with console.pager():\n                    console.print(make_test_card())\n\n        Returns:\n            PagerContext: A context manager.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A context manager to display anything printed within a \"pager\". The pager used\n        is defined by the system and will typically support at less pressing a key to scroll.\n\n        Args:\n            pager (Pager, optional): A pager object, or None to use :class:~rich.pager.SystemPager`. Defaults to None.\n            styles (bool, optional): Show styles in pager. Defaults to False.\n            links (bool, optional): Show links in pager. Defaults to False.\n\n        Example:\n            >>> from rich.console import Console\n            >>> from rich.__main__ import make_test_card\n            >>> console = Console()\n            >>> with console.pager():\n                    console.print(make_test_card())\n\n        Returns:\n            PagerContext: A context manager.", "file": "rich_console.py", "function": "Console.pager", "project": "rich", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n                edge_attr: OptTensor = None,\n                return_attention_weights: bool = None):\n        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa\n        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa\n        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n        \n        \n        H, C = self.heads, self.out_channels\n\n        x_l: OptTensor = None\n        x_r: OptTensor = None\n        if isinstance(x, Tensor):\n            assert x.dim() == 2\n            x_l = self.lin_l(x).view(-1, H, C)\n            if self.share_weights:\n                x_r = x_l\n            else:\n                x_r = self.lin_r(x).view(-1, H, C)\n        else:\n            x_l, x_r = x[0], x[1]\n            assert x[0].dim() == 2\n            x_l = self.lin_l(x_l).view(-1, H, C)\n            if x_r is not None:\n                x_r = self.lin_r(x_r).view(-1, H, C)\n\n        assert x_l is not None\n        assert x_r is not None\n\n        if self.add_self_loops:\n            if isinstance(edge_index, Tensor):\n                num_nodes = x_l.size(0)\n                if x_r is not None:\n                    num_nodes = min(num_nodes, x_r.size(0))\n                edge_index, edge_attr = remove_self_loops(\n                    edge_index, edge_attr)\n                edge_index, edge_attr = add_self_loops(\n                    edge_index, edge_attr, fill_value=self.fill_value,\n                    num_nodes=num_nodes)\n            elif isinstance(edge_index, SparseTensor):\n                if self.edge_dim is None:\n                    edge_index = torch_sparse.set_diag(edge_index)\n                else:\n                    raise NotImplementedError(\n                        \"The usage of 'edge_attr' and 'add_self_loops' \"\n                        \"simultaneously is currently not yet supported for \"\n                        \"'edge_index' in a 'SparseTensor' form\")\n\n        # propagate_type: (x: PairTensor, edge_attr: OptTensor)\n        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr,\n                             size=None)\n\n        alpha = self._alpha\n        assert alpha is not None\n        self._alpha = None\n\n        if self.concat:\n            out = out.view(-1, self.heads * self.out_channels)\n        else:\n            out = out.mean(dim=1)\n\n        if self.bias is not None:\n            out = out + self.bias\n\n        if isinstance(return_attention_weights, bool):\n            if isinstance(edge_index, Tensor):\n                if is_torch_sparse_tensor(edge_index):\n                    # TODO TorchScript requires to return a tuple\n                    adj = set_sparse_value(edge_index, alpha)\n                    return out, (adj, alpha)\n                else:\n                    return out, (edge_index, alpha)\n            elif isinstance(edge_index, SparseTensor):\n                return out, edge_index.set_value(alpha, layout='coo')\n        else:\n            return out```\n\nUpdated code:\n```python\ndef forward(\n        self,\n        x: Union[Tensor, PairTensor],\n        edge_index: Adj,\n        edge_attr: OptTensor = None,\n        return_attention_weights: bool = None,\n    ):\n        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa\n        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa\n        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n        \n        \n        H, C = self.heads, self.out_channels\n\n        x_l: OptTensor = None\n        x_r: OptTensor = None\n        if isinstance(x, Tensor):\n            assert x.dim() == 2\n            x_l = self.lin_l(x).view(-1, H, C)\n            if self.share_weights:\n                x_r = x_l\n            else:\n                x_r = self.lin_r(x).view(-1, H, C)\n        else:\n            x_l, x_r = x[0], x[1]\n            assert x[0].dim() == 2\n            x_l = self.lin_l(x_l).view(-1, H, C)\n            if x_r is not None:\n                x_r = self.lin_r(x_r).view(-1, H, C)\n\n        assert x_l is not None\n        assert x_r is not None\n\n        if self.add_self_loops:\n            if isinstance(edge_index, Tensor):\n                num_nodes = x_l.size(0)\n                if x_r is not None:\n                    num_nodes = min(num_nodes, x_r.size(0))\n                edge_index, edge_attr = remove_self_loops(\n                    edge_index, edge_attr)\n                edge_index, edge_attr = add_self_loops(\n                    edge_index, edge_attr, fill_value=self.fill_value,\n                    num_nodes=num_nodes)\n            elif isinstance(edge_index, SparseTensor):\n                if self.edge_dim is None:\n                    edge_index = torch_sparse.set_diag(edge_index)\n                else:\n                    raise NotImplementedError(\n                        \"The usage of 'edge_attr' and 'add_self_loops' \"\n                        \"simultaneously is currently not yet supported for \"\n                        \"'edge_index' in a 'SparseTensor' form\")\n\n        # propagate_type: (x: PairTensor, edge_attr: OptTensor)\n        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr,\n                             size=None)\n\n        alpha = self._alpha\n        assert alpha is not None\n        self._alpha = None\n\n        if self.concat:\n            out = out.view(-1, self.heads * self.out_channels)\n        else:\n            out = out.mean(dim=1)\n\n        if self.bias is not None:\n            out = out + self.bias\n\n        if isinstance(return_attention_weights, bool):\n            if isinstance(edge_index, Tensor):\n                if is_torch_sparse_tensor(edge_index):\n                    # TODO TorchScript requires to return a tuple\n                    adj = set_sparse_value(edge_index, alpha)\n                    return out, (adj, alpha)\n                else:\n                    return out, (edge_index, alpha)\n            elif isinstance(edge_index, SparseTensor):\n                return out, edge_index.set_value(alpha, layout='coo')\n        else:\n            return out\n```\n\nOriginal documentation:\n```python\nr\"\"\"Runs the forward pass of the module.\n\n        Args:\n            return_attention_weights (bool, optional): If set to :obj:`True`,\n                will additionally return the tuple\n                :obj:`(edge_index, attention_weights)`, holding the computed\n                attention weights for each edge. (default: :obj:`None`)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Runs the forward pass of the module.\n\n        Args:\n            x (torch.Tensor or (torch.Tensor, torch.Tensor)): The input node\n                features.\n            edge_index (torch.Tensor or SparseTensor): The edge indices.\n            edge_attr (torch.Tensor, optional): The edge features.\n                (default: :obj:`None`)\n            return_attention_weights (bool, optional): If set to :obj:`True`,\n                will additionally return the tuple\n                :obj:`(edge_index, attention_weights)`, holding the computed\n                attention weights for each edge. (default: :obj:`None`)", "file": "torch_geometric_nn_conv_gatv2_conv.py", "function": "GATv2Conv.forward", "project": "pytorch_geometric", "change_stats": {"code_changes": 12, "doc_changes": 7, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef time_limit(self, task_name, soft=None, hard=None, **kwargs):\n        \n\n        \n        return self.broadcast(\"time_limit\",\n                              arguments={\"task_name\": task_name,\n                                         \"hard\": hard, \"soft\": soft},\n                              **kwargs)```\n\nUpdated code:\n```python\ndef time_limit(self, task_name, soft=None, hard=None, **kwargs):\n        \n\n        \n        return self.broadcast(\"time_limit\",\n                              arguments={\"task_name\": task_name,\n                                         \"hard\": hard, \"soft\": soft}, **kwargs)\n```\n\nOriginal documentation:\n```python\nSet time limits for task by type.\n\n        :param task_name: Name of task to change time limits for.\n        :keyword soft: New soft time limit (in seconds).\n        :keyword hard: New hard time limit (in seconds).\n\n        Any additional keyword arguments are passed on to :meth:`broadcast`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tell all (or specific) workers to set time limits for\n        a task by type.\n\n        :param task_name: Name of task to change time limits for.\n        :keyword soft: New soft time limit (in seconds).\n        :keyword hard: New hard time limit (in seconds).\n\n        Any additional keyword arguments are passed on to :meth:`broadcast`.", "file": "celery_app_control.py", "function": "Control.time_limit", "project": "celery", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(\n            self,\n            inputs: torch.Tensor,\n            input_lengths: torch.Tensor,\n            hidden_states: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        \n        self_attn_mask = None\n        batch = inputs.size(0)\n\n        inputs = inputs[inputs != self.eos_id].view(batch, -1)\n\n        if len(inputs.size()) == 1:  # validate, evaluation\n            inputs = inputs.unsqueeze(1)\n            target_lens = inputs.size(1)\n\n            embedding_output = self.embedding(inputs) * self.scale\n            positional_encoding_output = self.positional_encoding(target_lens)\n            inputs = embedding_output + positional_encoding_output\n\n        else:  # train\n            inputs = inputs[inputs != self.eos_id].view(batch, -1)\n            target_lengths = inputs.size(1)\n\n            embedding_output = self.embedding(inputs) * self.scale\n            positional_encoding_output = self.positional_encoding(target_lengths)\n            inputs = embedding_output + positional_encoding_output\n\n            dec_self_attn_pad_mask = get_attn_pad_mask(inputs, input_lengths, inputs.size(1))\n            dec_self_attn_subsequent_mask = get_attn_subsequent_mask(inputs)\n            self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n\n        outputs = self.input_dropout(inputs)\n\n        for encoder_layer in self.encoder_layers:\n            outputs, _ = encoder_layer(outputs, self_attn_mask)\n\n        return outputs, input_lengths```\n\nUpdated code:\n```python\ndef forward(\n            self,\n            inputs: torch.Tensor,\n            input_lengths: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        \n        batch = inputs.size(0)\n\n        if len(inputs.size()) == 1:  # validate, evaluation\n            inputs = inputs.unsqueeze(1)\n            target_lengths = inputs.size(1)\n\n            outputs = self.forward_step(\n                decoder_inputs=inputs,\n                decoder_input_lengths=input_lengths,\n                positional_encoding_length=target_lengths,\n            )\n\n        else:  # train\n            inputs = inputs[inputs != self.eos_id].view(batch, -1)\n            target_lengths = inputs.size(1)\n\n            outputs = self.forward_step(\n                decoder_inputs=inputs,\n                decoder_input_lengths=input_lengths,\n                positional_encoding_length=target_lengths,\n            )\n\n        return outputs, input_lengths\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Forward propagate a `inputs` for label encoder.\n\n        Args:\n            inputs (torch.LongTensor): A input sequence passed to label encoder. Typically inputs will be a padded\n                `LongTensor` of size ``(batch, target_length)``\n            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n\n        Returns:\n            * outputs (Tensor): ``(batch, seq_length, dimension)``\n            * input_lengths (Tensor):  ``(batch)``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Forward propagate a `inputs` for label encoder.\n\n        Args:\n            inputs (torch.LongTensor): A input sequence passed to label encoder. Typically inputs will be a padded\n                `LongTensor` of size ``(batch, target_length)``\n            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n\n        Returns:\n            * outputs (Tensor): ``(batch, seq_length, dimension)``\n            * output_lengths (Tensor):  ``(batch)``", "file": "openspeech_decoders_transformer_transducer_decoder.py", "function": "TransformerTransducerDecoder.forward", "project": "openspeech", "change_stats": {"code_changes": 33, "doc_changes": 4, "total_changes": 37}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef recognize_beam(self, h, lpz, recog_args, char_list=None, rnnlm=None):\n        \n\n        \n        logging.info(\"input lengths: \" + str(h.shape[1]))\n\n        # initialization\n        n_len = h.shape[1]\n        xp = self.xp\n        h_mask = xp.ones((1, n_len))\n\n        # search parms\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = recog_args.ctc_weight\n\n        # prepare sos\n        y = self.sos\n        if recog_args.maxlenratio == 0:\n            maxlen = n_len\n        else:\n            maxlen = max(1, int(recog_args.maxlenratio * n_len))\n        minlen = int(recog_args.minlenratio * n_len)\n        logging.info(\"max output length: \" + str(maxlen))\n        logging.info(\"min output length: \" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {\"score\": 0.0, \"yseq\": [y], \"rnnlm_prev\": None}\n        else:\n            hyp = {\"score\": 0.0, \"yseq\": [y]}\n\n        if lpz is not None:\n            ctc_prefix_score = CTCPrefixScore(lpz, 0, self.eos, self.xp)\n            hyp[\"ctc_state_prev\"] = ctc_prefix_score.initial_state()\n            hyp[\"ctc_score_prev\"] = 0.0\n            if ctc_weight != 1.0:\n                # pre-pruning based on attention scores\n                ctc_beam = min(lpz.shape[-1], int(beam * CTC_SCORING_RATIO))\n            else:\n                ctc_beam = lpz.shape[-1]\n\n        hyps = [hyp]\n        ended_hyps = []\n\n        for i in six.moves.range(maxlen):\n            logging.debug(\"position \" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                ys = F.expand_dims(xp.array(hyp[\"yseq\"]), axis=0).data\n                out = self.decoder(ys, h, h_mask)\n\n                # get nbest local scores and their ids\n                local_att_scores = F.log_softmax(out[:, -1], axis=-1).data\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(\n                        hyp[\"rnnlm_prev\"], hyp[\"yseq\"][i]\n                    )\n                    local_scores = (\n                        local_att_scores + recog_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                if lpz is not None:\n                    local_best_ids = xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :ctc_beam\n                    ]\n                    ctc_scores, ctc_states = ctc_prefix_score(\n                        hyp[\"yseq\"], local_best_ids, hyp[\"ctc_state_prev\"]\n                    )\n                    local_scores = (1.0 - ctc_weight) * local_att_scores[\n                        :, local_best_ids\n                    ] + ctc_weight * (ctc_scores - hyp[\"ctc_score_prev\"])\n                    if rnnlm:\n                        local_scores += (\n                            recog_args.lm_weight * local_lm_scores[:, local_best_ids]\n                        )\n                    joint_best_ids = xp.argsort(local_scores, axis=1)[0, ::-1][:beam]\n                    local_best_scores = local_scores[:, joint_best_ids]\n                    local_best_ids = local_best_ids[joint_best_ids]\n                else:\n                    local_best_ids = self.xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :beam\n                    ]\n                    local_best_scores = local_scores[:, local_best_ids]\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    new_hyp[\"score\"] = hyp[\"score\"] + float(local_best_scores[0, j])\n                    new_hyp[\"yseq\"] = [0] * (1 + len(hyp[\"yseq\"]))\n                    new_hyp[\"yseq\"][: len(hyp[\"yseq\"])] = hyp[\"yseq\"]\n                    new_hyp[\"yseq\"][len(hyp[\"yseq\"])] = int(local_best_ids[j])\n                    if rnnlm:\n                        new_hyp[\"rnnlm_prev\"] = rnnlm_state\n                    if lpz is not None:\n                        new_hyp[\"ctc_state_prev\"] = ctc_states[joint_best_ids[j]]\n                        new_hyp[\"ctc_score_prev\"] = ctc_scores[joint_best_ids[j]]\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[\"score\"], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(\"number of pruned hypothesis: \" + str(len(hyps)))\n            if char_list is not None:\n                logging.debug(\n                    \"best hypo: \"\n                    + \"\".join([char_list[int(x)] for x in hyps[0][\"yseq\"][1:]])\n                    + \" score: \"\n                    + str(hyps[0][\"score\"])\n                )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(\"adding <eos> in the last postion in the loop\")\n                for hyp in hyps:\n                    hyp[\"yseq\"].append(self.eos)\n\n            # add ended hypothes to a final list, and removed them from current hypothes\n            # (this will be a probmlem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[\"yseq\"][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[\"yseq\"]) > minlen:\n                        hyp[\"score\"] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[\"score\"] += recog_args.lm_weight * rnnlm.final(\n                                hyp[\"rnnlm_prev\"]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n            if end_detect(ended_hyps, i) and recog_args.maxlenratio == 0.0:\n                logging.info(\"end detected at %d\", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(\"remained hypothes: \" + str(len(hyps)))\n            else:\n                logging.info(\"no hypothesis. Finish decoding.\")\n                break\n            if char_list is not None:\n                for hyp in hyps:\n                    logging.debug(\n                        \"hypo: \" + \"\".join([char_list[int(x)] for x in hyp[\"yseq\"][1:]])\n                    )\n\n            logging.debug(\"number of ended hypothes: \" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(\n            ended_hyps, key=lambda x: x[\"score\"], reverse=True\n        )  # [:min(len(ended_hyps), recog_args.nbest)]\n\n        logging.debug(nbest_hyps)\n        # check number of hypotheis\n        if len(nbest_hyps) == 0:\n            logging.warn(\n                \"there is no N-best results, perform recognition \"\n                \"again with smaller minlenratio.\"\n            )\n            # should copy becasuse Namespace will be overwritten globally\n            recog_args = Namespace(**vars(recog_args))\n            recog_args.minlenratio = max(0.0, recog_args.minlenratio - 0.1)\n            return self.recognize_beam(h, lpz, recog_args, char_list, rnnlm)\n\n        logging.info(\"total log probability: \" + str(nbest_hyps[0][\"score\"]))\n        logging.info(\n            \"normalized log probability: \"\n            + str(nbest_hyps[0][\"score\"] / len(nbest_hyps[0][\"yseq\"]))\n        )\n        # remove sos\n        return nbest_hyps```\n\nUpdated code:\n```python\ndef recognize_beam(self, h, lpz, recog_args, char_list=None, rnnlm=None):\n        \n\n        \n        logging.info(\"input lengths: \" + str(h.shape[1]))\n\n        # initialization\n        n_len = h.shape[1]\n        xp = self.xp\n        h_mask = xp.ones((1, n_len))\n\n        # search parms\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = recog_args.ctc_weight\n\n        # prepare sos\n        y = self.sos\n        if recog_args.maxlenratio == 0:\n            maxlen = n_len\n        else:\n            maxlen = max(1, int(recog_args.maxlenratio * n_len))\n        minlen = int(recog_args.minlenratio * n_len)\n        logging.info(\"max output length: \" + str(maxlen))\n        logging.info(\"min output length: \" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {\"score\": 0.0, \"yseq\": [y], \"rnnlm_prev\": None}\n        else:\n            hyp = {\"score\": 0.0, \"yseq\": [y]}\n\n        if lpz is not None:\n            ctc_prefix_score = CTCPrefixScore(lpz, 0, self.eos, self.xp)\n            hyp[\"ctc_state_prev\"] = ctc_prefix_score.initial_state()\n            hyp[\"ctc_score_prev\"] = 0.0\n            if ctc_weight != 1.0:\n                # pre-pruning based on attention scores\n                ctc_beam = min(lpz.shape[-1], int(beam * CTC_SCORING_RATIO))\n            else:\n                ctc_beam = lpz.shape[-1]\n\n        hyps = [hyp]\n        ended_hyps = []\n\n        for i in six.moves.range(maxlen):\n            logging.debug(\"position \" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                ys = F.expand_dims(xp.array(hyp[\"yseq\"]), axis=0).data\n                out = self.decoder(ys, h, h_mask)\n\n                # get nbest local scores and their ids\n                local_att_scores = F.log_softmax(out[:, -1], axis=-1).data\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(\n                        hyp[\"rnnlm_prev\"], hyp[\"yseq\"][i]\n                    )\n                    local_scores = (\n                        local_att_scores + recog_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                if lpz is not None:\n                    local_best_ids = xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :ctc_beam\n                    ]\n                    ctc_scores, ctc_states = ctc_prefix_score(\n                        hyp[\"yseq\"], local_best_ids, hyp[\"ctc_state_prev\"]\n                    )\n                    local_scores = (1.0 - ctc_weight) * local_att_scores[\n                        :, local_best_ids\n                    ] + ctc_weight * (ctc_scores - hyp[\"ctc_score_prev\"])\n                    if rnnlm:\n                        local_scores += (\n                            recog_args.lm_weight * local_lm_scores[:, local_best_ids]\n                        )\n                    joint_best_ids = xp.argsort(local_scores, axis=1)[0, ::-1][:beam]\n                    local_best_scores = local_scores[:, joint_best_ids]\n                    local_best_ids = local_best_ids[joint_best_ids]\n                else:\n                    local_best_ids = self.xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :beam\n                    ]\n                    local_best_scores = local_scores[:, local_best_ids]\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    new_hyp[\"score\"] = hyp[\"score\"] + float(local_best_scores[0, j])\n                    new_hyp[\"yseq\"] = [0] * (1 + len(hyp[\"yseq\"]))\n                    new_hyp[\"yseq\"][: len(hyp[\"yseq\"])] = hyp[\"yseq\"]\n                    new_hyp[\"yseq\"][len(hyp[\"yseq\"])] = int(local_best_ids[j])\n                    if rnnlm:\n                        new_hyp[\"rnnlm_prev\"] = rnnlm_state\n                    if lpz is not None:\n                        new_hyp[\"ctc_state_prev\"] = ctc_states[joint_best_ids[j]]\n                        new_hyp[\"ctc_score_prev\"] = ctc_scores[joint_best_ids[j]]\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[\"score\"], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(\"number of pruned hypothesis: \" + str(len(hyps)))\n            if char_list is not None:\n                logging.debug(\n                    \"best hypo: \"\n                    + \"\".join([char_list[int(x)] for x in hyps[0][\"yseq\"][1:]])\n                    + \" score: \"\n                    + str(hyps[0][\"score\"])\n                )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(\"adding <eos> in the last position in the loop\")\n                for hyp in hyps:\n                    hyp[\"yseq\"].append(self.eos)\n\n            # add ended hypothes to a final list, and removed them from current hypothes\n            # (this will be a probmlem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[\"yseq\"][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[\"yseq\"]) > minlen:\n                        hyp[\"score\"] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[\"score\"] += recog_args.lm_weight * rnnlm.final(\n                                hyp[\"rnnlm_prev\"]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n            if end_detect(ended_hyps, i) and recog_args.maxlenratio == 0.0:\n                logging.info(\"end detected at %d\", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(\"remained hypothes: \" + str(len(hyps)))\n            else:\n                logging.info(\"no hypothesis. Finish decoding.\")\n                break\n            if char_list is not None:\n                for hyp in hyps:\n                    logging.debug(\n                        \"hypo: \" + \"\".join([char_list[int(x)] for x in hyp[\"yseq\"][1:]])\n                    )\n\n            logging.debug(\"number of ended hypothes: \" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(\n            ended_hyps, key=lambda x: x[\"score\"], reverse=True\n        )  # [:min(len(ended_hyps), recog_args.nbest)]\n\n        logging.debug(nbest_hyps)\n        # check number of hypotheis\n        if len(nbest_hyps) == 0:\n            logging.warn(\n                \"there is no N-best results, perform recognition \"\n                \"again with smaller minlenratio.\"\n            )\n            # should copy becasuse Namespace will be overwritten globally\n            recog_args = Namespace(**vars(recog_args))\n            recog_args.minlenratio = max(0.0, recog_args.minlenratio - 0.1)\n            return self.recognize_beam(h, lpz, recog_args, char_list, rnnlm)\n\n        logging.info(\"total log probability: \" + str(nbest_hyps[0][\"score\"]))\n        logging.info(\n            \"normalized log probability: \"\n            + str(nbest_hyps[0][\"score\"] / len(nbest_hyps[0][\"yseq\"]))\n        )\n        # remove sos\n        return nbest_hyps\n```\n\nOriginal documentation:\n```python\nE2E beam search.\n\n        Args:\n            h (ndarray): Encoder ouput features (B, T, D) or (T, D).\n            lpz (ndarray): Log probabilities from CTC.\n            recog_args (Namespace): Argment namespace contraining options.\n            char_list (List[str]): List of characters.\n            rnnlm (chainer.Chain): Language model module defined at\n            `espnet.lm.chainer_backend.lm`.\n\n        Returns:\n            List: N-best decoding results.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "E2E beam search.\n\n        Args:\n            h (ndarray): Encoder output features (B, T, D) or (T, D).\n            lpz (ndarray): Log probabilities from CTC.\n            recog_args (Namespace): Argment namespace contraining options.\n            char_list (List[str]): List of characters.\n            rnnlm (chainer.Chain): Language model module defined at\n            `espnet.lm.chainer_backend.lm`.\n\n        Returns:\n            List: N-best decoding results.", "file": "espnet_nets_chainer_backend_e2e_asr_transformer.py", "function": "E2E.recognize_beam", "project": "espnet", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef softmax_cross_entropy(\n        x, t, use_cudnn=True, normalize=True, store_forward=True):\n    \n\n    \n    return SoftmaxCrossEntropy(use_cudnn, normalize, store_forward)(x, t)```\n\nUpdated code:\n```python\ndef softmax_cross_entropy(\n        x, t, use_cudnn=True, normalize=True, cache_score=True):\n    \n\n    \n    return SoftmaxCrossEntropy(use_cudnn, normalize, cache_score)(x, t)\n```\n\nOriginal documentation:\n```python\nComputes cross entropy loss for pre-softmax activations.\n\n    Args:\n        x (Variable): Variable holding a multidimensional array whose element\n            indicates unnormalized log probability: the first axis of the\n            variable represents the number of samples, and the second axis\n            represents the number of classes. While this function computes\n            a usual softmax cross entropy if the number of dimensions is equal\n            to 2, it computes a cross entropy of the replicated softmax if the\n            number of dimensions is greater than 2.\n        t (Variable): Variable holding an int32 vector of groundtruth labels.\n            If ``t[i] == -1``, correspondig ``x[i]`` is ignored.\n        normalize (Variable): Variable holding a boolean value which\n            determines the normalization constant. If true, this function\n            normalizes the cross entropy loss across all instances. If else,\n            it only normalizes along a batch size.\n        store_forward (bool): When it is ``True``, the function stores result\n            of forward computation to use it on backward computation. It\n            reduces computational cost though consumes more memories.\n\n    Returns:\n        Variable: A variable holding a scalar array of the cross entropy loss.\n\n    .. note::\n\n       This function is differentiable only by ``x``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes cross entropy loss for pre-softmax activations.\n\n    Args:\n        x (Variable): Variable holding a multidimensional array whose element\n            indicates unnormalized log probability: the first axis of the\n            variable represents the number of samples, and the second axis\n            represents the number of classes. While this function computes\n            a usual softmax cross entropy if the number of dimensions is equal\n            to 2, it computes a cross entropy of the replicated softmax if the\n            number of dimensions is greater than 2.\n        t (Variable): Variable holding an int32 vector of groundtruth labels.\n            If ``t[i] == -1``, correspondig ``x[i]`` is ignored.\n        normalize (Variable): Variable holding a boolean value which\n            determines the normalization constant. If true, this function\n            normalizes the cross entropy loss across all instances. If else,\n            it only normalizes along a batch size.\n        cache_score (bool): When it is ``True``, the function stores result\n            of forward computation to use it on backward computation. It\n            reduces computational cost though consumes more memories.\n\n    Returns:\n        Variable: A variable holding a scalar array of the cross entropy loss.\n\n    .. note::\n\n       This function is differentiable only by ``x``.", "file": "chainer_", "function": "softmax_cross_entropy", "project": "chainer", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef decrypt(self, content: str, key: int) -> list[str]:\n        \n        \n        \n\n        # precondition\n        assert isinstance(key, int)\n        assert isinstance(content, list)\n\n        key = key or self.__key or 1\n\n        # make sure key is an appropriate size\n        key %= 255\n\n        return [chr(ord(ch) ^ key) for ch in content]```\n\nUpdated code:\n```python\ndef decrypt(self, content: str, key: int) -> list[str]:\n        \n        \n        \n\n        # precondition\n        assert isinstance(key, int)\n        assert isinstance(content, str)\n\n        key = key or self.__key or 1\n\n        # make sure key is an appropriate size\n        key %= 256\n\n        return [chr(ord(ch) ^ key) for ch in content]\n```\n\nOriginal documentation:\n```python\ninput: 'content' of type list and 'key' of type int\n        output: decrypted string 'content' as a list of chars\n        if key not passed the method uses the key by the constructor.\n        otherwise key = 1\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "input: 'content' of type list and 'key' of type int\n        output: decrypted string 'content' as a list of chars\n        if key not passed the method uses the key by the constructor.\n        otherwise key = 1\n\n        Empty list\n        >>> XORCipher().decrypt(\"\", 5)\n        []\n\n        One key\n        >>> XORCipher().decrypt(\"hallo welt\", 1)\n        ['i', '`', 'm', 'm', 'n', '!', 'v', 'd', 'm', 'u']\n\n        Normal key\n        >>> XORCipher().decrypt(\"HALLO WELT\", 32)\n        ['h', 'a', 'l', 'l', 'o', '\\\\x00', 'w', 'e', 'l', 't']\n\n        Key greater than 255\n        >>> XORCipher().decrypt(\"hallo welt\", 256)\n        ['h', 'a', 'l', 'l', 'o', ' ', 'w', 'e', 'l', 't']", "file": "ciphers_xor_cipher.py", "function": "XORCipher.decrypt", "project": "Python", "change_stats": {"code_changes": 6, "doc_changes": 18, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, offset=1.0):\n        \n\n        \n        super(DurationPredictorLoss, self).__init__()\n        self.criterion = torch.nn.MSELoss()\n        self.offset = offset```\n\nUpdated code:\n```python\ndef __init__(self, offset=1.0, reduction=\"mean\"):\n        \n\n        \n        super(DurationPredictorLoss, self).__init__()\n        self.criterion = torch.nn.MSELoss(reduction=reduction)\n        self.offset = offset\n```\n\nOriginal documentation:\n```python\nInitilize duration predictor loss module.\n\n        Args:\n            offset (float, optional): Offset value to avoid nan in log domain.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initilize duration predictor loss module.\n\n        Args:\n            offset (float, optional): Offset value to avoid nan in log domain.\n            reduction (str): Reduction type in loss calculation.", "file": "espnet_nets_pytorch_backend_fastspeech_duration_predictor.py", "function": "DurationPredictorLoss.__init__", "project": "espnet", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef kruskal(self) -> GraphUndirectedWeighted:\n        # Kruskal's Algorithm to generate a Minimum Spanning Tree (MST) of a graph\n        \n        \n        \n\n        # getting the edges in ascending order of weights\n        edges = []\n        seen = set()\n        for start in self.connections:\n            for end in self.connections[start]:\n                if (start, end) not in seen:\n                    seen.add((end, start))\n                    edges.append((start, end, self.connections[start][end]))\n        edges.sort(key=lambda x: x[2])\n        # creating the disjoint set\n        disjoint_set = DisjointSetTree()\n        [disjoint_set.make_set(node) for node in self.connections]\n        # MST generation\n        num_edges = 0\n        index = 0\n        graph = GraphUndirectedWeighted()\n        while num_edges < len(self.connections) - 1:\n            u, v, w = edges[index]\n            index += 1\n            parentu = disjoint_set.find_set(u)\n            parentv = disjoint_set.find_set(v)\n            if parentu != parentv:\n                num_edges += 1\n                graph.add_edge(u, v, w)\n                disjoint_set.union(u, v)\n        return graph```\n\nUpdated code:\n```python\ndef kruskal(self) -> GraphUndirectedWeighted[T]:\n        # Kruskal's Algorithm to generate a Minimum Spanning Tree (MST) of a graph\n        \n        \n        \n\n        # getting the edges in ascending order of weights\n        edges = []\n        seen = set()\n        for start in self.connections:\n            for end in self.connections[start]:\n                if (start, end) not in seen:\n                    seen.add((end, start))\n                    edges.append((start, end, self.connections[start][end]))\n        edges.sort(key=lambda x: x[2])\n\n        # creating the disjoint set\n        disjoint_set = DisjointSetTree[T]()\n        for node in self.connections:\n            disjoint_set.make_set(node)\n\n        # MST generation\n        num_edges = 0\n        index = 0\n        graph = GraphUndirectedWeighted[T]()\n        while num_edges < len(self.connections) - 1:\n            u, v, w = edges[index]\n            index += 1\n            parent_u = disjoint_set.find_set(u)\n            parent_v = disjoint_set.find_set(v)\n            if parent_u != parent_v:\n                num_edges += 1\n                graph.add_edge(u, v, w)\n                disjoint_set.union(u, v)\n        return graph\n```\n\nOriginal documentation:\n```python\nDetails: https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n\n        Example:\n\n        >>> graph = GraphUndirectedWeighted()\n        >>> graph.add_edge(1, 2, 1)\n        >>> graph.add_edge(2, 3, 2)\n        >>> graph.add_edge(3, 4, 1)\n        >>> graph.add_edge(3, 5, 100) # Removed in MST\n        >>> graph.add_edge(4, 5, 5)\n        >>> assert 5 in graph.connections[3]\n        >>> mst = graph.kruskal()\n        >>> assert 5 not in mst.connections[3]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Details: https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n\n        Example:\n        >>> g1 = GraphUndirectedWeighted[int]()\n        >>> g1.add_edge(1, 2, 1)\n        >>> g1.add_edge(2, 3, 2)\n        >>> g1.add_edge(3, 4, 1)\n        >>> g1.add_edge(3, 5, 100) # Removed in MST\n        >>> g1.add_edge(4, 5, 5)\n        >>> assert 5 in g1.connections[3]\n        >>> mst = g1.kruskal()\n        >>> assert 5 not in mst.connections[3]\n\n        >>> g2 = GraphUndirectedWeighted[str]()\n        >>> g2.add_edge('A', 'B', 1)\n        >>> g2.add_edge('B', 'C', 2)\n        >>> g2.add_edge('C', 'D', 1)\n        >>> g2.add_edge('C', 'E', 100) # Removed in MST\n        >>> g2.add_edge('D', 'E', 5)\n        >>> assert 'E' in g2.connections[\"C\"]\n        >>> mst = g2.kruskal()\n        >>> assert 'E' not in mst.connections['C']", "file": "graphs_minimum_spanning_tree_kruskal2.py", "function": "GraphUndirectedWeighted.kruskal", "project": "Python", "change_stats": {"code_changes": 19, "doc_changes": 29, "total_changes": 48}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_cache_multiple_builds_same_prev_clean():\n    \n    \n    \n    c = TestClient()\n    c.save({\"conanfile.py\": GenConanfile(\"pkg\", \"0.1\")})\n    c.run(\"create .\")\n    create_out = c.out\n    c.run(\"cache path pkg/0.1:da39a3ee5e6b4b0d3255bfef95601890afd80709\")\n    path1 = str(c.stdout)\n    assert path1 in create_out\n    c.run(\"create .\")\n    create_out = c.out\n    c.run(\"cache path pkg/0.1:da39a3ee5e6b4b0d3255bfef95601890afd80709\")\n    path2 = str(c.stdout)\n    assert path2 in create_out\n    assert path1 != path2\n\n    builds_folder = os.path.join(c.cache_folder, \"p\", \"b\")\n    assert len(os.listdir(builds_folder)) == 1  # only one build\n    c.run('cache clean')\n    assert len(os.listdir(builds_folder)) == 1  # one build not cleaned\n    c.run('remove * -c')\n    assert len(os.listdir(builds_folder)) == 0```\n\nUpdated code:\n```python\ndef test_cache_multiple_builds_same_prev_clean():\n    \n    \n    \n    c = TestClient()\n    c.save({\"conanfile.py\": GenConanfile(\"pkg\", \"0.1\")})\n    c.run(\"create .\")\n    create_out = c.out\n    c.run(\"cache path pkg/0.1:da39a3ee5e6b4b0d3255bfef95601890afd80709\")\n    path1 = str(c.stdout)\n    assert path1 in create_out\n    c.run(\"create .\")\n    create_out = c.out\n    c.run(\"cache path pkg/0.1:da39a3ee5e6b4b0d3255bfef95601890afd80709\")\n    path2 = str(c.stdout)\n    assert path2 in create_out\n    assert path1 == path2\n\n    builds_folder = os.path.join(c.cache_folder, \"p\", \"b\")\n    assert len(os.listdir(builds_folder)) == 1  # only one build\n    c.run('cache clean')\n    assert len(os.listdir(builds_folder)) == 1  # one build not cleaned\n    c.run('remove * -c')\n    assert len(os.listdir(builds_folder)) == 0\n```\n\nOriginal documentation:\n```python\nDifferent consecutive builds will create different folders, even if for the\n    same exact prev, leaving trailing non-referenced folders\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Different consecutive builds will create exactly the same folder, for the\n    same exact prev, not leaving trailing non-referenced folders", "file": "conans_test_integration_command_v2_test_cache_clean.py", "function": "test_cache_multiple_builds_same_prev_clean", "project": "conan", "change_stats": {"code_changes": 4, "doc_changes": 6, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef clip_grad_norm_(\n        self, max_norm: Union[float, int], norm_type: Union[float, int] = 2.0\n    ) -> torch.Tensor:\n        \n        \n        \n        _lazy_init(self, self)\n        if not self._is_root:\n            raise RuntimeError(\n                \"`clip_grad_norm_()` should only be called on the root FSDP instance\"\n            )\n        self._assert_state(TrainingState.IDLE)\n        # If every FSDP instance uses `NO_SHARD`, then we can directly use\n        # the normal `nn.utils` one targeting local gradients\n        all_no_shard = all(\n            not handle.uses_sharded_strategy\n            for handle in FullyShardedDataParallel._fsdp_handles(self)\n        )\n        if all_no_shard:\n            return torch.nn.utils.clip_grad_norm_(\n                self.parameters(), max_norm, norm_type\n            )\n        # Otherwise, there exists some FSDP instance using a sharded strategy,\n        # where sharded and non-sharded parameters must be handled separately\n        max_norm = float(max_norm)\n        norm_type = float(norm_type)\n        sharded_params = set()\n        nonsharded_params = set()  # `NO_SHARD` or not FSDP-managed\n        grads: List[torch.Tensor] = []\n        for handle in FullyShardedDataParallel._fsdp_handles(self):\n            target_set = (\n                sharded_params if handle.uses_sharded_strategy else nonsharded_params\n            )\n            if handle._use_orig_params:\n                for param in handle.flat_param._params:\n                    target_set.add(param)\n                    if param.grad is not None:\n                        grads.append(param.grad)\n            else:\n                target_set.add(handle.flat_param)\n                if handle.flat_param.grad is not None:\n                    grads.append(handle.flat_param.grad)\n        for param in self.parameters():\n            not_fsdp_managed = (\n                param not in sharded_params and param not in nonsharded_params\n            )\n            if not_fsdp_managed:\n                nonsharded_params.add(param)\n                if param.grad is not None:\n                    grads.append(param.grad)\n        local_sharded_norm = _get_grad_norm(sharded_params, norm_type).to(\n            self.compute_device\n        )\n        local_nonsharded_norm = _get_grad_norm(nonsharded_params, norm_type).to(\n            self.compute_device\n        )\n        # Reconstruct the total gradient norm depending on the norm type\n        if norm_type == math.inf:\n            total_norm = torch.maximum(local_sharded_norm, local_nonsharded_norm)\n            dist.all_reduce(\n                total_norm, op=torch.distributed.ReduceOp.MAX, group=self.process_group\n            )\n        else:\n            total_norm = local_sharded_norm**norm_type\n            dist.all_reduce(total_norm, group=self.process_group)\n            # All-reducing the local non-sharded norm would count it an extra\n            # world-size-many times\n            total_norm += local_nonsharded_norm**norm_type\n            total_norm = total_norm ** (1.0 / norm_type)\n        if self.cpu_offload.offload_params:\n            total_norm = total_norm.cpu()\n\n        clip_coef = max_norm / (total_norm + 1e-6)\n        # Multiplying by the clamped coefficient is meaningless when it is\n        # equal to 1, but it avoids the host-device sync that would result from\n        # `if clip_coef < 1`\n        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n        for grad in grads:\n            grad.detach().mul_(clip_coef_clamped.to(grad.device))\n        return total_norm```\n\nUpdated code:\n```python\ndef clip_grad_norm_(\n        self, max_norm: Union[float, int], norm_type: Union[float, int] = 2.0\n    ) -> torch.Tensor:\n        \n        \n        \n        _lazy_init(self, self)\n        if not self._is_root:\n            raise RuntimeError(\n                \"`clip_grad_norm_()` should only be called on the root FSDP instance\"\n            )\n        self._assert_state(TrainingState.IDLE)\n        # If every FSDP instance uses `NO_SHARD`, then we can directly use\n        # the normal `nn.utils` one targeting local gradients\n        all_no_shard = all(\n            not handle.uses_sharded_strategy\n            for handle in FullyShardedDataParallel._fsdp_handles(self)\n        )\n        if all_no_shard:\n            return torch.nn.utils.clip_grad_norm_(\n                self.parameters(), max_norm, norm_type\n            )\n        # Otherwise, there exists some FSDP instance using a sharded strategy,\n        # where sharded and non-sharded parameters must be handled separately\n        max_norm = float(max_norm)\n        norm_type = float(norm_type)\n        sharded_params = set()\n        nonsharded_params = set()  # `NO_SHARD` or not FSDP-managed\n        grads: List[torch.Tensor] = []\n        for handle in FullyShardedDataParallel._fsdp_handles(self):\n            target_set = (\n                sharded_params if handle.uses_sharded_strategy else nonsharded_params\n            )\n            if handle._use_orig_params:\n                for param in handle.flat_param._params:\n                    target_set.add(param)\n                    if param.grad is not None:\n                        grads.append(param.grad)\n            else:\n                target_set.add(handle.flat_param)\n                if handle.flat_param.grad is not None:\n                    grads.append(handle.flat_param.grad)\n        for param in self.parameters():\n            not_fsdp_managed = (\n                param not in sharded_params and param not in nonsharded_params\n            )\n            if not_fsdp_managed:\n                nonsharded_params.add(param)\n                if param.grad is not None:\n                    grads.append(param.grad)\n        # Compute local norms (forced to be in FP32)\n        local_sharded_norm = _get_grad_norm(sharded_params, norm_type).to(\n            self.compute_device\n        )\n        local_nonsharded_norm = _get_grad_norm(nonsharded_params, norm_type).to(\n            self.compute_device\n        )\n        # Reconstruct the total gradient norm depending on the norm type\n        if norm_type == math.inf:\n            total_norm = torch.maximum(local_sharded_norm, local_nonsharded_norm)\n            dist.all_reduce(\n                total_norm, op=torch.distributed.ReduceOp.MAX, group=self.process_group\n            )\n        else:\n            total_norm = local_sharded_norm**norm_type\n            dist.all_reduce(total_norm, group=self.process_group)\n            # All-reducing the local non-sharded norm would count it an extra\n            # world-size-many times\n            total_norm += local_nonsharded_norm**norm_type\n            total_norm = total_norm ** (1.0 / norm_type)\n        if self.cpu_offload.offload_params:\n            total_norm = total_norm.cpu()\n\n        clip_coef = max_norm / (total_norm + 1e-6)\n        # Multiplying by the clamped coefficient is meaningless when it is\n        # equal to 1, but it avoids the host-device sync that would result from\n        # `if clip_coef < 1`\n        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n        for grad in grads:\n            grad.detach().mul_(clip_coef_clamped.to(grad.device, grad.dtype))\n        # Use the \"largest\" dtype by type promotion semantics to use the same\n        # dtype as if we did not force local norm computation to be in FP32\n        total_norm_dtype = functools.reduce(\n            lambda dtype1, dtype2: torch.promote_types(dtype1, dtype2),\n            [grad.dtype for grad in grads],\n        )\n        return total_norm.to(total_norm_dtype)\n```\n\nOriginal documentation:\n```python\nClips the gradient norm of all parameters. The norm is computed over\n        all parameters' gradients as viewed as a single vector, and the\n        gradients are modified in-place.\n\n        Args:\n            max_norm (float or int): max norm of the gradients\n            norm_type (float or int): type of the used p-norm. Can be ``'inf'``\n                for infinity norm.\n\n        Returns:\n            Total norm of the parameters (viewed as a single vector).\n\n        .. note:: This is analogous to ``torch.nn.utils.clip_grad_norm_`` but\n            handles the partitioning and multiple devices per rank under the\n            hood. The default torch util is not applicable here, because each\n            rank only has a partial view of all the grads in the model, so\n            calling it for FSDP models would lead to different scaling being\n            applied per subset of model parameters.\n\n        .. warning:: This needs to be called on all ranks since it uses\n            collective communications.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Clips the gradient norm of all parameters. The norm is computed over\n        all parameters' gradients as viewed as a single vector, and the\n        gradients are modified in-place.\n\n        Args:\n            max_norm (float or int): max norm of the gradients\n            norm_type (float or int): type of the used p-norm. Can be ``'inf'``\n                for infinity norm.\n\n        Returns:\n            Total norm of the parameters (viewed as a single vector).\n\n        .. note:: If every FSDP instance uses ``NO_SHARD``, meaning that no\n            gradients are sharded across ranks, then you may directly use\n            :func:`torch.nn.utils.clip_grad_norm_`.\n\n        .. note:: If at least some FSDP instance uses a sharded strategy (i.e.\n            one other than ``NO_SHARD``), then you should use this method\n            instead of :func:`torch.nn.utils.clip_grad_norm_` since this method\n            handles the fact that gradients are sharded across ranks.\n\n        .. note:: The total norm returned will have the \"largest\" dtype across\n            all parameters/gradients as defined by PyTorch's type promotion\n            semantics. For example, if *all* parameters/gradients use a low\n            precision dtype, then the returned norm's dtype will be that low\n            precision dtype, but if there exists at least one parameter/\n            gradient using FP32, then the returned norm's dtype will be FP32.\n\n        .. warning:: This needs to be called on all ranks since it uses\n            collective communications.", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel.clip_grad_norm_", "project": "pytorch", "change_stats": {"code_changes": 13, "doc_changes": 23, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(self, num):\n         \n        \n\n        dnum = decimal.Decimal(str(num))\n\n        sign = 1\n\n        if dnum < 0:  # pragma: no cover\n            sign = -1\n            dnum = -dnum\n\n        if dnum != 0:\n            pow10 = decimal.Decimal(int(math.floor(dnum.log10()/3)*3))\n        else:\n            pow10 = decimal.Decimal(0)\n\n        pow10 = pow10.min(max(self.ENG_PREFIXES.keys()))\n        pow10 = pow10.max(min(self.ENG_PREFIXES.keys()))\n        int_pow10 = int(pow10)\n\n        if self.use_eng_prefix:\n            prefix = self.ENG_PREFIXES[int_pow10]\n        else:\n            if int_pow10 < 0:\n                prefix = 'E-%02d' % (-int_pow10)\n            else:\n                prefix = 'E+%02d' % int_pow10\n\n        mant = sign*dnum/(10**pow10)\n\n        if self.precision is None:  # pragma: no cover\n            format_str = u\"%g%s\"\n        elif self.precision == 0:\n            format_str = u\"%i%s\"\n        elif self.precision > 0:\n            format_str = (u\"%% .%if%%s\" % self.precision)\n\n        formatted = format_str % (mant, prefix)\n\n        return formatted```\n\nUpdated code:\n```python\ndef __call__(self, num):\n         \n        \n\n        dnum = decimal.Decimal(str(num))\n\n        sign = 1\n\n        if dnum < 0:  # pragma: no cover\n            sign = -1\n            dnum = -dnum\n\n        if dnum != 0:\n            pow10 = decimal.Decimal(int(math.floor(dnum.log10()/3)*3))\n        else:\n            pow10 = decimal.Decimal(0)\n\n        pow10 = pow10.min(max(self.ENG_PREFIXES.keys()))\n        pow10 = pow10.max(min(self.ENG_PREFIXES.keys()))\n        int_pow10 = int(pow10)\n\n        if self.use_eng_prefix:\n            prefix = self.ENG_PREFIXES[int_pow10]\n        else:\n            if int_pow10 < 0:\n                prefix = 'E-%02d' % (-int_pow10)\n            else:\n                prefix = 'E+%02d' % int_pow10\n\n        mant = sign*dnum/(10**pow10)\n\n        if self.accuracy is None:  # pragma: no cover\n            format_str = u\"%g%s\"\n        elif self.accuracy == 0:\n            format_str = u\"%i%s\"\n        elif self.accuracy > 0:\n            format_str = (u\"%% .%if%%s\" % self.accuracy )\n\n        formatted = format_str % (mant, prefix)\n\n        return formatted\n```\n\nOriginal documentation:\n```python\nFormats a number in engineering notation, appending a letter\n        representing the power of 1000 of the original number. Some examples:\n\n        >>> format_eng(0)      # for self.precision = 0\n        '0'\n\n        >>> format_eng(1000000) # for self.precision = 1,\n                                #     self.use_eng_prefix = True\n        '1.0M'\n\n        >>> format_eng(\"-1e-6\") # for self.precision = 2\n                                #     self.use_eng_prefix = False\n        '-1.00E-06'\n\n        @param num: the value to represent\n        @type num: either a numeric value or a string that can be converted to\n                   a numeric value (as per decimal.Decimal constructor)\n\n        @return: engineering formatted string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Formats a number in engineering notation, appending a letter\n        representing the power of 1000 of the original number. Some examples:\n\n        >>> format_eng(0)       # for self.accuracy = 0\n        '0'\n\n        >>> format_eng(1000000) # for self.accuracy = 1,\n                                #     self.use_eng_prefix = True\n        '1.0M'\n\n        >>> format_eng(\"-1e-6\") # for self.accuracy = 2\n                                #     self.use_eng_prefix = False\n        '-1.00E-06'\n\n        @param num: the value to represent\n        @type num: either a numeric value or a string that can be converted to\n                   a numeric value (as per decimal.Decimal constructor)\n\n        @return: engineering formatted string", "file": "pandas_core_common.py", "function": "EngFormatter.__call__", "project": "pandas", "change_stats": {"code_changes": 10, "doc_changes": 8, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef conv2d_grad_wrt_inputs(output_grad,\n                           filters,\n                           input_shape,\n                           filter_shape=None,\n                           border_mode='valid',\n                           subsample=(1, 1),\n                           filter_flip=True,\n                           filter_dilation=(1, 1),\n                           num_groups=1):\n    \n\n    \n\n    filters = as_tensor_variable(filters)\n    output_grad = as_tensor_variable(output_grad)\n\n    # checking the type of input_shape\n    for dim in [0, 1]:\n        assert isinstance(input_shape[dim], (theano.tensor.TensorConstant,\n                                             integer_types, type(None)))\n    for dim in [2, 3]:\n        assert isinstance(input_shape[dim], (theano.tensor.TensorVariable,\n                                             theano.tensor.TensorConstant,\n                                             integer_types))\n\n    # checking the type of filter_shape\n    if filter_shape is not None:\n        for dim in [0, 1, 2, 3]:\n            assert isinstance(filter_shape[dim], (theano.tensor.TensorConstant,\n                                                  integer_types, type(None)))\n\n    # setting the last two dimensions of input_shape to None, if\n    # the type of these dimensions is TensorVariable.\n    numerical_input_shape = list(input_shape)\n    for dim in [2, 3]:\n        if isinstance(input_shape[dim], theano.tensor.TensorVariable):\n            numerical_input_shape[dim] = None\n\n    grad_input_op = AbstractConv2d_gradInputs(imshp=numerical_input_shape,\n                                              kshp=filter_shape,\n                                              border_mode=border_mode,\n                                              subsample=subsample,\n                                              filter_flip=filter_flip,\n                                              filter_dilation=filter_dilation,\n                                              num_groups=num_groups)\n\n    return grad_input_op(filters, output_grad, input_shape[-2:])```\n\nUpdated code:\n```python\ndef conv2d_grad_wrt_inputs(output_grad,\n                           filters,\n                           input_shape,\n                           filter_shape=None,\n                           border_mode='valid',\n                           subsample=(1, 1),\n                           filter_flip=True,\n                           filter_dilation=(1, 1),\n                           num_groups=1,\n                           unshared=False):\n    \n\n    \n\n    filters = as_tensor_variable(filters)\n    output_grad = as_tensor_variable(output_grad)\n\n    # checking the type of input_shape\n    for dim in [0, 1]:\n        assert isinstance(input_shape[dim], (theano.tensor.TensorConstant,\n                                             integer_types, type(None)))\n    for dim in [2, 3]:\n        assert isinstance(input_shape[dim], (theano.tensor.TensorVariable,\n                                             theano.tensor.TensorConstant,\n                                             integer_types))\n\n    # checking the type of filter_shape\n    if filter_shape is not None:\n        for dim in [0, 1, 2, 3]:\n            assert isinstance(filter_shape[dim], (theano.tensor.TensorConstant,\n                                                  integer_types, type(None)))\n        if unshared:\n            for dim in [4, 5]:\n                assert isinstance(filter_shape[dim], (theano.tensor.TensorConstant,\n                                                      integer_types, type(None)))\n\n    # setting the last two dimensions of input_shape to None, if\n    # the type of these dimensions is TensorVariable.\n    numerical_input_shape = list(input_shape)\n    for dim in [2, 3]:\n        if isinstance(input_shape[dim], theano.tensor.TensorVariable):\n            numerical_input_shape[dim] = None\n\n    grad_input_op = AbstractConv2d_gradInputs(imshp=numerical_input_shape,\n                                              kshp=filter_shape,\n                                              border_mode=border_mode,\n                                              subsample=subsample,\n                                              filter_flip=filter_flip,\n                                              filter_dilation=filter_dilation,\n                                              num_groups=num_groups,\n                                              unshared=unshared)\n\n    return grad_input_op(filters, output_grad, input_shape[-2:])\n```\n\nOriginal documentation:\n```python\nCompute conv output gradient w.r.t its inputs\n\n    This function builds the symbolic graph for getting the\n    gradient of the output of a convolution (namely output_grad)\n    w.r.t the input of the convolution, given a set of 2D filters\n    used by the convolution, such that the output_grad is upsampled\n    to the input_shape.\n\n    Parameters\n    ----------\n    output_grad : symbolic 4D tensor\n        mini-batch of feature map stacks, of shape (batch size, input\n        channels, input rows, input columns).  This is the tensor that\n        will be upsampled or the output gradient of the convolution\n        whose gradient will be taken with respect to the input of the\n        convolution.\n    filters : symbolic 4D tensor\n        set of filters used in CNN layer of shape (output channels,\n        input channels, filter rows, filter columns).  See the\n        optional parameter ``filter_shape``.\n    input_shape : [None/int/Constant] * 2 + [Tensor/int/Constant] * 2\n        The shape of the input (upsampled) parameter.\n        A tuple/list of len 4, with the first two dimensions\n        being None or int or Constant and the last two dimensions being\n        Tensor or int or Constant.\n        Not Optional, since given the output_grad shape\n        and the subsample values, multiple input_shape may be\n        plausible.\n    filter_shape : None or [None/int/Constant] * 4\n        The shape of the filters parameter. None or a tuple/list of len 4.\n        Optional, possibly used  to choose an optimal implementation.\n        You can give ``None`` for any element of the list to specify that\n        this element is not known at compile time.\n    border_mode : str, int or tuple of two int\n        Either of the following:\n\n          ``'valid'``\n            apply filter wherever it completely overlaps with the\n            input. Generates output of shape: input shape - filter\n            shape + 1\n\n          ``'full'``\n            apply filter wherever it partly overlaps with the input.\n            Generates output of shape: input shape + filter shape - 1\n\n          ``'half'``\n            pad input with a symmetric border of ``filter rows // 2``\n            rows and ``filter columns // 2`` columns, then perform a\n            valid convolution. For filters with an odd number of rows\n            and columns, this leads to the output shape being equal to\n            the input shape. It is known as 'same' elsewhere.\n\n          ``int``\n            pad input with a symmetric border of zeros of the given\n            width, then perform a valid convolution.\n\n          ``(int1, int2)``\n            pad input with a symmetric border of ``int1`` rows and\n            ``int2`` columns, then perform a valid convolution.\n\n    subsample : tuple of len 2\n        The subsampling used in the forward pass.  Also called strides\n        elsewhere.\n    filter_flip : bool\n        If ``True``, will flip the filter rows and columns before\n        sliding them over the input. This operation is normally\n        referred to as a convolution, and this is the default. If\n        ``False``, the filters are not flipped and the operation is\n        referred to as a cross-correlation.\n    filter_dilation : tuple of len 2\n        The filter dilation used in the forward pass.\n        Also known as input striding.\n    num_groups : int\n        Divides the image, kernel and output tensors into num_groups\n        separate groups. Each which carry out convolutions separately\n\n    Returns\n    -------\n    symbolic 4D tensor\n        set of feature maps generated by convolutional layer. Tensor\n        is of shape (batch size, output channels, output rows, output\n        columns)\n\n    Notes\n    -----\n\n    :note: If cuDNN is available, it will be used on the\n        GPU. Otherwise, it is the *CorrMM* convolution that will be used\n        \"caffe style convolution\".\n\n    :note: This is only supported in Theano 0.8 or the development\n        version until it is released.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute conv output gradient w.r.t its inputs\n\n    This function builds the symbolic graph for getting the\n    gradient of the output of a convolution (namely output_grad)\n    w.r.t the input of the convolution, given a set of 2D filters\n    used by the convolution, such that the output_grad is upsampled\n    to the input_shape.\n\n    Parameters\n    ----------\n    output_grad : symbolic 4D tensor\n        mini-batch of feature map stacks, of shape (batch size, input\n        channels, input rows, input columns).  This is the tensor that\n        will be upsampled or the output gradient of the convolution\n        whose gradient will be taken with respect to the input of the\n        convolution.\n    filters: symbolic 4D or 6D tensor\n        Set of filters used in CNN layer of shape\n        (output channels, input channels, filter rows, filter columns)\n        for normal convolution and\n        (output channels, output rows, output columns, input channels,\n        filter rows, filter columns)\n        for unshared convolution.\n        See the optional parameter ``filter_shape``.\n    input_shape : [None/int/Constant] * 2 + [Tensor/int/Constant] * 2\n        The shape of the input (upsampled) parameter.\n        A tuple/list of len 4, with the first two dimensions\n        being None or int or Constant and the last two dimensions being\n        Tensor or int or Constant.\n        Not Optional, since given the output_grad shape\n        and the subsample values, multiple input_shape may be\n        plausible.\n    filter_shape : None or [None/int/Constant] * (4 or 6)\n        The shape of the filters parameter. None or a tuple/list of len 4 or a\n        tuple/list of len 6 (for unshared convolution)\n        Optional, possibly used  to choose an optimal implementation.\n        You can give ``None`` for any element of the list to specify that\n        this element is not known at compile time.\n    border_mode : str, int or tuple of two int\n        Either of the following:\n\n          ``'valid'``\n            apply filter wherever it completely overlaps with the\n            input. Generates output of shape: input shape - filter\n            shape + 1\n\n          ``'full'``\n            apply filter wherever it partly overlaps with the input.\n            Generates output of shape: input shape + filter shape - 1\n\n          ``'half'``\n            pad input with a symmetric border of ``filter rows // 2``\n            rows and ``filter columns // 2`` columns, then perform a\n            valid convolution. For filters with an odd number of rows\n            and columns, this leads to the output shape being equal to\n            the input shape. It is known as 'same' elsewhere.\n\n          ``int``\n            pad input with a symmetric border of zeros of the given\n            width, then perform a valid convolution.\n\n          ``(int1, int2)``\n            pad input with a symmetric border of ``int1`` rows and\n            ``int2`` columns, then perform a valid convolution.\n\n    subsample : tuple of len 2\n        The subsampling used in the forward pass.  Also called strides\n        elsewhere.\n    filter_flip : bool\n        If ``True``, will flip the filter rows and columns before\n        sliding them over the input. This operation is normally\n        referred to as a convolution, and this is the default. If\n        ``False``, the filters are not flipped and the operation is\n        referred to as a cross-correlation.\n    filter_dilation : tuple of len 2\n        The filter dilation used in the forward pass.\n        Also known as input striding.\n    num_groups : int\n        Divides the image, kernel and output tensors into num_groups\n        separate groups. Each which carry out convolutions separately\n    unshared: bool\n        If true, then unshared or 'locally connected' convolution will be\n        performed. A different kernel will be used for each region of the\n        input.\n\n    Returns\n    -------\n    symbolic 4D tensor\n        set of feature maps generated by convolutional layer. Tensor\n        is of shape (batch size, output channels, output rows, output\n        columns)\n\n    Notes\n    -----\n\n    :note: If cuDNN is available, it will be used on the\n        GPU. Otherwise, it is the *CorrMM* convolution that will be used\n        \"caffe style convolution\".\n\n    :note: This is only supported in Theano 0.8 or the development\n        version until it is released.", "file": "theano_tensor_nnet_abstract_conv.py", "function": "conv2d_grad_wrt_inputs", "project": "Theano", "change_stats": {"code_changes": 12, "doc_changes": 23, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, x_transform, y_transform, **kwargs):\n        \n        \n        \n        # Here we ask: \"Does it blend?\"\n\n        Transform.__init__(self, **kwargs)\n        self._x = x_transform\n        self._y = y_transform\n        self.set_children(x_transform, y_transform)\n        self._affine = None```\n\nUpdated code:\n```python\ndef __init__(self, x_transform, y_transform, **kwargs):\n        \n        \n        \n        Transform.__init__(self, **kwargs)\n        self._x = x_transform\n        self._y = y_transform\n        self.set_children(x_transform, y_transform)\n        self._affine = None\n```\n\nOriginal documentation:\n```python\nCreate a new \"blended\" transform using *x_transform* to\n        transform the *x*-axis and *y_transform* to transform the\n        *y*-axis.\n\n        You will generally not call this constructor directly but use the\n        `blended_transform_factory` function instead, which can determine\n        automatically which kind of blended transform to create.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a new \"blended\" transform using *x_transform* to transform the\n        *x*-axis and *y_transform* to transform the *y*-axis.\n\n        You will generally not call this constructor directly but use the\n        `blended_transform_factory` function instead, which can determine\n        automatically which kind of blended transform to create.", "file": "lib_matplotlib_transforms.py", "function": "BlendedGenericTransform.__init__", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef visit_module(self, node: Module) -> None:\n        \n        self.current_module = node.name```\n\nUpdated code:\n```python\ndef visit_module(self, node: Module) -> None:\n        \n        if node.package:\n            self.current_package = node.name\n        else:\n            # Strip name of the current module\n            self.current_package = node.name[: node.name.rfind(\".\")]\n```\n\nOriginal documentation:\n```python\nCalled when a Import node is visited.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Called when a Module node is visited.", "file": "pylint_plugins_hass_imports.py", "function": "HassImportsFormatChecker.visit_module", "project": "core", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_box_actions_out_of_bound(env, seed):\n    \n    \n    OOB_VALUE = 100\n\n    env.reset(seed=seed)\n\n    oob_env = gym.make(env.spec.id)\n    oob_env.reset(seed=seed)\n\n    dtype = env.action_space.dtype\n\n    upper_bounds = env.action_space.high\n    lower_bounds = env.action_space.low\n\n    for i, (is_upper_bound, is_lower_bound) in enumerate(\n        zip(env.action_space.bounded_above, env.action_space.bounded_below)\n    ):\n        if is_upper_bound:\n            obs, _, _, _ = env.step(upper_bounds)\n            oob_action = upper_bounds.copy()\n            oob_action[i] += np.cast[dtype](OOB_VALUE)\n\n            assert oob_action[i] > upper_bounds[i]\n            oob_obs, _, _, _ = oob_env.step(oob_action)\n\n            assert np.alltrue(obs == oob_obs)\n\n        if is_lower_bound:\n            obs, _, _, _ = env.step(lower_bounds)\n            oob_action = lower_bounds.copy()\n            oob_action[i] -= np.cast[dtype](OOB_VALUE)\n\n            assert oob_action[i] < lower_bounds[i]\n            oob_obs, _, _, _ = oob_env.step(oob_action)\n\n            assert np.alltrue(obs == oob_obs)```\n\nUpdated code:\n```python\ndef test_box_actions_out_of_bound(env: gym.Env):\n    \n    \n    env.reset(seed=42)\n\n    oob_env = gym.make(env.spec.id, disable_env_checker=True)\n    oob_env.reset(seed=42)\n\n    assert isinstance(env.action_space, spaces.Box)\n    dtype = env.action_space.dtype\n    upper_bounds = env.action_space.high\n    lower_bounds = env.action_space.low\n\n    for i, (is_upper_bound, is_lower_bound) in enumerate(\n        zip(env.action_space.bounded_above, env.action_space.bounded_below)\n    ):\n        if is_upper_bound:\n            obs, _, _, _ = env.step(upper_bounds)\n            oob_action = upper_bounds.copy()\n            oob_action[i] += np.cast[dtype](OOB_VALUE)\n\n            assert oob_action[i] > upper_bounds[i]\n            oob_obs, _, _, _ = oob_env.step(oob_action)\n\n            assert np.alltrue(obs == oob_obs)\n\n        if is_lower_bound:\n            obs, _, _, _ = env.step(lower_bounds)\n            oob_action = lower_bounds.copy()\n            oob_action[i] -= np.cast[dtype](OOB_VALUE)\n\n            assert oob_action[i] < lower_bounds[i]\n            oob_obs, _, _, _ = oob_env.step(oob_action)\n\n            assert np.alltrue(obs == oob_obs)\n\n    env.close()\n```\n\nOriginal documentation:\n```python\nTest out of bound actions in Box action_space.\n    Environments with Box actions spaces perform clipping inside `step`.\n    The expected behaviour is that an action `out-of-bound` has the same effect\n    of an action with value exactly at the upper (or lower) bound.\n    Args:\n        env (gym.Env): the gym environment\n        seed (int): seed value for determinism\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test out of bound actions in Box action_space.\n\n    Environments with Box actions spaces perform clipping inside `step`.\n    The expected behaviour is that an action `out-of-bound` has the same effect\n    of an action with value exactly at the upper (or lower) bound.\n\n    Args:\n        env (gym.Env): the gym environment", "file": "tests_envs_test_action_dim_check.py", "function": "test_box_actions_out_of_bound", "project": "gym", "change_stats": {"code_changes": 16, "doc_changes": 5, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_web_ui(\n        self,\n        host=\"\",\n        port=8089,\n        auth_credentials=None,\n        tls_cert=None,\n        tls_key=None,\n        stats_csv_writer=None,\n        delayed_start=False,\n        enable_cors=False\n    ):\n        \n        \n        \n        self.web_ui = WebUI(\n            self,\n            host,\n            port,\n            auth_credentials=auth_credentials,\n            tls_cert=tls_cert,\n            tls_key=tls_key,\n            stats_csv_writer=stats_csv_writer,\n            delayed_start=delayed_start,\n            enable_cors=enable_cors\n        )\n        return self.web_ui```\n\nUpdated code:\n```python\ndef create_web_ui(\n        self,\n        host=\"\",\n        port=8089,\n        auth_credentials=None,\n        tls_cert=None,\n        tls_key=None,\n        stats_csv_writer=None,\n        delayed_start=False,\n    ):\n        \n        \n        \n        self.web_ui = WebUI(\n            self,\n            host,\n            port,\n            auth_credentials=auth_credentials,\n            tls_cert=tls_cert,\n            tls_key=tls_key,\n            stats_csv_writer=stats_csv_writer,\n            delayed_start=delayed_start,\n        )\n        return self.web_ui\n```\n\nOriginal documentation:\n```python\nCreates a :class:`WebUI <locust.web.WebUI>` instance for this Environment and start running the web server\n\n        :param host: Host/interface that the web server should accept connections to. Defaults to \"\"\n                     which means all interfaces\n        :param port: Port that the web server should listen to\n        :param auth_credentials: If provided (in format \"username:password\") basic auth will be enabled\n        :param tls_cert: An optional path (str) to a TLS cert. If this is provided the web UI will be\n                         served over HTTPS\n        :param tls_key: An optional path (str) to a TLS private key. If this is provided the web UI will be\n                        served over HTTPS\n        :param stats_csv_writer: `StatsCSV <stats_csv.StatsCSV>` instance.\n        :param delayed_start: Whether or not to delay starting web UI until `start()` is called. Delaying web UI start\n                              allows for adding Flask routes or Blueprints before accepting requests, avoiding errors.\n        :param enable_cors: Whether or not to enable Cross-Origin Resource Sharing. This is can be useful when integrating\n                            the Locust API with external services.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates a :class:`WebUI <locust.web.WebUI>` instance for this Environment and start running the web server\n\n        :param host: Host/interface that the web server should accept connections to. Defaults to \"\"\n                     which means all interfaces\n        :param port: Port that the web server should listen to\n        :param auth_credentials: If provided (in format \"username:password\") basic auth will be enabled\n        :param tls_cert: An optional path (str) to a TLS cert. If this is provided the web UI will be\n                         served over HTTPS\n        :param tls_key: An optional path (str) to a TLS private key. If this is provided the web UI will be\n                        served over HTTPS\n        :param stats_csv_writer: `StatsCSV <stats_csv.StatsCSV>` instance.\n        :param delayed_start: Whether or not to delay starting web UI until `start()` is called. Delaying web UI start\n                              allows for adding Flask routes or Blueprints before accepting requests, avoiding errors.", "file": "locust_env.py", "function": "Environment.create_web_ui", "project": "locust", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_inbound_mails(self, test_mails=None):\n\t\t\n\t\t\n\t\tif not self.enable_incoming:\n\t\t\treturn []\n\n\t\tif frappe.local.flags.in_test:\n\t\t\treturn [InboundMail(msg, self) for msg in test_mails or []]\n\n\t\temail_sync_rule = self.build_email_sync_rule()\n\t\ttry:\n\t\t\temail_server = self.get_incoming_server(in_receive=True, email_sync_rule=email_sync_rule)\n\t\t\tmessages = email_server.get_messages() or {}\n\t\texcept Exception:\n\t\t\traise\n\t\t\tprint(\"In exception...\")\n\t\t\tfrappe.log_error(title=_(\"Error while connecting to email account {0}\").format(self.name))\n\t\t\treturn []\n\n\t\tmails = []\n\t\tfor index, message in enumerate(messages.get(\"latest_messages\", [])):\n\t\t\tprint(\"RAW MESSAGE TYPE: \", type(message))\n\t\t\tuid = messages['uid_list'][index]\n\t\t\tseen_status = 1 if messages['seen_status'][uid]=='SEEN' else 0\n\t\t\tmails.append(InboundMail(message, self, uid, seen_status))\n\n\t\treturn mails```\n\nUpdated code:\n```python\ndef get_inbound_mails(self, test_mails=None):\n\t\t\n\n\t\t\n\t\tif not self.enable_incoming:\n\t\t\treturn []\n\n\t\tif frappe.local.flags.in_test:\n\t\t\treturn [InboundMail(msg, self) for msg in test_mails or []]\n\n\t\temail_sync_rule = self.build_email_sync_rule()\n\t\ttry:\n\t\t\temail_server = self.get_incoming_server(in_receive=True, email_sync_rule=email_sync_rule)\n\t\t\tmessages = email_server.get_messages() or {}\n\t\texcept Exception:\n\t\t\traise\n\t\t\tfrappe.log_error(title=_(\"Error while connecting to email account {0}\").format(self.name))\n\t\t\treturn []\n\n\t\tmails = []\n\t\tfor index, message in enumerate(messages.get(\"latest_messages\", [])):\n\t\t\tuid = messages['uid_list'][index]\n\t\t\tseen_status = 1 if messages['seen_status'][uid]=='SEEN' else 0\n\t\t\tmails.append(InboundMail(message, self, uid, seen_status))\n\n\t\treturn mails\n```\n\nOriginal documentation:\n```python\nretrive and return inbound mails.\n\n\t\tTODO: Handle for tests\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "retrive and return inbound mails.", "file": "frappe_email_doctype_email_account_email_account.py", "function": "EmailAccount.get_inbound_mails", "project": "frappe", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save_sys_module_state(self):\n        \n        \n        self._orig_sys_module_state = {}\n        self._orig_sys_module_state['stdin'] = sys.stdin\n        self._orig_sys_module_state['stdout'] = sys.stdout\n        self._orig_sys_module_state['stderr'] = sys.stderr\n        self._orig_sys_module_state['excepthook'] = sys.excepthook\n        try:\n            self._orig_sys_modules_main_name = self.user_ns['__name__']\n        except KeyError:\n            pass```\n\nUpdated code:\n```python\ndef save_sys_module_state(self):\n        \n        \n        self._orig_sys_module_state = {}\n        self._orig_sys_module_state['stdin'] = sys.stdin\n        self._orig_sys_module_state['stdout'] = sys.stdout\n        self._orig_sys_module_state['stderr'] = sys.stderr\n        self._orig_sys_module_state['excepthook'] = sys.excepthook\n        self._orig_sys_modules_main_name = self.user_module.__name__\n```\n\nOriginal documentation:\n```python\nSave the state of hooks in the sys module.\n\n        This has to be called after self.user_ns is created.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save the state of hooks in the sys module.\n\n        This has to be called after self.user_module is created.", "file": "IPython_core_interactiveshell.py", "function": "InteractiveShell.save_sys_module_state", "project": "ipython", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _pip_cache_cmd_argument(self):\n        \n        \n        \n        if (\n            (\n                settings.RTD_CLEAN_AFTER_BUILD or\n                self.project.has_feature(Feature.CLEAN_AFTER_BUILD)\n            ) and not self.project.has_feature(Feature.CACHED_ENVIRONMENT)\n        ):\n            return [\n                '--no-cache-dir',\n            ]\n        return [\n            '--cache-dir',\n            self.project.pip_cache_path,\n        ]```\n\nUpdated code:\n```python\ndef _pip_cache_cmd_argument(self):\n        \n        \n        \n        if (\n            # Cache is going to be removed anyways\n            settings.RTD_CLEAN_AFTER_BUILD or\n            self.project.has_feature(Feature.CLEAN_AFTER_BUILD) or\n            # Cache will be pushed/pulled each time and won't be used because\n            # packages are already installed in the environment\n            self.project.has_feature(Feature.CACHED_ENVIRONMENT)\n        ):\n            return [\n                '--no-cache-dir',\n            ]\n        return [\n            '--cache-dir',\n            self.project.pip_cache_path,\n        ]\n```\n\nOriginal documentation:\n```python\nReturn the pip command ``--cache-dir`` or ``--no-cache-dir`` argument.\n\n        The decision is made considering if the directories are going to be\n        cleaned after the build (``RTD_CLEAN_AFTER_BUILD=True`` or project has\n        the ``CLEAN_AFTER_BUILD`` feature enabled) and project has not the\n        feature ``CACHED_ENVIRONMENT``. In this case, there is no need to cache\n        anything.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the pip command ``--cache-dir`` or ``--no-cache-dir`` argument.\n\n        The decision is made considering if the directories are going to be\n        cleaned after the build (``RTD_CLEAN_AFTER_BUILD=True`` or project has\n        the ``CLEAN_AFTER_BUILD`` feature enabled) or project has the feature\n        ``CACHED_ENVIRONMENT``. In this case, there is no need to cache\n        anything.", "file": "readthedocs_doc_builder_python_environments.py", "function": "PythonEnvironment._pip_cache_cmd_argument", "project": "readthedocs.org", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(self, h: torch.Tensor) -> Union[List[Hypothesis], List[NSCHypothesis]]:\n        \n\n        \n        self.decoder.set_device(h.device)\n\n        if not hasattr(self.decoder, \"decoders\"):\n            self.decoder.set_data_type(h.dtype)\n\n        nbest_hyps = self.search_algorithm(h)\n\n        return nbest_hyps```\n\nUpdated code:\n```python\ndef __call__(\n        self, enc_out: torch.Tensor\n    ) -> Union[List[Hypothesis], List[ExtendedHypothesis]]:\n        \n\n        \n        self.decoder.set_device(enc_out.device)\n\n        nbest_hyps = self.search_algorithm(enc_out)\n\n        return nbest_hyps\n```\n\nOriginal documentation:\n```python\nPerform beam search.\n\n        Args:\n            h: Encoded speech features (T_max, D_enc)\n\n        Returns:\n            nbest_hyps: N-best decoding results\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform beam search.\n\n        Args:\n            enc_out: Encoder output sequence. (T, D_enc)\n\n        Returns:\n            nbest_hyps: N-best decoding results", "file": "espnet_nets_beam_search_transducer.py", "function": "BeamSearchTransducer.__call__", "project": "espnet", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_frame_result_type(result, objs):\n    \n    \n    \n\n    if result.blocks and (any(isinstance(obj, ABCSparseDataFrame) for obj in objs)):\n        from pandas.core.sparse.api import SparseDataFrame\n\n        return SparseDataFrame\n    else:\n        return next(obj for obj in objs if not isinstance(obj, ABCSparseDataFrame))```\n\nUpdated code:\n```python\ndef _get_frame_result_type(result, objs):\n    \n    \n    \n    # TODO: just inline this as _constructor.\n    return objs[0]\n```\n\nOriginal documentation:\n```python\nreturn appropriate class of DataFrame-like concat\n    if all blocks are sparse, return SparseDataFrame\n    otherwise, return 1st obj\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "return appropriate class of DataFrame-like concat", "file": "pandas_core_reshape_concat.py", "function": "_get_frame_result_type", "project": "pandas", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_event_loop():\n    \n    return get_event_loop_policy().get_event_loop()```\n\nUpdated code:\n```python\ndef get_event_loop():\n    \n    \n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n```\n\nOriginal documentation:\n```python\nEquivalent to calling get_event_loop_policy().get_event_loop().\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.", "file": "Lib_asyncio_events.py", "function": "get_event_loop", "project": "cpython", "change_stats": {"code_changes": 6, "doc_changes": 10, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef decode(loc, priors):\n        \n        \n        variances = [0.1, 0.2]\n        boxes = np.concatenate((priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n                                priors[:, 2:] * np.exp(loc[:, 2:] * variances[1])), axis=1)\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n        return boxes```\n\nUpdated code:\n```python\ndef decode(location, priors):\n        \n        \n        variances = [0.1, 0.2]\n        boxes = np.concatenate((priors[:, :2] + location[:, :2] * variances[0] * priors[:, 2:],\n                                priors[:, 2:] * np.exp(location[:, 2:] * variances[1])), axis=1)\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n        return boxes\n```\n\nOriginal documentation:\n```python\nDecode locations from predictions using priors to undo\n        the encoding we did for offset regression at train time.\n        Args:\n            loc (tensor): location predictions for loc layers,\n                Shape: [num_priors,4]\n            priors (tensor): Prior boxes in center-offset form.\n                Shape: [num_priors,4].\n            variances: (list[float]) Variances of prior boxes\n        Return:\n            decoded bounding box predictions\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Decode locations from predictions using priors to undo the encoding we did for offset\n        regression at train time.\n\n        Parameters\n        ----------\n        location: tensor\n            location predictions for location layers,\n        priors: tensor\n            Prior boxes in center-offset form.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            decoded bounding box predictions", "file": "plugins_extract_detect_s3fd.py", "function": "S3fd.decode", "project": "faceswap", "change_stats": {"code_changes": 8, "doc_changes": 24, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef do_list(self, arg):\n        \n        \n        self.lastcmd = 'list'\n        last = None\n        if arg and arg != '.':\n            try:\n                if ',' in arg:\n                    first, last = arg.split(',')\n                    first = int(first.strip())\n                    last = int(last.strip())\n                    if last < first:\n                        # assume it's a count\n                        last = first + last\n                else:\n                    first = int(arg.strip())\n                    first = max(1, first - 5)\n            except ValueError:\n                self.error('Error in argument: %r' % arg)\n                return\n        elif self.lineno is None or arg == '.':\n            first = max(1, self.curframe.f_lineno - 5)\n        else:\n            first = self.lineno + 1\n        if last is None:\n            last = first + 10\n        filename = self.curframe.f_code.co_filename\n        breaklist = self.get_file_breaks(filename)\n        try:\n            # XXX add tb_lineno feature\n            lines = linecache.getlines(filename, self.curframe.f_globals)\n            self._print_lines(lines[first-1:last], first, breaklist,\n                              self.curframe.f_lineno, -1)\n            self.lineno = min(last, len(lines))\n            if len(lines) < last:\n                self.message('[EOF]')\n        except KeyboardInterrupt:\n            pass```\n\nUpdated code:\n```python\ndef do_list(self, arg):\n        \n        \n        self.lastcmd = 'list'\n        last = None\n        if arg and arg != '.':\n            try:\n                if ',' in arg:\n                    first, last = arg.split(',')\n                    first = int(first.strip())\n                    last = int(last.strip())\n                    if last < first:\n                        # assume it's a count\n                        last = first + last\n                else:\n                    first = int(arg.strip())\n                    first = max(1, first - 5)\n            except ValueError:\n                self.error('Error in argument: %r' % arg)\n                return\n        elif self.lineno is None or arg == '.':\n            first = max(1, self.curframe.f_lineno - 5)\n        else:\n            first = self.lineno + 1\n        if last is None:\n            last = first + 10\n        filename = self.curframe.f_code.co_filename\n        breaklist = self.get_file_breaks(filename)\n        try:\n            lines = linecache.getlines(filename, self.curframe.f_globals)\n            self._print_lines(lines[first-1:last], first, breaklist,\n                              self.curframe)\n            self.lineno = min(last, len(lines))\n            if len(lines) < last:\n                self.message('[EOF]')\n        except KeyboardInterrupt:\n            pass\n```\n\nOriginal documentation:\n```python\nl(ist) [first [,last] | .]\n        List source code for the current file.\n        Without arguments, list 11 lines around the current line\n        or continue the previous listing.\n        With . as argument, list 11 lines around the current line.\n        With one argument, list 11 lines starting at that line.\n        With two arguments, list the given range;\n        if the second argument is less than the first, it is a count.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "l(ist) [first [,last] | .]\n\n        List source code for the current file.  Without arguments,\n        list 11 lines around the current line or continue the previous\n        listing.  With . as argument, list 11 lines around the current\n        line.  With one argument, list 11 lines starting at that line.\n        With two arguments, list the given range; if the second\n        argument is less than the first, it is a count.\n\n        The current line in the current frame is indicated by \"->\".\n        If an exception is being debugged, the line where the\n        exception was originally raised or propagated is indicated by\n        \">>\", if it differs from the current line.", "file": "Lib_pdb.py", "function": "Pdb.do_list", "project": "cpython", "change_stats": {"code_changes": 5, "doc_changes": 21, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef handle_gopy_data(self, data: str):\n        \n        \n        \n        try:\n            data_dict = json.loads(data)\n            message_type = data_dict[\"message_type\"]\n            message_contents = data_dict[\"message_contents\"]\n\n            if message_type == \"peer_update\":\n                # update in peers reliability or IP address.\n                self.process_go_update(message_contents)\n\n            elif message_type == \"go_data\":\n                # a peer request or update\n                self.process_go_data(message_contents)\n\n            else:\n                self.print(\"Invalid command: \" + message_type)\n\n        except json.decoder.JSONDecodeError:\n            self.print(\"Couldn't load message from pigeon - invalid json\")\n\n        except KeyError:\n            self.print(f\"Json from the pigeon: {data_dict} doesn't contain \"\n                       \"expected values message_type or message_contents\")```\n\nUpdated code:\n```python\ndef handle_gopy_data(self, data: str):\n        \n        \n        \n        try:\n            data_dict = json.loads(data)\n            message_type = data_dict[\"message_type\"]\n            message_contents = data_dict[\"message_contents\"]\n\n            if message_type == \"peer_update\":\n                # update in peers reliability or IP address.\n                self.process_go_update(message_contents)\n\n            elif message_type == \"go_data\":\n                # a peer request or update\n                self.process_go_data(message_contents)\n\n            else:\n                self.print(\"Invalid command: \" + message_type)\n\n        except json.decoder.JSONDecodeError:\n            self.print(f\"Couldn't load message from pigeon - invalid Json from the pigeon: {data_dict}\")\n\n        except KeyError:\n            self.print(f\"Json from the pigeon: {data_dict} doesn't contain \"\n                       \"expected values message_type or message_contents\")\n```\n\nOriginal documentation:\n```python\nMethod that receives raw data sent into p2p_gopy redis channel\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Method that receives raw data from peers sent into p2p_gopy redis channel", "file": "modules_p2ptrust_utils_go_director.py", "function": "GoDirector.handle_gopy_data", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef mktime_tz(data):\n    \n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = time.mktime(data[:8] + (0,))\n        return t - data[9] - time.timezone```\n\nUpdated code:\n```python\ndef mktime_tz(data):\n    \n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = calendar.timegm(data)\n        return t - data[9]\n```\n\nOriginal documentation:\n```python\nTurn a 10-tuple as returned by parsedate_tz() into a UTC timestamp.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Turn a 10-tuple as returned by parsedate_tz() into a POSIX timestamp.", "file": "Lib_email__parseaddr.py", "function": "mktime_tz", "project": "cpython", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_multihead_self_attn_two_masks_fast_path_mock(self, device):\n        \n        \n        \n        if device not in ['cpu', 'cuda']:\n            self.skipTest(\"Fastpath only runs on CPU and CUDA.\")\n\n        with torch.autocast(device_type=device, enabled=False):\n            embed_dim = 16\n            num_heads = 8\n            batch_size = 8\n            src_len = 5\n\n            query = value = key = torch.rand(batch_size, src_len, embed_dim).to(device)\n            # Create masks of two different types\n            attn_mask = torch.randint(0, 2, (src_len, src_len)).bool().to(device)\n            key_padding_mask = torch.randint(0, 2, (batch_size, src_len)).bool().to(device)\n\n            with mock.patch('torch._native_multi_head_attention', new=mock.MagicMock(\n                    return_value=(torch.Tensor(), torch.Tensor()))\n            ) as fastpath_mock:\n                # Compute attention on the fast path\n                mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device).eval()\n                mta_model.training = False\n                mta_model(query, key, value, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n                # If mock was called, fastpath was taken\n                self.assertTrue(fastpath_mock.called)```\n\nUpdated code:\n```python\ndef test_multihead_self_attn_two_masks_fast_path_mock(self, device):\n        \n        \n        \n        device = device.rstrip(':0123456789')\n        if device not in ['cpu', 'cuda', torch._C._get_privateuse1_backend_name()]:\n            self.skipTest(\"Fastpath only runs on CPU and CUDA and PrivateUse1.\")\n\n        with torch.autocast(device_type=device, enabled=False):\n            embed_dim = 16\n            num_heads = 8\n            batch_size = 8\n            src_len = 5\n\n            query = value = key = torch.rand(batch_size, src_len, embed_dim).to(device)\n            # Create masks of two different types\n            attn_mask = torch.randint(0, 2, (src_len, src_len)).bool().to(device)\n            key_padding_mask = torch.randint(0, 2, (batch_size, src_len)).bool().to(device)\n\n            with mock.patch('torch._native_multi_head_attention', new=mock.MagicMock(\n                    return_value=(torch.Tensor(), torch.Tensor()))\n            ) as fastpath_mock:\n                # Compute attention on the fast path\n                mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, device=device).eval()\n                mta_model.training = False\n                mta_model(query, key, value, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n                # If mock was called, fastpath was taken\n                self.assertTrue(fastpath_mock.called)\n```\n\nOriginal documentation:\n```python\nMultihead self-attention should take fast path when both attention mask (mask type 0)\n        and key padding mask (mask type 1) are provided at the same time on CPU and CUDA\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Multihead self-attention should take fast path when both attention mask (mask type 0)\n        and key padding mask (mask type 1) are provided at the same time on CPU and CUDA and PrivateUse1", "file": "test_nn_test_multihead_attention.py", "function": "TestMultiheadAttentionNNDeviceType.test_multihead_self_attn_two_masks_fast_path_mock", "project": "pytorch", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_frame_result_type(result, objs):\n    \n    \n    \n\n    if result.blocks and all(b.is_sparse for b in result.blocks):\n        from pandas.core.sparse.api import SparseDataFrame\n        return SparseDataFrame\n    else:\n        return next(obj for obj in objs if not isinstance(obj,\n                                                          ABCSparseDataFrame))```\n\nUpdated code:\n```python\ndef _get_frame_result_type(result, objs):\n    \n    \n    \n\n    if (result.blocks and (\n            all(is_sparse(b) for b in result.blocks) or\n            all(isinstance(obj, ABCSparseDataFrame) for obj in objs))):\n        from pandas.core.sparse.api import SparseDataFrame\n        return SparseDataFrame\n    else:\n        return next(obj for obj in objs if not isinstance(obj,\n                                                          ABCSparseDataFrame))\n```\n\nOriginal documentation:\n```python\nreturn appropriate class of DataFrame-like concat\n    if all blocks are SparseBlock, return SparseDataFrame\n    otherwise, return 1st obj\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "return appropriate class of DataFrame-like concat\n    if all blocks are sparse, return SparseDataFrame\n    otherwise, return 1st obj", "file": "pandas_core_dtypes_concat.py", "function": "_get_frame_result_type", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_canceled_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):\n        \n        \n        \n        self.load_markets()\n        request = {\n            'orderStatus': 'Cancelled',\n        }\n        return self.fetch_orders(symbol, since, limit, self.extend(request, params))```\n\nUpdated code:\n```python\ndef fetch_canceled_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):\n        \n        \n        \n        self.load_markets()\n        request = {\n            'orderStatus': 'Cancelled',\n        }\n        return self.fetch_canceled_and_closed_orders(symbol, since, limit, self.extend(request, params))\n```\n\nOriginal documentation:\n```python\nfetches information on multiple canceled orders made by the user\n        :see: https://bybit-exchange.github.io/docs/v5/order/order-list\n        :param str symbol: unified market symbol of the market orders were made in\n        :param int [since]: timestamp in ms of the earliest order, default is None\n        :param int [limit]: max number of orders to return, default is None\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param boolean [params.stop]: True if stop order\n        :param str [params.type]: market type, ['swap', 'option', 'spot']\n        :param str [params.subType]: market subType, ['linear', 'inverse']\n        :returns dict: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetches information on multiple canceled orders made by the user\n        :see: https://bybit-exchange.github.io/docs/v5/order/order-list\n        :param str [symbol]: unified market symbol of the market orders were made in\n        :param int [since]: timestamp in ms of the earliest order, default is None\n        :param int [limit]: max number of orders to return, default is None\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param boolean [params.stop]: True if stop order\n        :param str [params.type]: market type, ['swap', 'option', 'spot']\n        :param str [params.subType]: market subType, ['linear', 'inverse']\n        :param str [params.orderFilter]: 'Order' or 'StopOrder' or 'tpslOrder'\n        :param int [params.until]: the latest time in ms to fetch entries for\n        :param boolean [params.paginate]: default False, when True will automatically paginate by calling self endpoint multiple times. See in the docs all the [available parameters](https://github.com/ccxt/ccxt/wiki/Manual#pagination-params)\n        :returns dict: a list of `order structures <https://docs.ccxt.com/#/?id=order-structure>`", "file": "python_ccxt_bybit.py", "function": "bybit.fetch_canceled_orders", "project": "ccxt", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_empirical(self):\n        \n        \n        \n        return {name: dist.Empirical(value, self._log_weights)\n                for name, value in self._values.items()}```\n\nUpdated code:\n```python\ndef get_empirical(self):\n        \n        \n        \n        return {key: dist.Empirical(value, self.state._log_weights)\n                for key, value in self.state.items()}\n```\n\nOriginal documentation:\n```python\n:returns: a marginal distribution over every latent variable.\n        :rtype: a dictionary with keys which are latent variables and values\n            which are :class:`~pyro.distributions.Empirical` objects.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":returns: a marginal distribution over all state tensors.\n        :rtype: a dictionary with keys which are latent variables and values\n            which are :class:`~pyro.distributions.Empirical` objects.", "file": "pyro_infer_smcfilter.py", "function": "SMCFilter.get_empirical", "project": "pyro", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef location2node(location):\n    \n    \n    \n    return chr(location[0] + 64 + 1) + str(location[1] + 1)```\n\nUpdated code:\n```python\ndef location2node(location):\n    \n    \n    \n    x, y = location\n    if y >= 0:\n        y += 1\n    return col2name(x) + str(y)\n```\n\nOriginal documentation:\n```python\nArgs:\n        location(tuple): Example: (6, 4)\n\n    Returns:\n        str: Example: 'E3'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert location tuple to an Excel-like cell.\n    Accept negative values also.\n\n         -2   -1    0    1    2    3\n    -2 -B-2 -A-2  A-2  B-2  C-2  D-2\n    -1 -B-1 -A-1  A-1  B-1  C-1  D-1\n     0  -B1  -A1   A1   B1   C1   D1\n     1  -B2  -A2   A2   B2   C2   D2\n     2  -B3  -A3   A3   B3   C3   D3\n     3  -B4  -A4   A4   B4   C4   D4\n\n    # To generate the table above\n    index = range(-2, 4)\n    row = '   ' + ' '.join([str(i).rjust(4) for i in index])\n    print(row)\n    for y in index:\n        row = str(y).rjust(2) + ' ' + ' '.join([location2node((x, y)).rjust(4) for x in index])\n        print(row)\n\n    def check(node):\n        return point2str(*node2location(location2node(node)), length=2)\n    row = '   ' + ' '.join([str(i).rjust(8) for i in index])\n    print(row)\n    for y in index:\n        row = str(y).rjust(2) + ' ' + ' '.join([check((x, y)).rjust(4) for x in index])\n        print(row)\n\n    Args:\n        location (tuple[int]):\n\n    Returns:\n        str:", "file": "module_base_utils.py", "function": "location2node", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 7, "doc_changes": 35, "total_changes": 42}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef async_aiohttp_proxy_stream(hass, request, stream_coro, buffer_size=102400,\n                               timeout=10):\n    \n    response = None\n    stream = None\n\n    try:\n        with async_timeout.timeout(timeout, loop=hass.loop):\n            stream = yield from stream_coro\n\n        response = web.StreamResponse()\n        response.content_type = stream.headers.get(CONTENT_TYPE)\n\n        yield from response.prepare(request)\n\n        while True:\n            data = yield from stream.content.read(buffer_size)\n            if not data:\n                break\n            response.write(data)\n\n    except asyncio.TimeoutError:\n        raise HTTPGatewayTimeout()\n\n    except (aiohttp.errors.ClientError,\n            aiohttp.errors.ClientDisconnectedError):\n        pass\n\n    except (asyncio.CancelledError, ConnectionResetError):\n        response = None\n\n    finally:\n        if stream is not None:\n            stream.close()\n        if response is not None:\n            yield from response.write_eof()```\n\nUpdated code:\n```python\ndef async_aiohttp_proxy_stream(hass, request, stream, content_type,\n                               buffer_size=102400, timeout=10):\n    \n    response = web.StreamResponse()\n    response.content_type = content_type\n    yield from response.prepare(request)\n\n    try:\n        while True:\n            with async_timeout.timeout(timeout, loop=hass.loop):\n                data = yield from stream.read(buffer_size)\n\n            if not data:\n                yield from response.write_eof()\n                break\n\n            response.write(data)\n\n    except (asyncio.TimeoutError, aiohttp.ClientError):\n        pass\n\n    yield from response.write_eof()\n```\n\nOriginal documentation:\n```python\nStream websession request to aiohttp web response.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Stream a stream to aiohttp web response.", "file": "homeassistant_helpers_aiohttp_client.py", "function": "async_aiohttp_proxy_stream", "project": "core", "change_stats": {"code_changes": 42, "doc_changes": 4, "total_changes": 46}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _has_code_flag(f, flag):\n    \n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    if not (isfunction(f) or _signature_is_functionlike(f)):\n        return False\n    return bool(f.__code__.co_flags & flag)```\n\nUpdated code:\n```python\ndef _has_code_flag(f, flag):\n    \n    f = functools._unwrap_partialmethod(f)\n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    if not (isfunction(f) or _signature_is_functionlike(f)):\n        return False\n    return bool(f.__code__.co_flags & flag)\n```\n\nOriginal documentation:\n```python\nReturn true if ``f`` is a function (or a method or functools.partial\n    wrapper wrapping a function) whose code object has the given ``flag``\n    set in its flags.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return true if ``f`` is a function (or a method or functools.partial\n    wrapper wrapping a function or a functools.partialmethod wrapping a\n    function) whose code object has the given ``flag``\n    set in its flags.", "file": "Lib_inspect.py", "function": "_has_code_flag", "project": "cpython", "change_stats": {"code_changes": 3, "doc_changes": 5, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'pair': market['id'],\n            'type': side,\n            'amount': amount,\n            'price': self.price_to_precision(symbol, price),\n        }\n        response = self.privatePostOrder(self.extend(request, params))\n        if not response['success']:\n            raise InvalidOrder(self.id + ' ' + self.json(response))\n        order = self.parse_order(response, market)\n        orderAmount = str(order['amount'])\n        amount = order['amount'] if Precise.string_gt(orderAmount, '0') else amount\n        return self.extend(order, {\n            'amount': self.parse_number(amount),\n        })```\n\nUpdated code:\n```python\ndef create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):\n        \n        \n        \n        if type == 'market':\n            raise InvalidOrder(self.id + ' only limits orders are supported')\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'pair': market['id'],\n            'type': side,\n            'amount': amount,\n            'price': self.price_to_precision(symbol, price),\n        }\n        response = self.privatePostOrder(self.extend(request, params))\n        if not response['success']:\n            raise InvalidOrder(self.id + ' ' + self.json(response))\n        order = self.parse_order(response, market)\n        orderAmount = str(order['amount'])\n        amount = order['amount'] if Precise.string_gt(orderAmount, '0') else amount\n        return self.extend(order, {\n            'amount': self.parse_number(amount),\n        })\n```\n\nOriginal documentation:\n```python\ncreate a trade order\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'market' or 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much of currency you want to trade in units of base currency\n        :param float [price]: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict [params]: extra parameters specific to the btcalpha api endpoint\n        :returns dict: an `order structure <https://github.com/ccxt/ccxt/wiki/Manual#order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "see https://btc-alpha.github.io/api-docs/#create-order\n        create a trade order\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much of currency you want to trade in units of base currency\n        :param float [price]: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict [params]: extra parameters specific to the btcalpha api endpoint\n        :returns dict: an `order structure <https://github.com/ccxt/ccxt/wiki/Manual#order-structure>`", "file": "python_ccxt_btcalpha.py", "function": "btcalpha.create_order", "project": "ccxt", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef groups(self):\n          \n        return [ g for g in self.handle.walkGroups() if getattr(g._v_attrs,'pandas_type',None) ]```\n\nUpdated code:\n```python\ndef groups(self):\n          \n        return [g for g in self.handle.walkGroups() if getattr(g._v_attrs, 'pandas_type', None)]\n```\n\nOriginal documentation:\n```python\nreturn a list of all the top-level nodes (that are not themselves a pandas storage object)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "return a list of all the groups (that are not themselves a pandas storage object)", "file": "pandas_io_pytables.py", "function": "HDFStore.groups", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef extern(self, include: 'GlobPattern', *, exclude: 'GlobPattern' = ()):\n        \n\n        \n        self.patterns.append((_GlobGroup(include, exclude), self.save_extern_module))```\n\nUpdated code:\n```python\ndef extern(self, include: 'GlobPattern', *, exclude: 'GlobPattern' = (), allow_empty: bool = True):\n        \n\n        \n        self.patterns.append((_GlobGroup(include, exclude), self.save_extern_module, allow_empty))\n```\n\nOriginal documentation:\n```python\nInclude `module` in the list of external modules the package can import.\n        This will prevent dependency discover from saving\n        it in the package. The importer will load an external module directly from the standard import system.\n        Code for extern modules must also exist in the process loading the package.\n\n        Args:\n            include (Union[List[str], str]): A string e.g. \"my_package.my_subpackage\", or list of strings\n                for the names of the modules to be externed. This can also be a glob-style pattern, as described in :meth:`mock`\n\n            exclude (Union[List[str], str]): An optional pattern that excludes some patterns that match the include string.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Include `module` in the list of external modules the package can import.\n        This will prevent dependency discover from saving\n        it in the package. The importer will load an external module directly from the standard import system.\n        Code for extern modules must also exist in the process loading the package.\n\n        Args:\n            include (Union[List[str], str]): A string e.g. \"my_package.my_subpackage\", or list of strings\n                for the names of the modules to be externed. This can also be a glob-style pattern, as described in :meth:`mock`\n\n            exclude (Union[List[str], str]): An optional pattern that excludes some patterns that match the include string.\n\n            allow_empty (bool): An optional flag that specifies whether the extern modules specified by this call\n                to the `extern` method must be matched to some module during packaging. If an extern module glob pattern is added\n                with allow_empty=False, and `close` is called (either explicitly or via `__exit__`) before any modules match that\n                pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.", "file": "torch_package_package_exporter.py", "function": "PackageExporter.extern", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 7, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef asnumpy(a, stream=None, order='C', out=None):\n    \n\n    \n    if isinstance(a, ndarray):\n        return a.get(stream=stream, order=order, out=out)\n    elif hasattr(a, \"__cuda_array_interface__\"):\n        return array(a).get(stream=stream, order=order, out=out)\n    else:\n        temp = _numpy.asarray(a, order=order)\n        if out is not None:\n            out[...] = temp\n        else:\n            out = temp\n        return out```\n\nUpdated code:\n```python\ndef asnumpy(a, stream=None, order='C', out=None, *, blocking=True):\n    \n\n    \n    if isinstance(a, ndarray):\n        return a.get(stream=stream, order=order, out=out, blocking=blocking)\n    elif hasattr(a, \"__cuda_array_interface__\"):\n        return array(a).get(\n            stream=stream, order=order, out=out, blocking=blocking)\n    else:\n        temp = _numpy.asarray(a, order=order)\n        if out is not None:\n            out[...] = temp\n        else:\n            out = temp\n        return out\n```\n\nOriginal documentation:\n```python\nReturns an array on the host memory from an arbitrary source array.\n\n    Args:\n        a: Arbitrary object that can be converted to :class:`numpy.ndarray`.\n        stream (cupy.cuda.Stream): CUDA stream object. If it is specified, then\n            the device-to-host copy runs asynchronously. Otherwise, the copy is\n            synchronous. Note that if ``a`` is not a :class:`cupy.ndarray`\n            object, then this argument has no effect.\n        order ({'C', 'F', 'A'}): The desired memory layout of the host\n            array. When ``order`` is 'A', it uses 'F' if ``a`` is\n            fortran-contiguous and 'C' otherwise.\n        out (numpy.ndarray): The output array to be written to. It must have\n            compatible shape and dtype with those of ``a``'s.\n\n    Returns:\n        numpy.ndarray: Converted array on the host memory.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns an array on the host memory from an arbitrary source array.\n\n    Args:\n        a: Arbitrary object that can be converted to :class:`numpy.ndarray`.\n        stream (cupy.cuda.Stream): CUDA stream object. If given, the\n            stream is used to perform the copy. Otherwise, the current\n            stream is used. Note that if ``a`` is not a :class:`cupy.ndarray`\n            object, then this argument has no effect.\n        order ({'C', 'F', 'A'}): The desired memory layout of the host\n            array. When ``order`` is 'A', it uses 'F' if the array is\n            fortran-contiguous and 'C' otherwise. The ``order`` will be\n            ignored if ``out`` is specified.\n        out (numpy.ndarray): The output array to be written to. It must have\n            compatible shape and dtype with those of ``a``'s.\n        blocking (bool): If set to ``False``, the copy runs asynchronously\n            on the given (if given) or current stream, and users are\n            responsible for ensuring the stream order. Default is ``True``,\n            so the copy is synchronous (with respect to the host).\n\n    Returns:\n        numpy.ndarray: Converted array on the host memory.", "file": "cupy___init__.py", "function": "asnumpy", "project": "cupy", "change_stats": {"code_changes": 9, "doc_changes": 17, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __next__(self):\n        \n\n        \n        frame, pts = self._c.next()\n        if frame.numel() == 0:\n            raise StopIteration\n        return frame, pts```\n\nUpdated code:\n```python\ndef __next__(self):\n        \n\n        \n        frame, pts = self._c.next()\n        if frame.numel() == 0:\n            raise StopIteration\n        return {\"data\": frame, \"pts\": pts}\n```\n\nOriginal documentation:\n```python\nDecodes and returns the next frame of the current stream\n\n        Returns:\n            ([torch.Tensor, float]): list containing decoded frame and corresponding timestamp\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Decodes and returns the next frame of the current stream\n\n        Returns:\n            (dict): a dictionary with fields ``data`` and ``pts``\n            containing decoded frame and corresponding timestamp", "file": "torchvision_io___init__.py", "function": "VideoReader.__next__", "project": "vision", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _load_markets(self) -> None:\n          \n        try:\n            self._api.load_markets()\n            self._last_markets_refresh = arrow.utcnow().timestamp\n        except ccxt.BaseError as e:\n            logger.warning('Unable to initialize markets. Reason: %s', e)```\n\nUpdated code:\n```python\ndef _load_markets(self) -> None:\n          \n        try:\n            self._api.load_markets()\n            self._load_async_markets()\n            self._last_markets_refresh = arrow.utcnow().timestamp\n        except ccxt.BaseError as e:\n            logger.warning('Unable to initialize markets. Reason: %s', e)\n```\n\nOriginal documentation:\n```python\nInitialize markets\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize markets both sync and async", "file": "freqtrade_exchange_exchange.py", "function": "Exchange._load_markets", "project": "freqtrade", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_cookie(\n        self,\n        name: str,\n        value: Union[str, bytes],\n        domain: Optional[str] = None,\n        expires: Optional[Union[float, Tuple, datetime.datetime]] = None,\n        path: str = \"/\",\n        expires_days: Optional[float] = None,\n        # Keyword-only args start here for historical reasons.\n        *,\n        max_age: Optional[int] = None,\n        httponly: bool = False,\n        secure: bool = False,\n        samesite: Optional[str] = None,\n    ) -> None:\n        \n\n        \n        # The cookie library only accepts type str, in both python 2 and 3\n        name = escape.native_str(name)\n        value = escape.native_str(value)\n        if re.search(r\"[\\x00-\\x20]\", name + value):\n            # Don't let us accidentally inject bad stuff\n            raise ValueError(\"Invalid cookie %r: %r\" % (name, value))\n        if not hasattr(self, \"_new_cookie\"):\n            self._new_cookie = (\n                http.cookies.SimpleCookie()\n            )  # type: http.cookies.SimpleCookie\n        if name in self._new_cookie:\n            del self._new_cookie[name]\n        self._new_cookie[name] = value\n        morsel = self._new_cookie[name]\n        if domain:\n            morsel[\"domain\"] = domain\n        if expires_days is not None and not expires:\n            expires = datetime.datetime.utcnow() + datetime.timedelta(days=expires_days)\n        if expires:\n            morsel[\"expires\"] = httputil.format_timestamp(expires)\n        if path:\n            morsel[\"path\"] = path\n        if max_age:\n            # Note change from _ to -.\n            morsel[\"max-age\"] = str(max_age)\n        if httponly:\n            # Note that SimpleCookie ignores the value here. The presense of an\n            # httponly (or secure) key is treated as true.\n            morsel[\"httponly\"] = True\n        if secure:\n            morsel[\"secure\"] = True\n        if samesite:\n            morsel[\"samesite\"] = samesite```\n\nUpdated code:\n```python\ndef set_cookie(\n        self,\n        name: str,\n        value: Union[str, bytes],\n        domain: Optional[str] = None,\n        expires: Optional[Union[float, Tuple, datetime.datetime]] = None,\n        path: str = \"/\",\n        expires_days: Optional[float] = None,\n        # Keyword-only args start here for historical reasons.\n        *,\n        max_age: Optional[int] = None,\n        httponly: bool = False,\n        secure: bool = False,\n        samesite: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \n        \n        # The cookie library only accepts type str, in both python 2 and 3\n        name = escape.native_str(name)\n        value = escape.native_str(value)\n        if re.search(r\"[\\x00-\\x20]\", name + value):\n            # Don't let us accidentally inject bad stuff\n            raise ValueError(\"Invalid cookie %r: %r\" % (name, value))\n        if not hasattr(self, \"_new_cookie\"):\n            self._new_cookie = (\n                http.cookies.SimpleCookie()\n            )  # type: http.cookies.SimpleCookie\n        if name in self._new_cookie:\n            del self._new_cookie[name]\n        self._new_cookie[name] = value\n        morsel = self._new_cookie[name]\n        if domain:\n            morsel[\"domain\"] = domain\n        if expires_days is not None and not expires:\n            expires = datetime.datetime.utcnow() + datetime.timedelta(days=expires_days)\n        if expires:\n            morsel[\"expires\"] = httputil.format_timestamp(expires)\n        if path:\n            morsel[\"path\"] = path\n        if max_age:\n            # Note change from _ to -.\n            morsel[\"max-age\"] = str(max_age)\n        if httponly:\n            # Note that SimpleCookie ignores the value here. The presense of an\n            # httponly (or secure) key is treated as true.\n            morsel[\"httponly\"] = True\n        if secure:\n            morsel[\"secure\"] = True\n        if samesite:\n            morsel[\"samesite\"] = samesite\n        if kwargs:\n            # The setitem interface is case-insensitive, so continue to support\n            # kwargs for backwards compatibility until we can remove deprecated\n            # features.\n            for k, v in kwargs.items():\n                morsel[k] = v\n            warnings.warn(\n                f\"Deprecated arguments to set_cookie: {set(kwargs.keys())} \"\n                \"(should be lowercase)\",\n                DeprecationWarning,\n            )\n```\n\nOriginal documentation:\n```python\nSets an outgoing cookie name/value with the given options.\n\n        Newly-set cookies are not immediately visible via `get_cookie`;\n        they are not present until the next request.\n\n        Most arguments are passed directly to `http.cookies.Morsel` directly.\n        See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie\n        for more information.\n\n        ``expires`` may be a numeric timestamp as returned by `time.time`,\n        a time tuple as returned by `time.gmtime`, or a\n        `datetime.datetime` object. ``expires_days`` is provided as a convenience\n        to set an expiration time in days from today (if both are set, ``expires``\n        is used).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sets an outgoing cookie name/value with the given options.\n\n        Newly-set cookies are not immediately visible via `get_cookie`;\n        they are not present until the next request.\n\n        Most arguments are passed directly to `http.cookies.Morsel` directly.\n        See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie\n        for more information.\n\n        ``expires`` may be a numeric timestamp as returned by `time.time`,\n        a time tuple as returned by `time.gmtime`, or a\n        `datetime.datetime` object. ``expires_days`` is provided as a convenience\n        to set an expiration time in days from today (if both are set, ``expires``\n        is used).\n\n        .. deprecated:: 6.3\n           Keyword arguments are currently accepted case-insensitively.\n           In Tornado 7.0 this will be changed to only accept lowercase\n           arguments.", "file": "tornado_web.py", "function": "RequestHandler.set_cookie", "project": "tornado", "change_stats": {"code_changes": 15, "doc_changes": 7, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        height: int = 512,\n        width: int = 512,\n        prior_num_inference_steps: int = 60,\n        prior_timesteps: Optional[List[float]] = None,\n        prior_guidance_scale: float = 4.0,\n        num_inference_steps: int = 12,\n        decoder_timesteps: Optional[List[float]] = None,\n        decoder_guidance_scale: float = 0.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        num_images_per_prompt: int = 1,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        prior_callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        prior_callback_steps: int = 1,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n    ):\n        \n        \n        \n        prior_outputs = self.prior_pipe(\n            prompt=prompt if prompt_embeds is None else None,\n            height=height,\n            width=width,\n            num_inference_steps=prior_num_inference_steps,\n            timesteps=prior_timesteps,\n            guidance_scale=prior_guidance_scale,\n            negative_prompt=negative_prompt if negative_prompt_embeds is None else None,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            num_images_per_prompt=num_images_per_prompt,\n            generator=generator,\n            latents=latents,\n            output_type=\"pt\",\n            return_dict=False,\n            callback=prior_callback,\n            callback_steps=prior_callback_steps,\n        )\n        image_embeddings = prior_outputs[0]\n\n        outputs = self.decoder_pipe(\n            image_embeddings=image_embeddings,\n            prompt=prompt if prompt is not None else \"\",\n            num_inference_steps=num_inference_steps,\n            timesteps=decoder_timesteps,\n            guidance_scale=decoder_guidance_scale,\n            negative_prompt=negative_prompt,\n            generator=generator,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n        )\n\n        return outputs```\n\nUpdated code:\n```python\ndef __call__(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        height: int = 512,\n        width: int = 512,\n        prior_num_inference_steps: int = 60,\n        prior_timesteps: Optional[List[float]] = None,\n        prior_guidance_scale: float = 4.0,\n        num_inference_steps: int = 12,\n        decoder_timesteps: Optional[List[float]] = None,\n        decoder_guidance_scale: float = 0.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        num_images_per_prompt: int = 1,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        prior_callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        prior_callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n        **kwargs,\n    ):\n        \n        \n        \n        prior_kwargs = {}\n        if kwargs.get(\"prior_callback\", None) is not None:\n            prior_kwargs[\"callback\"] = kwargs.pop(\"prior_callback\")\n            deprecate(\n                \"prior_callback\",\n                \"1.0.0\",\n                \"Passing `prior_callback` as an input argument to `__call__` is deprecated, consider use `prior_callback_on_step_end`\",\n            )\n        if kwargs.get(\"prior_callback_steps\", None) is not None:\n            deprecate(\n                \"prior_callback_steps\",\n                \"1.0.0\",\n                \"Passing `prior_callback_steps` as an input argument to `__call__` is deprecated, consider use `prior_callback_on_step_end`\",\n            )\n            prior_kwargs[\"callback_steps\"] = kwargs.pop(\"prior_callback_steps\")\n\n        prior_outputs = self.prior_pipe(\n            prompt=prompt if prompt_embeds is None else None,\n            height=height,\n            width=width,\n            num_inference_steps=prior_num_inference_steps,\n            timesteps=prior_timesteps,\n            guidance_scale=prior_guidance_scale,\n            negative_prompt=negative_prompt if negative_prompt_embeds is None else None,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            num_images_per_prompt=num_images_per_prompt,\n            generator=generator,\n            latents=latents,\n            output_type=\"pt\",\n            return_dict=False,\n            callback_on_step_end=prior_callback_on_step_end,\n            callback_on_step_end_tensor_inputs=prior_callback_on_step_end_tensor_inputs,\n            **prior_kwargs,\n        )\n        image_embeddings = prior_outputs[0]\n\n        outputs = self.decoder_pipe(\n            image_embeddings=image_embeddings,\n            prompt=prompt if prompt is not None else \"\",\n            num_inference_steps=num_inference_steps,\n            timesteps=decoder_timesteps,\n            guidance_scale=decoder_guidance_scale,\n            negative_prompt=negative_prompt,\n            generator=generator,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback_on_step_end=callback_on_step_end,\n            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n            **kwargs,\n        )\n\n        return outputs\n```\n\nOriginal documentation:\n```python\nFunction invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation for the prior and decoder.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings for the prior. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings for the prior. Can be used to easily tweak text inputs, *e.g.*\n                prompt weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`\n                input argument.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            height (`int`, *optional*, defaults to 512):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to 512):\n                The width in pixels of the generated image.\n            prior_guidance_scale (`float`, *optional*, defaults to 4.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `prior_guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting\n                `prior_guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked\n                to the text `prompt`, usually at the expense of lower image quality.\n            prior_num_inference_steps (`Union[int, Dict[float, int]]`, *optional*, defaults to 60):\n                The number of prior denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference. For more specific timestep spacing, you can pass customized\n                `prior_timesteps`\n            num_inference_steps (`int`, *optional*, defaults to 12):\n                The number of decoder denoising steps. More denoising steps usually lead to a higher quality image at\n                the expense of slower inference. For more specific timestep spacing, you can pass customized\n                `timesteps`\n            prior_timesteps (`List[float]`, *optional*):\n                Custom timesteps to use for the denoising process for the prior. If not defined, equal spaced\n                `prior_num_inference_steps` timesteps are used. Must be in descending order.\n            decoder_timesteps (`List[float]`, *optional*):\n                Custom timesteps to use for the denoising process for the decoder. If not defined, equal spaced\n                `num_inference_steps` timesteps are used. Must be in descending order.\n            decoder_guidance_scale (`float`, *optional*, defaults to 0.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between: `\"pil\"` (`PIL.Image.Image`), `\"np\"`\n                (`np.array`) or `\"pt\"` (`torch.Tensor`).\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n            prior_callback (`Callable`, *optional*):\n                A function that will be called every `prior_callback_steps` steps during inference. The function will\n                be called with the following arguments: `prior_callback(step: int, timestep: int, latents:\n                torch.FloatTensor)`.\n            prior_callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.ImagePipelineOutput`] or `tuple` [`~pipelines.ImagePipelineOutput`] if `return_dict` is True,\n            otherwise a `tuple`. When returning a tuple, the first element is a list with the generated images.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation for the prior and decoder.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings for the prior. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings for the prior. Can be used to easily tweak text inputs, *e.g.*\n                prompt weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`\n                input argument.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            height (`int`, *optional*, defaults to 512):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to 512):\n                The width in pixels of the generated image.\n            prior_guidance_scale (`float`, *optional*, defaults to 4.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `prior_guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting\n                `prior_guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked\n                to the text `prompt`, usually at the expense of lower image quality.\n            prior_num_inference_steps (`Union[int, Dict[float, int]]`, *optional*, defaults to 60):\n                The number of prior denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference. For more specific timestep spacing, you can pass customized\n                `prior_timesteps`\n            num_inference_steps (`int`, *optional*, defaults to 12):\n                The number of decoder denoising steps. More denoising steps usually lead to a higher quality image at\n                the expense of slower inference. For more specific timestep spacing, you can pass customized\n                `timesteps`\n            prior_timesteps (`List[float]`, *optional*):\n                Custom timesteps to use for the denoising process for the prior. If not defined, equal spaced\n                `prior_num_inference_steps` timesteps are used. Must be in descending order.\n            decoder_timesteps (`List[float]`, *optional*):\n                Custom timesteps to use for the denoising process for the decoder. If not defined, equal spaced\n                `num_inference_steps` timesteps are used. Must be in descending order.\n            decoder_guidance_scale (`float`, *optional*, defaults to 0.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between: `\"pil\"` (`PIL.Image.Image`), `\"np\"`\n                (`np.array`) or `\"pt\"` (`torch.Tensor`).\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n            prior_callback_on_step_end (`Callable`, *optional*):\n                A function that calls at the end of each denoising steps during the inference. The function is called\n                with the following arguments: `prior_callback_on_step_end(self: DiffusionPipeline, step: int, timestep:\n                int, callback_kwargs: Dict)`.\n            prior_callback_on_step_end_tensor_inputs (`List`, *optional*):\n                The list of tensor inputs for the `prior_callback_on_step_end` function. The tensors specified in the\n                list will be passed as `callback_kwargs` argument. You will only be able to include variables listed in\n                the `._callback_tensor_inputs` attribute of your pipeine class.\n            callback_on_step_end (`Callable`, *optional*):\n                A function that calls at the end of each denoising steps during the inference. The function is called\n                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n                `callback_on_step_end_tensor_inputs`.\n            callback_on_step_end_tensor_inputs (`List`, *optional*):\n                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n                `._callback_tensor_inputs` attribute of your pipeine class.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.ImagePipelineOutput`] or `tuple` [`~pipelines.ImagePipelineOutput`] if `return_dict` is True,\n            otherwise a `tuple`. When returning a tuple, the first element is a list with the generated images.", "file": "src_diffusers_pipelines_wuerstchen_pipeline_wuerstchen_combined.py", "function": "WuerstchenCombinedPipeline.__call__", "project": "diffusers", "change_stats": {"code_changes": 37, "doc_changes": 32, "total_changes": 69}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_check(self, name, description):\n        \n        \n        \n        checks = []\n        updated_ids = []\n\n        q = select(self.ConfChecksTable).filter(self.ConfChecksTable.c.name == name)\n        results = self.conn.execute(q).all()\n\n        # create new check\n        if not results:\n            context = locals()\n            new_check = dict(((column, context[column]) for column in ('name', 'description')))\n            checks = [new_check]\n        # update existing check results data\n        else:\n            for check in results:\n                check_data = check._asdict()\n                for column in ('name', 'description'):\n                    check_data[column] = locals()[column]\n\n                # only add check result to be updated if it has changed\n                if check_data not in check:\n                    checks.append(check_data)\n                    updated_ids.append(check_data[\"id\"])\n        cme_logger.debug(f\"Update Check: {checks}\")\n\n        # TODO: find a way to abstract this away to a single Upsert call\n        q = Insert(self.ConfChecksTable)  # .returning(self.HostsTable.c.id)\n        update_columns = {col.name: col for col in q.excluded if col.name not in \"id\"}\n        q = q.on_conflict_do_update(index_elements=self.ConfChecksTable.primary_key, set_=update_columns)\n\n        self.conn.execute(q, checks)  # .scalar()\n        # we only return updated IDs for now - when RETURNING clause is allowed we can return inserted\n        if updated_ids:\n            cme_logger.debug(f\"add_check_result() - Checks IDs Updated: {updated_ids}\")\n            return updated_ids```\n\nUpdated code:\n```python\ndef add_check(self, name, description):\n        \n        \n        \n        q = select(self.ConfChecksTable).filter(self.ConfChecksTable.c.name == name)\n        select_results = self.conn.execute(q).all()\n        context = locals()\n        new_row = dict(((column, context[column]) for column in ('name', 'description')))\n        updated_ids = self.insert_data(self.ConfChecksTable, select_results, **new_row)\n\n        if updated_ids:\n            cme_logger.debug(f\"add_check() - Checks IDs Updated: {updated_ids}\")\n            return updated_ids\n```\n\nOriginal documentation:\n```python\nCheck if this check has already been added to the database, if not, add it in.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if this check item has already been added to the database, if not, add it in.", "file": "cme_protocols_smb_database.py", "function": "database.add_check", "project": "CrackMapExec", "change_stats": {"code_changes": 39, "doc_changes": 4, "total_changes": 43}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(\n        self,\n        ref: torch.Tensor,\n        est: torch.Tensor,\n    ) -> torch.Tensor:\n        \n        \n\n        sdr_loss = fast_bss_eval.sdr_loss(\n            est=est,\n            ref=ref,\n            filter_length=self.filter_length,\n            use_cg_iter=self.use_cg_iter,\n            zero_mean=self.zero_mean,\n            clamp_db=self.clamp_db,\n            load_diag=self.load_diag,\n            pairwise=False,\n        )\n\n        return sdr_loss```\n\nUpdated code:\n```python\ndef forward(self, ref: torch.Tensor, est: torch.Tensor) -> torch.Tensor:\n        \n        \n\n        sdr_loss = fast_bss_eval.sdr_loss(\n            est=est,\n            ref=ref,\n            filter_length=self.filter_length,\n            use_cg_iter=self.use_cg_iter,\n            zero_mean=self.zero_mean,\n            clamp_db=self.clamp_db,\n            load_diag=self.load_diag,\n            pairwise=False,\n        )\n\n        return sdr_loss\n```\n\nOriginal documentation:\n```python\nThe forward function\n\n        Args:\n            ref: Tensor, (..., n_samples)\n                reference signal\n            est: Tensor (..., n_samples)\n                estimated signal\n\n        Returns:\n            loss: (...,)\n                the SDR loss (negative sdr)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "SDR forward.\n\n        Args:\n            ref: Tensor, (..., n_samples)\n                reference signal\n            est: Tensor (..., n_samples)\n                estimated signal\n\n        Returns:\n            loss: (...,)\n                the SDR loss (negative sdr)", "file": "espnet2_enh_loss_criterions_time_domain.py", "function": "SDRLoss.forward", "project": "espnet", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_download_malicious_file(outputQueue, database,url):\n    \n    update_manager = create_update_manager_instance(outputQueue)\n    old_etag = update_manager.get_e_tag_from_web(url)\n    database.set_TI_file_info(url.split('/')[-1], {'e-tag':old_etag})\n    assert update_manager.update_TI_file(url) == True```\n\nUpdated code:\n```python\ndef test_download_malicious_file(outputQueue, database, url):\n    \n    update_manager = create_update_manager_instance(outputQueue)\n    # modify old e-tag of this file and store it in the database\n    old_etag = update_manager.get_e_tag_from_web(url)\n    old_etag = '*' + old_etag[1:]\n    database.set_TI_file_info(url.split('/')[-1], {'e-tag':old_etag})\n    # we call this function to set the new self.new_e_tag\n    # to something different than the old modified one\n    assert update_manager._UpdateFileManager__check_if_update(url) == True\n```\n\nOriginal documentation:\n```python\nwe're tetsing this condition old_e_tag == new_e_tag\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "we're tetsing this condition old_e_tag != new_e_tag", "file": "tests_test_update_file_manager.py", "function": "test_download_malicious_file", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        file_path: str,\n        *,\n        headers: Optional[Dict] = None,\n        extract_images: bool = False,\n    ) -> None:\n        \n        try:\n            from pdfminer.high_level import extract_text  # noqa:F401\n        except ImportError:\n            raise ImportError(\n                \"`pdfminer` package not found, please install it with \"\n                \"`pip install pdfminer.six`\"\n            )\n\n        super().__init__(file_path, headers=headers)\n        self.parser = PDFMinerParser(extract_images=extract_images)```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        file_path: str,\n        *,\n        headers: Optional[Dict] = None,\n        extract_images: bool = False,\n        concatenate_pages: bool = True,\n    ) -> None:\n        \n        \n        try:\n            from pdfminer.high_level import extract_text  # noqa:F401\n        except ImportError:\n            raise ImportError(\n                \"`pdfminer` package not found, please install it with \"\n                \"`pip install pdfminer.six`\"\n            )\n\n        super().__init__(file_path, headers=headers)\n        self.parser = PDFMinerParser(\n            extract_images=extract_images, concatenate_pages=concatenate_pages\n        )\n```\n\nOriginal documentation:\n```python\nInitialize with file path.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize with file path.\n\n        Args:\n            extract_images: Whether to extract images from PDF.\n            concatenate_pages: If True, concatenate all PDF pages into one a single\n                               document. Otherwise, return one document per page.", "file": "libs_langchain_langchain_document_loaders_pdf.py", "function": "PDFMinerLoader.__init__", "project": "langchain", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save_state_dict_shards(sharded_state_dict: Iterator[Tuple[OrderedDict, int]],\n                           checkpoint: str,\n                           index_file: \"CheckpointIndexFile\",\n                           base_filename: str,\n                           is_master: bool,\n                           use_safetensors: bool = False) -> int:\n    '''\n    \n    '''\n\n    total_size = 0\n    for idx, shard_pair in enumerate(sharded_state_dict):\n        shard, current_size = shard_pair\n        if not is_master:\n            del shard\n            continue\n        shard_file = get_shard_filename(base_filename, idx)\n        total_size = total_size + current_size\n        for key in shard.keys():\n            index_file.append_weight_map(key, shard_file)\n        checkpoint_file_path = os.path.join(checkpoint, shard_file)\n\n        # Only save on master rank.\n        save_state_dict(shard, checkpoint_file_path, use_safetensors=use_safetensors)\n        del shard\n\n    return total_size```\n\nUpdated code:\n```python\ndef save_state_dict_shards(sharded_state_dict: Iterator[Tuple[OrderedDict, int]],\n                           checkpoint: str,\n                           index_file: \"CheckpointIndexFile\",\n                           base_filename: str,\n                           is_master: bool,\n                           use_safetensors: bool = False,\n                           use_pp_format: bool = False) -> int:\n    '''\n    \n    '''\n\n    total_size = 0\n    shard_filenames = []\n    for idx, shard_pair in enumerate(sharded_state_dict):\n        shard, current_size = shard_pair\n        if not is_master:\n            del shard\n            continue\n        shard_file = get_shard_filename(base_filename, idx)\n        total_size = total_size + current_size\n        for key in shard.keys():\n            index_file.append_weight_map(key, shard_file)\n        checkpoint_file_path = os.path.join(checkpoint, shard_file)\n\n        # Only save on master rank.\n        save_state_dict(shard, checkpoint_file_path, use_safetensors=use_safetensors)\n        shard_filenames.append(shard_file)\n        del shard\n\n    # Clean folder, deleted unneeded files.\n    clean_folder(checkpoint, base_filename, shard_filenames, is_master=is_master, use_pp_format=use_pp_format)\n\n    return total_size\n```\n\nOriginal documentation:\n```python\nSave sharded state dict only on master rank, this method can be used by both model and optimizer states.\n    Args:\n        sharded_state_dict (Iterator[Tuple[OrderedDict, int]]): a generator of shards, each shard contains state dict and shard size.\n        checkpoint (str): The path of checkpoint directory as string.\n        index_file (CheckpointIndexFile): The index file object to be updated.\n        base_filename (str): Decides the prefix of filenames of shards.\n        is_master (bool): Whether current rank is master.\n        use_safetensors (bool): Whether to use safetensors to save checkpoint.\n\n    Returns:\n        int: the total size of shards\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save sharded state dict only on master rank, this method can be used by both model and optimizer states.\n    Args:\n        sharded_state_dict (Iterator[Tuple[OrderedDict, int]]): a generator of shards, each shard contains state dict and shard size.\n        checkpoint (str): The path of checkpoint directory as string.\n        index_file (CheckpointIndexFile): The index file object to be updated.\n        base_filename (str): Decides the prefix of filenames of shards.\n        is_master (bool): Whether current rank is main process.\n        use_safetensors (bool, optional): Whether to use safetensors to save checkpoint. Defaults to False.\n        use_pp_format: (bool, optional): Whether to save the files in pipeline format including stage information. Defaults to False.\n\n    Returns:\n        int: the total size of shards", "file": "colossalai_checkpoint_io_utils.py", "function": "save_state_dict_shards", "project": "ColossalAI", "change_stats": {"code_changes": 10, "doc_changes": 7, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_cross_segment_issues_per_subnet_pair(scans, source_subnet, target_subnet):\n        \n        \n        \n        if source_subnet == target_subnet:\n            return []\n        source_subnet_range = NetworkRange.get_range_obj(source_subnet)\n        target_subnet_range = NetworkRange.get_range_obj(target_subnet)\n\n        cross_segment_issues = []\n\n        scans.rewind()  # If we iterated over scans already we need to rewind.\n        for scan in scans:\n            target_ip = scan[\"data\"][\"machine\"][\"ip_addr\"]\n            if target_subnet_range.is_in_range(str(target_ip)):\n                monkey = NodeService.get_monkey_by_guid(scan[\"monkey_guid\"])\n                cross_segment_ip = get_ip_in_src_and_not_in_dst(\n                    monkey[\"ip_addresses\"], source_subnet_range, target_subnet_range\n                )\n\n                if cross_segment_ip is not None:\n                    cross_segment_issues.append(\n                        {\n                            \"source\": cross_segment_ip,\n                            \"hostname\": monkey[\"hostname\"],\n                            \"target\": target_ip,\n                            \"services\": scan[\"data\"][\"machine\"][\"services\"],\n                            \"icmp\": scan[\"data\"][\"machine\"][\"icmp\"],\n                            \"is_self\": False,\n                        }\n                    )\n\n        return cross_segment_issues + ReportService.get_cross_segment_issues_of_single_machine(\n            source_subnet_range, target_subnet_range\n        )```\n\nUpdated code:\n```python\ndef get_cross_segment_issues_per_subnet_pair(\n        cls,\n        scans: Sequence[Union[PingScanEvent, TCPScanEvent]],\n        source_subnet: str,\n        target_subnet: str,\n    ) -> List[Dict[str, Any]]:\n        \n        \n        \n        if source_subnet == target_subnet:\n            return []\n\n        if cls._agent_repository is None:\n            raise RuntimeError()\n        if cls._machine_repository is None:\n            raise RuntimeError()\n\n        source_subnet_range = NetworkRange.get_range_obj(source_subnet)\n        target_subnet_range = NetworkRange.get_range_obj(target_subnet)\n\n        cross_segment_issues = []\n\n        for scan in scans:\n            target_ip = scan.target\n            if target_subnet_range.is_in_range(str(target_ip)):\n                agent = cls._agent_repository.get_agent_by_id(scan.source)\n                machine = cls._machine_repository.get_machine_by_id(agent.machine_id)\n                machine_ips = [iface.ip for iface in machine.network_interfaces]\n                cross_segment_ip = get_ip_in_src_and_not_in_dst(\n                    machine_ips, source_subnet_range, target_subnet_range\n                )\n                if cross_segment_ip is not None:\n                    cross_segment_issues.append(\n                        {\n                            \"source\": cross_segment_ip,\n                            \"hostname\": machine.hostname,\n                            \"target\": target_ip,\n                            \"services\": {a: s for a, s in machine.network_services},\n                            \"icmp\": type(scan) is PingScanEvent,\n                            \"is_self\": False,\n                        }\n                    )\n\n        return cross_segment_issues + ReportService.get_cross_segment_issues_of_single_machine(\n            source_subnet_range, target_subnet_range\n        )\n```\n\nOriginal documentation:\n```python\nGets list of cross segment issues from source_subnet to target_subnet.\n\n        :param scans: List of all scan telemetry entries. Must have monkey_guid,ip_addr and\n         services. This should be a PyMongo cursor object.\n        :param source_subnet:   The subnet which shouldn't be able to access target_subnet.\n        :param target_subnet:   The subnet which shouldn't be accessible from source_subnet.\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Gets list of cross segment issues from source_subnet to target_subnet.\n\n        :param scans: List of all scan events.\n        :param source_subnet:   The subnet which shouldn't be able to access target_subnet.\n        :param target_subnet:   The subnet which shouldn't be accessible from source_subnet.\n        :return:", "file": "monkey_monkey_island_cc_services_reporting_report.py", "function": "ReportService.get_cross_segment_issues_per_subnet_pair", "project": "monkey", "change_stats": {"code_changes": 31, "doc_changes": 5, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = None\n        if symbol is not None:\n            market = self.market(symbol)\n        request = {\n            # 'from': 12345678,  # A 'trade Id'. The query begins at \u2018from'.\n            # 'direction': 'PRE',  # PRE, NEXT The direction before or after \u2018from'.\n        }\n        if since is not None:\n            request['startTime'] = since\n        if limit is not None:\n            request['limit'] = limit\n        response = self.privateGetTrades(self.extend(request, params))\n        #\n        #     [\n        #         {\n        #             \"id\": \"32164924331503616\",\n        #             \"symbol\": \"LINK_USDT\",\n        #             \"accountType\": \"SPOT\",\n        #             \"orderId\": \"32164923987566592\",\n        #             \"side\": \"SELL\",\n        #             \"type\": \"MARKET\",\n        #             \"matchRole\": \"TAKER\",\n        #             \"createTime\": 1648635115525,\n        #             \"price\": \"11\",\n        #             \"quantity\": \"0.5\",\n        #             \"amount\": \"5.5\",\n        #             \"feeCurrency\": \"USDT\",\n        #             \"feeAmount\": \"0.007975\",\n        #             \"pageId\": \"32164924331503616\",\n        #             \"clientOrderId\": \"myOwnId-321\"\n        #         }\n        #     ]\n        #\n        result = self.parse_trades(response, market)\n        return self.filter_by_since_limit(result, since, limit)```\n\nUpdated code:\n```python\ndef fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        paginate = False\n        paginate, params = self.handle_option_and_params(params, 'fetchMyTrades', 'paginate')\n        if paginate:\n            return self.fetch_paginated_call_dynamic('fetchMyTrades', symbol, since, limit, params)\n        market = None\n        if symbol is not None:\n            market = self.market(symbol)\n        request = {\n            # 'from': 12345678,  # A 'trade Id'. The query begins at \u2018from'.\n            # 'direction': 'PRE',  # PRE, NEXT The direction before or after \u2018from'.\n        }\n        if since is not None:\n            request['startTime'] = since\n        if limit is not None:\n            request['limit'] = limit\n        request, params = self.handle_until_option('endTime', request, params)\n        response = self.privateGetTrades(self.extend(request, params))\n        #\n        #     [\n        #         {\n        #             \"id\": \"32164924331503616\",\n        #             \"symbol\": \"LINK_USDT\",\n        #             \"accountType\": \"SPOT\",\n        #             \"orderId\": \"32164923987566592\",\n        #             \"side\": \"SELL\",\n        #             \"type\": \"MARKET\",\n        #             \"matchRole\": \"TAKER\",\n        #             \"createTime\": 1648635115525,\n        #             \"price\": \"11\",\n        #             \"quantity\": \"0.5\",\n        #             \"amount\": \"5.5\",\n        #             \"feeCurrency\": \"USDT\",\n        #             \"feeAmount\": \"0.007975\",\n        #             \"pageId\": \"32164924331503616\",\n        #             \"clientOrderId\": \"myOwnId-321\"\n        #         }\n        #     ]\n        #\n        result = self.parse_trades(response, market)\n        return self.filter_by_since_limit(result, since, limit)\n```\n\nOriginal documentation:\n```python\nfetch all trades made by the user\n        see https://docs.poloniex.com/#authenticated-endpoints-trades-trade-history\n        :param str symbol: unified market symbol\n        :param int [since]: the earliest time in ms to fetch trades for\n        :param int [limit]: the maximum number of trades structures to retrieve\n        :param dict [params]: extra parameters specific to the poloniex api endpoint\n        :returns Trade[]: a list of `trade structures <https://github.com/ccxt/ccxt/wiki/Manual#trade-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch all trades made by the user\n        see https://docs.poloniex.com/#authenticated-endpoints-trades-trade-history\n        :param str symbol: unified market symbol\n        :param int [since]: the earliest time in ms to fetch trades for\n        :param int [limit]: the maximum number of trades structures to retrieve\n        :param dict [params]: extra parameters specific to the poloniex api endpoint\n        :param int [params.until]: the latest time in ms to fetch entries for\n        :param boolean [params.paginate]: default False, when True will automatically paginate by calling self endpoint multiple times. See in the docs all the [availble parameters]  (ttps://github.com/ccxt/ccxt/wiki/Manual#pagination-params)\n        :returns Trade[]: a list of `trade structures <https://github.com/ccxt/ccxt/wiki/Manual#trade-structure>`", "file": "python_ccxt_poloniex.py", "function": "poloniex.fetch_my_trades", "project": "ccxt", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef resize(image, output_shape, order=1, mode='reflect', cval=0, clip=True,\n           preserve_range=False, anti_aliasing=True, anti_aliasing_sigma=None):\n    \n\n    \n    output_shape = tuple(output_shape)\n    output_ndim = len(output_shape)\n    input_shape = image.shape\n    if output_ndim > image.ndim:\n        # append dimensions to input_shape\n        input_shape = input_shape + (1, ) * (output_ndim - image.ndim)\n        image = np.reshape(image, input_shape)\n    elif output_ndim == image.ndim - 1:\n        # multichannel case: append shape of last axis\n        output_shape = output_shape + (image.shape[-1], )\n    elif output_ndim < image.ndim - 1:\n        raise ValueError(\"len(output_shape) cannot be smaller than the image \"\n                         \"dimensions\")\n\n    factors = (np.asarray(input_shape, dtype=float) /\n               np.asarray(output_shape, dtype=float))\n\n    if anti_aliasing:\n        if anti_aliasing_sigma is None:\n            anti_aliasing_sigma = np.maximum(0, (factors - 1) / 2)\n        else:\n            anti_aliasing_sigma = \\\n                np.atleast_1d(anti_aliasing_sigma) * np.ones_like(factors)\n            if np.any(anti_aliasing_sigma < 0):\n                raise ValueError(\"Anti-aliasing standard deviation must be \"\n                                 \"greater than or equal to zero\")\n            elif np.any((anti_aliasing_sigma > 0) & (factors <= 1)):\n                warn(\"Anti-aliasing standard deviation greater than zero but \"\n                     \"not down-sampling along all axes\")\n\n        image = ndi.gaussian_filter(image, anti_aliasing_sigma,\n                                    cval=cval, mode=mode)\n\n    # 2-dimensional interpolation\n    if len(output_shape) == 2 or (len(output_shape) == 3 and\n                                  output_shape[2] == input_shape[2]):\n        rows = output_shape[0]\n        cols = output_shape[1]\n        input_rows = input_shape[0]\n        input_cols = input_shape[1]\n        if rows == 1 and cols == 1:\n            tform = AffineTransform(translation=(input_cols / 2.0 - 0.5,\n                                                 input_rows / 2.0 - 0.5))\n        else:\n            # 3 control points necessary to estimate exact AffineTransform\n            src_corners = np.array([[1, 1], [1, rows], [cols, rows]]) - 1\n            dst_corners = np.zeros(src_corners.shape, dtype=np.double)\n            # take into account that 0th pixel is at position (0.5, 0.5)\n            dst_corners[:, 0] = factors[1] * (src_corners[:, 0] + 0.5) - 0.5\n            dst_corners[:, 1] = factors[0] * (src_corners[:, 1] + 0.5) - 0.5\n\n            tform = AffineTransform()\n            tform.estimate(src_corners, dst_corners)\n\n        # Make sure the transform is exactly metric, to ensure fast warping.\n        tform.params[2] = (0, 0, 1)\n        tform.params[0, 1] = 0\n        tform.params[1, 0] = 0\n\n        out = warp(image, tform, output_shape=output_shape, order=order,\n                   mode=mode, cval=cval, clip=clip,\n                   preserve_range=preserve_range)\n\n    else:  # n-dimensional interpolation\n        coord_arrays = [factors[i] * (np.arange(d) + 0.5) - 0.5\n                        for i, d in enumerate(output_shape)]\n\n        coord_map = np.array(np.meshgrid(*coord_arrays,\n                                         sparse=False,\n                                         indexing='ij'))\n\n        image = convert_to_float(image, preserve_range)\n\n        ndi_mode = _to_ndimage_mode(mode)\n        out = ndi.map_coordinates(image, coord_map, order=order,\n                                  mode=ndi_mode, cval=cval)\n\n        _clip_warp_output(image, out, order, mode, cval, clip)\n\n    return out```\n\nUpdated code:\n```python\ndef resize(image, output_shape, order=1, mode='reflect', cval=0, clip=True,\n           preserve_range=False, anti_aliasing=True, anti_aliasing_sigma=None):\n    \n\n    \n    output_shape = tuple(output_shape)\n    output_ndim = len(output_shape)\n    input_shape = image.shape\n    if output_ndim > image.ndim:\n        # append dimensions to input_shape\n        input_shape = input_shape + (1, ) * (output_ndim - image.ndim)\n        image = np.reshape(image, input_shape)\n    elif output_ndim == image.ndim - 1:\n        # multichannel case: append shape of last axis\n        output_shape = output_shape + (image.shape[-1], )\n    elif output_ndim < image.ndim - 1:\n        raise ValueError(\"len(output_shape) cannot be smaller than the image \"\n                         \"dimensions\")\n\n    factors = (np.asarray(input_shape, dtype=float) /\n               np.asarray(output_shape, dtype=float))\n\n    if anti_aliasing:\n        if anti_aliasing_sigma is None:\n            anti_aliasing_sigma = np.maximum(0, (factors - 1) / 2)\n        else:\n            anti_aliasing_sigma = \\\n                np.atleast_1d(anti_aliasing_sigma) * np.ones_like(factors)\n            if np.any(anti_aliasing_sigma < 0):\n                raise ValueError(\"Anti-aliasing standard deviation must be \"\n                                 \"greater than or equal to zero\")\n            elif np.any((anti_aliasing_sigma > 0) & (factors <= 1)):\n                warn(\"Anti-aliasing standard deviation greater than zero but \"\n                     \"not down-sampling along all axes\")\n\n        # Translate modes used by np.pad to those used by ndi.gaussian_filter\n        np_pad_to_ndimage = {\n            'constant': 'constant',\n            'edge': 'nearest',\n            'symmetric': 'reflect',\n            'reflect': 'mirror',\n            'wrap': 'wrap'\n        }\n        try:\n            ndi_mode = np_pad_to_ndimage[mode]\n        except KeyError:\n            raise ValueError(\"Unknown mode, or cannot translate mode. The \"\n                             \"mode should be one of 'constant', 'edge', \"\n                             \"'symmetric', 'reflect', or 'wrap'. See the \"\n                             \"documentation of numpy.pad for more info.\")\n\n        image = ndi.gaussian_filter(image, anti_aliasing_sigma,\n                                    cval=cval, mode=ndi_mode)\n\n    # 2-dimensional interpolation\n    if len(output_shape) == 2 or (len(output_shape) == 3 and\n                                  output_shape[2] == input_shape[2]):\n        rows = output_shape[0]\n        cols = output_shape[1]\n        input_rows = input_shape[0]\n        input_cols = input_shape[1]\n        if rows == 1 and cols == 1:\n            tform = AffineTransform(translation=(input_cols / 2.0 - 0.5,\n                                                 input_rows / 2.0 - 0.5))\n        else:\n            # 3 control points necessary to estimate exact AffineTransform\n            src_corners = np.array([[1, 1], [1, rows], [cols, rows]]) - 1\n            dst_corners = np.zeros(src_corners.shape, dtype=np.double)\n            # take into account that 0th pixel is at position (0.5, 0.5)\n            dst_corners[:, 0] = factors[1] * (src_corners[:, 0] + 0.5) - 0.5\n            dst_corners[:, 1] = factors[0] * (src_corners[:, 1] + 0.5) - 0.5\n\n            tform = AffineTransform()\n            tform.estimate(src_corners, dst_corners)\n\n        # Make sure the transform is exactly metric, to ensure fast warping.\n        tform.params[2] = (0, 0, 1)\n        tform.params[0, 1] = 0\n        tform.params[1, 0] = 0\n\n        out = warp(image, tform, output_shape=output_shape, order=order,\n                   mode=mode, cval=cval, clip=clip,\n                   preserve_range=preserve_range)\n\n    else:  # n-dimensional interpolation\n        coord_arrays = [factors[i] * (np.arange(d) + 0.5) - 0.5\n                        for i, d in enumerate(output_shape)]\n\n        coord_map = np.array(np.meshgrid(*coord_arrays,\n                                         sparse=False,\n                                         indexing='ij'))\n\n        image = convert_to_float(image, preserve_range)\n\n        ndi_mode = _to_ndimage_mode(mode)\n        out = ndi.map_coordinates(image, coord_map, order=order,\n                                  mode=ndi_mode, cval=cval)\n\n        _clip_warp_output(image, out, order, mode, cval, clip)\n\n    return out\n```\n\nOriginal documentation:\n```python\nResize image to match a certain size.\n\n    Performs interpolation to up-size or down-size images. Note that anti-\n    aliasing should be enabled when down-sizing images to avoid aliasing\n    artifacts. For down-sampling N-dimensional images with an integer factor\n    also see `skimage.transform.downscale_local_mean`.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    output_shape : tuple or ndarray\n        Size of the generated output image `(rows, cols[, ...][, dim])`. If\n        `dim` is not provided, the number of channels is preserved. In case the\n        number of input channels does not equal the number of output channels a\n        n-dimensional interpolation is applied.\n\n    Returns\n    -------\n    resized : ndarray\n        Resized version of the input.\n\n    Other parameters\n    ----------------\n    order : int, optional\n        The order of the spline interpolation, default is 1. The order has to\n        be in the range 0-5. See `skimage.transform.warp` for detail.\n    mode : {'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional\n        Points outside the boundaries of the input are filled according\n        to the given mode.  Modes match the behaviour of `numpy.pad`.\n    cval : float, optional\n        Used in conjunction with mode 'constant', the value outside\n        the image boundaries.\n    clip : bool, optional\n        Whether to clip the output to the range of values of the input image.\n        This is enabled by default, since higher order interpolation may\n        produce values outside the given input range.\n    preserve_range : bool, optional\n        Whether to keep the original range of values. Otherwise, the input\n        image is converted according to the conventions of `img_as_float`.\n        Also see https://scikit-image.org/docs/dev/user_guide/data_types.html\n    anti_aliasing : bool, optional\n        Whether to apply a Gaussian filter to smooth the image prior to\n        down-scaling. It is crucial to filter when down-sampling the image to\n        avoid aliasing artifacts.\n    anti_aliasing_sigma : {float, tuple of floats}, optional\n        Standard deviation for Gaussian filtering to avoid aliasing artifacts.\n        By default, this value is chosen as (1 - s) / 2 where s is the\n        down-scaling factor.\n\n    Notes\n    -----\n    Modes 'reflect' and 'symmetric' are similar, but differ in whether the edge\n    pixels are duplicated during the reflection.  As an example, if an array\n    has values [0, 1, 2] and was padded to the right by four values using\n    symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it\n    would be [0, 1, 2, 1, 0, 1, 2].\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.transform import resize\n    >>> image = data.camera()\n    >>> resize(image, (100, 100)).shape\n    (100, 100)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Resize image to match a certain size.\n\n    Performs interpolation to up-size or down-size images. Note that anti-\n    aliasing should be enabled when down-sizing images to avoid aliasing\n    artifacts. For down-sampling N-dimensional images with an integer factor\n    also see `skimage.transform.downscale_local_mean`.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    output_shape : tuple or ndarray\n        Size of the generated output image `(rows, cols[, ...][, dim])`. If\n        `dim` is not provided, the number of channels is preserved. In case the\n        number of input channels does not equal the number of output channels a\n        n-dimensional interpolation is applied.\n\n    Returns\n    -------\n    resized : ndarray\n        Resized version of the input.\n\n    Other parameters\n    ----------------\n    order : int, optional\n        The order of the spline interpolation, default is 1. The order has to\n        be in the range 0-5. See `skimage.transform.warp` for detail.\n    mode : {'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional\n        Points outside the boundaries of the input are filled according\n        to the given mode.  Modes match the behaviour of `numpy.pad`.\n    cval : float, optional\n        Used in conjunction with mode 'constant', the value outside\n        the image boundaries.\n    clip : bool, optional\n        Whether to clip the output to the range of values of the input image.\n        This is enabled by default, since higher order interpolation may\n        produce values outside the given input range.\n    preserve_range : bool, optional\n        Whether to keep the original range of values. Otherwise, the input\n        image is converted according to the conventions of `img_as_float`.\n        Also see http://scikit-image.org/docs/dev/user_guide/data_types.html\n    anti_aliasing : bool, optional\n        Whether to apply a Gaussian filter to smooth the image prior to\n        down-scaling. It is crucial to filter when down-sampling the image to\n        avoid aliasing artifacts.\n    anti_aliasing_sigma : {float, tuple of floats}, optional\n        Standard deviation for Gaussian filtering to avoid aliasing artifacts.\n        By default, this value is chosen as (1 - s) / 2 where s is the\n        down-scaling factor.\n\n    Notes\n    -----\n    Modes 'reflect' and 'symmetric' are similar, but differ in whether the edge\n    pixels are duplicated during the reflection.  As an example, if an array\n    has values [0, 1, 2] and was padded to the right by four values using\n    symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it\n    would be [0, 1, 2, 1, 0, 1, 2].\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.transform import resize\n    >>> image = data.camera()\n    >>> resize(image, (100, 100)).shape\n    (100, 100)", "file": "skimage_transform__warps.py", "function": "resize", "project": "scikit-image", "change_stats": {"code_changes": 20, "doc_changes": 4, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get(self, url, header=None, retry_time=5, timeout=30,\n            retry_flag=list(), retry_interval=5, *args, **kwargs):\n        \n        \n        \n        headers = self.header\n        if header and isinstance(header, dict):\n            headers.update(header)\n        while True:\n            try:\n                html = requests.get(url, headers=headers, timeout=timeout, **kwargs)\n                if any(f in html.content for f in retry_flag):\n                    raise Exception\n                return html\n            except Exception as e:\n                print(e)\n                retry_time -= 1\n                if retry_time <= 0:\n                    # \u591a\u6b21\u8bf7\u6c42\u5931\u8d25\n                    resp = Response()\n                    resp.status_code = 200\n                    return resp\n                time.sleep(retry_interval)```\n\nUpdated code:\n```python\ndef get(self, url, header=None, retry_time=5, retry_interval=5, timeout=30, *args, **kwargs):\n        \n        \n        \n        headers = self.header\n        if header and isinstance(header, dict):\n            headers.update(header)\n        while True:\n            try:\n                self.response = requests.get(url, headers=headers, timeout=timeout, *args, **kwargs)\n                return self\n            except Exception as e:\n                self.log.error(\"requests: %s error: %s\" % (url, str(e)))\n                retry_time -= 1\n                if retry_time <= 0:\n                    resp = Response()\n                    resp.status_code = 200\n                    return self\n                self.log.info(\"retry %s second after\" % retry_interval)\n                time.sleep(retry_interval)\n```\n\nOriginal documentation:\n```python\nget method\n        :param url: target url\n        :param header: headers\n        :param retry_time: retry time when network error\n        :param timeout: network timeout\n        :param retry_flag: if retry_flag in content. do retry\n        :param retry_interval: retry interval(second)\n        :param args:\n        :param kwargs:\n        :return:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "get method\n        :param url: target url\n        :param header: headers\n        :param retry_time: retry time\n        :param retry_interval: retry interval\n        :param timeout: network timeout\n        :return:", "file": "util_webRequest.py", "function": "WebRequest.get", "project": "proxy_pool", "change_stats": {"code_changes": 17, "doc_changes": 9, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _save(self, image, genre, timestamp):\n        \n        \n        \n        try:\n            folder = os.path.join(str(self.config.DropRecord_SaveFolder), genre)\n            os.makedirs(folder, exist_ok=True)\n            file = os.path.join(folder, f'{timestamp}.png')\n            image.save(file)\n            logger.info(f'Image save success, file: {file}')\n            return True\n        except Exception as e:\n            logger.exception(e)\n\n        return False```\n\nUpdated code:\n```python\ndef _save(self, image, genre, filename):\n        \n        \n        \n        try:\n            folder = os.path.join(str(self.config.DropRecord_SaveFolder), genre)\n            os.makedirs(folder, exist_ok=True)\n            file = os.path.join(folder, filename)\n            save_image(image, file)\n            logger.info(f'Image save success, file: {file}')\n            return True\n        except Exception as e:\n            logger.exception(e)\n\n        return False\n```\n\nOriginal documentation:\n```python\nArgs:\n            image: Image to save.\n            genre (str): Name of sub folder.\n            timestamp (int): Millisecond timestamp.\n\n        Returns:\n            bool: If success\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            image: Image to save.\n            genre (str): Name of sub folder.\n            filename (str): 'xxx.png'\n\n        Returns:\n            bool: If success", "file": "module_statistics_azurstats.py", "function": "AzurStats._save", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef borrow_margin(self, code: str, amount, symbol: Optional[str] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        marginMode, query = self.handle_margin_mode_and_params('borrowMargin', params)\n        if marginMode == 'isolated':\n            raise NotSupported(self.id + ' borrowMargin() cannot use isolated margin')\n        request = {\n            'coin': currency['id'],\n            'qty': self.currency_to_precision(code, amount),\n        }\n        response = self.privatePostSpotV3PrivateCrossMarginLoan(self.extend(request, query))\n        #\n        #     {\n        #         \"retCode\": 0,\n        #         \"retMsg\": \"success\",\n        #         \"result\": {\n        #             \"transactId\": \"14143\"\n        #         },\n        #         \"retExtInfo\": null,\n        #         \"time\": 1662617848970\n        #     }\n        #\n        result = self.safe_value(response, 'result', {})\n        transaction = self.parse_margin_loan(result, currency)\n        return self.extend(transaction, {\n            'symbol': symbol,\n            'amount': amount,\n        })```\n\nUpdated code:\n```python\ndef borrow_margin(self, code: str, amount, symbol: Optional[str] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        currency = self.currency(code)\n        marginMode, query = self.handle_margin_mode_and_params('borrowMargin', params)\n        if marginMode == 'isolated':\n            raise NotSupported(self.id + ' borrowMargin() cannot use isolated margin')\n        request = {\n            'coin': currency['id'],\n            'qty': self.currency_to_precision(code, amount),\n        }\n        response = self.privatePostV5SpotCrossMarginTradeLoan(self.extend(request, query))\n        #\n        #     {\n        #         \"retCode\": 0,\n        #         \"retMsg\": \"success\",\n        #         \"result\": {\n        #             \"transactId\": \"14143\"\n        #         },\n        #         \"retExtInfo\": null,\n        #         \"time\": 1662617848970\n        #     }\n        #\n        result = self.safe_value(response, 'result', {})\n        transaction = self.parse_margin_loan(result, currency)\n        return self.extend(transaction, {\n            'symbol': symbol,\n            'amount': amount,\n        })\n```\n\nOriginal documentation:\n```python\ncreate a loan to borrow margin\n        see https://bybit-exchange.github.io/docs/spot/v3/#t-borrowmarginloan\n        :param str code: unified currency code of the currency to borrow\n        :param float amount: the amount to borrow\n        :param str symbol: not used by bybit.borrowMargin()\n        :param dict [params]: extra parameters specific to the bybit api endpoint\n        :returns dict: a `margin loan structure <https://github.com/ccxt/ccxt/wiki/Manual#margin-loan-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "create a loan to borrow margin\n        see https://bybit-exchange.github.io/docs/v5/spot-margin-normal/borrow\n        :param str code: unified currency code of the currency to borrow\n        :param float amount: the amount to borrow\n        :param str symbol: not used by bybit.borrowMargin()\n        :param dict [params]: extra parameters specific to the bybit api endpoint\n        :returns dict: a `margin loan structure <https://github.com/ccxt/ccxt/wiki/Manual#margin-loan-structure>`", "file": "python_ccxt_bybit.py", "function": "bybit.borrow_margin", "project": "ccxt", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef array2string(a, max_line_width=None, precision=None,\n                 suppress_small=None, separator=' ', prefix=\"\",\n                 style=numpy._NoValue, formatter=None, threshold=None,\n                 edgeitems=None, sign=None, floatmode=None, suffix=\"\",\n                 legacy=None):\n    \n    \n    return numpy.array2string(cupy.asnumpy(a), max_line_width=max_line_width,\n                              precision=precision,\n                              suppress_small=suppress_small,\n                              separator=separator,\n                              prefix=prefix,\n                              formatter=formatter,\n                              threshold=threshold,\n                              edgeitems=edgeitems,\n                              sign=sign,\n                              floatmode=floatmode,\n                              suffix=suffix,\n                              legacy=legacy)```\n\nUpdated code:\n```python\ndef array2string(arr, *args, **kwargs):\n    \n    \n    return numpy.array2string(cupy.asnumpy(arr), *args, **kwargs)\n```\n\nOriginal documentation:\n```python\nReturn a string representation of an array.\n\n    Args:\n        a (array_like): Input array. It should be able to feed to\n            :func:`cupy.asnumpy`.\n        max_line_width : int, optional\n            Inserts newlines if text is longer than `max_line_width`.\n            Defaults to ``numpy.get_printoptions()['linewidth']``.\n        precision : int or None, optional\n            Floating point precision.\n            Defaults to ``numpy.get_printoptions()['precision']``.\n        suppress_small : bool, optional\n            Represent numbers \"very close\" to zero as zero; default is False.\n            Very close is defined by precision: if the precision is 8, e.g.,\n            numbers smaller (in absolute value) than 5e-9 are represented as\n            zero.\n            Defaults to ``numpy.get_printoptions()['suppress']``.\n        separator : str, optional\n            Inserted between elements.\n        prefix : str, optional\n        suffix : str, optional\n            The length of the prefix and suffix strings are used to\n            respectively\n            align and wrap the output. An array is typically printed as::\n            prefix + array2string(a) + suffix\n            The output is left-padded by the length of the prefix string, and\n            wrapping is forced at the column ``max_line_width - len(suffix)``.\n            It should be noted that the content of prefix and suffix strings\n            are\n            not included in the output.\n        style : _NoValue, optional\n            Has no effect, do not use.\n            .. deprecated:: 1.14.0\n        formatter : dict of callables, optional\n            If not None, the keys should indicate the type(s) that the\n            respective\n            formatting function applies to.  Callables should return a string.\n            Types that are not specified (by their corresponding keys) are\n            handled\n            by the default formatters.  Individual types for which a formatter\n            can be set are:\n            - 'bool'\n            - 'int'\n            - 'timedelta' : a `numpy.timedelta64`\n            - 'datetime' : a `numpy.datetime64`\n            - 'float'\n            - 'longfloat' : 128-bit floats\n            - 'complexfloat'\n            - 'longcomplexfloat' : composed of two 128-bit floats\n            - 'void' : type `numpy.void`\n            - 'numpystr' : types `numpy.string_` and `numpy.unicode_`\n            Other keys that can be used to set a group of types at once are:\n            - 'all' : sets all types\n            - 'int_kind' : sets 'int'\n            - 'float_kind' : sets 'float' and 'longfloat'\n            - 'complex_kind' : sets 'complexfloat' and 'longcomplexfloat'\n            - 'str_kind' : sets 'numpystr'\n        threshold : int, optional\n            Total number of array elements which trigger summarization\n            rather than full repr.\n            Defaults to ``numpy.get_printoptions()['threshold']``.\n        edgeitems : int, optional\n            Number of array items in summary at beginning and end of\n            each dimension.\n            Defaults to ``numpy.get_printoptions()['edgeitems']``.\n        sign : string, either '-', '+', or ' ', optional\n            Controls printing of the sign of floating-point types. If '+',\n            always\n            print the sign of positive values. If ' ', always prints a space\n            (whitespace character) in the sign position of positive values.  If\n            '-', omit the sign character of positive values.\n            Defaults to ``numpy.get_printoptions()['sign']``.\n        floatmode : str, optional\n            Controls the interpretation of the `precision` option for\n            floating-point types.\n            Defaults to ``numpy.get_printoptions()['floatmode']``.\n            Can take the following values:\n            - 'fixed': Always print exactly `precision` fractional digits,\n            even if this would print more or fewer digits than\n            necessary to specify the value uniquely.\n            - 'unique': Print the minimum number of fractional digits necessary\n            to represent each value uniquely. Different elements may\n            have a different number of digits.  The value of the\n            `precision` option is ignored.\n            - 'maxprec': Print at most `precision` fractional digits, but if\n            an element can be uniquely represented with fewer digits\n            only print it with that many.\n            - 'maxprec_equal': Print at most `precision` fractional digits,\n            but if every element in the array can be uniquely\n            represented with an equal number of fewer digits, use that\n            many digits for all elements.\n        legacy : string or `False`, optional\n            If set to the string `'1.13'` enables 1.13 legacy printing mode.\n            This\n            approximates numpy 1.13 print output by including a space in the\n            sign\n            position of floats and different behavior for 0d arrays. If set to\n            `False`, disables legacy mode. Unrecognized strings will be ignored\n            with a warning for forward compatibility.\n            .. versionadded:: 1.14.0\n\n        .. seealso:: :func:`numpy.array2string`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a string representation of an array.\n\n\n        .. seealso:: :func:`numpy.array2string`", "file": "cupy__io_formatting.py", "function": "array2string", "project": "cupy", "change_stats": {"code_changes": 21, "doc_changes": 100, "total_changes": 121}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_metadata(self, request: Request) -> Dict[Text, Any]:\n        \n        \n        content_type = request.headers.get(\"content-type\")\n\n        # Slack API sends either a JSON-encoded or a URL-encoded body depending on the content\n        if content_type == \"application/json\":\n            # if JSON-encoded message is received\n            slack_event = request.json\n            event = slack_event.get(\"event\", {})\n            thread_id = event.get(\"thread_ts\", event.get(\"ts\"))\n\n            users = []\n            if \"authed_users\" in slack_event:\n                users = slack_event.get(\"authed_users\")\n            elif (\n                \"authorizations\" in slack_event\n                and len(slack_event.get(\"authorizations\")) > 0\n            ):\n                users.append(slack_event.get(\"authorizations\")[0].get(\"user_id\"))\n\n            return {\n                \"out_channel\": event.get(\"channel\"),\n                \"thread_id\": thread_id,\n                \"users\": users,\n            }\n\n        if content_type == \"application/x-www-form-urlencoded\":\n            # if URL-encoded message is received\n            output = request.form\n            payload = json.loads(output[\"payload\"][0])\n            message = payload.get(\"message\", {})\n            thread_id = message.get(\"thread_ts\", message.get(\"ts\"))\n\n            users = []\n            if payload.get(\"user\", {}).get(\"id\"):\n                users.append(payload.get(\"user\", {}).get(\"id\"))\n\n            return {\n                \"out_channel\": payload.get(\"channel\", {}).get(\"id\"),\n                \"thread_id\": thread_id,\n                \"users\": users,\n            }\n\n        return {}```\n\nUpdated code:\n```python\ndef get_metadata(self, request: Request) -> Dict[Text, Any]:\n        \n        \n        content_type = request.headers.get(\"content-type\")\n\n        # Slack API sends either a JSON-encoded or a URL-encoded body depending on the\n        # content\n        if content_type == \"application/json\":\n            # if JSON-encoded message is received\n            slack_event = request.json\n            event = slack_event.get(\"event\", {})\n            thread_id = event.get(\"thread_ts\", event.get(\"ts\"))\n\n            users = []\n            if \"authed_users\" in slack_event:\n                users = slack_event.get(\"authed_users\")\n            elif (\n                \"authorizations\" in slack_event\n                and len(slack_event.get(\"authorizations\")) > 0\n            ):\n                users.append(slack_event.get(\"authorizations\")[0].get(\"user_id\"))\n\n            return {\n                \"out_channel\": event.get(\"channel\"),\n                \"thread_id\": thread_id,\n                \"users\": users,\n            }\n\n        if content_type == \"application/x-www-form-urlencoded\":\n            # if URL-encoded message is received\n            output = request.form\n            payload = json.loads(output[\"payload\"][0])\n            message = payload.get(\"message\", {})\n            thread_id = message.get(\"thread_ts\", message.get(\"ts\"))\n\n            users = []\n            if payload.get(\"user\", {}).get(\"id\"):\n                users.append(payload.get(\"user\", {}).get(\"id\"))\n\n            return {\n                \"out_channel\": payload.get(\"channel\", {}).get(\"id\"),\n                \"thread_id\": thread_id,\n                \"users\": users,\n            }\n\n        return {}\n```\n\nOriginal documentation:\n```python\nExtracts the metadata from a slack API event (https://api.slack.com/types/event).\n\n        Args:\n            request: A `Request` object that contains a slack API event in the body.\n\n        Returns:\n            Metadata extracted from the sent event payload. This includes the output channel for the response,\n            and users that have installed the bot.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extracts the metadata from a slack API event.\n\n        Slack Documentation: https://api.slack.com/types/event\n\n        Args:\n            request: A `Request` object that contains a slack API event in the body.\n\n        Returns:\n            Metadata extracted from the sent event payload. This includes the output\n                channel for the response, and users that have installed the bot.", "file": "rasa_core_channels_slack.py", "function": "SlackInput.get_metadata", "project": "rasa", "change_stats": {"code_changes": 5, "doc_changes": 10, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_models_for_finetuning(\n    previous_model_file: Optional[Text], trained_models_directory: Text,\n) -> Tuple[Optional[\"Agent\"], Optional[\"Interpreter\"]]:\n    \n    \n    from rasa.core.agent import Agent\n    from rasa.core.interpreter import RasaNLUInterpreter\n    from rasa.nlu.model import Interpreter\n\n    if previous_model_file == USE_LATEST_MODEL_FOR_FINE_TUNING:\n        previous_model_file = get_latest_model(trained_models_directory)\n\n    if previous_model_file is None or not Path(previous_model_file).is_file():\n        return None, None\n\n    with unpack_model(previous_model_file) as unpacked:\n        core, nlu = None, None\n        try:\n            core = Agent.load(unpacked)\n        except ModelNotFound:\n            # Something went wrong when loading Core model. Apparently it's not there.\n            pass\n\n        # Try using loaded `interpreter` from `Agent` to avoid loading it twice.\n        if isinstance(core.interpreter, RasaNLUInterpreter):\n            return core, core.interpreter.interpreter\n\n        try:\n            _, nlu_directory = get_model_subdirectories(unpacked)\n            nlu = Interpreter.load(nlu_directory)\n        except Exception:\n            # Anything might go wrong. In that case we skip model finetuning.\n            pass\n\n    return core, nlu```\n\nUpdated code:\n```python\ndef get_models_for_finetuning(\n    previous_model_file: Optional[Union[Path, Text]]\n) -> Tuple[Optional[\"Agent\"], Optional[\"Interpreter\"]]:\n    \n    \n    from rasa.core.interpreter import RasaNLUInterpreter\n\n    if Path(previous_model_file).is_dir():\n        logger.debug(\n            f\"Trying to load latest model from '{previous_model_file}' for \"\n            f\"finetuning.\"\n        )\n        previous_model_file = get_latest_model(previous_model_file)\n\n    if previous_model_file is None or not Path(previous_model_file).is_file():\n        logger.debug(\n            \"No valid model for finetuning found as directory either \"\n            \"contains no model or model file cannot be found.\"\n        )\n        return None, None\n\n    with unpack_model(previous_model_file) as unpacked:\n        core = _load_core_model(unpacked)\n\n        # Try using loaded `interpreter` from `Agent` to avoid loading it twice.\n        if core and isinstance(core.interpreter, RasaNLUInterpreter):\n            nlu = core.interpreter.interpreter\n        else:\n            nlu = _load_nlu_model(unpacked)\n\n    return core, nlu\n```\n\nOriginal documentation:\n```python\nRetrieves loaded models which can be used for finetuning.\n\n    Args:\n        previous_model_file: Path to model which should be used for finetuning or\n            `USE_LATEST_MODEL_FOR_FINE_TUNING` in case the latest trained model should\n            be used.\n        trained_models_directory: Directory which contains trained models. Rasa Open\n            Source will try to retrieve the latest trained model from here in case\n            `previous_model_file` has the value of `USE_LATEST_MODEL_FOR_FINE_TUNING`.\n\n    Returns:\n        Loaded Core model and NLU model.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Retrieves loaded models which can be used for finetuning.\n\n    Args:\n        previous_model_file: Path to model file which should be used for finetuning or\n            a directory in case the latest trained model should be used.\n\n    Returns:\n        Loaded Core model and NLU model.", "file": "rasa_model.py", "function": "get_models_for_finetuning", "project": "rasa", "change_stats": {"code_changes": 38, "doc_changes": 10, "total_changes": 48}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef all_args(self) -> List[Namespace]:\n        \n        \n        all_args = (\n            ([self.pod_args['uses_before']] if self.pod_args['uses_before'] else [])\n            + ([self.pod_args['uses_after']] if self.pod_args['uses_after'] else [])\n            + ([self.pod_args['head']] if self.pod_args['head'] else [])\n        )\n        for shard_id in self.pod_args['pods']:\n            all_args += self.pod_args['pods'][shard_id]\n        return all_args```\n\nUpdated code:\n```python\ndef all_args(self) -> List[Namespace]:\n        \n        \n        all_args = (\n            ([self.pod_args['uses_before']] if self.pod_args['uses_before'] else [])\n            + ([self.pod_args['uses_after']] if self.pod_args['uses_after'] else [])\n            + ([self.pod_args['head']] if self.pod_args['head'] else [])\n            + ([self.pod_args['gateway']] if self.pod_args['gateway'] else [])\n        )\n        for shard_id in self.pod_args['pods']:\n            all_args += self.pod_args['pods'][shard_id]\n        return all_args\n```\n\nOriginal documentation:\n```python\nGet all arguments of all Pods in this BaseDeployment.\n\n        .. # noqa: DAR201\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get all arguments of all Pods in this Deployment.\n\n        .. # noqa: DAR201", "file": "jina_orchestrate_deployments___init__.py", "function": "Deployment.all_args", "project": "jina", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _set_datapipe_valid_iterator_id(datapipe):\n    \n    \n    if hasattr(datapipe, \"_is_child_datapipe\") and datapipe._is_child_datapipe is True:\n        if hasattr(datapipe, \"_set_main_datapipe_valid_iterator_id\"):\n            datapipe._set_main_datapipe_valid_iterator_id()  # reset() is called within this method when appropriate\n        else:\n            raise RuntimeError(\"ChildDataPipe must have method `_set_main_datapipe_valid_iterator_id`.\")\n    else:\n        if datapipe._valid_iterator_id is None:\n            datapipe._valid_iterator_id = 0\n        else:\n            datapipe._valid_iterator_id += 1\n    return datapipe._valid_iterator_id```\n\nUpdated code:\n```python\ndef _set_datapipe_valid_iterator_id(datapipe):\n    \n    \n    if hasattr(datapipe, \"_is_child_datapipe\") and datapipe._is_child_datapipe is True:\n        if hasattr(datapipe, \"_set_main_datapipe_valid_iterator_id\"):\n            datapipe._set_main_datapipe_valid_iterator_id()  # reset() is called within this method when appropriate\n        else:\n            raise RuntimeError(\"ChildDataPipe must have method `_set_main_datapipe_valid_iterator_id`.\")\n    else:\n        if datapipe._valid_iterator_id is None:\n            datapipe._valid_iterator_id = 0\n        else:\n            datapipe._valid_iterator_id += 1\n        datapipe.reset()\n    return datapipe._valid_iterator_id\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n    Given a DataPipe, updates its valid iterator ID.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n    Given a DataPipe, updates its valid iterator ID and reset the DataPipe.", "file": "torch_utils_data_datapipes__typing.py", "function": "_set_datapipe_valid_iterator_id", "project": "pytorch", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef os_archive(self):\n        \n        \n        \n        shop = VoucherShop(self.config, self.device)\n        while 1:\n            # In case logger bought manually,\n            # finish pre-existing archive zone\n            self.os_finish_daily_mission(question=False, rescan=False)\n\n            logger.hr('OS voucher', level=1)\n            self._os_voucher_enter()\n            bought = shop.run_once()\n            self._os_voucher_exit()\n            if not bought:\n                break\n\n        next_reset = get_os_next_reset()\n        logger.info('All archive zones finished, delay to next reset')\n        logger.attr('OpsiNextReset', next_reset)\n        self.config.task_delay(target=next_reset)```\n\nUpdated code:\n```python\ndef os_archive(self):\n        \n        \n        \n        shop = VoucherShop(self.config, self.device)\n        while 1:\n            # In case logger bought manually,\n            # finish pre-existing archive zone\n            self.os_finish_daily_mission(question=False, rescan=False)\n\n            logger.hr('OS voucher', level=1)\n            self._os_voucher_enter()\n            bought = shop.run_once()\n            self._os_voucher_exit()\n            if not bought:\n                break\n\n        # Reset to nearest 'Wednesday' date\n        next_reset = get_nearest_weekday_date(target=2)\n        logger.info('All archive zones finished, delay to next reset')\n        logger.attr('OpsiNextReset', next_reset)\n        self.config.task_delay(target=next_reset)\n```\n\nOriginal documentation:\n```python\nUnused func, not currently a monthly trend\n        Retain in case AL devs add as official feature\n\n        Complete active archive zone in daily mission\n        Purchase next available logger archive then repeat\n        until exhausted\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Complete active archive zone in daily mission\n        Purchase next available logger archive then repeat\n        until exhausted\n\n        Run on weekly basis, AL devs seemingly add new logger\n        archives after random scheduled maintenances", "file": "module_os_operation_siren.py", "function": "OperationSiren.os_archive", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 5, "doc_changes": 10, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_random_state(seed):\n    \n    \n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        \"%r cannot be used to seed a np.random.RandomState\" \" instance\" % seed\n    )```\n\nUpdated code:\n```python\ndef check_random_state(seed):\n    \n    \n    \n\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        \"%r cannot be used to seed a np.random.RandomState\" \" instance\" % seed\n    )\n```\n\nOriginal documentation:\n```python\nTurn seed into a np.random.RandomState instance\n    Parameters\n    ----------\n    seed : None | int | instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Turn seed into a np.random.RandomState instance.\n\n    Arguments\n    ---------\n    seed : None | int | instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.", "file": "speechbrain_processing_diarization.py", "function": "check_random_state", "project": "speechbrain", "change_stats": {"code_changes": 4, "doc_changes": 9, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef preset_mode(self):\n        \n        if self._tado_zone_data.is_away:\n            return PRESET_AWAY\n        return PRESET_HOME```\n\nUpdated code:\n```python\ndef preset_mode(self):\n        \n\n        if \"presenceLocked\" in self._tado_geofence_data:\n            if not self._tado_geofence_data[\"presenceLocked\"]:\n                return PRESET_AUTO\n        if self._tado_zone_data.is_away:\n            return PRESET_AWAY\n        return PRESET_HOME\n```\n\nOriginal documentation:\n```python\nReturn the current preset mode (home, away).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the current preset mode (home, away or auto).", "file": "homeassistant_components_tado_climate.py", "function": "TadoClimate.preset_mode", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_dir_path_handler(dir_path_str: str) -> PathHandler:\n        \n        \n        dir_path = Path(dir_path_str).resolve()\n        file_handler = PatternMatchingEventHandler(\n            patterns=[\"*\"], ignore_patterns=[], ignore_directories=False, case_sensitive=True\n        )\n        return PathHandler(path=dir_path, event_handler=file_handler, recursive=True, static_folder=True)```\n\nUpdated code:\n```python\ndef get_dir_path_handler(dir_path_str: str, ignore_regexes: Optional[List[str]] = None) -> PathHandler:\n        \n        \n        dir_path = Path(dir_path_str).resolve()\n\n        case_sensitive = platform.system().lower() != \"windows\"\n\n        file_handler = RegexMatchingEventHandler(\n            regexes=[\"^.*$\"],\n            ignore_regexes=ignore_regexes,\n            ignore_directories=False,\n            case_sensitive=case_sensitive,\n        )\n        return PathHandler(path=dir_path, event_handler=file_handler, recursive=True, static_folder=True)\n```\n\nOriginal documentation:\n```python\nGet PathHandler for watching a single directory\n\n        Parameters\n        ----------\n        dir_path_str : str\n            Folder path in string\n\n        Returns\n        -------\n        PathHandler\n            The PathHandler for the folder specified\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get PathHandler for watching a single directory\n\n        Parameters\n        ----------\n        dir_path_str : str\n            Folder path in string\n        ignore_regexes : List[str], Optional\n            List of regexes that should be ignored\n\n        Returns\n        -------\n        PathHandler\n            The PathHandler for the folder specified", "file": "samcli_lib_utils_resource_trigger.py", "function": "ResourceTrigger.get_dir_path_handler", "project": "aws-sam-cli", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update(self, *args, **kwargs):\n        \n        new = dict(*args, **kwargs)\n        if any(isinstance(v, dict) for v in new.values()):\n            update_dict_recursive(self, new)\n        else:\n            # super().update(*args, **kwargs)\n            super().update(new)\n        self.save()```\n\nUpdated code:\n```python\ndef update(self, *args, **kwargs):\n        \n        super().update(*args, **kwargs)\n        self.save()\n```\n\nOriginal documentation:\n```python\nUpdates a setting value in the current settings and saves the settings.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Updates a setting value in the current settings.", "file": "ultralytics_utils___init__.py", "function": "SettingsManager.update", "project": "ultralytics", "change_stats": {"code_changes": 9, "doc_changes": 4, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef chunk_forward(\n        self,\n        x: torch.Tensor,\n        pos_enc: torch.Tensor,\n        mask: torch.Tensor,\n        left_context: int = 0,\n        right_context: int = 0,\n    ) -> torch.Tensor:\n        \n\n        \n        for block_idx, block in enumerate(self.blocks):\n            x, pos_enc = block.chunk_forward(\n                x,\n                pos_enc,\n                mask,\n                left_context=left_context,\n                right_context=right_context,\n            )\n\n        x = self.norm_blocks(x)\n\n        return x```\n\nUpdated code:\n```python\ndef chunk_forward(\n        self,\n        x: torch.Tensor,\n        pos_enc: torch.Tensor,\n        mask: torch.Tensor,\n        left_context: int = 0,\n    ) -> torch.Tensor:\n        \n\n        \n        for block in self.blocks:\n            x, pos_enc = block.chunk_forward(\n                x,\n                pos_enc,\n                mask,\n                left_context=left_context,\n            )\n\n        x = self.norm_blocks(x)\n\n        return x\n```\n\nOriginal documentation:\n```python\nForward each block of the encoder architecture.\n\n        Args:\n            x: MultiBlocks input sequences. (B, T, D_block_1)\n            pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_att)\n            mask: Source mask. (B, T_2)\n            left_context: Number of frames in left context.\n            right_context: Number of frames in right context.\n\n        Returns:\n            x: MultiBlocks output sequences. (B, T, D_block_N)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward each block of the encoder architecture.\n\n        Args:\n            x: MultiBlocks input sequences. (B, T, D_block_1)\n            pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_att)\n            mask: Source mask. (B, T_2)\n            left_context: Number of frames in left context.\n\n        Returns:\n            x: MultiBlocks output sequences. (B, T, D_block_N)", "file": "espnet2_asr_transducer_encoder_modules_multi_blocks.py", "function": "MultiBlocks.chunk_forward", "project": "espnet", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _exclude_border(mask, exclude_border):\n    \n    \n    \n    # zero out the image borders\n    for i, excluded in enumerate(exclude_border):\n        if excluded == 0:\n            continue\n        mask[(slice(None),) * i + (slice(None, excluded),)] = False\n        mask[(slice(None),) * i + (slice(-excluded, None),)] = False\n    return mask```\n\nUpdated code:\n```python\ndef _exclude_border(label, border_width):\n    \n\n    \n    # zero out label borders\n    for i, width in enumerate(border_width):\n        if width == 0:\n            continue\n        label[(slice(None),) * i + (slice(None, width),)] = 0\n        label[(slice(None),) * i + (slice(-width, None),)] = 0\n    return label\n```\n\nOriginal documentation:\n```python\nRemove peaks near the borders\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set label border values to 0.", "file": "skimage_feature_peak.py", "function": "_exclude_border", "project": "scikit-image", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self, markers: List[Marker], negated: bool = False, name: Optional[Text] = None\n    ) -> None:\n        \n        \n        super().__init__(name=name, negated=negated)\n        self.sub_markers: List[Marker] = markers\n        expected_num = self.expected_number_of_sub_markers()\n        if expected_num is not None and len(markers) != expected_num:\n            raise InvalidMarkerConfig(\n                f\"Expected {expected_num} sub-marker(s) to be specified for marker \"\n                f\"'{self}' ({self.get_tag()}) but found {len(markers)}. \"\n                f\"Please adapt your configuration so that there are exactly \"\n                f\"{expected_num} sub-markers. \"\n            )\n        elif len(markers) == 0:\n            raise InvalidMarkerConfig(\n                f\"Expected some sub-markers to be specified for {self} but \"\n                f\"found none. Please adapt your configuration so that there is \"\n                f\"at least one sub-marker. \"\n            )```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        markers: List[Marker],\n        negated: bool = False,\n        name: Optional[Text] = None,\n        description: Optional[Text] = None,\n    ) -> None:\n        \n        \n        super().__init__(name=name, negated=negated)\n        self.sub_markers: List[Marker] = markers\n        expected_num = self.expected_number_of_sub_markers()\n        if expected_num is not None and len(markers) != expected_num:\n            raise InvalidMarkerConfig(\n                f\"Expected {expected_num} sub-marker(s) to be specified for marker \"\n                f\"'{self}' ({self.get_tag()}) but found {len(markers)}. \"\n                f\"Please adapt your configuration so that there are exactly \"\n                f\"{expected_num} sub-markers. \"\n            )\n        elif len(markers) == 0:\n            raise InvalidMarkerConfig(\n                f\"Expected some sub-markers to be specified for {self} but \"\n                f\"found none. Please adapt your configuration so that there is \"\n                f\"at least one sub-marker. \"\n            )\n```\n\nOriginal documentation:\n```python\nInstantiates a marker.\n\n        Args:\n            markers: the list of markers to combine\n            negated: whether this marker should be negated (i.e. a negated marker\n                applies if and only if the non-negated marker does not apply)\n            name: a custom name that can be used to replace the default string\n                conversion of this marker\n        Raises:\n            `InvalidMarkerConfig` if the given number of sub-markers does not match\n            the expected number of sub-markers\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Instantiates a marker.\n\n        Args:\n            markers: the list of markers to combine\n            negated: whether this marker should be negated (i.e. a negated marker\n                applies if and only if the non-negated marker does not apply)\n            name: a custom name that can be used to replace the default string\n                conversion of this marker\n            description: an optional description of the marker. It is not used\n                internally but can be used to document the marker.\n        Raises:\n            `InvalidMarkerConfig` if the given number of sub-markers does not match\n            the expected number of sub-markers", "file": "rasa_core_evaluation_marker_base.py", "function": "OperatorMarker.__init__", "project": "rasa", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef score(self, examples):\n        \n        \n        scores = {}\n\n        if hasattr(self.nlp.tokenizer, \"score\"):\n            scores.update(self.nlp.tokenizer.score(examples, **self.cfg))\n        for name, component in self.nlp.pipeline:\n            if hasattr(component, \"score\"):\n                scores.update(component.score(examples, **self.cfg))\n\n        return scores```\n\nUpdated code:\n```python\ndef score(self, examples: Iterable[Example]) -> Dict[str, Any]:\n        \n        \n        scores = {}\n        if hasattr(self.nlp.tokenizer, \"score\"):\n            scores.update(self.nlp.tokenizer.score(examples, **self.cfg))\n        for name, component in self.nlp.pipeline:\n            if hasattr(component, \"score\"):\n                scores.update(component.score(examples, **self.cfg))\n        return scores\n```\n\nOriginal documentation:\n```python\nEvaluate a list of Examples.\n\n        examples (Iterable[Example]): The predicted annotations + correct annotations.\n        RETURNS (Dict): A dictionary of scores.\n        DOCS: https://spacy.io/api/scorer#score\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Evaluate a list of Examples.\n\n        examples (Iterable[Example]): The predicted annotations + correct annotations.\n        RETURNS (Dict): A dictionary of scores.\n\n        DOCS: https://spacy.io/api/scorer#score", "file": "spacy_scorer.py", "function": "Scorer.score", "project": "spaCy", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _token_clusters(tokens: List[Token]) -> List[List[Token]]:\n        \n        \n\n        \n        token_index_clusters = []\n\n        for idx in range(1, len(tokens)):\n            # two token belong to the same word if there is no other character\n            # between them\n            if tokens[idx].start == tokens[idx - 1].end:\n                if token_index_clusters and token_index_clusters[-1][-1] == idx - 1:\n                    token_index_clusters[-1].append(idx)\n                else:\n                    token_index_clusters.append([idx - 1, idx])\n\n        token_clusters = []\n\n        for cluster in token_index_clusters:\n            cluster.sort()\n            token_clusters.append([tokens[idx] for idx in cluster])\n\n        return token_clusters```\n\nUpdated code:\n```python\ndef _token_clusters(tokens: List[Token]) -> List[List[Token]]:\n        \n        \n\n        \n        token_index_clusters = []\n\n        for idx in range(1, len(tokens)):\n            # two token belong to the same word if there is no other character\n            # between them\n            if tokens[idx].start == tokens[idx - 1].end:\n                if token_index_clusters and token_index_clusters[-1][-1] == idx - 1:\n                    token_index_clusters[-1].append(idx)\n                else:\n                    token_index_clusters.append([idx - 1, idx])\n            else:\n                if idx == 1:\n                    token_index_clusters.append([idx - 1])\n                token_index_clusters.append([idx])\n\n        return [[tokens[idx] for idx in cluster] for cluster in token_index_clusters]\n```\n\nOriginal documentation:\n```python\nGet tokens that belong to one word.\n\n        Args:\n            tokens: list of tokens\n\n        Returns: list of token clusters\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Build clusters of tokens that belong to one word.\n\n        Args:\n            tokens: list of tokens\n\n        Returns: list of token clusters", "file": "rasa_nlu_extractors_extractor.py", "function": "EntityExtractor._token_clusters", "project": "rasa", "change_stats": {"code_changes": 14, "doc_changes": 4, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        slack_token: Text,\n        slack_channel: Optional[Text] = None,\n        proxy: Optional[Text] = None,\n        slack_retry_reason_header: Optional[Text] = None,\n        slack_retry_number_header: Optional[Text] = None,\n        errors_ignore_retry: Optional[List[Text]] = None,\n        use_threads: Optional[bool] = False,\n    ) -> None:\n        \n\n        \n        self.slack_token = slack_token\n        self.slack_channel = slack_channel\n        self.proxy = proxy\n        self.errors_ignore_retry = errors_ignore_retry or (\"http_timeout\",)\n        self.retry_reason_header = slack_retry_reason_header\n        self.retry_num_header = slack_retry_number_header\n        self.use_threads = use_threads```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        slack_token: Text,\n        slack_channel: Optional[Text] = None,\n        proxy: Optional[Text] = None,\n        slack_retry_reason_header: Optional[Text] = None,\n        slack_retry_number_header: Optional[Text] = None,\n        errors_ignore_retry: Optional[List[Text]] = None,\n        use_threads: Optional[bool] = False,\n        slack_signing_secret: Optional[Text] = None,\n    ) -> None:\n        \n\n        \n        self.slack_token = slack_token\n        self.slack_channel = slack_channel\n        self.proxy = proxy\n        self.errors_ignore_retry = errors_ignore_retry or (\"http_timeout\",)\n        self.retry_reason_header = slack_retry_reason_header\n        self.retry_num_header = slack_retry_number_header\n        self.use_threads = use_threads\n        self.slack_signing_secret = slack_signing_secret\n\n        self._raise_deprecation_warnings()\n```\n\nOriginal documentation:\n```python\nCreate a Slack input channel.\n\n        Needs a couple of settings to properly authenticate and validate\n        messages. Details to setup:\n\n        https://github.com/slackapi/python-slackclient\n\n        Args:\n            slack_token: Your Slack Authentication token. You can create a\n                Slack app and get your Bot User OAuth Access Token\n                `here <https://api.slack.com/slack-apps>`_.\n            slack_channel: the string identifier for a channel to which\n                the bot posts, or channel name (e.g. '#bot-test')\n                If not set, messages will be sent back\n                to the \"App\" DM channel of your bot's name.\n            proxy: A Proxy Server to route your traffic through\n            slack_retry_reason_header: Slack HTTP header name indicating reason that slack send retry request.\n            slack_retry_number_header: Slack HTTP header name indicating the attempt number\n            errors_ignore_retry: Any error codes given by Slack\n                included in this list will be ignored.\n                Error codes are listed\n                `here <https://api.slack.com/events-api#errors>`_.\n            use_threads: If set to True, your bot will send responses in Slack as a threaded message.\n                Responses will appear as a normal Slack message if set to False.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a Slack input channel.\n\n        Needs a couple of settings to properly authenticate and validate\n        messages. Details to setup:\n\n        https://github.com/slackapi/python-slackclient\n\n        Args:\n            slack_token: Your Slack Authentication token. You can create a\n                Slack app and get your Bot User OAuth Access Token\n                `here <https://api.slack.com/slack-apps>`_.\n            slack_channel: the string identifier for a channel to which\n                the bot posts, or channel name (e.g. '#bot-test')\n                If not set, messages will be sent back\n                to the \"App\" DM channel of your bot's name.\n            proxy: A Proxy Server to route your traffic through\n            slack_retry_reason_header: Slack HTTP header name indicating reason\n                that slack send retry request.\n            slack_retry_number_header: Slack HTTP header name indicating\n                the attempt number.\n            errors_ignore_retry: Any error codes given by Slack\n                included in this list will be ignored.\n                Error codes are listed\n                `here <https://api.slack.com/events-api#errors>`_.\n            use_threads: If set to `True`, your bot will send responses in Slack as\n                a threaded message. Responses will appear as a normal Slack message\n                if set to `False`.\n            slack_signing_secret: Slack creates a unique string for your app and\n                shares it with you. This allows us to verify requests from Slack\n                with confidence by verifying signatures using your signing secret.", "file": "rasa_core_channels_slack.py", "function": "SlackInput.__init__", "project": "rasa", "change_stats": {"code_changes": 6, "doc_changes": 16, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_uniform_grid(method, seeds=[0, 1, 2], n_iter=1000):\n    \n    for seed in seeds:\n        tsne = TSNE(n_components=2, init='random', random_state=seed,\n                    perplexity=10, n_iter=n_iter, method=method)\n        Y = tsne.fit_transform(X_2d_grid)\n\n        # Ensure that the convergence criterion has been triggered\n        assert tsne.n_iter_ < n_iter\n\n        # Ensure that the resulting embedding leads to approximately\n        # uniformly spaced points: the distance to the closest neighbors\n        # should be non-zero and approximately constant.\n        nn = NearestNeighbors(n_neighbors=1).fit(Y)\n        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()\n        assert dist_to_nn.min() > 0.1\n\n        smallest_to_mean = dist_to_nn.min() / np.mean(dist_to_nn)\n        largest_to_mean = dist_to_nn.max() / np.mean(dist_to_nn)\n\n        try_name = \"{}_{}\".format(method, seed)\n        assert_greater(smallest_to_mean, .5, msg=try_name)\n        assert_less(largest_to_mean, 2, msg=try_name)```\n\nUpdated code:\n```python\ndef check_uniform_grid(method, seeds=[0, 1, 2], n_iter=1000):\n    \n    \n    for seed in seeds:\n        tsne = TSNE(n_components=2, init='random', random_state=seed,\n                    perplexity=20, n_iter=n_iter, method=method)\n        Y = tsne.fit_transform(X_2d_grid)\n\n        try_name = \"{}_{}\".format(method, seed)\n        try:\n            assert_uniform_grid(Y, try_name)\n        except AssertionError:\n            # If the test fails a first time, re-run with init=Y to see if\n            # this was caused by a bad initialization. Note that this will\n            # also run an early_exaggeration step.\n            try_name += \":rerun\"\n            tsne.init = Y\n            Y = tsne.fit_transform(X_2d_grid)\n            assert_uniform_grid(Y, try_name)\n```\n\nOriginal documentation:\n```python\nMake sure that TSNE can approximately recover a uniform 2D grid\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Make sure that TSNE can approximately recover a uniform 2D grid\n\n    Due to ties in distances between point in X_2d_grid, this test is platform\n    dependent for ``method='barnes_hut'`` due to numerical imprecision.\n\n    Also, t-SNE is not assured to converge to the right solution because bad\n    initialization can lead to convergence to bad local minimum (the\n    optimization problem is non-convex). To avoid breaking the test too often,\n    we re-run t-SNE from the final point when the convergence is not good\n    enough.", "file": "sklearn_manifold_tests_test_t_sne.py", "function": "check_uniform_grid", "project": "scikit-learn", "change_stats": {"code_changes": 30, "doc_changes": 11, "total_changes": 41}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ensure_edge_insight(self, reverse=False, preset=None, swipe_limit=(3, 2)):\n        \n        \n        \n        logger.info(f'Ensure edge in sight.')\n        record = []\n        self._correct_camera = True\n        x_swipe, y_swipe = np.multiply(swipe_limit, random_direction(self.config.MAP_ENSURE_EDGE_INSIGHT_CORNER))\n\n        while 1:\n            if len(record) == 0:\n                self.update()\n                if preset is not None:\n                    self.map_swipe(preset)\n                    record.append(preset)\n\n            x = 0 if self.view.left_edge or self.view.right_edge else x_swipe\n            y = 0 if self.view.lower_edge or self.view.upper_edge else y_swipe\n\n            if len(record) > 0:\n                # Swipe even if two edges insight, this will avoid some embarrassing camera position.\n                self.map_swipe((x, y))\n\n            record.append((x, y))\n\n            if x == 0 and y == 0:\n                break\n\n        # self._correct_camera = False\n\n        if reverse:\n            logger.info('Reverse swipes.')\n            for vector in record[::-1]:\n                x, y = vector\n                if x != 0 or y != 0:\n                    self.map_swipe((-x, -y))\n\n        return record```\n\nUpdated code:\n```python\ndef ensure_edge_insight(self, reverse=False, preset=None, swipe_limit=(3, 2), skip_first_update=True):\n        \n        \n        \n        logger.info(f'Ensure edge in sight.')\n        record = []\n        x_swipe, y_swipe = np.multiply(swipe_limit, random_direction(self.config.MAP_ENSURE_EDGE_INSIGHT_CORNER))\n\n        while 1:\n            if len(record) == 0:\n                if not skip_first_update:\n                    self.update()\n                if preset is not None:\n                    self.map_swipe(preset)\n                    record.append(preset)\n\n            x = 0 if self.view.left_edge or self.view.right_edge else x_swipe\n            y = 0 if self.view.lower_edge or self.view.upper_edge else y_swipe\n\n            if len(record) > 0:\n                # Swipe even if two edges insight, this will avoid some embarrassing camera position.\n                self.map_swipe((x, y))\n\n            record.append((x, y))\n\n            if x == 0 and y == 0:\n                break\n\n        if reverse:\n            logger.info('Reverse swipes.')\n            for vector in record[::-1]:\n                x, y = vector\n                if x != 0 or y != 0:\n                    self.map_swipe((-x, -y))\n\n        return record\n```\n\nOriginal documentation:\n```python\nSwipe to bottom left until two edges insight.\n        Edges are used to locate camera.\n\n        Args:\n            reverse (bool): Reverse swipes.\n            preset (tuple(int)): Set in map swipe manually.\n            swipe_limit (tuple): (x, y). Limit swipe in (-x, -y, x, y).\n\n        Returns:\n            list[tuple]: Swipe record.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Swipe to bottom left until two edges insight.\n        Edges are used to locate camera.\n\n        Args:\n            reverse (bool): Reverse swipes.\n            preset (tuple(int)): Set in map swipe manually.\n            swipe_limit (tuple): (x, y). Limit swipe in (-x, -y, x, y).\n            skip_first_update (bool): Usually to be True. Use False if you are calling ensure_edge_insight manually.\n\n        Returns:\n            list[tuple]: Swipe record.", "file": "module_map_camera.py", "function": "Camera.ensure_edge_insight", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 10, "doc_changes": 3, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef session_ids(self):\n          \n        return self._log_files.session_ids```\n\nUpdated code:\n```python\ndef session_ids(self) -> List[int]:\n          \n        return self._log_files.session_ids\n```\n\nOriginal documentation:\n```python\nlist: Sorted list of integers of available session ids.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "list[int]: Sorted list of integers of available session ids.", "file": "lib_gui_analysis_event_reader.py", "function": "TensorBoardLogs.session_ids", "project": "faceswap", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef summary_string(self, already_printed: Set[str] = None) -> str:\n        \n        if already_printed is None:\n            already_printed = set()\n\n        if self.needs_stats_actor:\n            ac = self.stats_actor\n            # TODO(chengsu): this is a super hack, clean it up.\n            stats_map, self.time_total_s = ray.get(ac.get.remote(self.stats_uuid))\n            if DatasetContext.get_current().block_splitting_enabled:\n                self.stages[\"read\"] = []\n                for _, blocks_metadata in sorted(stats_map.items()):\n                    self.stages[\"read\"] += blocks_metadata\n            else:\n                for i, metadata in stats_map.items():\n                    self.stages[\"read\"][i] = metadata[0]\n        out = \"\"\n        if self.parents:\n            for p in self.parents:\n                parent_sum = p.summary_string(already_printed)\n                if parent_sum:\n                    out += parent_sum\n                    out += \"\\n\"\n        if len(self.stages) == 1:\n            stage_name, metadata = next(iter(self.stages.items()))\n            stage_uuid = self.dataset_uuid + stage_name\n            out += \"Stage {} {}: \".format(self.number, stage_name)\n            if stage_uuid in already_printed:\n                out += \"[execution cached]\\n\"\n            else:\n                already_printed.add(stage_uuid)\n                out += self._summarize_blocks(metadata, is_substage=False)\n        elif len(self.stages) > 1:\n            rounded_total = round(self.time_total_s, 2)\n            if rounded_total <= 0:\n                # Handle -0.0 case.\n                rounded_total = 0\n            out += \"Stage {} {}: executed in {}s\\n\".format(\n                self.number, self.base_name, rounded_total\n            )\n            for n, (stage_name, metadata) in enumerate(self.stages.items()):\n                stage_uuid = self.dataset_uuid + stage_name\n                out += \"\\n\"\n                out += \"\\tSubstage {} {}: \".format(n, stage_name)\n                if stage_uuid in already_printed:\n                    out += \"\\t[execution cached]\\n\"\n                else:\n                    already_printed.add(stage_uuid)\n                    out += self._summarize_blocks(metadata, is_substage=True)\n        out += self._summarize_iter()\n        return out```\n\nUpdated code:\n```python\ndef summary_string(\n        self, already_printed: Set[str] = None, include_parent: bool = True\n    ) -> str:\n        \n        \n        if already_printed is None:\n            already_printed = set()\n\n        if self.needs_stats_actor:\n            ac = self.stats_actor\n            # TODO(chengsu): this is a super hack, clean it up.\n            stats_map, self.time_total_s = ray.get(ac.get.remote(self.stats_uuid))\n            if DatasetContext.get_current().block_splitting_enabled:\n                self.stages[\"read\"] = []\n                for _, blocks_metadata in sorted(stats_map.items()):\n                    self.stages[\"read\"] += blocks_metadata\n            else:\n                for i, metadata in stats_map.items():\n                    self.stages[\"read\"][i] = metadata[0]\n        out = \"\"\n        if self.parents and include_parent:\n            for p in self.parents:\n                parent_sum = p.summary_string(already_printed)\n                if parent_sum:\n                    out += parent_sum\n                    out += \"\\n\"\n        if len(self.stages) == 1:\n            stage_name, metadata = next(iter(self.stages.items()))\n            stage_uuid = self.dataset_uuid + stage_name\n            out += \"Stage {} {}: \".format(self.number, stage_name)\n            if stage_uuid in already_printed:\n                out += \"[execution cached]\\n\"\n            else:\n                already_printed.add(stage_uuid)\n                out += self._summarize_blocks(metadata, is_substage=False)\n        elif len(self.stages) > 1:\n            rounded_total = round(self.time_total_s, 2)\n            if rounded_total <= 0:\n                # Handle -0.0 case.\n                rounded_total = 0\n            out += \"Stage {} {}: executed in {}s\\n\".format(\n                self.number, self.base_name, rounded_total\n            )\n            for n, (stage_name, metadata) in enumerate(self.stages.items()):\n                stage_uuid = self.dataset_uuid + stage_name\n                out += \"\\n\"\n                out += \"\\tSubstage {} {}: \".format(n, stage_name)\n                if stage_uuid in already_printed:\n                    out += \"\\t[execution cached]\\n\"\n                else:\n                    already_printed.add(stage_uuid)\n                    out += self._summarize_blocks(metadata, is_substage=True)\n        out += self._summarize_iter()\n        return out\n```\n\nOriginal documentation:\n```python\nReturn a human-readable summary of this Dataset's stats.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a human-readable summary of this Dataset's stats.\n\n        Args:\n        already_printed: Set of stage IDs that have already had its stats printed out.\n        include_parent: If true, also include parent stats summary; otherwise, only\n        log stats of the latest stage.", "file": "python_ray_data__internal_stats.py", "function": "DatasetStats.summary_string", "project": "ray", "change_stats": {"code_changes": 9, "doc_changes": 7, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create(\n        self,\n        arn: str,\n        version: str,\n        endpoint_options: Optional[DomainEndpointOptions] = None,\n        preferred_port: Optional[int] = None,\n    ) -> Server:\n        \n        \n        \n\n        # determine custom domain endpoint\n        custom_endpoint = determine_custom_endpoint(endpoint_options)\n\n        # determine engine type\n        engine_type = versions.get_engine_type(version)\n\n        # build final endpoint and cluster url\n        endpoint = build_cluster_endpoint(\n            DomainKey.from_arn(arn), custom_endpoint, engine_type, preferred_port\n        )\n        url = f\"http://{endpoint}\" if \"://\" not in endpoint else endpoint\n\n        # call abstract cluster factory\n        cluster = self._create_cluster(arn, url, version, custom_endpoint)\n        cluster.start()\n\n        # save cluster into registry and return\n        self.clusters[arn] = cluster\n        return cluster```\n\nUpdated code:\n```python\ndef create(\n        self,\n        arn: str,\n        version: str,\n        endpoint_options: Optional[DomainEndpointOptions] = None,\n        security_options: Optional[SecurityOptions] = None,\n        preferred_port: Optional[int] = None,\n    ) -> Server:\n        \n        \n        \n\n        # determine custom domain endpoint\n        custom_endpoint = determine_custom_endpoint(endpoint_options)\n\n        # determine engine type\n        engine_type = versions.get_engine_type(version)\n\n        # build final endpoint and cluster url\n        endpoint = build_cluster_endpoint(\n            DomainKey.from_arn(arn), custom_endpoint, engine_type, preferred_port\n        )\n        url = f\"http://{endpoint}\" if \"://\" not in endpoint else endpoint\n\n        # call abstract cluster factory\n        cluster = self._create_cluster(arn, url, version, custom_endpoint, security_options)\n        cluster.start()\n\n        # save cluster into registry and return\n        self.clusters[arn] = cluster\n        return cluster\n```\n\nOriginal documentation:\n```python\nCreates a new cluster.\n\n        :param arn: of the cluster to create\n        :param version: of the cluster to start (string including the EngineType)\n        :param endpoint_options: DomainEndpointOptions (may contain information about a custom endpoint url)\n        :param preferred_port: port which should be preferred (only if OPENSEARCH_ENDPOINT_STRATEGY == \"port\")\n        :return: None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates a new cluster.\n\n        :param arn: of the cluster to create\n        :param version: of the cluster to start (string including the EngineType)\n        :param endpoint_options: DomainEndpointOptions (may contain information about a custom endpoint url)\n        :param security_options: SecurityOptions (may contain info on the security plugin config)\n        :param preferred_port: port which should be preferred (only if OPENSEARCH_ENDPOINT_STRATEGY == \"port\")\n        :return: None", "file": "localstack_services_opensearch_cluster_manager.py", "function": "ClusterManager.create", "project": "localstack", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef plot(self, label=None, kind='line', rot=30, axes=None, style='-',\n             **kwds): # pragma: no cover\n        \n        \n        \n        import matplotlib.pyplot as plt\n\n        if label is not None:\n            kwds = kwds.copy()\n            kwds['label'] = label\n\n        N = len(self)\n\n        if axes is None:\n            axes = plt.gca()\n\n        if kind == 'line':\n            axes.plot(self.index, self.values, style, **kwds)\n        elif kind == 'bar':\n            xinds = np.arange(N) + 0.25\n            axes.bar(xinds, self.values, 0.5, bottom=np.zeros(N), linewidth=1)\n\n            if N < 10:\n                fontsize = 12\n            else:\n                fontsize = 10\n\n            axes.set_xticks(xinds + 0.25)\n            axes.set_xticklabels(self.index, rotation=rot, fontsize=fontsize)\n\n        plt.draw_if_interactive()```\n\nUpdated code:\n```python\ndef plot(self, label=None, kind='line', use_index=True, rot=30, ax=None,\n             style='-', **kwds): # pragma: no cover\n        \n        \n        \n        import matplotlib.pyplot as plt\n\n        if label is not None:\n            kwds = kwds.copy()\n            kwds['label'] = label\n\n        N = len(self)\n\n        if ax is None:\n            ax = plt.gca()\n\n        if kind == 'line':\n            if use_index:\n                x = self.index\n            else:\n                x = range(len(self))\n\n            ax.plot(x, self.values, style, **kwds)\n        elif kind == 'bar':\n            xinds = np.arange(N) + 0.25\n            ax.bar(xinds, self.values, 0.5, bottom=np.zeros(N), linewidth=1)\n\n            if N < 10:\n                fontsize = 12\n            else:\n                fontsize = 10\n\n            ax.set_xticks(xinds + 0.25)\n            ax.set_xticklabels(self.index, rotation=rot, fontsize=fontsize)\n\n        plt.draw_if_interactive()\n```\n\nOriginal documentation:\n```python\nPlot the input series with the index on the x-axis using\n        matplotlib / pylab.\n\n        Parameters\n        ----------\n        label : label argument to provide to plot\n        kind : {'line', 'bar', 'hist'}\n            Default: line for TimeSeries, hist for Series\n        kwds : other plotting keyword arguments\n\n        Notes\n        -----\n        See matplotlib documentation online for more on this subject\n\n        Default plot-types: TimeSeries (line), Series (bar)\n\n        Intended to be used in ipython -pylab mode\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Plot the input series with the index on the x-axis using\n        matplotlib / pylab.\n\n        Parameters\n        ----------\n        label : label argument to provide to plot\n        kind : {'line', 'bar', 'hist'}\n            Default: line for TimeSeries, hist for Series\n        auto_x : if True, it will use range(len(self)) as x-axis\n        kwds : other plotting keyword arguments\n\n        Notes\n        -----\n        See matplotlib documentation online for more on this subject\n\n        Default plot-types: TimeSeries (line), Series (bar)\n\n        Intended to be used in ipython -pylab mode", "file": "pandas_core_series.py", "function": "Series.plot", "project": "pandas", "change_stats": {"code_changes": 23, "doc_changes": 3, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef write_error(self, status_code, **kwargs):\n        \n        \n        if hasattr(self, 'get_error_html'):\n            if 'exc_info' in kwargs:\n                exc_info = kwargs.pop('exc_info')\n                kwargs['exception'] = exc_info[1]\n                try:\n                    # Put the traceback into sys.exc_info()\n                    raise_exc_info(exc_info)\n                except Exception:\n                    self.finish(self.get_error_html(status_code, **kwargs))\n            else:\n                self.finish(self.get_error_html(status_code, **kwargs))\n            return\n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            # in debug mode, try to send a traceback\n            self.set_header('Content-Type', 'text/plain')\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                self.write(line)\n            self.finish()\n        else:\n            self.finish(\"<html><title>%(code)d: %(message)s</title>\"\n                        \"<body>%(code)d: %(message)s</body></html>\" % {\n                            \"code\": status_code,\n                            \"message\": self._reason,\n                        })```\n\nUpdated code:\n```python\ndef write_error(self, status_code, **kwargs):\n        \n        \n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            # in debug mode, try to send a traceback\n            self.set_header('Content-Type', 'text/plain')\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                self.write(line)\n            self.finish()\n        else:\n            self.finish(\"<html><title>%(code)d: %(message)s</title>\"\n                        \"<body>%(code)d: %(message)s</body></html>\" % {\n                            \"code\": status_code,\n                            \"message\": self._reason,\n                        })\n```\n\nOriginal documentation:\n```python\nOverride to implement custom error pages.\n\n        ``write_error`` may call `write`, `render`, `set_header`, etc\n        to produce output as usual.\n\n        If this error was caused by an uncaught exception (including\n        HTTPError), an ``exc_info`` triple will be available as\n        ``kwargs[\"exc_info\"]``.  Note that this exception may not be\n        the \"current\" exception for purposes of methods like\n        ``sys.exc_info()`` or ``traceback.format_exc``.\n\n        For historical reasons, if a method ``get_error_html`` exists,\n        it will be used instead of the default ``write_error`` implementation.\n        ``get_error_html`` returned a string instead of producing output\n        normally, and had different semantics for exception handling.\n        Users of ``get_error_html`` are encouraged to convert their code\n        to override ``write_error`` instead.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Override to implement custom error pages.\n\n        ``write_error`` may call `write`, `render`, `set_header`, etc\n        to produce output as usual.\n\n        If this error was caused by an uncaught exception (including\n        HTTPError), an ``exc_info`` triple will be available as\n        ``kwargs[\"exc_info\"]``.  Note that this exception may not be\n        the \"current\" exception for purposes of methods like\n        ``sys.exc_info()`` or ``traceback.format_exc``.", "file": "tornado_web.py", "function": "RequestHandler.write_error", "project": "tornado", "change_stats": {"code_changes": 14, "doc_changes": 9, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, vmin=None, vmax=None, clip = True):\n        \n        \n        \n        self.vmin = vmin\n        self.vmax = vmax\n        self.clip = clip```\n\nUpdated code:\n```python\ndef __init__(self, vmin=None, vmax=None, clip=False):\n        \n        \n        \n        self.vmin = vmin\n        self.vmax = vmax\n        self.clip = clip\n```\n\nOriginal documentation:\n```python\nIf vmin or vmax is not given, they are taken from the input's\n        minimum and maximum value respectively.  If clip is True and\n        the given value falls outside the range, the returned value\n        will be 0 or 1, whichever is closer. Returns 0 if vmin==vmax.\n        Works with scalars or arrays, including masked arrays.  If\n        clip is True, masked values are set to 1; otherwise they\n        remain masked.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "If vmin or vmax is not given, they are taken from the input's\n        minimum and maximum value respectively.  If clip is True and\n        the given value falls outside the range, the returned value\n        will be 0 or 1, whichever is closer. Returns 0 if vmin==vmax.\n        Works with scalars or arrays, including masked arrays.  If\n        clip is True, masked values are set to 1; otherwise they\n        remain masked.  Clipping silently defeats the purpose of setting\n        the over, under, and masked colors in the colormap, so it is\n        likely to lead to surprises; therefore the default is\n        clip=False.", "file": "lib_matplotlib_colors.py", "function": "Normalize.__init__", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _batch_diag(bmat):\n    \n    \n    \n    return bmat.contiguous().view(bmat.shape[:-2] + (-1,))[..., ::bmat.size(-1) + 1]```\n\nUpdated code:\n```python\ndef _batch_diag(bmat):\n    \n    \n    return bmat.reshape(bmat.shape[:-2] + (-1,))[..., ::bmat.size(-1) + 1]\n```\n\nOriginal documentation:\n```python\nReturns the diagonals of a batch of square matrices.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n    Returns the diagonals of a batch of square matrices.", "file": "torch_distributions_multivariate_normal.py", "function": "_batch_diag", "project": "pytorch", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _enhance_choose(self, skip_first_screenshot=True):\n        \n        \n        \n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if self.appear(EQUIP_CONFIRM, offset=(30, 30)):\n                return True\n\n            self.equip_sidebar_ensure(index=4)\n            self.wait_until_appear(ENHANCE_RECOMMEND, offset=(5, 5), skip_first_screenshot=True)\n\n            status = color_bar_percentage(self.device.image, area=ENHANCE_RELOAD.area, prev_color=(231, 178, 74))\n            logger.attr('Reload_enhanced', f'{int(status * 100)}%')\n            choose = np.sum(np.array(self.device.image.crop(ENHANCE_FILLED.area)) > 200) > 100\n\n            if self.info_bar_count():\n                if status > 0.98:\n                    logger.info('Fully enhanced for this ship')\n                    self.equip_view_next(check_button=ENHANCE_RECOMMEND)\n                    self.ensure_no_info_bar()\n                    continue\n                else:\n                    if choose:\n                        logger.info('Unable to enhance this ship')\n                        self.equip_view_next(check_button=ENHANCE_RECOMMEND)\n                        self.ensure_no_info_bar()\n                        continue\n                    else:\n                        logger.info('Enhancement material exhausted')\n                        return False\n\n            if self.appear_then_click(ENHANCE_RECOMMEND, offset=(5, 5), interval=2):\n                self.device.sleep(0.3)\n                self.appear_then_click(ENHANCE_CONFIRM)```\n\nUpdated code:\n```python\ndef _enhance_choose(self, skip_first_screenshot=True):\n        \n        \n        \n        end_activate_timer = Timer(2, count=2)\n        while 1:\n            if skip_first_screenshot:\n                skip_first_screenshot = False\n            else:\n                self.device.screenshot()\n\n            if self.appear(EQUIP_CONFIRM, offset=(30, 30)):\n                return True\n\n            if not end_activate_timer.reached_and_reset():\n                continue\n\n            ensured = self.equip_sidebar_ensure(index=4)\n            if ensured:\n                self.wait_until_appear(ENHANCE_RECOMMEND, offset=(5, 5), skip_first_screenshot=True)\n            else:\n                continue\n\n            status = color_bar_percentage(self.device.image, area=ENHANCE_RELOAD.area, prev_color=(231, 178, 74))\n            logger.attr('Reload_enhanced', f'{int(status * 100)}%')\n            choose = np.sum(np.array(self.device.image.crop(ENHANCE_FILLED.area)) > 200) > 100\n\n            if self.info_bar_count():\n                if status > 0.98:\n                    logger.info('Fully enhanced for this ship')\n                    swiped = self.equip_view_next(check_button=ENHANCE_RECOMMEND)\n                    self.ensure_no_info_bar()\n                    if not swiped:\n                        return False\n                    continue\n                else:\n                    if choose:\n                        logger.info('Unable to enhance this ship')\n                        swiped = self.equip_view_next(check_button=ENHANCE_RECOMMEND)\n                        self.ensure_no_info_bar()\n                        if not swiped:\n                            return False\n                        continue\n                    else:\n                        logger.info('Enhancement material exhausted')\n                        return False\n\n            if self.appear_then_click(ENHANCE_RECOMMEND, offset=(5, 5), interval=2):\n                self.device.sleep(0.3)\n                self.device.click(ENHANCE_CONFIRM)\n```\n\nOriginal documentation:\n```python\nPage require: page_ship_enhance, without info_bar\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Pages:\n            in: page_ship_enhance, without info_bar\n            out: EQUIP_CONFIRM", "file": "module_retire_enhancement.py", "function": "Enhancement._enhance_choose", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 23, "doc_changes": 6, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef NewRecord(net, schema):\n    \n    \n    \n    if isinstance(schema, Scalar):\n        result = schema.clone()\n        result.set_value(\n            blob=ScopedBlobReference(net.NextName('unnamed_scalar'))\n        )\n        return result\n\n    assert isinstance(schema, Field), 'Record must be a schema.Field instance.'\n    blob_refs = [\n        net.AddExternalInput(net.NextName(prefix=name))\n        for name in schema.field_names()\n    ]\n    return from_blob_list(schema, blob_refs)```\n\nUpdated code:\n```python\ndef NewRecord(net, schema):\n    \n    \n    \n    if isinstance(schema, Scalar):\n        result = schema.clone()\n        result.set_value(\n            blob=net.NextScopedBlob('unnamed_scalar')\n        )\n        return result\n\n    assert isinstance(schema, Field), 'Record must be a schema.Field instance.'\n    blob_refs = [\n        net.NextScopedBlob(prefix=name)\n        for name in schema.field_names()\n    ]\n    return from_blob_list(schema, blob_refs)\n```\n\nOriginal documentation:\n```python\nGiven a record of np.arrays, create a BlobReference for each one of them,\n    returning a record containing BlobReferences. The BlobReferences will be\n    added as ExternalInputs of the given net.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Given a record of np.arrays, create a BlobReference for each one of them,\n    returning a record containing BlobReferences. The name of each returned blob\n    is NextScopedBlob(field_name), which guarantees unique name in the current\n    net. Use NameScope explicitly to avoid name conflictions between different\n    nets.", "file": "caffe2_python_schema.py", "function": "NewRecord", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unique_id(self):\n        \n        return self._device_info.serial_num```\n\nUpdated code:\n```python\ndef unique_id(self) -> str:\n        \n        return self._unique_id\n```\n\nOriginal documentation:\n```python\nReturn a unique ID.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the unique ID for this entity.", "file": "homeassistant_components_roku_remote.py", "function": "RokuRemote.unique_id", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _ensure_listlike_indexer(self, key, axis=None):\n        \n        \n        \n        column_axis = 1\n\n        # column only exists in 2-dimensional DataFrame\n        if self.ndim != 2:\n            return\n\n        if isinstance(key, tuple):\n            # key may be a tuple if key is a _LocIndexer key\n            # in that case, set key to the column part of key\n            key = key[column_axis]\n            axis = column_axis\n\n        if (\n            axis == column_axis\n            and not isinstance(self.obj.columns, ABCMultiIndex)\n            and is_list_like_indexer(key)\n            and not com.is_bool_indexer(key)\n            and all(is_hashable(k) for k in key)\n        ):\n            for k in key:\n                try:\n                    self.obj[k]\n                except KeyError:\n                    self.obj[k] = np.nan```\n\nUpdated code:\n```python\ndef _ensure_listlike_indexer(self, key, axis=None):\n        \n        \n        \n        column_axis = 1\n\n        # column only exists in 2-dimensional DataFrame\n        if self.ndim != 2:\n            return\n\n        if isinstance(key, tuple):\n            # key may be a tuple if we are .loc\n            # in that case, set key to the column part of key\n            key = key[column_axis]\n            axis = column_axis\n\n        if (\n            axis == column_axis\n            and not isinstance(self.obj.columns, ABCMultiIndex)\n            and is_list_like_indexer(key)\n            and not com.is_bool_indexer(key)\n            and all(is_hashable(k) for k in key)\n        ):\n            for k in key:\n                if k not in self.obj:\n                    self.obj[k] = np.nan\n```\n\nOriginal documentation:\n```python\nEnsure that a list-like of column labels are all present by adding them if\n        they do not already exist.\n\n        Parameters\n        ----------\n        key : _LocIndexer key or list-like of column labels\n            Target labels.\n        axis : key axis if known\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure that a list-like of column labels are all present by adding them if\n        they do not already exist.\n\n        Parameters\n        ----------\n        key : list-like of column labels\n            Target labels.\n        axis : key axis if known", "file": "pandas_core_indexing.py", "function": "_LocationIndexer._ensure_listlike_indexer", "project": "pandas", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef validate_version(version: Text) -> bool:\n    \n    \n    \n    if isinstance(pep440_version.parse(version), pep440_version.LegacyVersion):\n        return False\n\n    version_object = Version.coerce(version)\n    return not version_object.prerelease or is_alpha_version(version_object)```\n\nUpdated code:\n```python\ndef validate_version(version: Text) -> bool:\n    \n    \n    \n    if isinstance(pep440_version.parse(version), pep440_version.LegacyVersion):\n        return False\n\n    version_object = Version.coerce(version)\n    return not version_object.prerelease or is_prerelease_version(version_object)\n```\n\nOriginal documentation:\n```python\nEnsure that the version follows semver\n    and that the alpha follows the format `a1`, `a2`, etc...\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure that the version follows semver\n    and that the prerelease follows the format `a1`, `rc2`, etc...", "file": "scripts_release.py", "function": "validate_version", "project": "rasa", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef reduce_model_alter_delete(self, operation, other):\n        \n        \n        \n        if operation.name == other.name:\n            return [other]\n        return None```\n\nUpdated code:\n```python\ndef reduce_model_alter_delete(self, operation, other):\n        \n        \n        \n        if operation.name.lower() == other.name.lower():\n            return [other]\n```\n\nOriginal documentation:\n```python\nFolds an AlterModelSomething and a DeleteModel into nothing.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Folds an AlterModelSomething and a DeleteModel into just delete.", "file": "django_db_migrations_optimizer.py", "function": "MigrationOptimizer.reduce_model_alter_delete", "project": "django", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_positions(self, symbols: Strings = None, params={}) -> List[Position]:\n        \n        \n        \n        self.load_markets()\n        paginate = False\n        paginate, params = self.handle_option_and_params(params, 'fetchPositions', 'paginate')\n        if paginate:\n            return self.fetch_paginated_call_cursor('fetchPositions', None, None, None, params, 'endId', 'idLessThan')\n        fetchPositionsOptions = self.safe_value(self.options, 'fetchPositions', {})\n        method = self.safe_string(fetchPositionsOptions, 'method', 'privateMixGetV2MixPositionAllPosition')\n        market = None\n        if symbols is not None:\n            first = self.safe_string(symbols, 0)\n            sandboxMode = self.safe_bool(self.options, 'sandboxMode', False)\n            if sandboxMode:\n                sandboxSymbol = self.convert_symbol_for_sandbox(first)\n                market = self.market(sandboxSymbol)\n            else:\n                market = self.market(first)\n        productType = None\n        productType, params = self.handle_product_type_and_params(market, params)\n        request = {\n            'productType': productType,\n        }\n        response = None\n        isHistory = False\n        if method == 'privateMixGetV2MixPositionAllPosition':\n            marginCoin = self.safe_string(params, 'marginCoin', 'USDT')\n            if symbols is not None:\n                marginCoin = market['settleId']\n            elif productType == 'USDT-FUTURES':\n                marginCoin = 'USDT'\n            elif productType == 'USDC-FUTURES':\n                marginCoin = 'USDC'\n            elif productType == 'SUSDT-FUTURES':\n                marginCoin = 'SUSDT'\n            elif productType == 'SUSDC-FUTURES':\n                marginCoin = 'SUSDC'\n            elif (productType == 'SCOIN-FUTURES') or (productType == 'COIN-FUTURES'):\n                if marginCoin is None:\n                    raise ArgumentsRequired(self.id + ' fetchPositions() requires a marginCoin parameter that matches the productType')\n            request['marginCoin'] = marginCoin\n            response = self.privateMixGetV2MixPositionAllPosition(self.extend(request, params))\n        else:\n            isHistory = True\n            if market is not None:\n                request['symbol'] = market['id']\n            response = self.privateMixGetV2MixPositionHistoryPosition(self.extend(request, params))\n        #\n        # privateMixGetV2MixPositionAllPosition\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1700807810221,\n        #         \"data\": [\n        #             {\n        #                 \"marginCoin\": \"USDT\",\n        #                 \"symbol\": \"BTCUSDT\",\n        #                 \"holdSide\": \"long\",\n        #                 \"openDelegateSize\": \"0\",\n        #                 \"marginSize\": \"3.73555\",\n        #                 \"available\": \"0.002\",\n        #                 \"locked\": \"0\",\n        #                 \"total\": \"0.002\",\n        #                 \"leverage\": \"20\",\n        #                 \"achievedProfits\": \"0\",\n        #                 \"openPriceAvg\": \"37355.5\",\n        #                 \"marginMode\": \"crossed\",\n        #                 \"posMode\": \"hedge_mode\",\n        #                 \"unrealizedPL\": \"0.03\",\n        #                 \"liquidationPrice\": \"31725.023602417\",\n        #                 \"keepMarginRate\": \"0.004\",\n        #                 \"markPrice\": \"37370.5\",\n        #                 \"marginRatio\": \"0.029550120396\",\n        #                 \"cTime\": \"1700807507275\"\n        #             }\n        #         ]\n        #     }\n        #\n        # privateMixGetV2MixPositionHistoryPosition\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1700808051002,\n        #         \"data\": {\n        #             \"list\": [\n        #                 {\n        #                     \"symbol\": \"BTCUSDT\",\n        #                     \"marginCoin\": \"USDT\",\n        #                     \"holdSide\": \"long\",\n        #                     \"openAvgPrice\": \"37272.1\",\n        #                     \"closeAvgPrice\": \"37271.4\",\n        #                     \"marginMode\": \"crossed\",\n        #                     \"openTotalPos\": \"0.001\",\n        #                     \"closeTotalPos\": \"0.001\",\n        #                     \"pnl\": \"-0.0007\",\n        #                     \"netProfit\": \"-0.0454261\",\n        #                     \"totalFunding\": \"0\",\n        #                     \"openFee\": \"-0.02236326\",\n        #                     \"closeFee\": \"-0.02236284\",\n        #                     \"utime\": \"1700720700400\",\n        #                     \"ctime\": \"1700720651684\"\n        #                 },\n        #             ],\n        #             \"endId\": \"1099351653866962944\"\n        #         }\n        #     }\n        #\n        position = []\n        if not isHistory:\n            position = self.safe_value(response, 'data', [])\n        else:\n            data = self.safe_value(response, 'data', {})\n            position = self.safe_value(data, 'list', [])\n        result = []\n        for i in range(0, len(position)):\n            result.append(self.parse_position(position[i], market))\n        symbols = self.market_symbols(symbols)\n        return self.filter_by_array_positions(result, 'symbol', symbols, False)```\n\nUpdated code:\n```python\ndef fetch_positions(self, symbols: Strings = None, params={}) -> List[Position]:\n        \n        \n        \n        self.load_markets()\n        paginate = False\n        paginate, params = self.handle_option_and_params(params, 'fetchPositions', 'paginate')\n        if paginate:\n            return self.fetch_paginated_call_cursor('fetchPositions', None, None, None, params, 'endId', 'idLessThan')\n        method = None\n        useHistoryEndpoint = self.safe_bool(params, 'useHistoryEndpoint', False)\n        if useHistoryEndpoint:\n            method = 'privateMixGetV2MixPositionHistoryPosition'\n        else:\n            method, params = self.handle_option_and_params(params, 'fetchPositions', 'method', 'privateMixGetV2MixPositionAllPosition')\n        market = None\n        if symbols is not None:\n            first = self.safe_string(symbols, 0)\n            sandboxMode = self.safe_bool(self.options, 'sandboxMode', False)\n            if sandboxMode:\n                sandboxSymbol = self.convert_symbol_for_sandbox(first)\n                market = self.market(sandboxSymbol)\n            else:\n                market = self.market(first)\n        productType = None\n        productType, params = self.handle_product_type_and_params(market, params)\n        request = {\n            'productType': productType,\n        }\n        response = None\n        isHistory = False\n        if method == 'privateMixGetV2MixPositionAllPosition':\n            marginCoin = self.safe_string(params, 'marginCoin', 'USDT')\n            if symbols is not None:\n                marginCoin = market['settleId']\n            elif productType == 'USDT-FUTURES':\n                marginCoin = 'USDT'\n            elif productType == 'USDC-FUTURES':\n                marginCoin = 'USDC'\n            elif productType == 'SUSDT-FUTURES':\n                marginCoin = 'SUSDT'\n            elif productType == 'SUSDC-FUTURES':\n                marginCoin = 'SUSDC'\n            elif (productType == 'SCOIN-FUTURES') or (productType == 'COIN-FUTURES'):\n                if marginCoin is None:\n                    raise ArgumentsRequired(self.id + ' fetchPositions() requires a marginCoin parameter that matches the productType')\n            request['marginCoin'] = marginCoin\n            response = self.privateMixGetV2MixPositionAllPosition(self.extend(request, params))\n        else:\n            isHistory = True\n            if market is not None:\n                request['symbol'] = market['id']\n            response = self.privateMixGetV2MixPositionHistoryPosition(self.extend(request, params))\n        #\n        # privateMixGetV2MixPositionAllPosition\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1700807810221,\n        #         \"data\": [\n        #             {\n        #                 \"marginCoin\": \"USDT\",\n        #                 \"symbol\": \"BTCUSDT\",\n        #                 \"holdSide\": \"long\",\n        #                 \"openDelegateSize\": \"0\",\n        #                 \"marginSize\": \"3.73555\",\n        #                 \"available\": \"0.002\",\n        #                 \"locked\": \"0\",\n        #                 \"total\": \"0.002\",\n        #                 \"leverage\": \"20\",\n        #                 \"achievedProfits\": \"0\",\n        #                 \"openPriceAvg\": \"37355.5\",\n        #                 \"marginMode\": \"crossed\",\n        #                 \"posMode\": \"hedge_mode\",\n        #                 \"unrealizedPL\": \"0.03\",\n        #                 \"liquidationPrice\": \"31725.023602417\",\n        #                 \"keepMarginRate\": \"0.004\",\n        #                 \"markPrice\": \"37370.5\",\n        #                 \"marginRatio\": \"0.029550120396\",\n        #                 \"cTime\": \"1700807507275\"\n        #             }\n        #         ]\n        #     }\n        #\n        # privateMixGetV2MixPositionHistoryPosition\n        #\n        #     {\n        #         \"code\": \"00000\",\n        #         \"msg\": \"success\",\n        #         \"requestTime\": 1700808051002,\n        #         \"data\": {\n        #             \"list\": [\n        #                 {\n        #                     \"symbol\": \"BTCUSDT\",\n        #                     \"marginCoin\": \"USDT\",\n        #                     \"holdSide\": \"long\",\n        #                     \"openAvgPrice\": \"37272.1\",\n        #                     \"closeAvgPrice\": \"37271.4\",\n        #                     \"marginMode\": \"crossed\",\n        #                     \"openTotalPos\": \"0.001\",\n        #                     \"closeTotalPos\": \"0.001\",\n        #                     \"pnl\": \"-0.0007\",\n        #                     \"netProfit\": \"-0.0454261\",\n        #                     \"totalFunding\": \"0\",\n        #                     \"openFee\": \"-0.02236326\",\n        #                     \"closeFee\": \"-0.02236284\",\n        #                     \"utime\": \"1700720700400\",\n        #                     \"ctime\": \"1700720651684\"\n        #                 },\n        #             ],\n        #             \"endId\": \"1099351653866962944\"\n        #         }\n        #     }\n        #\n        position = []\n        if not isHistory:\n            position = self.safe_list(response, 'data', [])\n        else:\n            data = self.safe_dict(response, 'data', {})\n            position = self.safe_list(data, 'list', [])\n        result = []\n        for i in range(0, len(position)):\n            result.append(self.parse_position(position[i], market))\n        symbols = self.market_symbols(symbols)\n        return self.filter_by_array_positions(result, 'symbol', symbols, False)\n```\n\nOriginal documentation:\n```python\nfetch all open positions\n        :see: https://www.bitget.com/api-doc/contract/position/get-all-position\n        :see: https://www.bitget.com/api-doc/contract/position/Get-History-Position\n        :param str[]|None symbols: list of unified market symbols\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.marginCoin]: the settle currency of the positions, needs to match the productType\n        :param str [params.productType]: 'USDT-FUTURES', 'USDC-FUTURES', 'COIN-FUTURES', 'SUSDT-FUTURES', 'SUSDC-FUTURES' or 'SCOIN-FUTURES'\n        :param boolean [params.paginate]: default False, when True will automatically paginate by calling self endpoint multiple times. See in the docs all the [available parameters](https://github.com/ccxt/ccxt/wiki/Manual#pagination-params)\n        :returns dict[]: a list of `position structure <https://docs.ccxt.com/#/?id=position-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch all open positions\n        :see: https://www.bitget.com/api-doc/contract/position/get-all-position\n        :see: https://www.bitget.com/api-doc/contract/position/Get-History-Position\n        :param str[] [symbols]: list of unified market symbols\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.marginCoin]: the settle currency of the positions, needs to match the productType\n        :param str [params.productType]: 'USDT-FUTURES', 'USDC-FUTURES', 'COIN-FUTURES', 'SUSDT-FUTURES', 'SUSDC-FUTURES' or 'SCOIN-FUTURES'\n        :param boolean [params.paginate]: default False, when True will automatically paginate by calling self endpoint multiple times. See in the docs all the [available parameters](https://github.com/ccxt/ccxt/wiki/Manual#pagination-params)\n        :param boolean [params.useHistoryEndpoint]: default False, when True  will use the historic endpoint to fetch positions\n        :param str [params.method]: either(default) 'privateMixGetV2MixPositionAllPosition' or 'privateMixGetV2MixPositionHistoryPosition'\n        :returns dict[]: a list of `position structure <https://docs.ccxt.com/#/?id=position-structure>`", "file": "python_ccxt_bitget.py", "function": "bitget.fetch_positions", "project": "ccxt", "change_stats": {"code_changes": 16, "doc_changes": 6, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _data(self):\n        \n        \n        \n        if self._cached_data is None:\n            self._cached_data = np.arange(\n                self.start, self.stop, self.step, dtype=np.int64\n            )\n        return self._cached_data```\n\nUpdated code:\n```python\ndef _data(self):\n        \n        \n        \n        return np.arange(self.start, self.stop, self.step, dtype=np.int64)\n```\n\nOriginal documentation:\n```python\nAn int array that for performance reasons is created only when needed.\n\n        The constructed array is saved in ``_cached_data``. This allows us to\n        check if the array has been created without accessing ``_data`` and\n        triggering the construction.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "An int array that for performance reasons is created only when needed.\n\n        The constructed array is saved in ``_cache``.", "file": "pandas_core_indexes_range.py", "function": "RangeIndex._data", "project": "pandas", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef verify_whitelist(self, pairlist: List[str], logmethod) -> List[str]:\n        \n        \n        \n        try:\n            whitelist = self.expanded_whitelist\n        except ValueError as err:\n            logger.error(f\"Pair blacklist contains an invalid Wildcard: {err}\")\n            return []\n        return whitelist```\n\nUpdated code:\n```python\ndef verify_whitelist(self, pairlist: List[str], logmethod,\n                         keep_invalid: bool = False) -> List[str]:\n        \n        \n        \n        if keep_invalid:\n            try:\n                whitelist = self.expanded_whitelist_keep_invalid\n            except ValueError as err:\n                logger.error(f\"Pair blacklist contains an invalid Wildcard: {err}\")\n                return []\n        else:\n            try:\n                whitelist = self.expanded_whitelist\n            except ValueError as err:\n                logger.error(f\"Pair blacklist contains an invalid Wildcard: {err}\")\n                return []\n        return whitelist\n```\n\nOriginal documentation:\n```python\nVerify and remove items from pairlist - returning a filtered pairlist.\n        Logs a warning or info depending on `aswarning`.\n        Pairlist Handlers explicitly using this method shall use\n        `logmethod=logger.info` to avoid spamming with warning messages\n        :return: pairlist - blacklisted pairs\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Verify and remove items from pairlist - returning a filtered pairlist.\n        Logs a warning or info depending on `aswarning`.\n        Pairlist Handlers explicitly using this method shall use\n        `logmethod=logger.info` to avoid spamming with warning messages\n        :param pairlist: Pairlist to validate\n        :param logmethod: Function that'll be called, `logger.info` or `logger.warning`\n        :param keep_invalid: If sets to True, drops invalid pairs silently while expanding regexes.\n        :return: pairlist - whitelisted pairs", "file": "freqtrade_plugins_pairlistmanager.py", "function": "PairListManager.verify_whitelist", "project": "freqtrade", "change_stats": {"code_changes": 22, "doc_changes": 7, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lora_apply_weights(self: torch.nn.Conv2d | torch.nn.Linear):\n    \n    \n    \n\n    current_names = getattr(self, \"lora_current_names\", ())\n    wanted_names = tuple((x.name, x.multiplier) for x in loaded_loras)\n\n    weights_backup = getattr(self, \"lora_weights_backup\", None)\n    if weights_backup is None:\n        weights_backup = self.weight.to(devices.cpu, copy=True)\n        self.lora_weights_backup = weights_backup\n\n    if current_names != wanted_names:\n        if weights_backup is not None:\n            self.weight.copy_(weights_backup)\n\n        lora_layer_name = getattr(self, 'lora_layer_name', None)\n        for lora in loaded_loras:\n            module = lora.modules.get(lora_layer_name, None)\n            if module is None:\n                continue\n\n            with torch.no_grad():\n                up = module.up.weight.to(self.weight.device, dtype=self.weight.dtype)\n                down = module.down.weight.to(self.weight.device, dtype=self.weight.dtype)\n\n                if up.shape[2:] == (1, 1) and down.shape[2:] == (1, 1):\n                    updown = (up.squeeze(2).squeeze(2) @ down.squeeze(2).squeeze(2)).unsqueeze(2).unsqueeze(3)\n                else:\n                    updown = up @ down\n\n                self.weight += updown * lora.multiplier * (module.alpha / module.up.weight.shape[1] if module.alpha else 1.0)\n\n        setattr(self, \"lora_current_names\", wanted_names)```\n\nUpdated code:\n```python\ndef lora_apply_weights(self: torch.nn.Conv2d | torch.nn.Linear | torch.nn.MultiheadAttention):\n    \n    \n    \n\n    lora_layer_name = getattr(self, 'lora_layer_name', None)\n    if lora_layer_name is None:\n        return\n\n    current_names = getattr(self, \"lora_current_names\", ())\n    wanted_names = tuple((x.name, x.multiplier) for x in loaded_loras)\n\n    weights_backup = getattr(self, \"lora_weights_backup\", None)\n    if weights_backup is None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            weights_backup = (self.in_proj_weight.to(devices.cpu, copy=True), self.out_proj.weight.to(devices.cpu, copy=True))\n        else:\n            weights_backup = self.weight.to(devices.cpu, copy=True)\n\n        self.lora_weights_backup = weights_backup\n\n    if current_names != wanted_names:\n        if weights_backup is not None:\n            if isinstance(self, torch.nn.MultiheadAttention):\n                self.in_proj_weight.copy_(weights_backup[0])\n                self.out_proj.weight.copy_(weights_backup[1])\n            else:\n                self.weight.copy_(weights_backup)\n\n        for lora in loaded_loras:\n            module = lora.modules.get(lora_layer_name, None)\n            if module is not None and hasattr(self, 'weight'):\n                self.weight += lora_calc_updown(lora, module, self.weight)\n                continue\n\n            module_q = lora.modules.get(lora_layer_name + \"_q_proj\", None)\n            module_k = lora.modules.get(lora_layer_name + \"_k_proj\", None)\n            module_v = lora.modules.get(lora_layer_name + \"_v_proj\", None)\n            module_out = lora.modules.get(lora_layer_name + \"_out_proj\", None)\n\n            if isinstance(self, torch.nn.MultiheadAttention) and module_q and module_k and module_v and module_out:\n                updown_q = lora_calc_updown(lora, module_q, self.in_proj_weight)\n                updown_k = lora_calc_updown(lora, module_k, self.in_proj_weight)\n                updown_v = lora_calc_updown(lora, module_v, self.in_proj_weight)\n                updown_qkv = torch.vstack([updown_q, updown_k, updown_v])\n\n                self.in_proj_weight += updown_qkv\n                self.out_proj.weight += lora_calc_updown(lora, module_out, self.out_proj.weight)\n                continue\n\n            if module is None:\n                continue\n\n            print(f'failed to calculate lora weights for layer {lora_layer_name}')\n\n        setattr(self, \"lora_current_names\", wanted_names)\n```\n\nOriginal documentation:\n```python\nApplies the currently selected set of Loras to the weight of torch layer self.\n    If weights already have this particular set of loras applied, does nothing.\n    If not, restores orginal weights from backup and alters weights according to loras.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Applies the currently selected set of Loras to the weights of torch layer self.\n    If weights already have this particular set of loras applied, does nothing.\n    If not, restores orginal weights from backup and alters weights according to loras.", "file": "extensions-builtin_Lora_lora.py", "function": "lora_apply_weights", "project": "stable-diffusion-webui", "change_stats": {"code_changes": 51, "doc_changes": 4, "total_changes": 55}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_gpu(array, device_id=None, stream=None):\n    \n\n    \n    check_cuda_available()\n    with get_device_from_id(device_id):\n        array_dev = get_device_from_array(array)\n        if array_dev.id == cupy.cuda.device.get_device_id():\n            return array\n\n        if stream is not None:\n            ret = cupy.empty_like(array)\n            mem = None\n            if array_dev.id == -1:\n                # cpu to gpu\n                mem = cupy.cuda.alloc_pinned_memory(array.nbytes)\n                src = numpy.frombuffer(\n                    mem, array.dtype, array.size).reshape(array.shape)\n                src[...] = array\n                ret.set(src, stream)\n            else:\n                # gpu to gpu\n                with array_dev:\n                    src = array.copy()\n                    event = cupy.cuda.Event()\n                    event.record()\n                stream.wait_event(event)\n                ret.data.copy_from_device_async(src.data, src.nbytes, stream)\n\n            # to hold a reference until the end of the asynchronous memcpy\n            stream.add_callback(lambda *x: None, (src, mem, ret))\n\n            return ret\n\n        if array_dev.id == -1:\n            return cupy.asarray(array)\n\n        # Need to make a copy when an array is copied to another device\n        return cupy.array(array, copy=True)```\n\nUpdated code:\n```python\ndef to_gpu(array, device=None, stream=None):\n    \n\n    \n    check_cuda_available()\n    if type(device) in _integer_types:\n        device = get_device_from_id(device)\n    with device:\n        array_dev = get_device_from_array(array)\n        if array_dev.id == cupy.cuda.device.get_device_id():\n            return array\n\n        if stream is not None:\n            ret = cupy.empty_like(array)\n            mem = None\n            if array_dev.id == -1:\n                # cpu to gpu\n                mem = cupy.cuda.alloc_pinned_memory(array.nbytes)\n                src = numpy.frombuffer(\n                    mem, array.dtype, array.size).reshape(array.shape)\n                src[...] = array\n                ret.set(src, stream)\n            else:\n                # gpu to gpu\n                with array_dev:\n                    src = array.copy()\n                    event = cupy.cuda.Event()\n                    event.record()\n                stream.wait_event(event)\n                ret.data.copy_from_device_async(src.data, src.nbytes, stream)\n\n            # to hold a reference until the end of the asynchronous memcpy\n            stream.add_callback(lambda *x: None, (src, mem, ret))\n\n            return ret\n\n        if array_dev.id == -1:\n            return cupy.asarray(array)\n\n        # Need to make a copy when an array is copied to another device\n        return cupy.array(array, copy=True)\n```\n\nOriginal documentation:\n```python\nCopies the given CPU array to specified device.\n\n    Args:\n        array: Array to be sent to GPU.\n        device_id (int): A target device ID.\n        stream (cupy.cuda.Stream): CUDA stream. If not ``None``, the copy runs\n            asynchronously.\n\n    Returns:\n        cupy.ndarray: Array on GPU.\n\n        If ``array`` is already on GPU, then this function just returns\n        ``array`` without performing any copy. Note that this function does not\n        copy :class:`cupy.ndarray` into specified device.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Copies the given CPU array to specified device.\n\n    Args:\n        array: Array to be sent to GPU.\n        device: Device specifier.\n        stream (cupy.cuda.Stream): CUDA stream. If not ``None``, the copy runs\n            asynchronously.\n\n    Returns:\n        cupy.ndarray: Array on GPU.\n\n        If ``array`` is already on GPU, then this function just returns\n        ``array`` without performing any copy. Note that this function does not\n        copy :class:`cupy.ndarray` into specified device.", "file": "chainer_cuda.py", "function": "to_gpu", "project": "chainer", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef format_evidence_causing_this_alert(\n            self,\n            all_evidence: Dict[str, dict],\n            profileid: str,\n            twid: str,\n            flow_datetime: str\n    ) -> str:\n        \n        \n        \n        # alerts in slips consists of several evidence,\n        # each evidence has a threat_level\n        # once we reach a certain threshold of accumulated\n        # threat_levels, we produce an alert\n        # Now instead of printing the last evidence only,\n        # we print all of them\n        try:\n            twid_num = twid.split('timewindow')[1]\n            srcip = profileid.split(self.separator)[1]\n            # Get the start time of this TW\n            twid_start_time = None\n            while twid_start_time is None:\n                # give the database time to retreive the time\n                twid_start_time = self.db.getTimeTW(profileid, twid)\n\n            tw_start_time_str = utils.convert_format(twid_start_time,\n                                                     '%Y/%m/%d %H:%M:%S')\n            # datetime obj\n            tw_start_time_datetime = utils.convert_to_datetime(tw_start_time_str)\n\n            # Convert the tw width to deltatime\n            # tw_width_in_seconds_delta = timedelta(seconds=int(self.width))\n            delta_width = utils.to_delta(self.width)\n\n            # Get the stop time of the TW\n            tw_stop_time_datetime = (tw_start_time_datetime + delta_width)\n\n            tw_stop_time_str = utils.convert_format(\n                tw_stop_time_datetime,\n                 '%Y/%m/%d %H:%M:%S'\n            )\n\n            hostname = self.db.get_hostname_from_profile(profileid)\n            # if there's no hostname, set it as ' '\n            hostname = hostname or ''\n            if hostname:\n                hostname = f'({hostname})'\n\n            alert_to_print = (\n                f'{Fore.RED}IP {srcip} {hostname} detected as malicious in timewindow {twid_num} '\n                f'(start {tw_start_time_str}, stop {tw_stop_time_str}) \\n'\n                f'given the following evidence:{Style.RESET_ALL}\\n'\n            )\n        except Exception:\n            exception_line = sys.exc_info()[2].tb_lineno\n            self.print(\n                f'Problem on format_evidence_causing_this_alert() '\n                f'line {exception_line}',0,1,\n            )\n            self.print(traceback.print_exc(),0,1)\n            return True\n\n        for evidence in all_evidence.values():\n            evidence: dict\n            description: str = evidence.get('description')\n\n            evidence_string = f'Detected {description}'\n            evidence_string = self.line_wrap(evidence_string)\n\n            alert_to_print += (\n                f'\\t{Fore.CYAN}- {evidence_string}{Style.RESET_ALL}\\n'\n            )\n\n        # Add the timestamp to the alert. The datetime printed will be of the last evidence only\n        readable_datetime = utils.convert_format(flow_datetime, utils.alerts_format)\n        alert_to_print = f'{Fore.RED}{readable_datetime}{Style.RESET_ALL} {alert_to_print}'\n        return alert_to_print```\n\nUpdated code:\n```python\ndef format_evidence_causing_this_alert(\n            self,\n            all_evidence: Dict[str, Evidence],\n            profileid: ProfileID,\n            twid: TimeWindow,\n            flow_datetime: str\n    ) -> str:\n        \n        \n        \n        # alerts in slips consists of several evidence,\n        # each evidence has a threat_level\n        # once we reach a certain threshold of accumulated\n        # threat_levels, we produce an alert\n        # Now instead of printing the last evidence only,\n        # we print all of them\n        # Get the start and end time of this TW\n        twid_start_time: Optional[float] = \\\n            self.db.get_timewindow_start_time(str(profileid), str(twid))\n        tw_stop_time: float = twid_start_time + self.width\n\n        # format them both for printing\n        time_format = '%Y/%m/%d %H:%M:%S'\n        twid_start_time: str = utils.convert_format(\n            twid_start_time, time_format\n        )\n        tw_stop_time: str = utils.convert_format(\n            tw_stop_time, time_format\n        )\n\n        alert_to_print = f'IP {profileid.ip} '\n        hostname: Optional[str] = self.db.get_hostname_from_profile(\n            str(profileid)\n            )\n        if hostname:\n            alert_to_print += f'({hostname}) '\n\n        alert_to_print += (\n                f\"detected as malicious in timewindow {twid.number} \"\n                f\"(start {twid_start_time}, stop {tw_stop_time}) \\n\"\n                f\"given the following evidence:\\n\"\n        )\n        alert_to_print: str = red(alert_to_print)\n\n        for evidence in all_evidence.values():\n            evidence: Evidence\n            description: str = evidence.description\n            evidence_string = self.line_wrap(f'Detected {description}')\n            alert_to_print += cyan(f'\\t- {evidence_string}\\n')\n\n        # Add the timestamp to the alert.\n        # The datetime printed will be of the last evidence only\n        readable_datetime: str = utils.convert_format(\n            flow_datetime,\n            utils.alerts_format\n            )\n        alert_to_print: str = red(f'{readable_datetime} ') + alert_to_print\n        return alert_to_print\n```\n\nOriginal documentation:\n```python\nFunction to format the string with all evidence causing an alert\n        flow_datetime: time of the last evidence received\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Function to format the string with all evidence causing an alert\n        : param flow_datetime: time of the last evidence received", "file": "slips_files_core_evidencehandler.py", "function": "EvidenceHandler.format_evidence_causing_this_alert", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 95, "doc_changes": 4, "total_changes": 99}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef latex_matches(self, text):\n        \n        \n        slashpos = text.rfind('\\\\')\n        if slashpos > -1:\n            s = text[slashpos:]\n            if s in latex_symbols:\n                # Try to complete a full latex symbol to unicode\n                # \\\\alpha -> \u03b1\n                return s, [latex_symbols[s]]\n            else:\n                # If a user has partially typed a latex symbol, give them\n                # a full list of options \\al -> [\\aleph, \\alpha]\n                matches = [k for k in latex_symbols if k.startswith(s)]\n                if matches:\n                    return s, matches\n        return u'', []```\n\nUpdated code:\n```python\ndef latex_matches(self, text:str) -> Tuple[str, Sequence[str]]:\n        \n        \n        slashpos = text.rfind('\\\\')\n        if slashpos > -1:\n            s = text[slashpos:]\n            if s in latex_symbols:\n                # Try to complete a full latex symbol to unicode\n                # \\\\alpha -> \u03b1\n                return s, [latex_symbols[s]]\n            else:\n                # If a user has partially typed a latex symbol, give them\n                # a full list of options \\al -> [\\aleph, \\alpha]\n                matches = [k for k in latex_symbols if k.startswith(s)]\n                if matches:\n                    return s, matches\n        return '', ()\n```\n\nOriginal documentation:\n```python\nu\"\"\"Match Latex syntax for unicode characters.\n\n        This does both ``\\\\alp`` -> ``\\\\alpha`` and ``\\\\alpha`` -> ``\u03b1``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Match Latex syntax for unicode characters.\n\n        This does both ``\\\\alp`` -> ``\\\\alpha`` and ``\\\\alpha`` -> ``\u03b1``", "file": "IPython_core_completer.py", "function": "IPCompleter.latex_matches", "project": "ipython", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n        \n        \n        \n\n        if text is None and images is None:\n            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n\n        if text is not None:\n            encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)\n\n        if images is not None:\n            image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n\n        if text is not None and images is not None:\n            encoding[\"pixel_values\"] = image_features.pixel_values\n            return encoding\n        elif text is not None:\n            return encoding\n        else:\n            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)```\n\nUpdated code:\n```python\ndef __call__(self, text=None, images=None, visual_prompt=None, return_tensors=None, **kwargs):\n        \n        \n        \n        if text is None and visual_prompt is None and images is None:\n            raise ValueError(\"You have to specify either text, visual prompt or images.\")\n\n        if text is not None and visual_prompt is not None:\n            raise ValueError(\"You have to specify exactly one type of prompt. Either text or visual prompt.\")\n\n        if text is not None:\n            encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)\n\n        if visual_prompt is not None:\n            prompt_features = self.image_processor(visual_prompt, return_tensors=return_tensors, **kwargs)\n\n        if images is not None:\n            image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n\n        if visual_prompt is not None and images is not None:\n            encoding = {\n                \"pixel_values\": image_features.pixel_values,\n                \"conditional_pixel_values\": prompt_features.pixel_values,\n            }\n            return encoding\n        elif text is not None and images is not None:\n            encoding[\"pixel_values\"] = image_features.pixel_values\n            return encoding\n        elif text is not None:\n            return encoding\n        elif visual_prompt is not None:\n            encoding = {\n                \"conditional_pixel_values\": prompt_features.pixel_values,\n            }\n            return encoding\n        else:\n            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n```\n\nOriginal documentation:\n```python\nMain method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n        and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n        ViTImageProcessor's [`~ViTImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring of\n        the above two methods for more information.\n\n        Args:\n            text (`str`, `List[str]`, `List[List[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                number of channels, H and W are image height and width.\n\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n\n                - `'tf'`: Return TensorFlow `tf.constant` objects.\n                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                - `'np'`: Return NumPy `np.ndarray` objects.\n                - `'jax'`: Return JAX `jnp.ndarray` objects.\n\n        Returns:\n            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n        and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n        ViTImageProcessor's [`~ViTImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring of\n        the above two methods for more information.\n\n        Args:\n            text (`str`, `List[str]`, `List[List[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                number of channels, H and W are image height and width.\n            visual_prompt (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                The visual prompt image or batch of images to be prepared. Each visual prompt image can be a PIL image,\n                NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape\n                (C, H, W), where C is a number of channels, H and W are image height and width.\n\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n\n                - `'tf'`: Return TensorFlow `tf.constant` objects.\n                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                - `'np'`: Return NumPy `np.ndarray` objects.\n                - `'jax'`: Return JAX `jnp.ndarray` objects.\n\n        Returns:\n            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.", "file": "src_transformers_models_clipseg_processing_clipseg.py", "function": "CLIPSegProcessor.__call__", "project": "transformers", "change_stats": {"code_changes": 26, "doc_changes": 6, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_data_interval(self):\n        ''\n        raise NotImplementedError('Derived must override')```\n\nUpdated code:\n```python\ndef get_data_interval(self):\n        \n        raise NotImplementedError('Derived must override')\n```\n\nOriginal documentation:\n```python\nreturn the Interval instance for this axis data limits\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the Interval instance for this axis data limits.", "file": "lib_matplotlib_axis.py", "function": "Axis.get_data_interval", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef delay_load(self, Gamma0: NumberLike, d: Number, unit: str = 'deg', **kwargs) -> Network:\n        \n        \n        return self.line(d=d, unit=unit,**kwargs)**\\\n                self.load(Gamma0=Gamma0,**kwargs)```\n\nUpdated code:\n```python\ndef delay_load(self, Gamma0: NumberLike, d: Number, unit: str = 'deg', **kwargs) -> Network:\n        \n        \n        return self.line(d=d, unit=unit, **kwargs) ** self.load(Gamma0=Gamma0, **kwargs)\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Delayed load.\n\n        A load with reflection coefficient `Gamma0` at the end of a\n        matched line of length `d`.\n\n        Parameters\n        ----------\n        Gamma0 : number, array-like\n            reflection coefficient of load (not in dB)\n        d : number\n            the length of transmission line (see unit argument)\n        unit : ['deg','rad','m','cm','um','in','mil','s','us','ns','ps']\n            the units of d.  See :func:`to_meters`, for details\n        \\*\\*kwargs : key word arguments\n            passed to :func:`match`, which is called initially to create a\n            'blank' network.\n\n        Returns\n        -------\n        delay_load : :class:`~skrf.network.Network` object\n                a delayed load\n\n        Examples\n        ----------\n        >>> my_media.delay_load(-.5, 90, 'deg', Z0=50)\n\n        Note\n        ----\n        This calls ::\n\n            line(d, unit, **kwargs) ** load(Gamma0, **kwargs)\n\n        See Also\n        --------\n        line : creates the network for line\n        load : creates the network for the load\n        delay_short\n        delay_open\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Delayed load.\n\n        A load with reflection coefficient `Gamma0` at the end of a\n        matched line of length `d`.\n\n        Parameters\n        ----------\n        Gamma0 : number, array-like\n            reflection coefficient of load (not in dB)\n        d : number\n            the length of transmission line (see unit argument)\n        unit : ['deg','rad','m','cm','um','in','mil','s','us','ns','ps']\n            the units of d.  See :func:`to_meters`, for details\n        \\*\\*kwargs : key word arguments\n            passed to :func:`line` and :func:`load`\n\n        Returns\n        -------\n        delay_load : :class:`~skrf.network.Network` object\n                a delayed load\n\n        Examples\n        ----------\n        >>> my_media.delay_load(-.5, 90, 'deg', Z0=50)\n\n        Notes\n        -----\n        This calls ::\n\n            line(d, unit, **kwargs) ** load(Gamma0, **kwargs)\n\n        See Also\n        --------\n        line : creates the network for line\n        load : creates the network for the load\n        delay_short\n        delay_open", "file": "skrf_media_media.py", "function": "Media.delay_load", "project": "scikit-rf", "change_stats": {"code_changes": 5, "doc_changes": 9, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef stoploss_limit(self, pair: str, amount: float, stop_price: float, rate: float) -> Dict:\n        \n        \n        \n        ordertype = \"stop_loss_limit\"\n\n        stop_price = self.symbol_price_prec(pair, stop_price)\n\n        # Ensure rate is less than stop price\n        if stop_price <= rate:\n            raise OperationalException(\n                'In stoploss limit order, stop price should be more than limit price')\n\n        if self._config['dry_run']:\n            dry_order = self.dry_run_order(\n                pair, ordertype, \"sell\", amount, stop_price)\n            return dry_order\n\n        params = self._params.copy()\n        params.update({'stopPrice': stop_price})\n\n        order = self.create_order(pair, ordertype, 'sell', amount, rate, params)\n        logger.info('stoploss limit order added for %s. '\n                    'stop price: %s. limit: %s', pair, stop_price, rate)\n        return order```\n\nUpdated code:\n```python\ndef stoploss_limit(self, pair: str, amount: float, stop_price: float, rate: float) -> Dict:\n        \n        \n        \n\n        raise OperationalException(f\"stoploss_limit is not implemented for {self.name}.\")\n```\n\nOriginal documentation:\n```python\ncreates a stoploss limit order.\n        NOTICE: it is not supported by all exchanges. only binance is tested for now.\n        TODO: implementation maybe needs to be moved to the binance subclass\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "creates a stoploss limit order.\n        Since ccxt does not unify stoploss-limit orders yet, this needs to be implemented in each\n        exchange's subclass.\n        The exception below should never raise, since we disallow\n        starting the bot in validate_ordertypes()\n        Note: Changes to this interface need to be applied to all sub-classes too.", "file": "freqtrade_exchange_exchange.py", "function": "Exchange.stoploss_limit", "project": "freqtrade", "change_stats": {"code_changes": 23, "doc_changes": 9, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef gemv(transa, alpha, a, x, beta, y):\n    \n    \n    dtype = a.dtype.char\n    if dtype == 'f':\n        func = cublas.sgemv\n    elif dtype == 'd':\n        func = cublas.dgemv\n    elif dtype == 'F':\n        func = cublas.cgemv\n    elif dtype == 'D':\n        func = cublas.zgemv\n    else:\n        raise TypeError('invalid dtype')\n    assert a.ndim == 2\n    assert x.ndim == y.ndim == 1\n    assert a.dtype == x.dtype == y.dtype\n    m, n = a.shape\n    if transa == 'N' or transa == 0:\n        transa = cublas.CUBLAS_OP_N\n        xlen, ylen = n, m\n    elif transa == 'T' or transa == 1:\n        transa = cublas.CUBLAS_OP_T\n        xlen, ylen = m, n\n    elif transa == 'C' or transa == 2:\n        transa = cublas.CUBLAS_OP_C\n        xlen, ylen = m, n\n    else:\n        raise TypeError('invalid transa (actual: {})'.fromat(transa))\n    assert x.shape[0] == xlen\n    assert y.shape[0] == ylen\n\n    alpha, alpha_ptr = _get_scalar_ptr(alpha, a.dtype)\n    beta, beta_ptr = _get_scalar_ptr(beta, a.dtype)\n    handle = device.get_cublas_handle()\n    orig_mode = cublas.getPointerMode(handle)\n    if isinstance(alpha, cupy.ndarray) or isinstance(beta, cupy.ndarray):\n        if not isinstance(alpha, cupy.ndarray):\n            alpha = cupy.array(alpha)\n            alpha_ptr = alpha.data.ptr\n        if not isinstance(beta, cupy.ndarray):\n            beta = cupy.array(beta)\n            beta_ptr = beta.data.ptr\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_DEVICE)\n    else:\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_HOST)\n\n    if a._f_contiguous:\n        func(handle, transa, m, n, alpha_ptr, a.data.ptr, m, x.data.ptr, 1,\n             beta_ptr, y.data.ptr, 1)\n    elif a._c_contiguous and transa != cublas.CUBLAS_OP_C:\n        if transa == cublas.CUBLAS_OP_N:\n            transa = cublas.CUBLAS_OP_T\n        else:\n            transa = cublas.CUBLAS_OP_N\n        func(handle, transa, n, m, alpha_ptr, a.data.ptr, n, x.data.ptr, 1,\n             beta_ptr, y.data.ptr, 1)\n    else:\n        a = a.copy(order='F')\n        func(handle, transa, m, n, alpha_ptr, a.data.ptr, m, x.data.ptr, 1,\n             beta_ptr, y.data.ptr, 1)\n    cublas.setPointerMode(handle, orig_mode)```\n\nUpdated code:\n```python\ndef gemv(transa, alpha, a, x, beta, y):\n    \n    \n    dtype = a.dtype.char\n    if dtype == 'f':\n        func = cublas.sgemv\n    elif dtype == 'd':\n        func = cublas.dgemv\n    elif dtype == 'F':\n        func = cublas.cgemv\n    elif dtype == 'D':\n        func = cublas.zgemv\n    else:\n        raise TypeError('invalid dtype')\n    assert a.ndim == 2\n    assert x.ndim == y.ndim == 1\n    assert a.dtype == x.dtype == y.dtype\n    m, n = a.shape\n    if transa == 'N':\n        transa = cublas.CUBLAS_OP_N\n        xlen, ylen = n, m\n    elif transa == 'T':\n        transa = cublas.CUBLAS_OP_T\n        xlen, ylen = m, n\n    elif transa == 'H':\n        transa = cublas.CUBLAS_OP_C\n        xlen, ylen = m, n\n    else:\n        raise TypeError('invalid transa (actual: {})'.fromat(transa))\n    assert x.shape[0] == xlen\n    assert y.shape[0] == ylen\n\n    alpha, alpha_ptr = _get_scalar_ptr(alpha, a.dtype)\n    beta, beta_ptr = _get_scalar_ptr(beta, a.dtype)\n    handle = device.get_cublas_handle()\n    orig_mode = cublas.getPointerMode(handle)\n    if isinstance(alpha, cupy.ndarray) or isinstance(beta, cupy.ndarray):\n        if not isinstance(alpha, cupy.ndarray):\n            alpha = cupy.array(alpha)\n            alpha_ptr = alpha.data.ptr\n        if not isinstance(beta, cupy.ndarray):\n            beta = cupy.array(beta)\n            beta_ptr = beta.data.ptr\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_DEVICE)\n    else:\n        cublas.setPointerMode(handle, cublas.CUBLAS_POINTER_MODE_HOST)\n\n    if a._f_contiguous:\n        func(handle, transa, m, n, alpha_ptr, a.data.ptr, m, x.data.ptr, 1,\n             beta_ptr, y.data.ptr, 1)\n    elif a._c_contiguous and transa != cublas.CUBLAS_OP_C:\n        if transa == cublas.CUBLAS_OP_N:\n            transa = cublas.CUBLAS_OP_T\n        else:\n            transa = cublas.CUBLAS_OP_N\n        func(handle, transa, n, m, alpha_ptr, a.data.ptr, n, x.data.ptr, 1,\n             beta_ptr, y.data.ptr, 1)\n    else:\n        a = a.copy(order='F')\n        func(handle, transa, m, n, alpha_ptr, a.data.ptr, m, x.data.ptr, 1,\n             beta_ptr, y.data.ptr, 1)\n    cublas.setPointerMode(handle, orig_mode)\n```\n\nOriginal documentation:\n```python\nComputes y = alpha * op(a) @ x + beta * y\n\n    op(a) = a if transa is 0 or 'N', op(a) = a.T if transa is 1 or 'T',\n    op(a) = a.T.conj() if transa is 2 or 'C'.\n\n    Note: ''y'' will be updated.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes y = alpha * op(a) @ x + beta * y\n\n    op(a) = a if transa is 'N', op(a) = a.T if transa is 'T',\n    op(a) = a.T.conj() if transa is 'H'.\n\n    Note: ''y'' will be updated.", "file": "cupy_cublas.py", "function": "gemv", "project": "cupy", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_czce_receipt_1(date: str = None, vars_list: List = cons.contract_symbols):\n    \n        \n    \n    date = cons.convert_date(date).strftime('%Y%m%d') if date is not None else datetime.date.today()\n    if date not in calendar:\n        warnings.warn('%s\u975e\u4ea4\u6613\u65e5' % date.strftime('%Y%m%d'))\n        return None\n    if date == '20090820':\n        return pd.DataFrame()\n    url = cons.CZCE_RECEIPT_URL_1 % date\n    r = requests_link(url, encoding='utf-8')\n    context = r.text\n    data = pd.read_html(context)[1]\n    records = pd.DataFrame()\n    indexes = [x for x in data.index if '\u54c1\u79cd\uff1a' in str(data[0].tolist()[x])]\n    ends = [x for x in data.index if '\u603b\u8ba1' in str(data[0].tolist()[x])]\n    for i in list(range(len(indexes))):\n        if i != len(indexes) - 1:\n            data_cut = data.loc[indexes[i]:ends[i], :]\n            data_cut = data_cut.fillna(method='pad')\n        else:\n            data_cut = data.loc[indexes[i]:, :]\n            data_cut = data_cut.fillna(method='pad')\n        if 'PTA' in data_cut[0].tolist()[0]:\n            var = 'TA'\n        else:\n            var = chinese_to_english(re.sub(r'[A-Z]+', '', data_cut[0].tolist()[0][3:]))\n        if var == 'CF':\n            receipt = data_cut[6].tolist()[-1]\n            receipt_chg = data_cut[7].tolist()[-1]\n        else:\n            receipt = data_cut[5].tolist()[-1]\n            receipt_chg = data_cut[6].tolist()[-1]\n        data_dict = {'var': var, 'receipt': int(receipt), 'receipt_chg': int(receipt_chg), 'date': date}\n        records = records.append(pd.DataFrame(data_dict, index=[0]))\n    if len(records.index) != 0:\n        records.index = records['var']\n        vars_in_market = [i for i in vars_list if i in records.index]\n        records = records.loc[vars_in_market, :]\n    return records.reset_index(drop=True)```\n\nUpdated code:\n```python\ndef get_czce_receipt_1(date: str = None, vars_list: List = cons.contract_symbols):\n    \n    \n    \n    date = cons.convert_date(date).strftime('%Y%m%d') if date is not None else datetime.date.today()\n    if date not in calendar:\n        warnings.warn('%s\u975e\u4ea4\u6613\u65e5' % date.strftime('%Y%m%d'))\n        return None\n    if date == '20090820':\n        return pd.DataFrame()\n    url = cons.CZCE_RECEIPT_URL_1 % date\n    r = requests_link(url, encoding='utf-8')\n    context = r.text\n    data = pd.read_html(context)[1]\n    records = pd.DataFrame()\n    indexes = [x for x in data.index if '\u54c1\u79cd\uff1a' in str(data[0].tolist()[x])]\n    ends = [x for x in data.index if '\u603b\u8ba1' in str(data[0].tolist()[x])]\n    for i in list(range(len(indexes))):\n        if i != len(indexes) - 1:\n            data_cut = data.loc[indexes[i]:ends[i], :]\n            data_cut = data_cut.fillna(method='pad')\n        else:\n            data_cut = data.loc[indexes[i]:, :]\n            data_cut = data_cut.fillna(method='pad')\n        if 'PTA' in data_cut[0].tolist()[0]:\n            var = 'TA'\n        else:\n            var = chinese_to_english(re.sub(r'[A-Z]+', '', data_cut[0].tolist()[0][3:]))\n        if var == 'CF':\n            receipt = data_cut[6].tolist()[-1]\n            receipt_chg = data_cut[7].tolist()[-1]\n        else:\n            receipt = data_cut[5].tolist()[-1]\n            receipt_chg = data_cut[6].tolist()[-1]\n        data_dict = {'var': var, 'receipt': int(receipt), 'receipt_chg': int(receipt_chg), 'date': date}\n        records = records.append(pd.DataFrame(data_dict, index=[0]))\n    if len(records.index) != 0:\n        records.index = records['var']\n        vars_in_market = [i for i in vars_list if i in records.index]\n        records = records.loc[vars_in_market, :]\n    return records.reset_index(drop=True)\n```\n\nOriginal documentation:\n```python\n\u6293\u53d6\u90d1\u5dde\u5546\u54c1\u4ea4\u6613\u6240\u6ce8\u518c\u4ed3\u5355\u6570\u636e\n        \u9002\u752820080222\u81f320100824(\u5305\u62ec)\n        Parameters\n        ------\n            date: \u5f00\u59cb\u65e5\u671f format\uff1aYYYY-MM-DD \u6216 YYYYMMDD \u6216 datetime.date\u5bf9\u8c61 \u4e3a\u7a7a\u65f6\u4e3a\u5f53\u5929\n            vars_list: \u5408\u7ea6\u54c1\u79cd\u5982CF\u3001TA\u7b49\u5217\u8868 \u4e3a\u7a7a\u65f6\u4e3a\u6240\u6709\u5546\u54c1\n        Return\n        -------\n            DataFrame:\n                \u5c55\u671f\u6536\u76ca\u7387\u6570\u636e(DataFrame):\n                    var             \u5546\u54c1\u54c1\u79cd                     string\n                    receipt         \u4ed3\u5355\u6570                       int\n                    date            \u65e5\u671f                         string YYYYMMDD\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u6293\u53d6\u90d1\u5dde\u5546\u54c1\u4ea4\u6613\u6240\u6ce8\u518c\u4ed3\u5355\u6570\u636e\n    \u9002\u752820080222\u81f320100824(\u5305\u62ec)\n    :param date: \u5f00\u59cb\u65e5\u671f format\uff1aYYYY-MM-DD \u6216 YYYYMMDD \u6216 datetime.date\u5bf9\u8c61 \u4e3a\u7a7a\u65f6\u4e3a\u5f53\u5929\n    :type date: str\n    :param vars_list: list\n    :type vars_list: \u5408\u7ea6\u54c1\u79cd\u5982CF\u3001TA\u7b49\u5217\u8868 \u4e3a\u7a7a\u65f6\u4e3a\u6240\u6709\u5546\u54c1\n    :return: \u5c55\u671f\u6536\u76ca\u7387\u6570\u636e\n    :rtype: pandas.DataFrame\n    var             \u5546\u54c1\u54c1\u79cd                     string\n    receipt         \u4ed3\u5355\u6570                       int\n    date            \u65e5\u671f                         string YYYYMMDD", "file": "akshare_futures_receipt.py", "function": "get_czce_receipt_1", "project": "akshare", "change_stats": {"code_changes": 4, "doc_changes": 24, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef reconcile_against_document(args):\n\t\n\t\t\n\t\n\tfor d in args:\n\n\t\tcheck_if_advance_entry_modified(d)\n\t\tvalidate_allocated_amount(d)\n\n\t\t# cancel advance entry\n\t\tdoc = frappe.get_doc(d.voucher_type, d.voucher_no)\n\n\t\tfrappe.flags.ignore_party_validation = True\n\t\tdoc.make_gl_entries(cancel=1, adv_adj=1)\n\n\t\t# update ref in advance entry\n\t\tif d.voucher_type == \"Journal Entry\":\n\t\t\tupdate_reference_in_journal_entry(d, doc)\n\t\telse:\n\t\t\tupdate_reference_in_payment_entry(d, doc)\n\n\t\t# re-submit advance entry\n\t\tdoc = frappe.get_doc(d.voucher_type, d.voucher_no)\n\t\tdoc.make_gl_entries(cancel = 0, adv_adj =1)\n\t\tfrappe.flags.ignore_party_validation = False\n\n\t\tif d.voucher_type in ('Payment Entry', 'Journal Entry'):\n\t\t\tdoc.update_expense_claim()```\n\nUpdated code:\n```python\ndef reconcile_against_document(args):\n\t\n\t\t\n\t\n\t# To optimize making GL Entry for PE or JV with multiple references\n\treconciled_entries = {}\n\tfor row in args:\n\t\tif not reconciled_entries.get((row.voucher_type, row.voucher_no)):\n\t\t\treconciled_entries[(row.voucher_type, row.voucher_no)] = []\n\n\t\treconciled_entries[(row.voucher_type, row.voucher_no)].append(row)\n\n\tfor key, entries in reconciled_entries.items():\n\t\tvoucher_type = key[0]\n\t\tvoucher_no = key[1]\n\n\t\t# cancel advance entry\n\t\tdoc = frappe.get_doc(voucher_type, voucher_no)\n\t\tfrappe.flags.ignore_party_validation = True\n\t\tdoc.make_gl_entries(cancel=1, adv_adj=1)\n\n\t\tfor entry in entries:\n\t\t\tcheck_if_advance_entry_modified(entry)\n\t\t\tvalidate_allocated_amount(entry)\n\n\t\t\t# update ref in advance entry\n\t\t\tif voucher_type == \"Journal Entry\":\n\t\t\t\tupdate_reference_in_journal_entry(entry, doc, do_not_save=True)\n\t\t\telse:\n\t\t\t\tupdate_reference_in_payment_entry(entry, doc, do_not_save=True)\n\n\t\tdoc.save(ignore_permissions=True)\n\t\t# re-submit advance entry\n\t\tdoc = frappe.get_doc(entry.voucher_type, entry.voucher_no)\n\t\tdoc.make_gl_entries(cancel = 0, adv_adj =1)\n\t\tfrappe.flags.ignore_party_validation = False\n\n\t\tif entry.voucher_type in ('Payment Entry', 'Journal Entry'):\n\t\t\tdoc.update_expense_claim()\n```\n\nOriginal documentation:\n```python\nCancel JV, Update aginst document, split if required and resubmit jv\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Cancel PE or JV, Update against document, split if required and resubmit", "file": "erpnext_accounts_utils.py", "function": "reconcile_against_document", "project": "erpnext", "change_stats": {"code_changes": 37, "doc_changes": 4, "total_changes": 41}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _annotate_landmarks(self, image: np.ndarray, landmarks: np.ndarray) -> None:\n         \n        \n        # Mesh\n        for area, indices in self._mesh_areas.items():\n            fill = area in (\"right_eye\", \"left_eye\", \"mouth\")\n            cv2.polylines(image, [landmarks[indices[0]:indices[1]]], fill, (255, 255, 0), 1)\n        # Landmarks\n        for (pos_x, pos_y) in landmarks:\n            cv2.circle(image, (pos_x, pos_y), 1, (0, 255, 255), -1)```\n\nUpdated code:\n```python\ndef _annotate_landmarks(self, image: np.ndarray, landmarks: np.ndarray) -> None:\n         \n        \n        # Mesh\n        for start, end, fill in LANDMARK_PARTS[LandmarkType.from_shape(landmarks.shape)].values():\n            cv2.polylines(image, [landmarks[start:end]], fill, (255, 255, 0), 1)\n        # Landmarks\n        for (pos_x, pos_y) in landmarks:\n            cv2.circle(image, (pos_x, pos_y), 1, (0, 255, 255), -1)\n```\n\nOriginal documentation:\n```python\nAnnotate the extract boxes onto the frame.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The frame that extract boxes are to be annotated on to\n        landmarks: :class:`numpy.ndarray`\n            The 68 point landmarks that are to be annotated onto the frame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Annotate the extract boxes onto the frame.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The frame that extract boxes are to be annotated on to\n        landmarks: :class:`numpy.ndarray`\n            The facial landmarks that are to be annotated onto the frame", "file": "tools_alignments_jobs_frames.py", "function": "Draw._annotate_landmarks", "project": "faceswap", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load(\n            self, path, offset=None, duration=None,\n            sample_rate=None, dtype=np.float32):\n         \n        \n        _check_ffmpeg_install()\n        if not isinstance(path, str):\n            path = path.decode()\n        try:\n            probe = ffmpeg.probe(path)\n        except ffmpeg._run.Error as e:\n            raise SpleeterError(\n                'An error occurs with ffprobe (see ffprobe output below)\\n\\n{}'\n                .format(e.stderr.decode()))\n        if 'streams' not in probe or len(probe['streams']) == 0:\n            raise SpleeterError('No stream was found with ffprobe')\n        metadata = next(\n            stream\n            for stream in probe['streams']\n            if stream['codec_type'] == 'audio')\n        n_channels = metadata['channels']\n        if sample_rate is None:\n            sample_rate = metadata['sample_rate']\n        output_kwargs = {'format': 'f32le', 'ar': sample_rate}\n        if duration is not None:\n            output_kwargs['t'] = _to_ffmpeg_time(duration)\n        if offset is not None:\n            output_kwargs['ss'] = _to_ffmpeg_time(offset)\n        process = (\n            ffmpeg\n            .input(path)\n            .output('pipe:', **output_kwargs)\n            .run_async(pipe_stdout=True, pipe_stderr=True))\n        buffer, _ = process.communicate()\n        waveform = np.frombuffer(buffer, dtype='<f4').reshape(-1, n_channels)\n        if not waveform.dtype == np.dtype(dtype):\n            waveform = waveform.astype(dtype)\n        return (waveform, sample_rate)```\n\nUpdated code:\n```python\ndef load(\n            _,\n            path: Union[Path, str],\n            offset: Optional[float] = None,\n            duration: Optional[float] = None,\n            sample_rate: Optional[float] = None,\n            dtype: np.dtype = np.float32) -> Signal:\n        \n            \n        \n        if isinstance(path, Path):\n            path = str(path)\n        if not isinstance(path, str):\n            path = path.decode()\n        try:\n            probe = ffmpeg.probe(path)\n        except ffmpeg._run.Error as e:\n            raise SpleeterError(\n                'An error occurs with ffprobe (see ffprobe output below)\\n\\n{}'\n                .format(e.stderr.decode()))\n        if 'streams' not in probe or len(probe['streams']) == 0:\n            raise SpleeterError('No stream was found with ffprobe')\n        metadata = next(\n            stream\n            for stream in probe['streams']\n            if stream['codec_type'] == 'audio')\n        n_channels = metadata['channels']\n        if sample_rate is None:\n            sample_rate = metadata['sample_rate']\n        output_kwargs = {'format': 'f32le', 'ar': sample_rate}\n        if duration is not None:\n            output_kwargs['t'] = str(dt.timedelta(seconds=duration))\n        if offset is not None:\n            output_kwargs['ss'] = str(dt.timedelta(seconds=offset))\n        process = (\n            ffmpeg\n            .input(path)\n            .output('pipe:', **output_kwargs)\n            .run_async(pipe_stdout=True, pipe_stderr=True))\n        buffer, _ = process.communicate()\n        waveform = np.frombuffer(buffer, dtype='<f4').reshape(-1, n_channels)\n        if not waveform.dtype == np.dtype(dtype):\n            waveform = waveform.astype(dtype)\n        return (waveform, sample_rate)\n```\n\nOriginal documentation:\n```python\nLoads the audio file denoted by the given path\n        and returns it data as a waveform.\n\n        :param path: Path of the audio file to load data from.\n        :param offset: (Optional) Start offset to load from in seconds.\n        :param duration: (Optional) Duration to load in seconds.\n        :param sample_rate: (Optional) Sample rate to load audio with.\n        :param dtype: (Optional) Numpy data type to use, default to float32.\n        :returns: Loaded data a (waveform, sample_rate) tuple.\n        :raise SpleeterError: If any error occurs while loading audio.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads the audio file denoted by the given path\n            and returns it data as a waveform.\n\n            Parameters:\n                path (Union[Path, str]:\n                    Path of the audio file to load data from.\n                offset (Optional[float]):\n                    Start offset to load from in seconds.\n                duration (Optional[float]):\n                    Duration to load in seconds.\n                sample_rate (Optional[float]):\n                    Sample rate to load audio with.\n                dtype (numpy.dtype):\n                    (Optional) Numpy data type to use, default to `float32`.\n\n            Returns:\n                Signal:\n                    Loaded data a (waveform, sample_rate) tuple.\n\n            Raises:\n                SpleeterError:\n                    If any error occurs while loading audio.", "file": "spleeter_audio_ffmpeg.py", "function": "FFMPEGProcessAudioAdapter.load", "project": "spleeter", "change_stats": {"code_changes": 20, "doc_changes": 30, "total_changes": 50}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_trial_info(trial, parameters, metrics, include_error_data=False):\n    \n    \n    result = flatten_dict(trial.last_result)\n    trial_info = [str(trial), trial.status]\n    trial_info += [_location_str(result.get(HOSTNAME), result.get(PID))]\n    if include_error_data:\n        # TODO(ujvl): File path is too long to display in a single row.\n        trial_info += [trial.num_failures, trial.error_file]\n    trial_info += [result.get(CONFIG_PREFIX + param) for param in parameters]\n    trial_info += [result.get(metric) for metric in metrics]\n    return trial_info```\n\nUpdated code:\n```python\ndef _get_trial_info(trial, parameters, metrics):\n    \n    \n    result = flatten_dict(trial.last_result)\n    trial_info = [str(trial), trial.status, str(trial.address)]\n    trial_info += [result.get(CONFIG_PREFIX + param) for param in parameters]\n    trial_info += [result.get(metric) for metric in metrics]\n    return trial_info\n```\n\nOriginal documentation:\n```python\nReturns the following information about a trial:\n\n    name | status | loc | # failures | error_file | params... | metrics...\n\n    Args:\n        trial (Trial): Trial to get information for.\n        parameters (List[str]): Names of trial parameters to include.\n        metrics (List[str]): Names of metrics to include.\n        include_error_data (bool): Include error file and # of failures.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the following information about a trial:\n\n    name | status | loc | params... | metrics...\n\n    Args:\n        trial (Trial): Trial to get information for.\n        parameters (List[str]): Names of trial parameters to include.\n        metrics (List[str]): Names of metrics to include.", "file": "python_ray_tune_progress_reporter.py", "function": "_get_trial_info", "project": "ray", "change_stats": {"code_changes": 10, "doc_changes": 5, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef resize_vectors(self):\n        \n        target_count = len(self.index_to_key)\n        prev_count = len(self.vectors)\n        if prev_count == target_count:\n            return ()\n        prev_vectors = self.vectors\n        if hasattr(self, 'mapfile_path') and self.mapfile_path:\n            self.vectors = np.memmap(self.mapfile_path, shape=(target_count, self.vector_size), mode='w+', dtype=REAL)\n        else:\n            self.vectors = np.zeros((target_count, self.vector_size), dtype=REAL)\n        self.vectors[0: min(prev_count, target_count), ] = prev_vectors[0: min(prev_count, target_count), ]\n        self.allocate_vecattrs()\n        self.norms = None\n        return range(prev_count, target_count)```\n\nUpdated code:\n```python\ndef resize_vectors(self, seed=0):\n        \n\n        target_shape = (len(self.index_to_key), self.vector_size)\n        self.vectors = prep_vectors(target_shape, prior_vectors=self.vectors, seed=seed)\n        # TODO: support memmap?\n#        if hasattr(self, 'mapfile_path') and self.mapfile_path:\n#            self.vectors = np.memmap(self.mapfile_path, shape=(target_count, self.vector_size), mode='w+', dtype=REAL)\n\n        self.allocate_vecattrs()\n        self.norms = None\n```\n\nOriginal documentation:\n```python\nMake underlying vectors match index_to_key size.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Make underlying vectors match index_to_key size; random-initialize any new rows.", "file": "gensim_models_keyedvectors.py", "function": "KeyedVectors.resize_vectors", "project": "gensim", "change_stats": {"code_changes": 22, "doc_changes": 4, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lena():\n    \n\n    \n    return load(\"lena.png\")```\n\nUpdated code:\n```python\ndef lena():\n    \n\n    \n    raise RuntimeError(\"This image has been removed due to copyright concerns.\")\n```\n\nOriginal documentation:\n```python\nColour \"Lena\" image.\n\n    The standard, yet sometimes controversial Lena test image was\n    scanned from the November 1972 edition of Playboy magazine.  From\n    an image processing perspective, this image is useful because it\n    contains smooth, textured, shaded as well as detail areas.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Colour \"Lena\" image.\n\n    **This image has been removed from scikit-image due to copyright\n    concerns.**\n\n    The standard, yet sometimes controversial Lena test image was\n    scanned from the November 1972 edition of Playboy magazine.  From\n    an image processing perspective, this image is useful because it\n    contains smooth, textured, shaded as well as detail areas.", "file": "skimage_data___init__.py", "function": "lena", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _choose_lineagename(self, domains: List[str], certname: Optional[str]) -> str:\n        \n\n        \n        if certname:\n            return certname\n        elif util.is_wildcard_domain(domains[0]):\n            # Don't make files and directories starting with *.\n            return domains[0][2:]\n        return domains[0]```\n\nUpdated code:\n```python\ndef _choose_lineagename(self, domains: List[str], certname: Optional[str]) -> str:\n        \n\n        \n        # Remember chosen name for new lineage\n        lineagename = None\n        if certname:\n            lineagename = certname\n        elif util.is_wildcard_domain(domains[0]):\n            # Don't make files and directories starting with *.\n            lineagename = domains[0][2:]\n        else:\n            lineagename = domains[0]\n        # Verify whether chosen lineage is valid\n        if self._is_valid_lineagename(lineagename):\n            return lineagename\n        else:\n            raise errors.Error(\n                \"The provided certname cannot be used as a lineage name because it contains \"\n                \"an illegal character (i.e. filepath separator).\" if certname else\n                \"Cannot use domain name as lineage name because it contains an illegal \"\n                \"character (i.e. filepath separator). Specify an explicit lineage name \"\n                \"with --cert-name.\")\n```\n\nOriginal documentation:\n```python\nChooses a name for the new lineage.\n\n        :param domains: domains in certificate request\n        :type domains: `list` of `str`\n        :param certname: requested name of lineage\n        :type certname: `str` or `None`\n\n        :returns: lineage name that should be used\n        :rtype: str\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Chooses a name for the new lineage.\n\n        :param domains: domains in certificate request\n        :type domains: `list` of `str`\n        :param certname: requested name of lineage\n        :type certname: `str` or `None`\n\n        :returns: lineage name that should be used\n        :rtype: str\n\n        :raises errors.Error: If the chosen lineage name is invalid.", "file": "certbot_certbot__internal_client.py", "function": "Client._choose_lineagename", "project": "certbot", "change_stats": {"code_changes": 21, "doc_changes": 4, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_over(self, color='k', alpha=None):\n        \n        \n        self._rgba_over = to_rgba(color, alpha)\n        if self._isinit:\n            self._set_extremes()```\n\nUpdated code:\n```python\ndef set_over(self, color='k', alpha=None):\n        \n        \n        \n        self._rgba_over = to_rgba(color, alpha)\n        if self._isinit:\n            self._set_extremes()\n```\n\nOriginal documentation:\n```python\nSet color to be used for high out-of-range values.\n           Requires norm.clip = False\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the color for high out-of-range values when ``norm.clip = False``.", "file": "lib_matplotlib_colors.py", "function": "Colormap.set_over", "project": "matplotlib", "change_stats": {"code_changes": 3, "doc_changes": 5, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef simulate_kafka_cannot_connect():\n    \n    \n    \n    with patch.object(TestKafkaProducer, \"__init__\") as init_mock:\n        init_mock.side_effect = KafkaError(\"failed to connect\")\n        yield```\n\nUpdated code:\n```python\ndef simulate_kafka_cannot_connect():\n    \n    \n    \n    with patch.object(KafkaProducerForTests, \"__init__\") as init_mock:\n        init_mock.side_effect = KafkaError(\"failed to connect\")\n        yield\n```\n\nOriginal documentation:\n```python\nCauses instantiation of a kafka producer to raise a `KafkaError`.\n\n    IMPORTANT: this is mocking the `TestKafkaProducer`, itselt a mock. I'm\n    hoping that the real producer raises similarly, and that that behaviour\n    doesn't change with version of the library. I have tested this manually\n    however locally with real Kafka connection, and it seems to function as\n    expected :fingerscrossed:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Causes instantiation of a kafka producer to raise a `KafkaError`.\n\n    IMPORTANT: this is mocking the `KafkaProducerForTests`, itself a mock. I'm\n    hoping that the real producer raises similarly, and that that behaviour\n    doesn't change with version of the library. I have tested this manually\n    however locally with real Kafka connection, and it seems to function as\n    expected :fingerscrossed:", "file": "posthog_test_test_health.py", "function": "simulate_kafka_cannot_connect", "project": "posthog", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef read_string1(f):\n    \n    \n\n    n = read_uint1(f)\n    assert n >= 0\n    data = f.read(n)\n    if len(data) == n:\n        return data\n    raise ValueError(\"expected %d bytes in a string1, but only %d remain\" %\n                     (n, len(data)))```\n\nUpdated code:\n```python\ndef read_string1(f):\n    \n    \n\n    n = read_uint1(f)\n    assert n >= 0\n    data = f.read(n)\n    if len(data) == n:\n        return data.decode(\"latin-1\")\n    raise ValueError(\"expected %d bytes in a string1, but only %d remain\" %\n                     (n, len(data)))\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n    >>> import StringIO\n    >>> read_string1(StringIO.StringIO(\"\\x00\"))\n    ''\n    >>> read_string1(StringIO.StringIO(\"\\x03abcdef\"))\n    'abc'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n    >>> import io\n    >>> read_string1(io.BytesIO(b\"\\x00\"))\n    ''\n    >>> read_string1(io.BytesIO(b\"\\x03abcdef\"))\n    'abc'", "file": "Lib_pickletools.py", "function": "read_string1", "project": "cpython", "change_stats": {"code_changes": 4, "doc_changes": 8, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef calculate_expectancy(trades: pd.DataFrame) -> float:\n    \n    \n    \n    if len(trades) == 0:\n        return 0\n\n    expectancy = 1\n\n    profit_sum = trades.loc[trades['profit_abs'] > 0, 'profit_abs'].sum()\n    loss_sum = abs(trades.loc[trades['profit_abs'] < 0, 'profit_abs'].sum())\n    nb_win_trades = len(trades.loc[trades['profit_abs'] > 0])\n    nb_loss_trades = len(trades.loc[trades['profit_abs'] < 0])\n\n    if (nb_win_trades > 0) and (nb_loss_trades > 0):\n        average_win = profit_sum / nb_win_trades\n        average_loss = loss_sum / nb_loss_trades\n        risk_reward_ratio = average_win / average_loss\n        winrate = nb_win_trades / len(trades)\n        expectancy = ((1 + risk_reward_ratio) * winrate) - 1\n    elif nb_win_trades == 0:\n        expectancy = 0\n\n    return expectancy```\n\nUpdated code:\n```python\ndef calculate_expectancy(trades: pd.DataFrame) -> float:\n    \n    \n    \n    if len(trades) == 0:\n        return 0\n\n    expectancy = 0\n\n    winning_trades = trades.loc[trades['profit_abs'] > 0]\n    losing_trades = trades.loc[trades['profit_abs'] < 0]\n    profit_sum = winning_trades['profit_abs'].sum()\n    loss_sum = abs(losing_trades['profit_abs'].sum())\n    nb_win_trades = len(winning_trades)\n    nb_loss_trades = len(losing_trades)\n\n    average_win = profit_sum / nb_win_trades\n    average_loss = loss_sum / nb_loss_trades\n    winrate = nb_win_trades / len(trades)\n    loserate = nb_loss_trades / len(trades)\n    expectancy = (winrate * average_win) - (loserate * average_loss)\n\n    return expectancy\n```\n\nOriginal documentation:\n```python\nCalculate expectancy\n    :param trades: DataFrame containing trades (requires columns close_date and profit_ratio)\n    :return: expectancy\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculate expectancy\n    :param trades: DataFrame containing trades (requires columns close_date and profit_abs)\n    :return: expectancy", "file": "freqtrade_data_metrics.py", "function": "calculate_expectancy", "project": "freqtrade", "change_stats": {"code_changes": 27, "doc_changes": 4, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_cursor(self, cursor):\n        \n        \n        \n        raise NotImplementedError```\n\nUpdated code:\n```python\ndef set_cursor(self, cursor):\n        \n        \n        \n        self.canvas.set_cursor(cursor)\n```\n\nOriginal documentation:\n```python\nSet the cursor.\n\n        This method has to be implemented per backend.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the cursor.", "file": "lib_matplotlib_backend_tools.py", "function": "SetCursorBase.set_cursor", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _convert_centering(self, image):\n         \n        \n        # Input image is sized up because of integer rounding\n        logger.trace(\"head_size: %s, image_size: %s, target_size: %s\",\n                     self._head_size, image.shape[0], self.size)\n        if self._head_size != image.shape[0]:\n            interp = cv2.INTER_CUBIC if image.shape[0] < self._head_size else cv2.INTER_AREA\n            image = cv2.resize(image, (self._head_size, self._head_size), interpolation=interp)\n\n        out = np.zeros((self.size, self.size, image.shape[-1]), dtype=image.dtype)\n        slices = self._get_cropped_slices()\n        out[slices[\"out\"][0], slices[\"out\"][1], :] = image[slices[\"in\"][0], slices[\"in\"][1], :]\n        logger.trace(\"Cropped from aligned extract: (centering: %s, in shape: %s, out shape: %s)\",\n                     self._centering, image.shape, out.shape)\n        return out```\n\nUpdated code:\n```python\ndef _convert_centering(self, image: np.ndarray) -> np.ndarray:\n         \n        \n        logger.trace(\"image_size: %s, target_size: %s, coverage_ratio: %s\",  # type: ignore\n                     image.shape[0], self.size, self._coverage_ratio)\n\n        img_size = image.shape[0]\n        target_size = get_centered_size(self._source_centering,\n                                        self._centering,\n                                        img_size,\n                                        self._coverage_ratio)\n        out = np.zeros((target_size, target_size, image.shape[-1]), dtype=image.dtype)\n\n        slices = self._get_cropped_slices(img_size, target_size)\n        out[slices[\"out\"][0], slices[\"out\"][1], :] = image[slices[\"in\"][0], slices[\"in\"][1], :]\n        logger.trace(\"Cropped from aligned extract: (centering: %s, in shape: %s, \"  # type: ignore\n                     \"out shape: %s)\", self._centering, image.shape, out.shape)\n        return out\n```\n\nOriginal documentation:\n```python\nWhen the face being loaded is pre-aligned, the loaded image will have 'head' centering\n        so it needs to be cropped out to the appropriate centering.\n\n        This function temporarily converts this object to a full head aligned face, extracts the\n        sub-cropped face to the correct centering, reverse the sub crop and returns the cropped\n        face.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The original head-centered aligned image\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The aligned image with the correct centering\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "When the face being loaded is pre-aligned, the loaded image will have 'head' centering\n        so it needs to be cropped out to the appropriate centering.\n\n        This function temporarily converts this object to a full head aligned face, extracts the\n        sub-cropped face to the correct centering, reverse the sub crop and returns the cropped\n        face at the selected coverage ratio.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The original head-centered aligned image\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The aligned image with the correct centering, scaled to image input size", "file": "lib_align_aligned_face.py", "function": "AlignedFace._convert_centering", "project": "faceswap", "change_stats": {"code_changes": 26, "doc_changes": 6, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __array__(self, *args, **kwargs):\n        \n        \n        \n        raise NotImplementedError()```\n\nUpdated code:\n```python\ndef __array__(self, *args, **kwargs):\n        \n        \n        \n        return self.get_affine().get_matrix()\n```\n\nOriginal documentation:\n```python\nArray interface to get at this Transform's matrix.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Array interface to get at this Transform's affine matrix.", "file": "lib_matplotlib_transforms.py", "function": "Transform.__array__", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rank(x):\n        \n        \n        \n        \n\tx      = numerix.asarray(x)\n\tu,s,v  = MLab.svd(x)\n\t# maxabs = MLab.max(numerix.absolute(s)) is also possible.\n\tmaxabs = norm(x)\t\n\tmaxdim = MLab.max(numerix.shape(x))\n\ttol    = maxabs*maxdim*_eps_approx\n\tr      = s>tol\n\treturn MLab.sum(r)```\n\nUpdated code:\n```python\ndef rank(x):\n        \n        \n        \n        \n\tx      = asarray(x)\n\tu,s,v  = numerix.mlab.svd(x)\n\t# maxabs = numerix.mlab.max(numerix.absolute(s)) is also possible.\n\tmaxabs = norm(x)\t\n\tmaxdim = numerix.mlab.max(numerix.shape(x))\n\ttol    = maxabs*maxdim*_eps_approx\n\tr      = s>tol\n\treturn asum(r)\n```\n\nOriginal documentation:\n```python\nReturns the rank of a matrix.\n        The rank is understood here as the an estimation of the number of\n        linearly independent rows or columns (depending on the size of the\n        matrix).\n        Note that MLab.rank() is not equivalent to Matlab's rank.\n        This function is!\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the rank of a matrix.\n        The rank is understood here as the an estimation of the number of\n        linearly independent rows or columns (depending on the size of the\n        matrix).\n        Note that numerix.mlab.rank() is not equivalent to Matlab's rank.\n        This function is!", "file": "lib_matplotlib_mlab.py", "function": "rank", "project": "matplotlib", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef match_template(image, template, pad_input=False, mode='constant',\n                   constant_values=0):\n    \n    \n    assert_nD(image, (2, 3))\n\n    if image.ndim < template.ndim:\n        raise ValueError(\"Dimensionality of template must be less than or \"\n                         \"equal to the dimensionality of image.\")\n    if np.any(np.less(image.shape, template.shape)):\n        raise ValueError(\"Image must be larger than template.\")\n\n    image_shape = image.shape\n\n    image = np.array(image, dtype=np.float32, copy=False)\n\n    pad_width = tuple((width, width) for width in template.shape)\n    if mode == 'constant':\n        image = np.pad(image, pad_width=pad_width, mode=mode,\n                       constant_values=constant_values)\n    else:\n        image = np.pad(image, pad_width=pad_width, mode=mode)\n\n    # Use special case for 2-D images for much better performance in\n    # computation of integral images\n    if image.ndim == 2:\n        image_window_sum = _window_sum_2d(image, template.shape)\n        image_window_sum2 = _window_sum_2d(image ** 2, template.shape)\n    elif image.ndim == 3:\n        image_window_sum = _window_sum_3d(image, template.shape)\n        image_window_sum2 = _window_sum_3d(image ** 2, template.shape)\n\n    template_mean = template.mean()\n    template_volume = np.prod(template.shape)\n    template_ssd = np.sum((template - template_mean) ** 2)\n\n    if image.ndim == 2:\n        xcorr = fftconvolve(image, template[::-1, ::-1],\n                            mode=\"valid\")[1:-1, 1:-1]\n    elif image.ndim == 3:\n        xcorr = fftconvolve(image, template[::-1, ::-1, ::-1],\n                            mode=\"valid\")[1:-1, 1:-1, 1:-1]\n\n    numerator = xcorr - image_window_sum * template_mean\n\n    denominator = image_window_sum2\n    np.multiply(image_window_sum, image_window_sum, out=image_window_sum)\n    np.divide(image_window_sum, template_volume, out=image_window_sum)\n    denominator -= image_window_sum\n    denominator *= template_ssd\n    np.maximum(denominator, 0, out=denominator)  # sqrt of negative number not allowed\n    np.sqrt(denominator, out=denominator)\n\n    response = np.zeros_like(xcorr, dtype=np.float32)\n\n    # avoid zero-division\n    mask = denominator > np.finfo(np.float32).eps\n\n    response[mask] = numerator[mask] / denominator[mask]\n\n    slices = []\n    for i in range(template.ndim):\n        if pad_input:\n            d0 = (template.shape[i] - 1) // 2\n            d1 = d0 + image_shape[i]\n        else:\n            d0 = template.shape[i] - 1\n            d1 = d0 + image_shape[i] - template.shape[i] + 1\n        slices.append(slice(d0, d1))\n\n    return response[slices]```\n\nUpdated code:\n```python\ndef match_template(image, template, pad_input=False, mode='constant',\n                   constant_values=0):\n    \n    \n    assert_nD(image, (2, 3))\n\n    if image.ndim < template.ndim:\n        raise ValueError(\"Dimensionality of template must be less than or \"\n                         \"equal to the dimensionality of image.\")\n    if np.any(np.less(image.shape, template.shape)):\n        raise ValueError(\"Image must be larger than template.\")\n\n    image_shape = image.shape\n\n    image = np.array(image, dtype=np.float64, copy=False)\n\n    pad_width = tuple((width, width) for width in template.shape)\n    if mode == 'constant':\n        image = pad(image, pad_width=pad_width, mode=mode,\n                    constant_values=constant_values)\n    else:\n        image = pad(image, pad_width=pad_width, mode=mode)\n\n    # Use special case for 2-D images for much better performance in\n    # computation of integral images\n    if image.ndim == 2:\n        image_window_sum = _window_sum_2d(image, template.shape)\n        image_window_sum2 = _window_sum_2d(image ** 2, template.shape)\n    elif image.ndim == 3:\n        image_window_sum = _window_sum_3d(image, template.shape)\n        image_window_sum2 = _window_sum_3d(image ** 2, template.shape)\n\n    template_mean = template.mean()\n    template_volume = np.prod(template.shape)\n    template_ssd = np.sum((template - template_mean) ** 2)\n\n    if image.ndim == 2:\n        xcorr = fftconvolve(image, template[::-1, ::-1],\n                            mode=\"valid\")[1:-1, 1:-1]\n    elif image.ndim == 3:\n        xcorr = fftconvolve(image, template[::-1, ::-1, ::-1],\n                            mode=\"valid\")[1:-1, 1:-1, 1:-1]\n\n    numerator = xcorr - image_window_sum * template_mean\n\n    denominator = image_window_sum2\n    np.multiply(image_window_sum, image_window_sum, out=image_window_sum)\n    np.divide(image_window_sum, template_volume, out=image_window_sum)\n    denominator -= image_window_sum\n    denominator *= template_ssd\n    np.maximum(denominator, 0, out=denominator)  # sqrt of negative number not allowed\n    np.sqrt(denominator, out=denominator)\n\n    response = np.zeros_like(xcorr, dtype=np.float64)\n\n    # avoid zero-division\n    mask = denominator > np.finfo(np.float64).eps\n\n    response[mask] = numerator[mask] / denominator[mask]\n\n    slices = []\n    for i in range(template.ndim):\n        if pad_input:\n            d0 = (template.shape[i] - 1) // 2\n            d1 = d0 + image_shape[i]\n        else:\n            d0 = template.shape[i] - 1\n            d1 = d0 + image_shape[i] - template.shape[i] + 1\n        slices.append(slice(d0, d1))\n\n    return response[slices]\n```\n\nOriginal documentation:\n```python\nMatch a template to a 2-D or 3-D image using normalized correlation.\n\n    The output is an array with values between -1.0 and 1.0. The value at a\n    given position corresponds to the correlation coefficient between the image\n    and the template.\n\n    For `pad_input=True` matches correspond to the center and otherwise to the\n    top-left corner of the template. To find the best match you must search for\n    peaks in the response (output) image.\n\n    Parameters\n    ----------\n    image : (M, N[, D]) array\n        2-D or 3-D input image.\n    template : (m, n[, d]) array\n        Template to locate. It must be `(m <= M, n <= N[, d <= D])`.\n    pad_input : bool\n        If True, pad `image` so that output is the same size as the image, and\n        output values correspond to the template center. Otherwise, the output\n        is an array with shape `(M - m + 1, N - n + 1)` for an `(M, N)` image\n        and an `(m, n)` template, and matches correspond to origin\n        (top-left corner) of the template.\n    mode : see `numpy.pad`, optional\n        Padding mode.\n    constant_values : see `numpy.pad`, optional\n        Constant values used in conjunction with ``mode='constant'``.\n\n    Returns\n    -------\n    output : array\n        Response image with correlation coefficients.\n\n    Notes\n    -----\n    Details on the cross-correlation are presented in [1]_. This implementation\n    uses FFT convolutions of the image and the template. Reference [2]_\n    presents similar derivations but the approximation presented in this\n    reference is not used in our implementation.\n\n    References\n    ----------\n    .. [1] J. P. Lewis, \"Fast Normalized Cross-Correlation\", Industrial Light\n           and Magic.\n    .. [2] Briechle and Hanebeck, \"Template Matching using Fast Normalized\n           Cross Correlation\", Proceedings of the SPIE (2001).\n           DOI:10.1117/12.421129\n\n    Examples\n    --------\n    >>> template = np.zeros((3, 3))\n    >>> template[1, 1] = 1\n    >>> template\n    array([[ 0.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  0.]])\n    >>> image = np.zeros((6, 6))\n    >>> image[1, 1] = 1\n    >>> image[4, 4] = -1\n    >>> image\n    array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  1.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.]])\n    >>> result = match_template(image, template)\n    >>> np.round(result, 3)\n    array([[ 1.   , -0.125,  0.   ,  0.   ],\n           [-0.125, -0.125,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.125, -1.   ]], dtype=float32)\n    >>> result = match_template(image, template, pad_input=True)\n    >>> np.round(result, 3)\n    array([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]], dtype=float32)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Match a template to a 2-D or 3-D image using normalized correlation.\n\n    The output is an array with values between -1.0 and 1.0. The value at a\n    given position corresponds to the correlation coefficient between the image\n    and the template.\n\n    For `pad_input=True` matches correspond to the center and otherwise to the\n    top-left corner of the template. To find the best match you must search for\n    peaks in the response (output) image.\n\n    Parameters\n    ----------\n    image : (M, N[, D]) array\n        2-D or 3-D input image.\n    template : (m, n[, d]) array\n        Template to locate. It must be `(m <= M, n <= N[, d <= D])`.\n    pad_input : bool\n        If True, pad `image` so that output is the same size as the image, and\n        output values correspond to the template center. Otherwise, the output\n        is an array with shape `(M - m + 1, N - n + 1)` for an `(M, N)` image\n        and an `(m, n)` template, and matches correspond to origin\n        (top-left corner) of the template.\n    mode : see `numpy.pad`, optional\n        Padding mode.\n    constant_values : see `numpy.pad`, optional\n        Constant values used in conjunction with ``mode='constant'``.\n\n    Returns\n    -------\n    output : array\n        Response image with correlation coefficients.\n\n    Notes\n    -----\n    Details on the cross-correlation are presented in [1]_. This implementation\n    uses FFT convolutions of the image and the template. Reference [2]_\n    presents similar derivations but the approximation presented in this\n    reference is not used in our implementation.\n\n    References\n    ----------\n    .. [1] J. P. Lewis, \"Fast Normalized Cross-Correlation\", Industrial Light\n           and Magic.\n    .. [2] Briechle and Hanebeck, \"Template Matching using Fast Normalized\n           Cross Correlation\", Proceedings of the SPIE (2001).\n           DOI:10.1117/12.421129\n\n    Examples\n    --------\n    >>> template = np.zeros((3, 3))\n    >>> template[1, 1] = 1\n    >>> template\n    array([[ 0.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  0.]])\n    >>> image = np.zeros((6, 6))\n    >>> image[1, 1] = 1\n    >>> image[4, 4] = -1\n    >>> image\n    array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  1.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.]])\n    >>> result = match_template(image, template)\n    >>> np.round(result, 3)\n    array([[ 1.   , -0.125,  0.   ,  0.   ],\n           [-0.125, -0.125,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.125, -1.   ]])\n    >>> result = match_template(image, template, pad_input=True)\n    >>> np.round(result, 3)\n    array([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])", "file": "skimage_feature_template.py", "function": "match_template", "project": "scikit-image", "change_stats": {"code_changes": 14, "doc_changes": 6, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cache_frame(self, filename, image):\n          \n        frame_no = int(re.search(self.re_search, filename).group())\n        self.cache[frame_no] = image\n        logger.trace(\"Added to cache. Frame no: %s\", frame_no)\n        logger.trace(\"Current cache: %s\", sorted(self.cache.keys()))```\n\nUpdated code:\n```python\ndef cache_frame(self, filename, image) -> None:\n         \n        \n        frame_no = int(re.search(self.re_search, filename).group())\n        self.cache[frame_no] = image\n        logger.trace(\"Added to cache. Frame no: %s\", frame_no)\n        logger.trace(\"Current cache: %s\", sorted(self.cache.keys()))\n```\n\nOriginal documentation:\n```python\nAdd the incoming frame to the cache\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add the incoming converted frame to the cache ready for writing out.\n\n        Used for ffmpeg and gif writers to ensure that the frames are written out in the correct\n        order.\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the incoming frame, where the frame index can be extracted from\n        image: class:`numpy.ndarray`\n            The converted frame corresponding to the given filename", "file": "plugins_convert_writer__base.py", "function": "Output.cache_frame", "project": "faceswap", "change_stats": {"code_changes": 7, "doc_changes": 14, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef init_to_mean(site=None):\n    \n    \n    \n    if site is None:\n        return init_to_mean\n\n    try:\n        # Try .mean() method.\n        value = site[\"fn\"].mean.detach()\n        if torch_isnan(value):\n            raise ValueError\n        if hasattr(site[\"fn\"], \"_validate_sample\"):\n            site[\"fn\"]._validate_sample(value)\n        return value\n    except (NotImplementedError, ValueError):\n        # Fall back to a median.\n        # This is required for distributions with infinite variance, e.g. Cauchy.\n        return init_to_median(site)```\n\nUpdated code:\n```python\ndef init_to_mean(\n    site=None,\n    *,\n    fallback: Optional[Callable] = init_to_median,\n):\n    \n    \n    \n    if site is None:\n        return functools.partial(init_to_mean, fallback=fallback)\n\n    try:\n        # Try .mean() method.\n        value = site[\"fn\"].mean.detach()\n        if torch_isnan(value):\n            raise ValueError\n        if hasattr(site[\"fn\"], \"_validate_sample\"):\n            site[\"fn\"]._validate_sample(value)\n        value._pyro_custom_init = False\n        return value\n    except (NotImplementedError, ValueError):\n        # This may happen for distributions with infinite variance, e.g. Cauchy.\n        pass\n    if fallback is not None:\n        return fallback(site)\n    raise ValueError(f\"No init strategy specified for site {repr(site['name'])}\")\n```\n\nOriginal documentation:\n```python\nInitialize to the prior mean; fallback to median if mean is undefined.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize to the prior mean; fallback to ``fallback`` (defaults to\n    :func:`init_to_median`) if mean is undefined.\n\n    :param callable fallback: Fallback init strategy, for sites not specified\n        in ``values``.\n    :raises ValueError: If ``fallback=None`` and no value for a site is given\n        in ``values``.", "file": "pyro_infer_autoguide_initialization.py", "function": "init_to_mean", "project": "pyro", "change_stats": {"code_changes": 19, "doc_changes": 10, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward(self, x):\n        \n\n        identity = x\n\n        out = self.conv1(x)\n        if self.with_norm:\n            out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        if self.with_norm:\n            out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        return out```\n\nUpdated code:\n```python\ndef forward(self, x: Tensor) -> Tensor:\n        \n\n        identity = x\n\n        out = self.conv1(x)\n        if self.with_norm:\n            out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        if self.with_norm:\n            out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        return out\n```\n\nOriginal documentation:\n```python\nForward function.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward function for SimplifiedBasicBlock.", "file": "mmdet_models_layers_res_layer.py", "function": "SimplifiedBasicBlock.forward", "project": "mmdetection", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_prev_action_sub_state(\n        tracker: \"DialogueStateTracker\",\n    ) -> Dict[Text, Text]:\n        \n        \n        return tracker.latest_action```\n\nUpdated code:\n```python\ndef _get_prev_action_sub_state(\n        tracker: \"DialogueStateTracker\",\n    ) -> Optional[Dict[Text, Text]]:\n        \n        \n        return tracker.latest_action\n```\n\nOriginal documentation:\n```python\nTurn the previous taken action into a state name.\n        Args:\n            tracker: dialog state tracker containing the dialog so far\n        Returns:\n            a dictionary with the information on latest action\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Turn the previous taken action into a state name.\n\n        Args:\n            tracker: dialog state tracker containing the dialog so far\n        Returns:\n            a dictionary with the information on latest action", "file": "rasa_shared_core_domain.py", "function": "Domain._get_prev_action_sub_state", "project": "rasa", "change_stats": {"code_changes": 4, "doc_changes": 3, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_leverage(self, leverage, symbol: Optional[str] = None, params={}):\n        \n        \n        \n        if symbol is None:\n            raise ArgumentsRequired(self.id + ' setLeverage() requires a symbol argument')\n        if (leverage < 1) or (leverage > 100):\n            raise BadRequest(self.id + ' leverage should be between 1 and 100')\n        self.load_markets()\n        self.load_accounts()\n        market = self.market(symbol)\n        if market['type'] != 'future':\n            raise BadSymbol(self.id + ' setLeverage() supports futures contracts only')\n        account = self.safe_value(self.accounts, 0, {})\n        accountGroup = self.safe_string(account, 'id')\n        request = {\n            'account-group': accountGroup,\n            'symbol': market['id'],\n            'leverage': leverage,\n        }\n        return self.v2PrivateAccountGroupPostFuturesLeverage(self.extend(request, params))```\n\nUpdated code:\n```python\ndef set_leverage(self, leverage, symbol: Optional[str] = None, params={}):\n        \n        \n        \n        self.check_required_symbol('setLeverage', symbol)\n        if (leverage < 1) or (leverage > 100):\n            raise BadRequest(self.id + ' leverage should be between 1 and 100')\n        self.load_markets()\n        self.load_accounts()\n        market = self.market(symbol)\n        if not market['swap']:\n            raise BadSymbol(self.id + ' setLeverage() supports swap contracts only')\n        account = self.safe_value(self.accounts, 0, {})\n        accountGroup = self.safe_string(account, 'id')\n        request = {\n            'account-group': accountGroup,\n            'symbol': market['id'],\n            'leverage': leverage,\n        }\n        return self.v2PrivateAccountGroupPostFuturesLeverage(self.extend(request, params))\n```\n\nOriginal documentation:\n```python\nset the level of leverage for a market\n        :param float leverage: the rate of leverage\n        :param str symbol: unified market symbol\n        :param dict [params]: extra parameters specific to the ascendex api endpoint\n        :returns dict: response from the exchange\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "set the level of leverage for a market\n        :see: https://ascendex.github.io/ascendex-futures-pro-api-v2/#change-contract-leverage\n        :param float leverage: the rate of leverage\n        :param str symbol: unified market symbol\n        :param dict [params]: extra parameters specific to the ascendex api endpoint\n        :returns dict: response from the exchange", "file": "python_ccxt_ascendex.py", "function": "ascendex.set_leverage", "project": "ccxt", "change_stats": {"code_changes": 9, "doc_changes": 3, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_ignore_user_permissions_if_missing(self):\n\t\t\n\t\tself.set_user_permission_doctypes(['Blog Category', 'Blog Post', 'Blogger'])\n\n\t\tfrappe.set_user(\"Administrator\")\n\t\t# add_user_permission(\"Blog Category\", \"_Test Blog Category\",\n\t\t# \t\"test2@example.com\")\n\t\tfrappe.set_user(\"test2@example.com\")\n\n\t\tdoc = frappe.get_doc({\n\t\t\t\"doctype\": \"Blog Post\",\n\t\t\t\"blog_category\": \"_Test Blog Category\",\n\t\t\t\"blogger\": \"_Test Blogger 1\",\n\t\t\t\"title\": \"_Test Blog Post Title\",\n\t\t\t\"content\": \"_Test Blog Post Content\"\n\t\t})\n\n\t\tself.assertFalse(doc.has_permission(\"write\"))\n\n\t\tfrappe.set_user(\"Administrator\")\n\t\tself.set_ignore_user_permissions_if_missing(1)\n\n\t\tfrappe.set_user(\"test2@example.com\")\n\t\tself.assertTrue(doc.has_permission(\"write\"))```\n\nUpdated code:\n```python\ndef test_ignore_user_permissions_if_missing(self):\n\t\t\n\n\t\tadd_user_permission(\"Blog Category\", \"_Test Blog Category\",\n\t\t\t\"test2@example.com\")\n\t\tfrappe.set_user(\"test2@example.com\")\n\n\t\tdoc = frappe.get_doc({\n\t\t\t\"doctype\": \"Blog Post\",\n\t\t\t\"blog_category\": \"_Test Blog Category 2\",\n\t\t\t\"blogger\": \"_Test Blogger 1\",\n\t\t\t\"title\": \"_Test Blog Post Title\",\n\t\t\t\"content\": \"_Test Blog Post Content\"\n\t\t})\n\n\t\tself.assertFalse(doc.has_permission(\"write\"))\n\n\t\tfrappe.set_user(\"Administrator\")\n\t\tremove_user_permission(\"Blog Category\", \"_Test Blog Category\",\n\t\t\t\"test2@example.com\")\n\n\t\tfrappe.set_user(\"test2@example.com\")\n\t\tself.assertTrue(doc.has_permission('write'))\n```\n\nOriginal documentation:\n```python\nIf `Ignore User Permissions If Missing` is checked in System Settings, show records even if User Permissions are missing for a linked doctype\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "If there are no user permissions, then allow as per role", "file": "frappe_tests_test_permissions.py", "function": "TestPermissions.test_ignore_user_permissions_if_missing", "project": "frappe", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_ultra_long_rendering(self) -> None:\n        \n        msg = \"mock rendered message\\n\" * settings.MAX_MESSAGE_LENGTH\n\n        with mock.patch(\"zerver.lib.markdown.timeout\", return_value=msg), mock.patch(\n            \"zerver.lib.markdown.markdown_logger\"\n        ):\n            with self.assertRaises(MarkdownRenderingException):\n                markdown_convert_wrapper(msg)```\n\nUpdated code:\n```python\ndef test_ultra_long_rendering(self) -> None:\n        \n        msg = \"mock rendered message\\n\" * 10 * settings.MAX_MESSAGE_LENGTH\n\n        with mock.patch(\"zerver.lib.markdown.timeout\", return_value=msg), mock.patch(\n            \"zerver.lib.markdown.markdown_logger\"\n        ):\n            with self.assertRaises(MarkdownRenderingException):\n                markdown_convert_wrapper(msg)\n```\n\nOriginal documentation:\n```python\nA rendered message with an ultra-long length (> 10 * MAX_MESSAGE_LENGTH)\n        throws an exception\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "A rendered message with an ultra-long length (> 100 * MAX_MESSAGE_LENGTH)\n        throws an exception", "file": "zerver_tests_test_markdown.py", "function": "MarkdownErrorTests.test_ultra_long_rendering", "project": "zulip", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fund_scale_close_sina() -> pd.DataFrame:\n    \n    \n    \n    url = \"http://vip.stock.finance.sina.com.cn/fund_center/data/jsonp.php/IO.XSRV2.CallbackList['_bjN6KvXOkfPy2Bu']/NetValueReturn_Service.NetValueReturnClose\"\n    params = {\n        \"page\": \"1\",\n        \"num\": \"1000\",\n        \"sort\": \"zmjgm\",\n        \"asc\": \"0\",\n        \"ccode\": \"\",\n        \"type2\": '',\n        \"type3\": \"\",\n    }\n    r = requests.get(url, params=params)\n    data_text = r.text\n    data_json = demjson.decode(data_text[data_text.find(\"({\") + 1 : -2])\n    temp_df = pd.DataFrame(data_json[\"data\"])\n    temp_df.reset_index(inplace=True)\n    temp_df[\"index\"] = range(1, len(temp_df) + 1)\n    temp_df.rename(\n        columns={\n            \"index\": \"\u5e8f\u53f7\",\n            \"symbol\": \"\u57fa\u91d1\u4ee3\u7801\",\n            \"sname\": \"\u57fa\u91d1\u7b80\u79f0\",\n            \"per_nav\": \"-\",\n            \"total_nav\": \"-\",\n            \"three_month\": \"-\",\n            \"six_month\": \"-\",\n            \"one_year\": \"-\",\n            \"form_year\": \"-\",\n            \"form_start\": \"-\",\n            \"name\": \"-\",\n            \"zmjgm\": \"\u603b\u52df\u96c6\u89c4\u6a21\",\n            \"clrq\": \"\u6210\u7acb\u65e5\u671f\",\n            \"jjjl\": \"\u57fa\u91d1\u7ecf\u7406\",\n            \"dwjz\": \"\u5355\u4f4d\u51c0\u503c\",\n            \"ljjz\": \"-\",\n            \"jzrq\": \"\u66f4\u65b0\u65e5\u671f\",\n            \"zjzfe\": \"\u6700\u8fd1\u603b\u4efd\u989d\",\n            \"jjglr_code\": \"-\",\n        },\n        inplace=True,\n    )\n    temp_df = temp_df[\n        [\n            \"\u5e8f\u53f7\",\n            \"\u57fa\u91d1\u4ee3\u7801\",\n            \"\u57fa\u91d1\u7b80\u79f0\",\n            \"\u5355\u4f4d\u51c0\u503c\",\n            \"\u603b\u52df\u96c6\u89c4\u6a21\",\n            \"\u6700\u8fd1\u603b\u4efd\u989d\",\n            \"\u6210\u7acb\u65e5\u671f\",\n            \"\u57fa\u91d1\u7ecf\u7406\",\n            \"\u66f4\u65b0\u65e5\u671f\",\n        ]\n    ]\n    temp_df[\"\u6210\u7acb\u65e5\u671f\"] = pd.to_datetime(temp_df[\"\u6210\u7acb\u65e5\u671f\"]).dt.date\n    temp_df[\"\u66f4\u65b0\u65e5\u671f\"] = pd.to_datetime(temp_df[\"\u66f4\u65b0\u65e5\u671f\"]).dt.date\n    temp_df[\"\u5355\u4f4d\u51c0\u503c\"] = pd.to_numeric(temp_df[\"\u5355\u4f4d\u51c0\u503c\"], errors=\"coerce\")\n    temp_df[\"\u603b\u52df\u96c6\u89c4\u6a21\"] = pd.to_numeric(temp_df[\"\u603b\u52df\u96c6\u89c4\u6a21\"], errors=\"coerce\")\n    temp_df[\"\u6700\u8fd1\u603b\u4efd\u989d\"] = pd.to_numeric(temp_df[\"\u6700\u8fd1\u603b\u4efd\u989d\"], errors=\"coerce\")\n    return temp_df```\n\nUpdated code:\n```python\ndef fund_scale_close_sina() -> pd.DataFrame:\n    \n    \n    \n    url = \"http://vip.stock.finance.sina.com.cn/fund_center/data/jsonp.php/IO.XSRV2.CallbackList['_bjN6KvXOkfPy2Bu']/NetValueReturn_Service.NetValueReturnClose\"\n    params = {\n        \"page\": \"1\",\n        \"num\": \"1000\",\n        \"sort\": \"zmjgm\",\n        \"asc\": \"0\",\n        \"ccode\": \"\",\n        \"type2\": '',\n        \"type3\": \"\",\n    }\n    r = requests.get(url, params=params)\n    data_text = r.text\n    data_json = demjson.decode(data_text[data_text.find(\"({\") + 1: -2])\n    temp_df = pd.DataFrame(data_json[\"data\"])\n    temp_df.reset_index(inplace=True)\n    temp_df[\"index\"] = range(1, len(temp_df) + 1)\n    temp_df.rename(\n        columns={\n            \"index\": \"\u5e8f\u53f7\",\n            \"symbol\": \"\u57fa\u91d1\u4ee3\u7801\",\n            \"sname\": \"\u57fa\u91d1\u7b80\u79f0\",\n            \"per_nav\": \"-\",\n            \"total_nav\": \"-\",\n            \"three_month\": \"-\",\n            \"six_month\": \"-\",\n            \"one_year\": \"-\",\n            \"form_year\": \"-\",\n            \"form_start\": \"-\",\n            \"name\": \"-\",\n            \"zmjgm\": \"\u603b\u52df\u96c6\u89c4\u6a21\",\n            \"clrq\": \"\u6210\u7acb\u65e5\u671f\",\n            \"jjjl\": \"\u57fa\u91d1\u7ecf\u7406\",\n            \"dwjz\": \"\u5355\u4f4d\u51c0\u503c\",\n            \"ljjz\": \"-\",\n            \"jzrq\": \"\u66f4\u65b0\u65e5\u671f\",\n            \"zjzfe\": \"\u6700\u8fd1\u603b\u4efd\u989d\",\n            \"jjglr_code\": \"-\",\n        },\n        inplace=True,\n    )\n    temp_df = temp_df[\n        [\n            \"\u5e8f\u53f7\",\n            \"\u57fa\u91d1\u4ee3\u7801\",\n            \"\u57fa\u91d1\u7b80\u79f0\",\n            \"\u5355\u4f4d\u51c0\u503c\",\n            \"\u603b\u52df\u96c6\u89c4\u6a21\",\n            \"\u6700\u8fd1\u603b\u4efd\u989d\",\n            \"\u6210\u7acb\u65e5\u671f\",\n            \"\u57fa\u91d1\u7ecf\u7406\",\n            \"\u66f4\u65b0\u65e5\u671f\",\n        ]\n    ]\n    temp_df[\"\u6210\u7acb\u65e5\u671f\"] = pd.to_datetime(temp_df[\"\u6210\u7acb\u65e5\u671f\"], errors=\"coerce\").dt.date\n    temp_df[\"\u66f4\u65b0\u65e5\u671f\"] = pd.to_datetime(temp_df[\"\u66f4\u65b0\u65e5\u671f\"], errors=\"coerce\").dt.date\n    temp_df[\"\u5355\u4f4d\u51c0\u503c\"] = pd.to_numeric(temp_df[\"\u5355\u4f4d\u51c0\u503c\"], errors=\"coerce\")\n    temp_df[\"\u603b\u52df\u96c6\u89c4\u6a21\"] = pd.to_numeric(temp_df[\"\u603b\u52df\u96c6\u89c4\u6a21\"], errors=\"coerce\")\n    temp_df[\"\u6700\u8fd1\u603b\u4efd\u989d\"] = pd.to_numeric(temp_df[\"\u6700\u8fd1\u603b\u4efd\u989d\"], errors=\"coerce\")\n    return temp_df\n```\n\nOriginal documentation:\n```python\n\u65b0\u6d6a\u8d22\u7ecf-\u57fa\u91d1\u6570\u636e\u4e2d\u5fc3-\u57fa\u91d1\u89c4\u6a21-\u5c01\u95ed\u5f0f\u57fa\u91d1\n    http://vip.stock.finance.sina.com.cn/fund_center/index.html#jjhqetf\n    :return: \u57fa\u91d1\u89c4\u6a21\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u65b0\u6d6a\u8d22\u7ecf-\u57fa\u91d1\u6570\u636e\u4e2d\u5fc3-\u57fa\u91d1\u89c4\u6a21-\u5c01\u95ed\u5f0f\u57fa\u91d1\n    https://vip.stock.finance.sina.com.cn/fund_center/index.html#jjhqetf\n    :return: \u57fa\u91d1\u89c4\u6a21\n    :rtype: pandas.DataFrame", "file": "akshare_fund_fund_scale_sina.py", "function": "fund_scale_close_sina", "project": "akshare", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef materialize_metadata(data):\n    \n    \n    event_type = get_event_type(data)\n    event_metadata = event_type.get_metadata(data)\n    return {\n        \"type\": event_type.key,\n        \"metadata\": event_metadata,\n        \"title\": event_type.get_title(event_metadata),\n        \"location\": event_type.get_location(event_metadata),\n    }```\n\nUpdated code:\n```python\ndef materialize_metadata(data, event_type, event_metadata):\n    \n\n    \n\n    # XXX(markus): Ideally this wouldn't take data or event_type, and instead\n    # calculate culprit + type from event_metadata\n\n    return {\n        \"type\": event_type.key,\n        \"culprit\": get_culprit(data),\n        \"metadata\": event_metadata,\n        \"title\": event_type.get_title(event_metadata),\n        \"location\": event_type.get_location(event_metadata),\n    }\n```\n\nOriginal documentation:\n```python\nReturns the materialized metadata to be merged with group or\n    event data.  This currently produces the keys `type`, `metadata`,\n    `title` and `location`.  This should most likely also produce\n    `culprit` here.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the materialized metadata to be merged with group or\n    event data.  This currently produces the keys `type`, `culprit`,\n    `metadata`, `title` and `location`.", "file": "src_sentry_event_manager.py", "function": "materialize_metadata", "project": "sentry", "change_stats": {"code_changes": 12, "doc_changes": 7, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef slots_for_entities(self, entities: List[Dict[Text, Any]]) -> List[SlotSet]:\n        \n        \n        if self.store_entities_as_slots:\n            slot_events = []\n            for slot in self.slots:\n                if slot.auto_fill:\n                    matching_entities = [\n                        entity.get(\"value\")\n                        for entity in entities\n                        if entity.get(\"entity\") == slot.name\n                    ]\n                    if matching_entities:\n                        if slot.type_name == \"list\":\n                            slot_events.append(SlotSet(slot.name, matching_entities))\n                        else:\n                            slot_events.append(\n                                SlotSet(slot.name, matching_entities[-1])\n                            )\n            return slot_events\n        else:\n            return []```\n\nUpdated code:\n```python\ndef slots_for_entities(self, entities: List[Dict[Text, Any]]) -> List[SlotSet]:\n        \n        \n        if self.store_entities_as_slots:\n            slot_events = []\n            for slot in self.slots:\n                matching_entities = [\n                    entity.get(\"value\")\n                    for entity in entities\n                    if any(\n                        [\n                            entity.get(\"entity\") == mapping.get(\"entity\")\n                            for mapping in slot.mappings\n                            if mapping.get(\"type\") == str(SlotMapping.FROM_ENTITY)\n                            and mapping.get(MAPPING_CONDITIONS) is None\n                        ]\n                    )\n                ]\n                if matching_entities:\n                    if slot.type_name == \"list\":\n                        slot_events.append(SlotSet(slot.name, matching_entities))\n                    else:\n                        slot_events.append(SlotSet(slot.name, matching_entities[-1]))\n            return slot_events\n        else:\n            return []\n```\n\nOriginal documentation:\n```python\nCreates slot events for entities if auto-filling is enabled.\n\n        Args:\n            entities: The list of entities.\n\n        Returns:\n            A list of `SlotSet` events.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates slot events for entities if from_entity mapping matches.\n\n        Args:\n            entities: The list of entities.\n\n        Returns:\n            A list of `SlotSet` events.", "file": "rasa_shared_core_domain.py", "function": "Domain.slots_for_entities", "project": "rasa", "change_stats": {"code_changes": 32, "doc_changes": 4, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _wait_for(self, method, args, timeout, msg):\n        \n        _wait = (\n            self._wd_wait if timeout is None else WebDriverWait(self.driver, timeout)\n        )\n        logger.debug(\n            \"method, timeout, poll => %s %s %s\",\n            method,\n            _wait._timeout,  # pylint: disable=protected-access\n            _wait._poll,  # pylint: disable=protected-access\n        )\n\n        return _wait.until(method(*args), msg)```\n\nUpdated code:\n```python\ndef _wait_for(self, method, args, timeout, msg):\n        \n        _wait = (\n            self._wd_wait\n            if timeout is None\n            else WebDriverWait(self.driver, timeout)\n        )\n        return _wait.until(method(*args), msg)\n```\n\nOriginal documentation:\n```python\nAbstract generic pattern for explicit WebDriverWait.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "abstract generic pattern for explicit webdriver wait", "file": "dash_testing_browser.py", "function": "Browser._wait_for", "project": "dash", "change_stats": {"code_changes": 13, "doc_changes": 4, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef benchmark(\n    funcs: List[Callable],\n    args: Union[Tuple[Any], List[Tuple[Any]]],\n    num_steps: int,\n    func_names: Optional[List[str]] = None,\n    num_warmups: int = 10,\n    backward: bool = False,\n    per_step: bool = False,\n    progress_bar: bool = False,\n):\n    \n    \n    from tabulate import tabulate\n\n    if num_steps <= 0:\n        raise ValueError(f\"'num_steps' must be a positive integer \"\n                         f\"(got {num_steps})\")\n\n    if num_warmups <= 0:\n        raise ValueError(f\"'num_warmups' must be a positive integer \"\n                         f\"(got {num_warmups})\")\n\n    if func_names is None:\n        func_names = [get_func_name(func) for func in funcs]\n\n    if len(funcs) != len(func_names):\n        raise ValueError(f\"Length of 'funcs' (got {len(funcs)}) and \"\n                         f\"'func_names' (got {len(func_names)}) must be equal\")\n\n    # Zero-copy `args` for each function (if necessary):\n    args_list = [args] * len(funcs) if isinstance(args, tuple) else args\n\n    iterator = zip(funcs, args_list, func_names)\n    if progress_bar:\n        from tqdm import tqdm\n        iterator = tqdm(iterator, total=len(funcs))\n\n    ts: List[List[str]] = []\n    for func, args, name in iterator:\n        t_forward = t_backward = 0\n        for i in range(num_warmups + num_steps):\n            args = require_grad(args, backward)\n\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            t_start = time.perf_counter()\n\n            out = func(*args)\n\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            if i >= num_warmups:\n                t_forward += time.perf_counter() - t_start\n\n            if backward:\n                if isinstance(out, (tuple, list)):\n                    out = sum(o.sum() for o in out if isinstance(o, Tensor))\n                elif isinstance(out, dict):\n                    out = out.values()\n                    out = sum(o.sum() for o in out if isinstance(o, Tensor))\n\n                out_grad = torch.randn_like(out)\n                t_start = time.perf_counter()\n\n                out.backward(out_grad)\n\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n                if i >= num_warmups:\n                    t_backward += time.perf_counter() - t_start\n\n        if per_step:\n            ts.append([name, f'{t_forward/num_steps:.6f}s'])\n        else:\n            ts.append([name, f'{t_forward:.4f}s'])\n        if backward:\n            if per_step:\n                ts[-1].append(f'{t_backward/num_steps:.6f}s')\n                ts[-1].append(f'{(t_forward + t_backward)/num_steps:.6f}s')\n            else:\n                ts[-1].append(f'{t_backward:.4f}s')\n                ts[-1].append(f'{t_forward + t_backward:.4f}s')\n\n    header = ['Name', 'Forward']\n    if backward:\n        header.extend(['Backward', 'Total'])\n\n    print(tabulate(ts, headers=header, tablefmt='psql'))```\n\nUpdated code:\n```python\ndef benchmark(\n    funcs: List[Callable],\n    args: Union[Tuple[Any], List[Tuple[Any]]],\n    num_steps: int,\n    func_names: Optional[List[str]] = None,\n    num_warmups: int = 10,\n    backward: bool = False,\n    per_step: bool = False,\n    progress_bar: bool = False,\n):\n    \n    \n    from tabulate import tabulate\n\n    if num_steps <= 0:\n        raise ValueError(f\"'num_steps' must be a positive integer \"\n                         f\"(got {num_steps})\")\n\n    if num_warmups <= 0:\n        raise ValueError(f\"'num_warmups' must be a positive integer \"\n                         f\"(got {num_warmups})\")\n\n    if func_names is None:\n        func_names = [get_func_name(func) for func in funcs]\n\n    if len(funcs) != len(func_names):\n        raise ValueError(f\"Length of 'funcs' (got {len(funcs)}) and \"\n                         f\"'func_names' (got {len(func_names)}) must be equal\")\n\n    # Zero-copy `args` for each function (if necessary):\n    args_list = [args] * len(funcs) if not isinstance(args, list) else args\n\n    iterator = zip(funcs, args_list, func_names)\n    if progress_bar:\n        from tqdm import tqdm\n        iterator = tqdm(iterator, total=len(funcs))\n\n    ts: List[List[str]] = []\n    for func, inputs, name in iterator:\n        t_forward = t_backward = 0\n        for i in range(num_warmups + num_steps):\n            args = inputs() if callable(inputs) else inputs\n            args = require_grad(args, backward)\n\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            t_start = time.perf_counter()\n\n            out = func(*args)\n\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            if i >= num_warmups:\n                t_forward += time.perf_counter() - t_start\n\n            if backward:\n                if isinstance(out, (tuple, list)):\n                    out = sum(o.sum() for o in out if isinstance(o, Tensor))\n                elif isinstance(out, dict):\n                    out = out.values()\n                    out = sum(o.sum() for o in out if isinstance(o, Tensor))\n\n                out_grad = torch.randn_like(out)\n                t_start = time.perf_counter()\n\n                out.backward(out_grad)\n\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n                if i >= num_warmups:\n                    t_backward += time.perf_counter() - t_start\n\n        if per_step:\n            ts.append([name, f'{t_forward/num_steps:.6f}s'])\n        else:\n            ts.append([name, f'{t_forward:.4f}s'])\n        if backward:\n            if per_step:\n                ts[-1].append(f'{t_backward/num_steps:.6f}s')\n                ts[-1].append(f'{(t_forward + t_backward)/num_steps:.6f}s')\n            else:\n                ts[-1].append(f'{t_backward:.4f}s')\n                ts[-1].append(f'{t_forward + t_backward:.4f}s')\n\n    header = ['Name', 'Forward']\n    if backward:\n        header.extend(['Backward', 'Total'])\n\n    print(tabulate(ts, headers=header, tablefmt='psql'))\n```\n\nOriginal documentation:\n```python\nr\"\"\"Benchmark a list of functions :obj:`funcs` that receive the same set\n    of arguments :obj:`args`.\n\n    Args:\n        funcs ([Callable]): The list of functions to benchmark.\n        args ((Any, ) or [(Any, )]): The arguments to pass to the functions.\n            Can be a list of arguments for each function in :obj:`funcs` in\n            case their headers differ.\n        num_steps (int): The number of steps to run the benchmark.\n        func_names ([str], optional): The names of the functions. If not given,\n            will try to infer the name from the function itself.\n            (default: :obj:`None`)\n        num_warmups (int, optional): The number of warmup steps.\n            (default: :obj:`10`)\n        backward (bool, optional): If set to :obj:`True`, will benchmark both\n            forward and backward passes. (default: :obj:`False`)\n        per_step (bool, optional): If set to :obj:`True`, will report runtimes\n            per step. (default: :obj:`False`)\n        progress_bar (bool, optional): If set to :obj:`True`, will print a\n            progress bar during benchmarking. (default: :obj:`False`)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Benchmark a list of functions :obj:`funcs` that receive the same set\n    of arguments :obj:`args`.\n\n    Args:\n        funcs ([Callable]): The list of functions to benchmark.\n        args ((Any, ) or [(Any, )]): The arguments to pass to the functions.\n            Can be a list of arguments for each function in :obj:`funcs` in\n            case their headers differ.\n            Alternatively, you can pass in functions that generate arguments\n            on-the-fly (e.g., useful for benchmarking models on various sizes).\n        num_steps (int): The number of steps to run the benchmark.\n        func_names ([str], optional): The names of the functions. If not given,\n            will try to infer the name from the function itself.\n            (default: :obj:`None`)\n        num_warmups (int, optional): The number of warmup steps.\n            (default: :obj:`10`)\n        backward (bool, optional): If set to :obj:`True`, will benchmark both\n            forward and backward passes. (default: :obj:`False`)\n        per_step (bool, optional): If set to :obj:`True`, will report runtimes\n            per step. (default: :obj:`False`)\n        progress_bar (bool, optional): If set to :obj:`True`, will print a\n            progress bar during benchmarking. (default: :obj:`False`)", "file": "torch_geometric_profile_benchmark.py", "function": "benchmark", "project": "pytorch_geometric", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_radius(self):\n        \n        \n        \n        return self.width / 2.```\n\nUpdated code:\n```python\ndef get_radius(self):\n        \n        return self.width / 2.\n```\n\nOriginal documentation:\n```python\nReturn the radius of the circle\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the radius of the circle.", "file": "lib_matplotlib_patches.py", "function": "Circle.get_radius", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef replay(fn=None, trace=None, sites=None):\n    \n    \n    \n    msngr = ReplayMessenger(trace=trace, sites=sites)\n    return msngr(fn) if fn is not None else msngr```\n\nUpdated code:\n```python\ndef replay(fn=None, trace=None, params=None):\n    \n    \n    \n    msngr = ReplayMessenger(trace=trace, params=params)\n    return msngr(fn) if fn is not None else msngr\n```\n\nOriginal documentation:\n```python\nGiven a callable that contains Pyro primitive calls,\n    return a callable that runs the original, reusing the values at sites in trace\n    at those sites in the new trace\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(\"s\", torch.tensor(0.5))\n        ...     z = pyro.sample(\"z\", dist.Normal(x, s))\n        ...     return z ** 2\n\n    ``replay`` makes ``sample`` statements behave as if they had sampled the values\n    at the corresponding sites in the trace:\n\n        >>> replayed_model = replay(model, trace=old_trace)\n        >>> replayed_model(0.0) == old_trace.nodes[\"_RETURN\"][\"value\"]\n        True\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param trace: a :class:`~pyro.poutine.Trace` data structure to replay against\n    :param sites: list or dict of names of sample sites in fn to replay against,\n        defaulting to all sites\n    :returns: a stochastic function decorated with a :class:`~pyro.poutine.replay_messenger.ReplayMessenger`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Given a callable that contains Pyro primitive calls,\n    return a callable that runs the original, reusing the values at sites in trace\n    at those sites in the new trace\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(\"s\", torch.tensor(0.5))\n        ...     z = pyro.sample(\"z\", dist.Normal(x, s))\n        ...     return z ** 2\n\n    ``replay`` makes ``sample`` statements behave as if they had sampled the values\n    at the corresponding sites in the trace:\n\n        >>> replayed_model = replay(model, trace=old_trace)\n        >>> replayed_model(0.0) == old_trace.nodes[\"_RETURN\"][\"value\"]\n        True\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param trace: a :class:`~pyro.poutine.Trace` data structure to replay against\n    :param params: dict of names of param sites and constrained values\n        in fn to replay against\n    :returns: a stochastic function decorated with a :class:`~pyro.poutine.replay_messenger.ReplayMessenger`", "file": "pyro_poutine_handlers.py", "function": "replay", "project": "pyro", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rotate_image_by_angle(image, angle, rotated_width=None, rotated_height=None):\n          \n        height, width = image.shape[:2]\n        image_center = (width/2, height/2)\n        rotation_matrix = cv2.getRotationMatrix2D(image_center, -1.*angle, 1.)\n        if rotated_width is None or rotated_height is None:\n            abs_cos = abs(rotation_matrix[0, 0])\n            abs_sin = abs(rotation_matrix[0, 1])\n            if rotated_width is None:\n                rotated_width = int(height*abs_sin + width*abs_cos)\n            if rotated_height is None:\n                rotated_height = int(height*abs_cos + width*abs_sin)\n        rotation_matrix[0, 2] += rotated_width/2 - image_center[0]\n        rotation_matrix[1, 2] += rotated_height/2 - image_center[1]\n        return cv2.warpAffine(image, rotation_matrix, (rotated_width, rotated_height))```\n\nUpdated code:\n```python\ndef rotate_image_by_angle(image, angle,\n                              rotated_width=None, rotated_height=None):\n          \n        height, width = image.shape[:2]\n        image_center = (width/2, height/2)\n        rotation_matrix = cv2.getRotationMatrix2D(image_center, -1.*angle, 1.)\n        if rotated_width is None or rotated_height is None:\n            abs_cos = abs(rotation_matrix[0, 0])\n            abs_sin = abs(rotation_matrix[0, 1])\n            if rotated_width is None:\n                rotated_width = int(height*abs_sin + width*abs_cos)\n            if rotated_height is None:\n                rotated_height = int(height*abs_cos + width*abs_sin)\n        rotation_matrix[0, 2] += rotated_width/2 - image_center[0]\n        rotation_matrix[1, 2] += rotated_height/2 - image_center[1]\n        return cv2.warpAffine(image,\n                              rotation_matrix,\n                              (rotated_width, rotated_height))\n```\n\nOriginal documentation:\n```python\nRotate an image by a given angle. From:\n            https://stackoverflow.com/questions/22041699\n            This is required by both Faces and Images so placed here for now\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Rotate an image by a given angle.\n            From: https://stackoverflow.com/questions/22041699\n\n            This is required by both Faces and Images\n            so placed here for now", "file": "scripts_fsmedia.py", "function": "Utils.rotate_image_by_angle", "project": "faceswap", "change_stats": {"code_changes": 9, "doc_changes": 10, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_device_changing_ips(\n            self,\n            flow_type,\n            smac,\n            profileid,\n            twid,\n            uid,\n            timestamp\n    ):\n        \n        \n        \n        if 'conn' not in flow_type:\n            return\n\n        if not smac:\n            return\n\n        saddr: str = profileid.split(\"_\")[-1]\n        if not (\n                validators.ipv4(saddr)\n                and utils.is_private_ip(ipaddress.ip_address(saddr))\n        ):\n            return\n\n        if self.db.was_ip_seen_in_connlog_before(saddr):\n            # we should only check once for the first time we're seeing this flow\n            return\n        self.db.mark_srcip_as_seen_in_connlog(saddr)\n\n\n        if old_ip_list := self.db.get_ip_of_mac(smac):\n            # old_ip is a list that may contain the ipv6 of this MAC\n            # this ipv6 may be of the same device that has the given saddr and MAC\n            # so this would be fp. so, make sure we're dealing with ipv4 only\n            for ip in json.loads(old_ip_list):\n                if validators.ipv4(ip):\n                    old_ip = ip\n                    break\n            else:\n                # all the IPs associated with the given macs are ipv6,\n                # 1 computer might have several ipv6, AND/OR a combination of ipv6 and 4\n                # so this detection will only work if both the\n                # old ip and the given saddr are ipv4 private ips\n                return\n\n            if old_ip != saddr:\n                # we found this smac associated with an ip other than this saddr\n                self.set_evidence.device_changing_ips(\n                    smac,\n                    old_ip,\n                    profileid,\n                    twid,\n                    uid,\n                    timestamp\n                )```\n\nUpdated code:\n```python\ndef check_device_changing_ips(\n            self,\n            flow_type,\n            smac,\n            profileid,\n            twid,\n            uid,\n            timestamp\n    ):\n        \n        \n        \n        if 'conn' not in flow_type:\n            return\n\n        if not smac:\n            return\n\n        saddr: str = profileid.split(\"_\")[-1]\n        if not (\n                validators.ipv4(saddr)\n                and utils.is_private_ip(ipaddress.ip_address(saddr))\n        ):\n            return\n\n        if self.db.was_ip_seen_in_connlog_before(saddr):\n            # we should only check once for the first\n            # time we're seeing this flow\n            return\n        self.db.mark_srcip_as_seen_in_connlog(saddr)\n\n\n        if old_ip_list := self.db.get_ip_of_mac(smac):\n            # old_ip is a list that may contain the ipv6 of this MAC\n            # this ipv6 may be of the same device that\n            # has the given saddr and MAC\n            # so this would be fp. so, make sure we're dealing with ipv4 only\n            for ip in json.loads(old_ip_list):\n                if validators.ipv4(ip):\n                    old_ip = ip\n                    break\n            else:\n                # all the IPs associated with the given macs are ipv6,\n                # 1 computer might have several ipv6,\n                # AND/OR a combination of ipv6 and 4\n                # so this detection will only work if both the\n                # old ip and the given saddr are ipv4 private ips\n                return\n\n            if old_ip != saddr:\n                # we found this smac associated with an\n                # ip other than this saddr\n                self.set_evidence.device_changing_ips(\n                    smac,\n                    old_ip,\n                    profileid,\n                    twid,\n                    uid,\n                    timestamp\n                )\n```\n\nOriginal documentation:\n```python\nEvery time we have a flow for a new ip (an ip that we're seeing for the first time)\n        we check if the MAC of this srcip was associated with another ip\n        this check is only done once for each source ip slips sees\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Every time we have a flow for a new ip\n            (an ip that we're seeing for the first time)\n        we check if the MAC of this srcip was associated with another ip\n        this check is only done once for each source ip slips sees", "file": "modules_flowalerts_flowalerts.py", "function": "FlowAlerts.check_device_changing_ips", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 14, "doc_changes": 5, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef find_data_files():\n    \n    \n    \n\n    docdirbase  = pjoin('share', 'doc', 'ipython')\n    manpagebase = pjoin('share', 'man', 'man1')\n\n    # Simple file lists can be made by hand\n    manpages = [f for f in glob(pjoin('docs','man','*.1.gz')) if isfile(f)]\n    if not manpages:\n        # When running from a source tree, the manpages aren't gzipped\n        manpages = [f for f in glob(pjoin('docs','man','*.1')) if isfile(f)]\n\n    igridhelpfiles = [f for f in glob(pjoin('IPython','extensions','igrid_help.*')) if isfile(f)]\n\n    # For nested structures, use the utility above\n    example_files = make_dir_struct(\n        'data',\n        pjoin('docs','examples'),\n        pjoin(docdirbase,'examples')\n    )\n    manual_files = make_dir_struct(\n        'data',\n        pjoin('docs','html'),\n        pjoin(docdirbase,'manual')\n    )\n\n    # And assemble the entire output list\n    data_files = [ (manpagebase, manpages),\n                   (pjoin(docdirbase, 'extensions'), igridhelpfiles),\n                   ] + manual_files + example_files\n\n    return data_files```\n\nUpdated code:\n```python\ndef find_data_files():\n    \n    \n    \n\n    manpagebase = pjoin('share', 'man', 'man1')\n\n    # Simple file lists can be made by hand\n    manpages = [f for f in glob(pjoin('docs','man','*.1.gz')) if isfile(f)]\n    if not manpages:\n        # When running from a source tree, the manpages aren't gzipped\n        manpages = [f for f in glob(pjoin('docs','man','*.1')) if isfile(f)]\n\n    # And assemble the entire output list\n    data_files = [ (manpagebase, manpages) ]\n\n    return data_files\n```\n\nOriginal documentation:\n```python\nFind IPython's data_files.\n\n    Most of these are docs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Find IPython's data_files.\n\n    Just man pages at this point.", "file": "setupbase.py", "function": "find_data_files", "project": "ipython", "change_stats": {"code_changes": 21, "doc_changes": 4, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef QA_fetch_index_min(\n    code,\n    start,\n    end,\n    format='numpy',\n    frequence='1min',\n    collections=DATABASE.index_min\n):\n    ''\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n    _data = []\n    code = QA_util_code_tolist(code)\n    cursor = collections.find(\n        {\n            'code': {\n                '$in': code\n            },\n            \"time_stamp\":\n                {\n                    \"$gte\": QA_util_time_stamp(start),\n                    \"$lte\": QA_util_time_stamp(end)\n                },\n            'type': frequence\n        },\n        {\"_id\": 0},\n        batch_size=10000\n    )\n    if format in ['dict', 'json']:\n        return [data for data in cursor]\n    # for item in cursor:\n    _data = pd.DataFrame([item for item in cursor])\n    _data = _data.assign(datetime=pd.to_datetime(_data['datetime']))\n    # _data.append([str(item['code']), float(item['open']),\n    # float(item['high']), float(\n    #     item['low']), float(item['close']), int(item['up_count']),\n    #     int(item['down_count']), float(item['vol']), float(item['amount']),\n    #     item['datetime'], item['time_stamp'], item['date'], item['type']])\n\n    # _data = DataFrame(_data, columns=[\n    #     'code', 'open', 'high', 'low', 'close', 'up_count', 'down_count',\n    #     'volume', 'amount', 'datetime', 'time_stamp', 'date', 'type'])\n\n    # _data['datetime'] = pd.to_datetime(_data['datetime'])\n    _data = _data.set_index('datetime', drop=False)\n    if format in ['numpy', 'np', 'n']:\n        return numpy.asarray(_data)\n    elif format in ['list', 'l', 'L']:\n        return numpy.asarray(_data).tolist()\n    elif format in ['P', 'p', 'pandas', 'pd']:\n        return _data```\n\nUpdated code:\n```python\ndef QA_fetch_index_min(\n    code,\n    start,\n    end,\n    format='numpy',\n    frequence='1min',\n    collections=DATABASE.index_min\n):\n    ''\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n    _data = []\n    code = QA_util_code_tolist(code)\n\n    cursor = collections.find(\n        {\n            'code': {\n                '$in': code\n            },\n            \"time_stamp\":\n                {\n                    \"$gte\": QA_util_time_stamp(start),\n                    \"$lte\": QA_util_time_stamp(end)\n                },\n            'type': frequence\n        },\n        {\"_id\": 0},\n        batch_size=10000\n    )\n\n    res = pd.DataFrame([item for item in cursor])\n    try:\n        res = res.assign(\n            volume=res.vol,\n            datetime=pd.to_datetime(res.datetime)\n        ).query('volume>1').drop_duplicates(['datetime',\n                                             'code']).set_index(\n                                                 'datetime',\n                                                 drop=False\n                                             )\n        # return res\n    except:\n        res = None\n\n    # \u591a\u79cd\u6570\u636e\u683c\u5f0f\n    if format in ['P', 'p', 'pandas', 'pd']:\n        return res\n    elif format in ['json', 'dict']:\n        return QA_util_to_json_from_pandas(res)\n        \n    elif format in ['n', 'N', 'numpy']:\n        return numpy.asarray(res)\n    elif format in ['list', 'l', 'L']:\n        return numpy.asarray(res).tolist()\n    else:\n        print(\n            \"QA Error QA_fetch_index_min format parameter %s is none of  \\\"P, p, pandas, pd , json, dict , n, N, numpy, list, l, L, !\\\" \"\n            % format\n        )\n        return None\n```\n\nOriginal documentation:\n```python\n\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u83b7\u53d6\u6307\u6570\u5206\u949f\u7ebf", "file": "QUANTAXIS_QAFetch_QAQuery.py", "function": "QA_fetch_index_min", "project": "QUANTAXIS", "change_stats": {"code_changes": 51, "doc_changes": 4, "total_changes": 55}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _guild_operations_get_dispatch(self):\n        \n        \n        \n        # Where the fleet switches are. | 1 | | 2 | | +`|\n        detection_area = (540, 370, 740, 410)\n        # Color of the red dot: | +`|\n        dot_color = (255, 93, 90)\n        # Offset inside to avoid clicking on edge\n        pad = 2\n\n        points = color_similarity_2d(self.device.image.crop(detection_area), color=dot_color)\n        points = np.array(np.where(points > 235)).T[:, ::-1]\n        if not len(points):\n            logger.info('No available dispatch')\n            return None\n\n        point = np.round(np.mean(points, axis=0)).astype(int) + detection_area[:2]\n        area = area_offset(area=(-25, 0, 0, 25), offset=point)\n        button = Button(area=area, color=(), button=area_pad(area, pad), name='DISPATCH_FLEET_SWITCH')\n        logger.info('Found available dispatch')\n        return button```\n\nUpdated code:\n```python\ndef _guild_operations_get_dispatch(self):\n        \n        \n        \n        # Fleet switch, for 4 situation\n        #          | 1 |\n        #       | 1 | | 2 |\n        #    | 1 | | 2 | | 3 |\n        # | 1 | | 2 | | 3 | | 4 |\n        #   0  1  2  3  4  5  6   buttons in switch_grid\n        switch_grid = ButtonGrid(origin=(573.5, 381), delta=(20.5, 0), button_shape=(11, 24), grid_shape=(7, 1))\n        # Color of inactive fleet switch\n        color_active = (74, 117, 222)\n        # Color of current fleet\n        color_inactive = (33, 48, 66)\n\n        text = []\n        index = 0\n        button = None\n        for switch in switch_grid.buttons():\n            if self.image_color_count(switch, color=color_inactive, threshold=235, count=30):\n                index += 1\n                text.append(f'| {index} |')\n                button = switch\n            elif self.image_color_count(switch, color=color_active, threshold=235, count=30):\n                index += 1\n                text.append(f'[ {index} ]')\n                button = switch\n\n        # log example: | 1 | | 2 | [ 3 ]\n        text = ' '.join(text)\n        logger.attr('Dispatch_fleet', text)\n        if text.endswith(']'):\n            logger.info('Already at the most right fleet')\n            return None\n        else:\n            return button\n```\n\nOriginal documentation:\n```python\nGet the button to switch available dispatch\n\n        Returns:\n            Button: Button to switch available dispatch. None if red dot not found.\n\n        Pages:\n            in: page_guild, guild operation, operation dispatch preparation (GUILD_DISPATCH_RECOMMEND)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the button to switch available dispatch\n        In previous version, this function detects the red dot on the switch.\n        But the red dot may not shows for unknown reason sometimes, so we detect the switch itself.\n\n        Returns:\n            Button: Button to switch available dispatch. None if already reach the most right fleet.\n\n        Pages:\n            in: page_guild, guild operation, operation dispatch preparation (GUILD_DISPATCH_RECOMMEND)", "file": "module_guild_operations.py", "function": "GuildOperations._guild_operations_get_dispatch", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 49, "doc_changes": 6, "total_changes": 55}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef densenet161(pretrained=False, **kwargs):\n    \n    \n    model = _densenet(\n        'densenet161', growth_rate=48, block_config=(6, 12, 36, 24), num_init_features=96,\n        pretrained=pretrained, **kwargs)\n    return model```\n\nUpdated code:\n```python\ndef densenet161(pretrained=False, **kwargs):\n    \n    \n    model = _densenet(\n        'densenet161', growth_rate=48, block_config=(6, 12, 36, 24), pretrained=pretrained, **kwargs)\n    return model\n```\n\nOriginal documentation:\n```python\nr\"\"\"Densenet-201 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Densenet-161 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`", "file": "timm_models_densenet.py", "function": "densenet161", "project": "pytorch-image-models", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef copyfile(src, dst, *, follow_symlinks=True):\n    \n\n    \n    if _samefile(src, dst):\n        raise SameFileError(\"{!r} and {!r} are the same file\".format(src, dst))\n\n    for fn in [src, dst]:\n        try:\n            st = os.stat(fn)\n        except OSError:\n            # File most likely does not exist\n            pass\n        else:\n            # XXX What about other special files? (sockets, devices...)\n            if stat.S_ISFIFO(st.st_mode):\n                raise SpecialFileError(\"`%s` is a named pipe\" % fn)\n\n    if not follow_symlinks and os.path.islink(src):\n        os.symlink(os.readlink(src), dst)\n    else:\n        with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:\n            if _HAS_SENDFILE:\n                try:\n                    _fastcopy_sendfile(fsrc, fdst)\n                    return dst\n                except _GiveupOnFastCopy:\n                    pass\n\n            if _HAS_FCOPYFILE:\n                try:\n                    _fastcopy_osx(fsrc, fdst, posix._COPYFILE_DATA)\n                    return dst\n                except _GiveupOnFastCopy:\n                    pass\n\n            _copybinfileobj(fsrc, fdst)\n\n    return dst```\n\nUpdated code:\n```python\ndef copyfile(src, dst, *, follow_symlinks=True):\n    \n\n    \n    if _samefile(src, dst):\n        raise SameFileError(\"{!r} and {!r} are the same file\".format(src, dst))\n\n    file_size = 0\n    for i, fn in enumerate([src, dst]):\n        try:\n            st = os.stat(fn)\n        except OSError:\n            # File most likely does not exist\n            pass\n        else:\n            # XXX What about other special files? (sockets, devices...)\n            if stat.S_ISFIFO(st.st_mode):\n                raise SpecialFileError(\"`%s` is a named pipe\" % fn)\n            if _WINDOWS and i == 0:\n                file_size = st.st_size\n\n    if not follow_symlinks and os.path.islink(src):\n        os.symlink(os.readlink(src), dst)\n    else:\n        with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:\n            # macOS\n            if _HAS_FCOPYFILE:\n                try:\n                    _fastcopy_fcopyfile(fsrc, fdst, posix._COPYFILE_DATA)\n                    return dst\n                except _GiveupOnFastCopy:\n                    pass\n            # Linux / Solaris\n            elif _HAS_SENDFILE:\n                try:\n                    _fastcopy_sendfile(fsrc, fdst)\n                    return dst\n                except _GiveupOnFastCopy:\n                    pass\n            # Windows, see:\n            # https://github.com/python/cpython/pull/7160#discussion_r195405230\n            elif _WINDOWS and file_size > 0:\n                _copyfileobj_readinto(fsrc, fdst, min(file_size, COPY_BUFSIZE))\n                return dst\n\n            copyfileobj(fsrc, fdst)\n\n    return dst\n```\n\nOriginal documentation:\n```python\nCopy data from src to dst.\n\n    If follow_symlinks is not set and src is a symbolic link, a new\n    symlink will be created instead of copying the file it points to.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Copy data from src to dst in the most efficient way possible.\n\n    If follow_symlinks is not set and src is a symbolic link, a new\n    symlink will be created instead of copying the file it points to.", "file": "Lib_shutil.py", "function": "copyfile", "project": "cpython", "change_stats": {"code_changes": 31, "doc_changes": 4, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _credit_card_type(cls, card_type=None):\n        \n        if card_type is None:\n            card_type = cls.random_element(cls.credit_card_types.keys())\n        elif isinstance(card_type, CreditCard):\n            return card_type\n        return cls.credit_card_types[card_type]```\n\nUpdated code:\n```python\ndef _credit_card_type(cls, card_type=None):\n          \n        if card_type is None:\n            card_type = cls.random_element(cls.credit_card_types.keys())\n        elif isinstance(card_type, CreditCard):\n            return card_type\n        return cls.credit_card_types[card_type]\n```\n\nOriginal documentation:\n```python\nreturns a random credit card type instance\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a random credit card type instance.", "file": "faker_providers_credit_card___init__.py", "function": "Provider._credit_card_type", "project": "faker", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_running_head_node(config: Dict[str, Any],\n                           printable_config_file: str,\n                           override_cluster_name: Optional[str],\n                           create_if_needed: bool = False,\n                           _provider: Optional[NodeProvider] = None) -> str:\n    \n    provider = _provider or _get_node_provider(config[\"provider\"],\n                                               config[\"cluster_name\"])\n    head_node_tags = {\n        TAG_RAY_NODE_KIND: NODE_KIND_HEAD,\n    }\n    nodes = provider.non_terminated_nodes(head_node_tags)\n    head_node = None\n    for node in nodes:\n        node_state = provider.node_tags(node).get(TAG_RAY_NODE_STATUS)\n        if node_state == STATUS_UP_TO_DATE:\n            head_node = node\n        else:\n            cli_logger.warning(f\"Head node ({node}) is in state {node_state}.\")\n\n    if head_node is not None:\n        return head_node\n    elif create_if_needed:\n        get_or_create_head_node(\n            config,\n            printable_config_file=printable_config_file,\n            restart_only=False,\n            no_restart=False,\n            yes=True,\n            override_cluster_name=override_cluster_name)\n        return _get_running_head_node(\n            config,\n            printable_config_file,\n            override_cluster_name,\n            create_if_needed=False)\n    else:\n        raise RuntimeError(\"Head node of cluster ({}) not found!\".format(\n            config[\"cluster_name\"]))```\n\nUpdated code:\n```python\ndef _get_running_head_node(\n        config: Dict[str, Any],\n        printable_config_file: str,\n        override_cluster_name: Optional[str],\n        create_if_needed: bool = False,\n        _provider: Optional[NodeProvider] = None,\n        _allow_uninitialized_state: bool = False,\n) -> str:\n    \n\n    \n    provider = _provider or _get_node_provider(config[\"provider\"],\n                                               config[\"cluster_name\"])\n    head_node_tags = {\n        TAG_RAY_NODE_KIND: NODE_KIND_HEAD,\n    }\n    nodes = provider.non_terminated_nodes(head_node_tags)\n    head_node = None\n    _backup_head_node = None\n    for node in nodes:\n        node_state = provider.node_tags(node).get(TAG_RAY_NODE_STATUS)\n        if node_state == STATUS_UP_TO_DATE:\n            head_node = node\n        else:\n            _backup_head_node = node\n            cli_logger.warning(f\"Head node ({node}) is in state {node_state}.\")\n\n    if head_node is not None:\n        return head_node\n    elif create_if_needed:\n        get_or_create_head_node(\n            config,\n            printable_config_file=printable_config_file,\n            restart_only=False,\n            no_restart=False,\n            yes=True,\n            override_cluster_name=override_cluster_name)\n        # NOTE: `_allow_uninitialized_state` is forced to False if\n        # `create_if_needed` is set to True. This is to ensure that the\n        # commands executed after creation occur on an actually running\n        # cluster.\n        return _get_running_head_node(\n            config,\n            printable_config_file,\n            override_cluster_name,\n            create_if_needed=False,\n            _allow_uninitialized_state=False)\n    else:\n        if _allow_uninitialized_state and _backup_head_node is not None:\n            cli_logger.warning(\n                f\"The head node being returned: {_backup_head_node} is not \"\n                \"`up-to-date`. If you are not debugging a startup issue \"\n                \"it is recommended to restart this head node with: {}\",\n                cf.bold(f\"  ray down  {printable_config_file}\"))\n\n            return _backup_head_node\n        raise RuntimeError(\"Head node of cluster ({}) not found!\".format(\n            config[\"cluster_name\"]))\n```\n\nOriginal documentation:\n```python\nGet a valid, running head node\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get a valid, running head node.\n    Args:\n        config (Dict[str, Any]): Cluster Config dictionary\n        printable_config_file (str): Used for printing formatted CLI commands.\n        override_cluster_name (str): Passed to `get_or_create_head_node` to\n            override the cluster name present in `config`.\n        create_if_needed (bool): Create a head node if one is not present.\n        _provider (NodeProvider): [For testing], a Node Provider to use.\n        _allow_uninitialized_state (bool): Whether to return a head node that\n            is not 'UP TO DATE'. This is used to allow `ray attach` and\n            `ray exec` to debug a cluster in a bad state.", "file": "python_ray_autoscaler__private_commands.py", "function": "_get_running_head_node", "project": "ray", "change_stats": {"code_changes": 34, "doc_changes": 14, "total_changes": 48}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _process(self, nb_assets: Optional[int] = 0) -> bool:\n        \n        \n        \n        state_changed = False\n        try:\n            # Refresh whitelist based on wallet maintenance\n            sanitized_list = self._refresh_whitelist(\n                self._gen_pair_whitelist(\n                    self.config['stake_currency']\n                ) if nb_assets else self.config['exchange']['pair_whitelist']\n            )\n\n            # Keep only the subsets of pairs wanted (up to nb_assets)\n            self.active_pair_whitelist = sanitized_list[:nb_assets] if nb_assets else sanitized_list\n\n            # Refreshing candles\n            self.exchange.refresh_tickers(self.active_pair_whitelist, self.strategy.ticker_interval)\n\n            # Query trades from persistence layer\n            trades = Trade.query.filter(Trade.is_open.is_(True)).all()\n\n            # First process current opened trades\n            for trade in trades:\n                state_changed |= self.process_maybe_execute_sell(trade)\n\n            # Then looking for buy opportunities\n            if len(trades) < self.config['max_open_trades']:\n                state_changed = self.process_maybe_execute_buy()\n\n            if 'unfilledtimeout' in self.config:\n                # Check and handle any timed out open orders\n                self.check_handle_timedout()\n                Trade.session.flush()\n\n        except TemporaryError as error:\n            logger.warning('%s, retrying in 30 seconds...', error)\n            time.sleep(constants.RETRY_TIMEOUT)\n        except OperationalException:\n            tb = traceback.format_exc()\n            hint = 'Issue `/start` if you think it is safe to restart.'\n            self.rpc.send_msg({\n                'type': RPCMessageType.STATUS_NOTIFICATION,\n                'status': f'OperationalException:\\n```\\n{tb}```{hint}'\n            })\n            logger.exception('OperationalException. Stopping trader ...')\n            self.state = State.STOPPED\n        return state_changed```\n\nUpdated code:\n```python\ndef _process(self) -> bool:\n        \n        \n        \n        state_changed = False\n        try:\n            nb_assets = self.config.get('dynamic_whitelist', None)\n            # Refresh whitelist based on wallet maintenance\n            sanitized_list = self._refresh_whitelist(\n                self._gen_pair_whitelist(\n                    self.config['stake_currency']\n                ) if nb_assets else self.config['exchange']['pair_whitelist']\n            )\n\n            # Keep only the subsets of pairs wanted (up to nb_assets)\n            self.active_pair_whitelist = sanitized_list[:nb_assets] if nb_assets else sanitized_list\n\n            # Refreshing candles\n            self.exchange.refresh_tickers(self.active_pair_whitelist, self.strategy.ticker_interval)\n\n            # Query trades from persistence layer\n            trades = Trade.query.filter(Trade.is_open.is_(True)).all()\n\n            # First process current opened trades\n            for trade in trades:\n                state_changed |= self.process_maybe_execute_sell(trade)\n\n            # Then looking for buy opportunities\n            if len(trades) < self.config['max_open_trades']:\n                state_changed = self.process_maybe_execute_buy()\n\n            if 'unfilledtimeout' in self.config:\n                # Check and handle any timed out open orders\n                self.check_handle_timedout()\n                Trade.session.flush()\n\n        except TemporaryError as error:\n            logger.warning('%s, retrying in 30 seconds...', error)\n            time.sleep(constants.RETRY_TIMEOUT)\n        except OperationalException:\n            tb = traceback.format_exc()\n            hint = 'Issue `/start` if you think it is safe to restart.'\n            self.rpc.send_msg({\n                'type': RPCMessageType.STATUS_NOTIFICATION,\n                'status': f'OperationalException:\\n```\\n{tb}```{hint}'\n            })\n            logger.exception('OperationalException. Stopping trader ...')\n            self.state = State.STOPPED\n        return state_changed\n```\n\nOriginal documentation:\n```python\nQueries the persistence layer for open trades and handles them,\n        otherwise a new trade is created.\n        :param: nb_assets: the maximum number of pairs to be traded at the same time\n        :return: True if one or more trades has been created or closed, False otherwise\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Queries the persistence layer for open trades and handles them,\n        otherwise a new trade is created.\n        :return: True if one or more trades has been created or closed, False otherwise", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot._process", "project": "freqtrade", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef call(self, features, return_covmat=False):\n    \n    \n    logits = super().call(features)\n\n    # Extracts logits and covariance matrix from model output.\n    if self.use_gp_layer:\n      logits, covmat = logits\n    else:\n      covmat = None\n\n    # Computes the uncertainty-adjusted logits.\n    logits = gaussian_process.mean_field_logits(\n        logits, covmat, mean_field_factor=self.temperature)\n\n    if return_covmat and covmat is not None:\n      return logits, covmat\n\n    return logits```\n\nUpdated code:\n```python\ndef call(self, features, training=False, return_covmat=False):\n    \n    \n    logits = super().call(features)\n\n    # Extracts logits and covariance matrix from model output.\n    if self.use_gp_layer:\n      logits, covmat = logits\n    else:\n      covmat = None\n\n    # Computes the uncertainty-adjusted logits during evaluation.\n    if not training:\n      logits = gaussian_process.mean_field_logits(\n          logits, covmat, mean_field_factor=self.temperature)\n\n    if return_covmat and covmat is not None:\n      return logits, covmat\n    return logits\n```\n\nOriginal documentation:\n```python\nReturns model output.\n\n    Arguments:\n      features: A tensor of input features, shape (batch_size, feature_dim).\n      return_covmat: Whether the model should also return covariance matrix if\n        `use_gp_layer=True`. During training, it is recommended to set\n        `return_covmat=False` to be compatible with the standard Keras pipelines\n        (e.g., `model.fit()`).\n\n    Returns:\n      logits: Uncertainty-adjusted predictive logits, shape\n        (batch_size, num_classes).\n      covmat: (Optional) Covariance matrix, shape (batch_size, batch_size).\n        Returned only when return_covmat=True.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns model output.\n\n    Dring training, the model returns raw logits. During evaluation, the model\n    returns uncertainty adjusted logits, and (optionally) the covariance matrix.\n\n    Arguments:\n      features: A tensor of input features, shape (batch_size, feature_dim).\n      training: Whether the model is in training mode.\n      return_covmat: Whether the model should also return covariance matrix if\n        `use_gp_layer=True`. During training, it is recommended to set\n        `return_covmat=False` to be compatible with the standard Keras pipelines\n        (e.g., `model.fit()`).\n\n    Returns:\n      logits: Uncertainty-adjusted predictive logits, shape\n        (batch_size, num_classes).\n      covmat: (Optional) Covariance matrix, shape (batch_size, batch_size).\n        Returned only when return_covmat=True.", "file": "official_nlp_modeling_layers_cls_head.py", "function": "GaussianProcessClassificationHead.call", "project": "models", "change_stats": {"code_changes": 12, "doc_changes": 6, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_file_path(self, file_path: str) -> Union[dict, None]:\n        \n\n        \n        if not file_path:\n            return None\n\n        ioc = self.check_file_name(os.path.basename(file_path))\n        if ioc:\n            return ioc\n\n        for ioc in self.get_iocs(\"file_paths\"):\n            # Strip any trailing slash from indicator paths to match directories.\n            if file_path.startswith(ioc[\"value\"].rstrip(\"/\")):\n                self.log.warning(\"Found a known suspicious file path \\\"%s\\\" matching indicators form \\\"%s\\\"\",\n                                 file_path, ioc[\"name\"])\n                return ioc\n\n        return None```\n\nUpdated code:\n```python\ndef check_file_path(self, file_path: str) -> Union[dict, None]:\n        \n\n        \n        if not file_path:\n            return None\n\n        ioc = self.check_file_name(os.path.basename(file_path))\n        if ioc:\n            return ioc\n\n        for ioc in self.get_iocs(\"file_paths\"):\n            # Strip any trailing slash from indicator paths to match\n            # directories.\n            if file_path.startswith(ioc[\"value\"].rstrip(\"/\")):\n                self.log.warning(\"Found a known suspicious file path \\\"%s\\\" \"\n                                 \"matching indicators form \\\"%s\\\"\",\n                                 file_path, ioc[\"name\"])\n                return ioc\n\n        return None\n```\n\nOriginal documentation:\n```python\nCheck the provided file path against the list of file indicators (both path and name).\n\n        :param file_path: File path or file name to check against file\n        indicators\n        :type file_path: str\n        :returns: Indicator details if matched, otherwise None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check the provided file path against the list of file indicators\n        (both path and name).\n\n        :param file_path: File path or file name to check against file\n        indicators\n        :type file_path: str\n        :returns: Indicator details if matched, otherwise None", "file": "mvt_common_indicators.py", "function": "Indicators.check_file_path", "project": "mvt", "change_stats": {"code_changes": 8, "doc_changes": 5, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef tricontourf(self, X, Y, Z, offset=None, zdir='z', *args, **kwargs):\n        '''\n        \n        '''\n        zdir = kwargs.pop('zdir', 'z')\n        offset = kwargs.pop('offset', None)\n\n        had_data = self.has_data()\n\n        jX, jY, jZ = art3d.rotate_axes(X, Y, Z, zdir)\n        cset = Axes.tricontourf(self, X, Y, Z, *args, **kwargs)\n        self.add_contourf_set(cset, zdir, offset)\n\n        self.auto_scale_xyz(X, Y, Z, had_data)\n        return cset```\n\nUpdated code:\n```python\ndef tricontourf(self, *args, **kwargs):\n        \n        \n        \n        zdir = kwargs.pop('zdir', 'z')\n        offset = kwargs.pop('offset', None)\n\n        had_data = self.has_data()\n\n        tri, args, kwargs = Triangulation.get_from_args_and_kwargs(\n                *args, **kwargs)\n        X = tri.x\n        Y = tri.y\n        Z = args[0]\n\n        # We do this so Z doesn't get passed as an arg to Axes.tricontour\n        args = args[1:]\n\n        jX, jY, jZ = art3d.rotate_axes(X, Y, Z, zdir)\n        tri = Triangulation(jX, jY, tri.triangles, tri.mask)\n\n        cset = Axes.tricontourf(self, tri, jZ, *args, **kwargs)\n        self.add_contourf_set(cset, zdir, offset)\n\n        self.auto_scale_xyz(X, Y, Z, had_data)\n        return cset\n```\n\nOriginal documentation:\n```python\nCreate a 3D contourf plot.\n\n        ==========  ================================================\n        Argument    Description\n        ==========  ================================================\n        *X*, *Y*,   Data values as numpy.arrays\n        *Z*\n        *zdir*      The direction to use: x, y or z (default)\n        *offset*    If specified plot a projection of the contour\n                    lines on this position in plane normal to zdir\n        ==========  ================================================\n\n        Other keyword arguments are passed on to\n        :func:`~matplotlib.axes.Axes.tricontour`\n\n        Returns a :class:`~matplotlib.axes.Axes.contour`\n\n        .. versionadded :: 1.1.0\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create a 3D contourf plot.\n\n        ==========  ================================================\n        Argument    Description\n        ==========  ================================================\n        *X*, *Y*,   Data values as numpy.arrays\n        *Z*\n        *zdir*      The direction to use: x, y or z (default)\n        *offset*    If specified plot a projection of the contour\n                    lines on this position in plane normal to zdir\n        ==========  ================================================\n\n        Other keyword arguments are passed on to\n        :func:`~matplotlib.axes.Axes.tricontour`\n\n        Returns a :class:`~matplotlib.axes.Axes.contour`\n\n        .. versionchanged :: 1.3.0\n            Added support for custom triangulations", "file": "lib_mpl_toolkits_mplot3d_axes3d.py", "function": "Axes3D.tricontourf", "project": "matplotlib", "change_stats": {"code_changes": 21, "doc_changes": 5, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_issue(self, issue_number: int) -> Dict[str, Any]:\n        \n        \n        \n        issue = self.github_repo_instance.get_issue(number=issue_number)\n        page = 0\n        comments: List[dict] = []\n        while len(comments) <= 10:\n            comments_page = issue.get_comments().get_page(page)\n            if len(comments_page) == 0:\n                break\n            for comment in comments_page:\n                comments.append({\"body\": comment.body, \"user\": comment.user.login})\n            page += 1\n\n        return {\n            \"title\": issue.title,\n            \"body\": issue.body,\n            \"comments\": str(comments),\n        }```\n\nUpdated code:\n```python\ndef get_issue(self, issue_number: int) -> Dict[str, Any]:\n        \n        \n        \n        issue = self.github_repo_instance.get_issue(number=issue_number)\n        page = 0\n        comments: List[dict] = []\n        while len(comments) <= 10:\n            comments_page = issue.get_comments().get_page(page)\n            if len(comments_page) == 0:\n                break\n            for comment in comments_page:\n                comments.append({\"body\": comment.body, \"user\": comment.user.login})\n            page += 1\n\n        opened_by = None\n        if issue.user and issue.user.login:\n            opened_by = issue.user.login\n\n        return {\n            \"number\": issue_number,\n            \"title\": issue.title,\n            \"body\": issue.body,\n            \"comments\": str(comments),\n            \"opened_by\": str(opened_by),\n        }\n```\n\nOriginal documentation:\n```python\nFetches a specific issue and its first 10 comments\n        Parameters:\n            issue_number(int): The number for the github issue\n        Returns:\n            dict: A doctionary containing the issue's title,\n            body, and comments as a string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fetches a specific issue and its first 10 comments\n        Parameters:\n            issue_number(int): The number for the github issue\n        Returns:\n            dict: A dictionary containing the issue's title,\n            body, comments as a string, and the username of the user\n            who opened the issue", "file": "libs_langchain_langchain_utilities_github.py", "function": "GitHubAPIWrapper.get_issue", "project": "langchain", "change_stats": {"code_changes": 8, "doc_changes": 7, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef who(self):\n        \n        return f\"{self.bulb.ip_addr} ({self.name})\"```\n\nUpdated code:\n```python\ndef who(self):\n        \n        return f\"{self.name} ({self.bulb.mac_addr})\"\n```\n\nOriginal documentation:\n```python\nReturn a string identifying the bulb.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a string identifying the bulb by name and mac.", "file": "homeassistant_components_lifx_light.py", "function": "LIFXLight.who", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef stock_board_concept_hist_em(symbol: str = \"\u6570\u5b57\u8d27\u5e01\", start_date: str = \"20220101\", end_date: str = \"20221128\", adjust: str = \"\") -> pd.DataFrame:\n    \n    \n    \n    stock_board_concept_em_map = stock_board_concept_name_em()\n    stock_board_code = stock_board_concept_em_map[\n        stock_board_concept_em_map[\"\u677f\u5757\u540d\u79f0\"] == symbol\n    ][\"\u677f\u5757\u4ee3\u7801\"].values[0]\n    adjust_map = {\"\": \"0\", \"qfq\": \"1\", \"hfq\": \"2\"}\n    url = \"http://91.push2his.eastmoney.com/api/qt/stock/kline/get\"\n    params = {\n        \"secid\": f\"90.{stock_board_code}\",\n        \"ut\": \"fa5fd1943c7b386f172d6893dbfba10b\",\n        \"fields1\": \"f1,f2,f3,f4,f5,f6\",\n        \"fields2\": \"f51,f52,f53,f54,f55,f56,f57,f58,f59,f60,f61\",\n        \"klt\": \"101\",\n        \"fqt\": adjust_map[adjust],\n        \"beg\": start_date,\n        \"end\": end_date,\n        \"smplmt\": \"10000\",\n        \"lmt\": \"1000000\",\n        \"_\": \"1626079488673\",\n    }\n    r = requests.get(url, params=params)\n    data_json = r.json()\n    temp_df = pd.DataFrame([item.split(\",\") for item in data_json[\"data\"][\"klines\"]])\n    temp_df.columns = [\n        \"\u65e5\u671f\",\n        \"\u5f00\u76d8\",\n        \"\u6536\u76d8\",\n        \"\u6700\u9ad8\",\n        \"\u6700\u4f4e\",\n        \"\u6210\u4ea4\u91cf\",\n        \"\u6210\u4ea4\u989d\",\n        \"\u632f\u5e45\",\n        \"\u6da8\u8dcc\u5e45\",\n        \"\u6da8\u8dcc\u989d\",\n        \"\u6362\u624b\u7387\",\n    ]\n    temp_df = temp_df[\n        [\n            \"\u65e5\u671f\",\n            \"\u5f00\u76d8\",\n            \"\u6536\u76d8\",\n            \"\u6700\u9ad8\",\n            \"\u6700\u4f4e\",\n            \"\u6da8\u8dcc\u5e45\",\n            \"\u6da8\u8dcc\u989d\",\n            \"\u6210\u4ea4\u91cf\",\n            \"\u6210\u4ea4\u989d\",\n            \"\u632f\u5e45\",\n            \"\u6362\u624b\u7387\",\n        ]\n    ]\n    temp_df[\"\u5f00\u76d8\"] = pd.to_numeric(temp_df[\"\u5f00\u76d8\"], errors=\"coerce\")\n    temp_df[\"\u6536\u76d8\"] = pd.to_numeric(temp_df[\"\u6536\u76d8\"], errors=\"coerce\")\n    temp_df[\"\u6700\u9ad8\"] = pd.to_numeric(temp_df[\"\u6700\u9ad8\"], errors=\"coerce\")\n    temp_df[\"\u6700\u4f4e\"] = pd.to_numeric(temp_df[\"\u6700\u4f4e\"], errors=\"coerce\")\n    temp_df[\"\u6da8\u8dcc\u5e45\"] = pd.to_numeric(temp_df[\"\u6da8\u8dcc\u5e45\"], errors=\"coerce\")\n    temp_df[\"\u6da8\u8dcc\u989d\"] = pd.to_numeric(temp_df[\"\u6da8\u8dcc\u989d\"], errors=\"coerce\")\n    temp_df[\"\u6210\u4ea4\u91cf\"] = pd.to_numeric(temp_df[\"\u6210\u4ea4\u91cf\"], errors=\"coerce\")\n    temp_df[\"\u6210\u4ea4\u989d\"] = pd.to_numeric(temp_df[\"\u6210\u4ea4\u989d\"], errors=\"coerce\")\n    temp_df[\"\u632f\u5e45\"] = pd.to_numeric(temp_df[\"\u632f\u5e45\"], errors=\"coerce\")\n    temp_df[\"\u6362\u624b\u7387\"] = pd.to_numeric(temp_df[\"\u6362\u624b\u7387\"], errors=\"coerce\")\n    return temp_df```\n\nUpdated code:\n```python\ndef stock_board_concept_hist_em(\n    symbol: str = \"\u6570\u5b57\u8d27\u5e01\",\n    period: str = \"daily\",\n    start_date: str = \"20220101\",\n    end_date: str = \"20221128\",\n    adjust: str = \"\",\n) -> pd.DataFrame:\n    \n    \n    \n    period_map = {\n        \"daily\": \"101\",\n        \"weekly\": \"102\",\n        \"monthly\": \"103\",\n    }\n    stock_board_concept_em_map = stock_board_concept_name_em()\n    stock_board_code = stock_board_concept_em_map[\n        stock_board_concept_em_map[\"\u677f\u5757\u540d\u79f0\"] == symbol\n    ][\"\u677f\u5757\u4ee3\u7801\"].values[0]\n    adjust_map = {\"\": \"0\", \"qfq\": \"1\", \"hfq\": \"2\"}\n    url = \"http://91.push2his.eastmoney.com/api/qt/stock/kline/get\"\n    params = {\n        \"secid\": f\"90.{stock_board_code}\",\n        \"ut\": \"fa5fd1943c7b386f172d6893dbfba10b\",\n        \"fields1\": \"f1,f2,f3,f4,f5,f6\",\n        \"fields2\": \"f51,f52,f53,f54,f55,f56,f57,f58,f59,f60,f61\",\n        \"klt\": period_map[period],\n        \"fqt\": adjust_map[adjust],\n        \"beg\": start_date,\n        \"end\": end_date,\n        \"smplmt\": \"10000\",\n        \"lmt\": \"1000000\",\n        \"_\": \"1626079488673\",\n    }\n    r = requests.get(url, params=params)\n    data_json = r.json()\n    temp_df = pd.DataFrame([item.split(\",\") for item in data_json[\"data\"][\"klines\"]])\n    temp_df.columns = [\n        \"\u65e5\u671f\",\n        \"\u5f00\u76d8\",\n        \"\u6536\u76d8\",\n        \"\u6700\u9ad8\",\n        \"\u6700\u4f4e\",\n        \"\u6210\u4ea4\u91cf\",\n        \"\u6210\u4ea4\u989d\",\n        \"\u632f\u5e45\",\n        \"\u6da8\u8dcc\u5e45\",\n        \"\u6da8\u8dcc\u989d\",\n        \"\u6362\u624b\u7387\",\n    ]\n    temp_df = temp_df[\n        [\n            \"\u65e5\u671f\",\n            \"\u5f00\u76d8\",\n            \"\u6536\u76d8\",\n            \"\u6700\u9ad8\",\n            \"\u6700\u4f4e\",\n            \"\u6da8\u8dcc\u5e45\",\n            \"\u6da8\u8dcc\u989d\",\n            \"\u6210\u4ea4\u91cf\",\n            \"\u6210\u4ea4\u989d\",\n            \"\u632f\u5e45\",\n            \"\u6362\u624b\u7387\",\n        ]\n    ]\n    temp_df[\"\u5f00\u76d8\"] = pd.to_numeric(temp_df[\"\u5f00\u76d8\"], errors=\"coerce\")\n    temp_df[\"\u6536\u76d8\"] = pd.to_numeric(temp_df[\"\u6536\u76d8\"], errors=\"coerce\")\n    temp_df[\"\u6700\u9ad8\"] = pd.to_numeric(temp_df[\"\u6700\u9ad8\"], errors=\"coerce\")\n    temp_df[\"\u6700\u4f4e\"] = pd.to_numeric(temp_df[\"\u6700\u4f4e\"], errors=\"coerce\")\n    temp_df[\"\u6da8\u8dcc\u5e45\"] = pd.to_numeric(temp_df[\"\u6da8\u8dcc\u5e45\"], errors=\"coerce\")\n    temp_df[\"\u6da8\u8dcc\u989d\"] = pd.to_numeric(temp_df[\"\u6da8\u8dcc\u989d\"], errors=\"coerce\")\n    temp_df[\"\u6210\u4ea4\u91cf\"] = pd.to_numeric(temp_df[\"\u6210\u4ea4\u91cf\"], errors=\"coerce\")\n    temp_df[\"\u6210\u4ea4\u989d\"] = pd.to_numeric(temp_df[\"\u6210\u4ea4\u989d\"], errors=\"coerce\")\n    temp_df[\"\u632f\u5e45\"] = pd.to_numeric(temp_df[\"\u632f\u5e45\"], errors=\"coerce\")\n    temp_df[\"\u6362\u624b\u7387\"] = pd.to_numeric(temp_df[\"\u6362\u624b\u7387\"], errors=\"coerce\")\n    return temp_df\n```\n\nOriginal documentation:\n```python\n\u4e1c\u65b9\u8d22\u5bcc\u7f51-\u6caa\u6df1\u677f\u5757-\u6982\u5ff5\u677f\u5757-\u5386\u53f2\u884c\u60c5\n    https://quote.eastmoney.com/bk/90.BK0715.html\n    :param symbol: \u677f\u5757\u540d\u79f0\n    :type symbol: str\n    :param start_date: \u5f00\u59cb\u65f6\u95f4\n    :type start_date: str\n    :param end_date: \u7ed3\u675f\u65f6\u95f4\n    :type end_date: str\n    :param adjust: choice of {'': \u4e0d\u590d\u6743, \"qfq\": \u524d\u590d\u6743, \"hfq\": \u540e\u590d\u6743}\n    :type adjust: str\n    :return: \u5386\u53f2\u884c\u60c5\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u4e1c\u65b9\u8d22\u5bcc\u7f51-\u6caa\u6df1\u677f\u5757-\u6982\u5ff5\u677f\u5757-\u5386\u53f2\u884c\u60c5\n    https://quote.eastmoney.com/bk/90.BK0715.html\n    :param symbol: \u677f\u5757\u540d\u79f0\n    :type symbol: str\n    :type period: \u5468\u671f; choice of {\"daily\", \"weekly\", \"monthly\"}\n    :param period: \u677f\u5757\u540d\u79f0\n    :param start_date: \u5f00\u59cb\u65f6\u95f4\n    :type start_date: str\n    :param end_date: \u7ed3\u675f\u65f6\u95f4\n    :type end_date: str\n    :param adjust: choice of {'': \u4e0d\u590d\u6743, \"qfq\": \u524d\u590d\u6743, \"hfq\": \u540e\u590d\u6743}\n    :type adjust: str\n    :return: \u5386\u53f2\u884c\u60c5\n    :rtype: pandas.DataFrame", "file": "akshare_stock_stock_board_concept_em.py", "function": "stock_board_concept_hist_em", "project": "akshare", "change_stats": {"code_changes": 17, "doc_changes": 4, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef cspline2d(signal, lamb=0.0, precision=-1.0):\n    \n    \n    \n    if lamb <= 1 / 144.0:\n        # Normal cubic spline\n        r = -2 + np.sqrt(3.0)\n        out = _symiirorder1_nd(signal, -r * 6.0, r, precision=precision,\n                               axis=-1)\n        out = _symiirorder1_nd(out - r * 6.0, r, precision=precision,\n                               axis=0)```\n\nUpdated code:\n```python\ndef cspline2d(signal, lamb=0.0, precision=-1.0):\n    \n    \n    \n    if lamb <= 1 / 144.0:\n        # Normal cubic spline\n        r = -2 + np.sqrt(3.0)\n        out = _symiirorder1_nd(signal, -r * 6.0, r, precision=precision,\n                               axis=-1)\n        out = _symiirorder1_nd(out, -r * 6.0, r, precision=precision,\n                               axis=0)\n        return out\n\n    r, omega = compute_root_from_lambda(lamb)\n    out = _symiirorder2_nd(signal, r, omega, precision=precision, axis=-1)\n    out = _symiirorder2_nd(out, r, omega, precision=precision, axis=0)\n    return out\n```\n\nOriginal documentation:\n```python\nCoefficients for 2-D cubic (3rd order) B-spline.\n\n    Return the third-order B-spline coefficients over a regularly spaced\n    input grid for the two-dimensional input image.\n\n    Parameters\n    ----------\n    input : ndarray\n        The input signal.\n    lambda : float\n        Specifies the amount of smoothing in the transfer function.\n    precision : float\n        Specifies the precision for computing the infinite sum needed to apply\n        mirror-symmetric boundary conditions.\n\n    Returns\n    -------\n    output : ndarray\n        The filtered signal.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Coefficients for 2-D cubic (3rd order) B-spline.\n\n    Return the third-order B-spline coefficients over a regularly spaced\n    input grid for the two-dimensional input image.\n\n    Parameters\n    ----------\n    input : ndarray\n        The input signal.\n    lamb : float\n        Specifies the amount of smoothing in the transfer function.\n    precision : float\n        Specifies the precision for computing the infinite sum needed to apply\n        mirror-symmetric boundary conditions.\n\n    Returns\n    -------\n    output : ndarray\n        The filtered signal.", "file": "cupyx_scipy_signal__bsplines.py", "function": "cspline2d", "project": "cupy", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_serializable_dict(self, mask_secrets=False):\n        \n        \n        \n        serializable_dict = {}\n        for k in sorted(six.iterkeys(self._fields)):\n            v = getattr(self, k)\n            v = str(v) if isinstance(v, JSON_UNFRIENDLY_TYPES) else v\n            serializable_dict[k] = v\n        return serializable_dict```\n\nUpdated code:\n```python\ndef to_serializable_dict(self, mask_secrets=False):\n        \n        \n        \n        serializable_dict = {}\n        for k in sorted(six.iterkeys(self._fields)):\n            v = getattr(self, k)\n            v = str(v) if isinstance(v, JSON_UNFRIENDLY_TYPES) else v\n            serializable_dict[k] = v\n\n        if mask_secrets:\n            serializable_dict = self.mask_secrets(value=serializable_dict)\n\n        return serializable_dict\n```\n\nOriginal documentation:\n```python\n:param mask_secrets: True to mask secrets in the resulting dict.\n        :type mask_secrets: ``boolean``\n\n        :rtype: ``dict``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Serialize database model to a dictionary.\n\n        :param mask_secrets: True to mask secrets in the resulting dict.\n        :type mask_secrets: ``boolean``\n\n        :rtype: ``dict``", "file": "st2common_st2common_models_db_stormbase.py", "function": "StormFoundationDB.to_serializable_dict", "project": "st2", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_options_completion(completer, complete_event):\n    \n    \n    \n\n    ps_opts = _get_command_option_names('ps')\n\n    _test_options_completion(\n        completer, complete_event, 'ps ', ps_opts, 0)\n\n    _test_options_completion(\n        completer, complete_event, 'ps --', [n for n in ps_opts if n.startswith('--')], -2)\n\n    _test_options_completion(\n        completer, complete_event, 'ps --h', ['--help'], -3)```\n\nUpdated code:\n```python\ndef test_options_completion(command, expected, expected_pos):\n    \n    \n    \n    c = completer()\n    e = complete_event()\n\n    position = len(command)\n\n    result = set(c.get_completions(\n        Document(text=command, cursor_position=position), e))\n\n    expected = set(map(lambda t: Completion(t, expected_pos), expected))\n\n    assert result == expected\n```\n\nOriginal documentation:\n```python\nAfter command name, suggest command options.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test command options suggestions.\n    :param command: string: text that user started typing\n    :param expected: list: expected completions", "file": "tests_test_completer.py", "function": "test_options_completion", "project": "wharfee", "change_stats": {"code_changes": 18, "doc_changes": 6, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef handle_action_point(self, zone, pinned, cost=None):\n        \n        \n        \n        if not self._is_in_action_point():\n            return False\n\n        # AP boxes have an animation to show\n        self.action_point_safe_get()\n        if cost is None:\n            cost = self.action_point_get_cost(zone, pinned)\n        buy_checked = False\n        if self._action_point_total <= self.config.OS_ACTION_POINT_PRESERVE:\n            logger.info(f'Reach the limit of action points, preserve={self.config.OS_ACTION_POINT_PRESERVE}')\n            self.action_point_quit()\n            raise ActionPointLimit\n\n        for _ in range(12):\n            # Having enough action points\n            if self._action_point_current >= cost:\n                logger.info('Having enough action points')\n                self.action_point_quit()\n                return True\n\n            # Buy action points\n            if self.config.OpsiGeneral_BuyActionPointLimit > 0 and not buy_checked:\n                if self.action_point_buy(preserve=self.config.OpsiGeneral_OilLimit):\n                    continue\n                else:\n                    buy_checked = True\n\n            # Sort action point boxes\n            box = []\n            for index in [1, 2, 3]:\n                if self._action_point_box[index] > 0:\n                    if self._action_point_current + ACTION_POINT_BOX[index] >= 200:\n                        box.append(index)\n                    else:\n                        box.insert(0, index)\n\n            # Use action point boxes\n            if len(box):\n                if self._action_point_total > self.config.OS_ACTION_POINT_PRESERVE:\n                    self.action_point_set_button(box[0])\n                    self.action_point_use()\n                    continue\n                else:\n                    logger.info(f'Reach the limit of action points, preserve={self.config.OS_ACTION_POINT_PRESERVE}')\n                    self.action_point_quit()\n                    raise ActionPointLimit\n            else:\n                logger.info('No more action point boxes')\n                self.action_point_quit()\n                raise ActionPointLimit\n\n        logger.warning('Failed to get action points after 12 trial')\n        return False```\n\nUpdated code:\n```python\ndef handle_action_point(self, zone, pinned, cost=None, keep_current_ap=True):\n        \n        \n        \n        if not self._is_in_action_point():\n            return False\n\n        # AP boxes have an animation to show\n        self.action_point_safe_get()\n        if cost is None:\n            cost = self.action_point_get_cost(zone, pinned)\n        buy_checked = False\n        time_now = get_server_next_update(daily_trigger=[datetime.now().strftime('%H:%M')])\n        today_rest = 144 - time_now.hour * 6 - time_now.minute / 10\n        if keep_current_ap:\n            if self._action_point_current + today_rest >= 200:\n                logger.info(f'The sum of the current action points and the rest action points that can be obtained today exceeds 200.')\n                logger.info(f'Current={self._action_point_current}  Rest={today_rest}')\n            elif self._action_point_total <= self.config.OS_ACTION_POINT_PRESERVE:\n                logger.info(f'Reach the limit of action points, preserve={self.config.OS_ACTION_POINT_PRESERVE}')\n                self.action_point_quit()\n                raise ActionPointLimit\n\n        for _ in range(12):\n            # Having enough action points\n            if self._action_point_current >= cost:\n                logger.info('Having enough action points')\n                self.action_point_quit()\n                return True\n\n            # Buy action points\n            if self.config.OpsiGeneral_BuyActionPointLimit > 0 and not buy_checked:\n                if self.action_point_buy(preserve=self.config.OpsiGeneral_OilLimit):\n                    continue\n                else:\n                    buy_checked = True\n\n            # Sort action point boxes\n            box = []\n            for index in [1, 2, 3]:\n                if self._action_point_box[index] > 0:\n                    if self._action_point_current + ACTION_POINT_BOX[index] >= 200:\n                        box.append(index)\n                    else:\n                        box.insert(0, index)\n\n            # Use action point boxes\n            if len(box):\n                if self._action_point_total > self.config.OS_ACTION_POINT_PRESERVE:\n                    self.action_point_set_button(box[0])\n                    self.action_point_use()\n                    continue\n                else:\n                    logger.info(f'Reach the limit of action points, preserve={self.config.OS_ACTION_POINT_PRESERVE}')\n                    self.action_point_quit()\n                    raise ActionPointLimit\n            else:\n                logger.info('No more action point boxes')\n                self.action_point_quit()\n                raise ActionPointLimit\n\n        logger.warning('Failed to get action points after 12 trial')\n        return False\n```\n\nOriginal documentation:\n```python\nArgs:\n            zone (Zone): Zone to enter.\n            pinned (str): Zone type. Available types: DANGEROUS, SAFE, OBSCURE, ABYSSAL, STRONGHOLD.\n            cost (int): Custom action point cost value.\n\n        Returns:\n            bool: If handled.\n\n        Raises:\n            ActionPointLimit: If not having enough action points.\n\n        Pages:\n            in: ACTION_POINT_USE\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            zone (Zone): Zone to enter.\n            pinned (str): Zone type. Available types: DANGEROUS, SAFE, OBSCURE, ABYSSAL, STRONGHOLD.\n            cost (int): Custom action point cost value.\n            keep_current_ap (bool): Check action points first to avoid using remaining AP\n                when it not enough for tomorrow's daily\n\n        Returns:\n            bool: If handled.\n\n        Raises:\n            ActionPointLimit: If not having enough action points.\n\n        Pages:\n            in: ACTION_POINT_USE", "file": "module_os_handler_action_point.py", "function": "ActionPointHandler.handle_action_point", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_line_short_enough(line: Line, *, line_length: int, line_str: str = \"\") -> bool:\n    \n    \n    if not line_str:\n        line_str = line_to_string(line)\n    return (\n        len(line_str) <= line_length\n        and \"\\n\" not in line_str  # multiline strings\n        and not line.contains_standalone_comments()\n    )```\n\nUpdated code:\n```python\ndef is_line_short_enough(  # noqa: C901\n    line: Line, *, mode: Mode, line_str: str = \"\"\n) -> bool:\n    \n    \n    if not line_str:\n        line_str = line_to_string(line)\n\n    if Preview.multiline_string_handling not in mode:\n        return (\n            len(line_str) <= mode.line_length\n            and \"\\n\" not in line_str  # multiline strings\n            and not line.contains_standalone_comments()\n        )\n\n    if line.contains_standalone_comments():\n        return False\n    if \"\\n\" not in line_str:\n        # No multiline strings (MLS) present\n        return len(line_str) <= mode.line_length\n\n    first, *_, last = line_str.split(\"\\n\")\n    if len(first) > mode.line_length or len(last) > mode.line_length:\n        return False\n\n    # Traverse the AST to examine the context of the multiline string (MLS),\n    # tracking aspects such as depth and comma existence,\n    # to determine whether to split the MLS or keep it together.\n    # Depth (which is based on the existing bracket_depth concept)\n    # is needed to determine nesting level of the MLS.\n    # Includes special case for trailing commas.\n    commas: List[int] = []  # tracks number of commas per depth level\n    multiline_string: Optional[Leaf] = None\n    # store the leaves that contain parts of the MLS\n    multiline_string_contexts: List[LN] = []\n\n    max_level_to_update = math.inf  # track the depth of the MLS\n    for i, leaf in enumerate(line.leaves):\n        if max_level_to_update == math.inf:\n            had_comma: Optional[int] = None\n            if leaf.bracket_depth + 1 > len(commas):\n                commas.append(0)\n            elif leaf.bracket_depth + 1 < len(commas):\n                had_comma = commas.pop()\n            if (\n                had_comma is not None\n                and multiline_string is not None\n                and multiline_string.bracket_depth == leaf.bracket_depth + 1\n            ):\n                # Have left the level with the MLS, stop tracking commas\n                max_level_to_update = leaf.bracket_depth\n                if had_comma > 0:\n                    # MLS was in parens with at least one comma - force split\n                    return False\n\n        if leaf.bracket_depth <= max_level_to_update and leaf.type == token.COMMA:\n            # Ignore non-nested trailing comma\n            # directly after MLS/MLS-containing expression\n            ignore_ctxs: List[Optional[LN]] = [None]\n            ignore_ctxs += multiline_string_contexts\n            if not (leaf.prev_sibling in ignore_ctxs and i == len(line.leaves) - 1):\n                commas[leaf.bracket_depth] += 1\n        if max_level_to_update != math.inf:\n            max_level_to_update = min(max_level_to_update, leaf.bracket_depth)\n\n        if is_multiline_string(leaf):\n            if len(multiline_string_contexts) > 0:\n                # >1 multiline string cannot fit on a single line - force split\n                return False\n            multiline_string = leaf\n            ctx: LN = leaf\n            # fetch the leaf components of the MLS in the AST\n            while str(ctx) in line_str:\n                multiline_string_contexts.append(ctx)\n                if ctx.parent is None:\n                    break\n                ctx = ctx.parent\n\n    # May not have a triple-quoted multiline string at all,\n    # in case of a regular string with embedded newlines and line continuations\n    if len(multiline_string_contexts) == 0:\n        return True\n\n    return all(val == 0 for val in commas)\n```\n\nOriginal documentation:\n```python\nReturn True if `line` is no longer than `line_length`.\n\n    Uses the provided `line_str` rendering, if any, otherwise computes a new one.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "For non-multiline strings, return True if `line` is no longer than `line_length`.\n    For multiline strings, looks at the context around `line` to determine\n    if it should be inlined or split up.\n    Uses the provided `line_str` rendering, if any, otherwise computes a new one.", "file": "src_black_lines.py", "function": "is_line_short_enough", "project": "black", "change_stats": {"code_changes": 88, "doc_changes": 7, "total_changes": 95}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _maybe_upcast(values, fill_value=np.nan, copy=False):\n      \n    new_dtype, fill_value = _maybe_promote(values.dtype, fill_value)\n    if new_dtype != values.dtype:\n        values = values.astype(new_dtype)\n    elif copy:\n        values = values.copy()\n    return values, fill_value```\n\nUpdated code:\n```python\ndef _maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):\n      \n\n    if dtype is None:\n        dtype = values.dtype\n    new_dtype, fill_value = _maybe_promote(dtype, fill_value)\n    if new_dtype != values.dtype:\n        values = values.astype(new_dtype)\n    elif copy:\n        values = values.copy()\n    return values, fill_value\n```\n\nOriginal documentation:\n```python\nprovide explicty type promotion and coercion\n        if copy == True, then a copy is created even if no upcast is required\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "provide explicty type promotion and coercion\n\n        Parameters\n        ----------\n        values : the ndarray that we want to maybe upcast\n        fill_value : what we want to fill with\n        dtype : if None, then use the dtype of the values, else coerce to this type\n        copy : if True always make a copy even if no upcast is required", "file": "pandas_core_common.py", "function": "_maybe_upcast", "project": "pandas", "change_stats": {"code_changes": 9, "doc_changes": 10, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_ticker(self, symbol: str, params={}) -> Ticker:\n        \n        \n        \n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'currencyPair': market['id'],\n        }\n        response = self.publicGetTicker(self.extend(request, params))\n        ticker = self.safe_value(response, 'data')\n        timestamp = self.safe_timestamp(ticker, 'timestamp')\n        last = self.safe_number(ticker, 'last')\n        return self.safe_ticker({\n            'symbol': market['symbol'],\n            'timestamp': timestamp,\n            'datetime': self.iso8601(timestamp),\n            'high': self.safe_number(ticker, 'high'),\n            'low': self.safe_number(ticker, 'low'),\n            'bid': self.safe_number(ticker, 'bid'),\n            'bidVolume': None,\n            'ask': self.safe_number(ticker, 'ask'),\n            'vwap': None,\n            'askVolume': None,\n            'open': None,\n            'close': last,\n            'last': last,\n            'previousClose': None,\n            'change': None,\n            'percentage': None,\n            'average': None,\n            'baseVolume': self.safe_number(ticker, 'amount'),\n            'quoteVolume': None,\n            'info': ticker,\n        }, market)```\n\nUpdated code:\n```python\ndef fetch_ticker(self, symbol: str, params={}) -> Ticker:\n        \n        \n        \n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'currencyPair': market['id'],\n        }\n        response = self.publicGetTicker(self.extend(request, params))\n        #\n        #     {\n        #         \"error\": False,\n        #         \"errorMessage\": null,\n        #         \"data\": {\n        #             \"last\": 0.55105,\n        #             \"high\": 0.56439,\n        #             \"low\": 0.54358,\n        #             \"amount\": 37038.993381,\n        #             \"bid\": 0.54595,\n        #             \"ask\": 0.55324,\n        #             \"change\": 3.03659243,\n        #             \"open\": 0.53481,\n        #             \"timestamp\": 1708074779\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data')\n        return self.parse_ticker(data, market)\n```\n\nOriginal documentation:\n```python\nfetches a price ticker, a statistical calculation with the information calculated over the past 24 hours for a specific market\n        :param str symbol: unified symbol of the market to fetch the ticker for\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :returns dict: a `ticker structure <https://docs.ccxt.com/#/?id=ticker-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetches a price ticker, a statistical calculation with the information calculated over the past 24 hours for a specific market\n        :see: https://coinmate.docs.apiary.io/#reference/ticker/get-ticker/get\n        :param str symbol: unified symbol of the market to fetch the ticker for\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :returns dict: a `ticker structure <https://docs.ccxt.com/#/?id=ticker-structure>`", "file": "python_ccxt_coinmate.py", "function": "coinmate.fetch_ticker", "project": "ccxt", "change_stats": {"code_changes": 46, "doc_changes": 3, "total_changes": 49}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_columns(self, table_name: str) -> StatusResponse:\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef get_columns(self, table_name: str) -> StatusResponse:\n        \n        \n        \n\n        query = f\"PRAGMA table_info([{table_name}]);\"\n        result = self.native_query(query)\n        df = result.data_frame\n        result.data_frame = df.rename(columns={'name': 'column_name', 'type': 'data_type'})\n        return result\n```\n\nOriginal documentation:\n```python\nReturns a list of entity columns.\n        Args:\n            table_name (str): name of one of tables returned by self.get_tables()\n        Returns:\n          HandlerResponse: should have same columns as information_schema.columns\n            (https://dev.mysql.com/doc/refman/8.0/en/information-schema-columns-table.html)\n            Column 'COLUMN_NAME' is mandatory, other is optional. Hightly\n            recomended to define also 'DATA_TYPE': it should be one of\n            python data types (by default it str).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a list of entity columns.\n        Args:\n            table_name (str): name of one of tables returned by self.get_tables()\n        Returns:\n          HandlerResponse", "file": "mindsdb_integrations_handlers_sqlite_handler_sqlite_handler.py", "function": "SQLiteHandler.get_columns", "project": "mindsdb", "change_stats": {"code_changes": 9, "doc_changes": 8, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef export_text(self, *, clear: bool = True, styles: bool = False) -> str:\n        \n\n        \n        assert (\n            self.record\n        ), \"To export console contents set record=True in the constructor or instance\"\n\n        with self._record_buffer_lock:\n            if styles:\n                text = \"\".join(\n                    (style.render(text) if style else text)\n                    for text, style, _ in self._record_buffer\n                )\n            else:\n                text = \"\".join(text for text, _, _ in self._record_buffer)\n            if clear:\n                del self._record_buffer[:]\n        return text```\n\nUpdated code:\n```python\ndef export_text(self, *, clear: bool = True, styles: bool = False) -> str:\n        \n\n        \n        assert (\n            self.record\n        ), \"To export console contents set record=True in the constructor or instance\"\n\n        with self._record_buffer_lock:\n            if styles:\n                text = \"\".join(\n                    (style.render(text) if style else text)\n                    for text, style, _ in self._record_buffer\n                )\n            else:\n                text = \"\".join(\n                    segment.text\n                    for segment in self._record_buffer\n                    if not segment.is_control\n                )\n            if clear:\n                del self._record_buffer[:]\n        return text\n```\n\nOriginal documentation:\n```python\nGenerate text from console contents (requires record=True argument in constructor).\n\n        Args:\n            clear (bool, optional): Set to ``True`` to clear the record buffer after exporting.\n            styles (bool, optional): If ``True``, ansi style codes will be included. ``False`` for plain text.\n                Defaults to ``False``.\n\n        Returns:\n            str: String containing console contents.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate text from console contents (requires record=True argument in constructor).\n\n        Args:\n            clear (bool, optional): Set to ``True`` to clear the record buffer after exporting.\n            styles (bool, optional): If ``True``, ansi escape codes will be included. ``False`` for plain text.\n                Defaults to ``False``.\n\n        Returns:\n            str: String containing console contents.", "file": "rich_console.py", "function": "Console.export_text", "project": "rich", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _image_directories(func):\n    \n    \n    \n    module_name = func.__module__\n    if module_name == '__main__':\n        # FIXME: this won't work for nested packages in matplotlib.tests\n        warnings.warn(\n            'Test module run as script. Guessing baseline image locations.')\n        module_path = Path(sys.argv[0]).resolve()\n        subdir = module_path.stem\n    else:\n        module_path = Path(sys.modules[func.__module__].__file__)\n        mods = module_name.split('.')\n        if len(mods) >= 3:\n            mods.pop(0)\n            # mods[0] will be the name of the package being tested (in\n            # most cases \"matplotlib\") However if this is a\n            # namespace package pip installed and run via the nose\n            # multiprocess plugin or as a specific test this may be\n            # missing. See https://github.com/matplotlib/matplotlib/issues/3314\n        if mods.pop(0) != 'tests':\n            warnings.warn(\n                \"Module {!r} does not live in a parent module named 'tests'. \"\n                \"This is probably ok, but we may not be able to guess the \"\n                \"correct subdirectory containing the baseline images. If \"\n                \"things go wrong please make sure that there is a parent \"\n                \"directory named 'tests' and that it contains a __init__.py \"\n                \"file (can be empty).\".format(module_name))\n        subdir = os.path.join(*mods)\n\n    baseline_dir = module_path.parent / 'baseline_images' / subdir\n    result_dir = Path().resolve() / 'result_images' / subdir\n    result_dir.mkdir(parents=True, exist_ok=True)\n\n    return str(baseline_dir), str(result_dir)```\n\nUpdated code:\n```python\ndef _image_directories(func):\n    \n    \n    \n    module_path = Path(sys.modules[func.__module__].__file__)\n    baseline_dir = module_path.parent / \"baseline_images\" / module_path.stem\n    result_dir = Path().resolve() / \"result_images\" / module_path.stem\n    result_dir.mkdir(parents=True, exist_ok=True)\n    return str(baseline_dir), str(result_dir)\n```\n\nOriginal documentation:\n```python\nCompute the baseline and result image directories for testing *func*.\n    Create the result directory if it doesn't exist.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Compute the baseline and result image directories for testing *func*.\n\n    For test module ``foo.bar.test_baz``, the baseline directory is at\n    ``foo/bar/baseline_images/test_baz`` and the result directory at\n    ``$(pwd)/result_images/test_baz``.  The result directory is created if it\n    doesn't exist.", "file": "lib_matplotlib_testing_decorators.py", "function": "_image_directories", "project": "matplotlib", "change_stats": {"code_changes": 35, "doc_changes": 8, "total_changes": 43}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef watch_path(self, new_path: str | Path) -> None:\n        \n        \n        self.path = Path(new_path)\n        self.reload()```\n\nUpdated code:\n```python\ndef watch_path(self) -> None:\n        \n        \n        self.reload()\n```\n\nOriginal documentation:\n```python\nWatch for changes to the `path` of the directory tree.\n\n        Args:\n            new_path: The new path being set.\n\n        If the path is changed the directory tree will be repopulated using\n        the new value as the root.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Watch for changes to the `path` of the directory tree.\n\n        If the path is changed the directory tree will be repopulated using\n        the new value as the root.", "file": "src_textual_widgets__directory_tree.py", "function": "DirectoryTree.watch_path", "project": "textual", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef apply_masks(\n        self,\n        masks: List[Union[torch.Tensor, ComplexTensor]],\n        spec_mags: torch.Tensor,\n        spec_phase: torch.Tensor,\n    ):\n        \n        \n        masked = []\n        for i in range(len(masks)):\n            # shape (B, T, F) --> (B, F, T)\n            mask_real = masks[i].real.permute(0, 2, 1)\n            mask_imag = masks[i].imag.permute(0, 2, 1)\n            if self.masking_mode == \"E\":\n                mask_mags = (mask_real ** 2 + mask_imag ** 2) ** 0.5\n                # mask_mags = (mask_real ** 2 + mask_imag ** 2 + EPS) ** 0.5\n                real_phase = mask_real / (mask_mags + EPS)\n                imag_phase = mask_imag / (mask_mags + EPS)\n                # mask_phase = torch.atan2(imag_phase + EPS, real_phase + EPS)\n                mask_phase = torch.atan2(imag_phase, real_phase)\n                mask_mags = torch.tanh(mask_mags)\n                est_mags = mask_mags * spec_mags\n                est_phase = spec_phase + mask_phase\n                real = est_mags * torch.cos(est_phase)\n                imag = est_mags * torch.sin(est_phase)\n            elif self.masking_mode == \"C\":\n                real, imag = (\n                    real * mask_real - imag * mask_imag,\n                    real * mask_imag + imag * mask_real,\n                )\n            elif self.masking_mode == \"R\":\n                real, imag = real * mask_real, imag * mask_imag\n\n            # shape (B, F, T) --> (B, T, F)\n            if is_torch_1_9_plus and self.use_builtin_complex:\n                masked.append(\n                    torch.complex(real.permute(0, 2, 1), imag.permute(0, 2, 1))\n                )\n            else:\n                masked.append(\n                    ComplexTensor(real.permute(0, 2, 1), imag.permute(0, 2, 1))\n                )\n        return masked```\n\nUpdated code:\n```python\ndef apply_masks(\n        self,\n        masks: List[Union[torch.Tensor, ComplexTensor]],\n        real: torch.Tensor,\n        imag: torch.Tensor,\n    ):\n        \n        \n        masked = []\n        for i in range(len(masks)):\n            # shape (B, T, F) --> (B, F, T)\n            mask_real = masks[i].real.permute(0, 2, 1)\n            mask_imag = masks[i].imag.permute(0, 2, 1)\n            if self.masking_mode == \"E\":\n                # shape (B, F, T)\n                spec_mags = torch.sqrt(real**2 + imag**2 + 1e-8)\n                # shape (B, F, T)\n                spec_phase = torch.atan2(imag, real)\n                mask_mags = (mask_real**2 + mask_imag**2) ** 0.5\n                # mask_mags = (mask_real ** 2 + mask_imag ** 2 + EPS) ** 0.5\n                real_phase = mask_real / (mask_mags + EPS)\n                imag_phase = mask_imag / (mask_mags + EPS)\n                # mask_phase = torch.atan2(imag_phase + EPS, real_phase + EPS)\n                mask_phase = torch.atan2(imag_phase, real_phase)\n                mask_mags = torch.tanh(mask_mags)\n                est_mags = mask_mags * spec_mags\n                est_phase = spec_phase + mask_phase\n                real = est_mags * torch.cos(est_phase)\n                imag = est_mags * torch.sin(est_phase)\n            elif self.masking_mode == \"C\":\n                real, imag = (\n                    real * mask_real - imag * mask_imag,\n                    real * mask_imag + imag * mask_real,\n                )\n            elif self.masking_mode == \"R\":\n                real, imag = real * mask_real, imag * mask_imag\n\n            # shape (B, F, T) --> (B, T, F)\n            if is_torch_1_9_plus and self.use_builtin_complex:\n                masked.append(\n                    torch.complex(real.permute(0, 2, 1), imag.permute(0, 2, 1))\n                )\n            else:\n                masked.append(\n                    ComplexTensor(real.permute(0, 2, 1), imag.permute(0, 2, 1))\n                )\n        return masked\n```\n\nOriginal documentation:\n```python\napply masks\n\n        Args:\n            masks (List[Union[torch.Tensor, ComplexTensor]]): estimated masks, [(B, T, F), ...]\n            spec_mags (torch.Tensor): magnitude of the noisy spectrum, (B, F, T)\n            spec_phase (torch.Tensor): phase of the noisy spectrum, (B, F, T)\n\n        Returns:\n            masked (List[Union(torch.Tensor, ComplexTensor)]): [(B, T, F), ...]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "apply masks\n\n        Args:\n            masks : est_masks, [(B, T, F), ...]\n            real (torch.Tensor): real part of the noisy spectrum, (B, F, T)\n            imag (torch.Tensor): imag part of the noisy spectrum, (B, F, T)\n\n        Returns:\n            masked (List[Union(torch.Tensor, ComplexTensor)]): [(B, T, F), ...]", "file": "espnet2_enh_separator_dccrn_separator.py", "function": "DCCRNSeparator.apply_masks", "project": "espnet", "change_stats": {"code_changes": 12, "doc_changes": 8, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef CPS_send_status(self, artist='', track='', album='', image='',\n                        **kwargs):\n        \n        \n        data = {'skill': self.name,\n                'artist': artist,\n                'album': artist,\n                'track': track,\n                'image': image,\n                'status': None  # TODO Add status system\n                }\n        data = {**data, **kwargs}  # Merge extra arguments\n        self.bus.emit(Message('play:status', data))```\n\nUpdated code:\n```python\ndef CPS_send_status(self, artist='', track='', album='', image='',\n                        uri='', track_length=None, elapsed_time=None,\n                        playlist_position=None,\n                        status=CPSTrackStatus.DISAMBIGUATION, **kwargs):\n        \n        \n        data = {'skill': self.name,\n                'uri': uri,\n                'artist': artist,\n                'album': album,\n                'track': track,\n                'image': image,\n                'track_length': track_length,\n                'elapsed_time': elapsed_time,\n                'playlist_position': playlist_position,\n                'status': status\n                }\n        data = {**data, **kwargs}  # Merge extra arguments\n        self.bus.emit(Message('play:status', data))\n```\n\nOriginal documentation:\n```python\nInform system of playback status.\n\n        If a skill is handling playback and wants the playback control to be\n        aware of it's current status it can emit this message indicating that\n        it's performing playback and can provide some standard info.\n\n        All parameters are optional so any can be left out. Also if extra\n        non-standard parameters are added, they too will be sent in the message\n        data.\n\n        Arguments:\n            artist (str): Current track artist\n            track (str): Track name\n            album (str): Album title\n            image (str): url for image to show\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Inform system of playback status.\n\n        If a skill is handling playback and wants the playback control to be\n        aware of it's current status it can emit this message indicating that\n        it's performing playback and can provide some standard info.\n\n        All parameters are optional so any can be left out. Also if extra\n        non-standard parameters are added, they too will be sent in the message\n        data.\n\n        Arguments:\n            artist (str): Current track artist\n            track (str): Track name\n            album (str): Album title\n            image (str): url for image to show\n            uri (str): uri for track\n            track_length (float): track length in seconds\n            elapsed_time (float): current offset into track in seconds\n            playlist_postion (int):", "file": "mycroft_skills_common_play_skill.py", "function": "CommonPlaySkill.CPS_send_status", "project": "mycroft-core", "change_stats": {"code_changes": 14, "doc_changes": 6, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               freeze_batchnorm,\n               inplace_batchnorm_update,\n               bifpn_min_level=3,\n               bifpn_max_level=7,\n               bifpn_num_iterations=3,\n               bifpn_num_filters=64,\n               bifpn_combine_method='fast_attention',\n               use_explicit_padding=None,\n               use_depthwise=None,\n               override_base_feature_extractor_hyperparams=None,\n               name='EfficientDet-D0'):\n    \n    \n    super(SSDEfficientNetB0BiFPNKerasFeatureExtractor, self).__init__(\n        is_training=is_training,\n        depth_multiplier=depth_multiplier,\n        min_depth=min_depth,\n        pad_to_multiple=pad_to_multiple,\n        conv_hyperparams=conv_hyperparams,\n        freeze_batchnorm=freeze_batchnorm,\n        inplace_batchnorm_update=inplace_batchnorm_update,\n        bifpn_min_level=bifpn_min_level,\n        bifpn_max_level=bifpn_max_level,\n        bifpn_num_iterations=bifpn_num_iterations,\n        bifpn_num_filters=bifpn_num_filters,\n        bifpn_combine_method=bifpn_combine_method,\n        efficientnet_version='efficientnet-b0',\n        use_explicit_padding=use_explicit_padding,\n        use_depthwise=use_depthwise,\n        override_base_feature_extractor_hyperparams=\n        override_base_feature_extractor_hyperparams,\n        name=name)```\n\nUpdated code:\n```python\ndef __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               freeze_batchnorm,\n               inplace_batchnorm_update,\n               bifpn_min_level=3,\n               bifpn_max_level=7,\n               bifpn_num_iterations=3,\n               bifpn_num_filters=64,\n               bifpn_combine_method='fast_attention',\n               use_explicit_padding=None,\n               use_depthwise=None,\n               use_native_resize_op=False,\n               override_base_feature_extractor_hyperparams=None,\n               name='EfficientDet-D0'):\n    \n    \n    super(SSDEfficientNetB0BiFPNKerasFeatureExtractor, self).__init__(\n        is_training=is_training,\n        depth_multiplier=depth_multiplier,\n        min_depth=min_depth,\n        pad_to_multiple=pad_to_multiple,\n        conv_hyperparams=conv_hyperparams,\n        freeze_batchnorm=freeze_batchnorm,\n        inplace_batchnorm_update=inplace_batchnorm_update,\n        bifpn_min_level=bifpn_min_level,\n        bifpn_max_level=bifpn_max_level,\n        bifpn_num_iterations=bifpn_num_iterations,\n        bifpn_num_filters=bifpn_num_filters,\n        bifpn_combine_method=bifpn_combine_method,\n        efficientnet_version='efficientnet-b0',\n        use_explicit_padding=use_explicit_padding,\n        use_depthwise=use_depthwise,\n        use_native_resize_op=use_native_resize_op,\n        override_base_feature_extractor_hyperparams=\n        override_base_feature_extractor_hyperparams,\n        name=name)\n```\n\nOriginal documentation:\n```python\nSSD Keras EfficientNet-b0 BiFPN (EfficientDet-d0) Feature Extractor.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: unsupported by EfficientNetBiFPN. float, depth\n        multiplier for the feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: a `hyperparams_builder.KerasLayerHyperparams` object\n        containing convolution hyperparameters for the layers added on top of\n        the base feature extractor.\n      freeze_batchnorm: whether to freeze batch norm parameters during training\n        or not. When training with a small batch size (e.g. 1), it is desirable\n        to freeze batch norm update and use pretrained batch norm params.\n      inplace_batchnorm_update: whether to update batch norm moving average\n        values inplace. When this is false train op must add a control\n        dependency on tf.graphkeys.UPDATE_OPS collection in order to update\n        batch norm statistics.\n      bifpn_min_level: the highest resolution feature map to use in BiFPN. The\n        valid values are {2, 3, 4, 5} which map to Resnet blocks {1, 2, 3, 4}\n        respectively.\n      bifpn_max_level: the smallest resolution feature map to use in the BiFPN.\n        BiFPN constructions uses features maps starting from bifpn_min_level\n        upto the bifpn_max_level. In the case that there are not enough feature\n        maps in the backbone network, additional feature maps are created by\n        applying stride 2 convolutions until we get the desired number of BiFPN\n        levels.\n      bifpn_num_iterations: number of BiFPN iterations. Overrided if\n        efficientdet_version is provided.\n      bifpn_num_filters: number of filters (channels) in all BiFPN layers.\n        Overrided if efficientdet_version is provided.\n      bifpn_combine_method: the method used to combine BiFPN nodes.\n      use_explicit_padding: unsupported by EfficientNetBiFPN. Whether to use\n        explicit padding when extracting features.\n      use_depthwise: unsupported by EfficientNetBiFPN, since BiFPN uses regular\n        convolutions when inputs to a node have a differing number of channels,\n        and use separable convolutions after combine operations.\n      override_base_feature_extractor_hyperparams: unsupported. Whether to\n        override hyperparameters of the base feature extractor with the one from\n        `conv_hyperparams`.\n      name: a string name scope to assign to the model. If 'None', Keras will\n        auto-generate one from the class name.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "SSD Keras EfficientNet-b0 BiFPN (EfficientDet-d0) Feature Extractor.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: unsupported by EfficientNetBiFPN. float, depth\n        multiplier for the feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: a `hyperparams_builder.KerasLayerHyperparams` object\n        containing convolution hyperparameters for the layers added on top of\n        the base feature extractor.\n      freeze_batchnorm: whether to freeze batch norm parameters during training\n        or not. When training with a small batch size (e.g. 1), it is desirable\n        to freeze batch norm update and use pretrained batch norm params.\n      inplace_batchnorm_update: whether to update batch norm moving average\n        values inplace. When this is false train op must add a control\n        dependency on tf.graphkeys.UPDATE_OPS collection in order to update\n        batch norm statistics.\n      bifpn_min_level: the highest resolution feature map to use in BiFPN. The\n        valid values are {2, 3, 4, 5} which map to Resnet blocks {1, 2, 3, 4}\n        respectively.\n      bifpn_max_level: the smallest resolution feature map to use in the BiFPN.\n        BiFPN constructions uses features maps starting from bifpn_min_level\n        upto the bifpn_max_level. In the case that there are not enough feature\n        maps in the backbone network, additional feature maps are created by\n        applying stride 2 convolutions until we get the desired number of BiFPN\n        levels.\n      bifpn_num_iterations: number of BiFPN iterations. Overrided if\n        efficientdet_version is provided.\n      bifpn_num_filters: number of filters (channels) in all BiFPN layers.\n        Overrided if efficientdet_version is provided.\n      bifpn_combine_method: the method used to combine BiFPN nodes.\n      use_explicit_padding: unsupported by EfficientNetBiFPN. Whether to use\n        explicit padding when extracting features.\n      use_depthwise: unsupported by EfficientNetBiFPN, since BiFPN uses regular\n        convolutions when inputs to a node have a differing number of channels,\n        and use separable convolutions after combine operations.\n      use_native_resize_op: If True, will use\n        tf.compat.v1.image.resize_nearest_neighbor for BiFPN unsampling.\n      override_base_feature_extractor_hyperparams: unsupported. Whether to\n        override hyperparameters of the base feature extractor with the one from\n        `conv_hyperparams`.\n      name: a string name scope to assign to the model. If 'None', Keras will\n        auto-generate one from the class name.", "file": "research_object_detection_models_ssd_efficientnet_bifpn_feature_extractor.py", "function": "SSDEfficientNetB0BiFPNKerasFeatureExtractor.__init__", "project": "models", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef notify_disconnect(server_ip: IPv4Address, server_port: int):\n    \n    \n    \n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as d_socket:\n        d_socket.settimeout(LONG_REQUEST_TIMEOUT)\n\n        try:\n            d_socket.connect((str(server_ip), server_port))\n            d_socket.sendall(RELAY_CONTROL_MESSAGE_REMOVE_FROM_WAITLIST)\n            logger.info(f\"Control message was sent to the server/relay {server_ip}:{server_port}\")\n        except OSError as err:\n            logger.error(f\"Error connecting to socket {server_ip}:{server_port}: {err}\")```\n\nUpdated code:\n```python\ndef notify_disconnect(server_address: SocketAddress):\n    \n    \n    \n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as d_socket:\n        d_socket.settimeout(LONG_REQUEST_TIMEOUT)\n\n        try:\n            d_socket.connect((str(server_address.ip), server_address.port))\n            d_socket.sendall(RELAY_CONTROL_MESSAGE_REMOVE_FROM_WAITLIST)\n            logger.info(\n                \"Control message was sent to the server/relay \"\n                f\"{server_address.ip}:{server_address.port}\"\n            )\n        except OSError as err:\n            logger.error(\n                f\"Error connecting to socket {server_address.ip}:{server_address.port}: {err}\"\n            )\n```\n\nOriginal documentation:\n```python\nTell upstream relay that we no longer need the relay.\n\n    :param server_ip: The IP address of the server to notify.\n    :param server_port: The port of the server to notify.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Tell upstream relay that we no longer need the relay\n\n    :param server_address: The address of the server to notify", "file": "monkey_infection_monkey_network_relay_utils.py", "function": "notify_disconnect", "project": "monkey", "change_stats": {"code_changes": 15, "doc_changes": 7, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef dequantize_helper(\n    g,\n    qtensor: _C.Value,\n    qdtype: Optional[torch.onnx.TensorProtoDataType] = None,\n) -> Tuple[_C.Value, _C.Value, _C.Value, Optional[_C.Value]]:\n    \n    \n    unpacked_qtensors = _unpack_tuple(qtensor)\n    tensor, scale, zero_point = unpacked_qtensors[:3]\n    axis = unpacked_qtensors[3] if len(unpacked_qtensors) >= 4 else None\n    axis_i = _get_const(axis, \"i\", \"axis\")\n    input_scalar_type = tensor.type().scalarType()\n    assert input_scalar_type is not None\n    input_qdtype = _type_utils.JitScalarType.from_name(tensor.type().scalarType())\n    if qdtype is None:\n        if input_qdtype is not None:\n            qdtype = input_qdtype.onnx_type()\n        else:\n            qdtype = _C_onnx.TensorProtoDataType.UINT8\n    value = g.op(\"Cast\", tensor, to_i=qdtype)\n    scale = g.op(\"Cast\", scale, to_i=_C_onnx.TensorProtoDataType.FLOAT)\n    zero_point = g.op(\"Cast\", zero_point, to_i=qdtype)\n\n    if axis_i is not None and GLOBALS.export_onnx_opset_version < 13:\n        _onnx_opset_unsupported_detailed(\n            \"DequantizeLinear\",\n            GLOBALS.export_onnx_opset_version,\n            13,\n            \"Attribute axis is not supported.\",\n        )\n\n    return (\n        g.op(\"DequantizeLinear\", value, scale, zero_point, axis_i=axis_i),\n        scale,\n        zero_point,\n        axis,\n    )```\n\nUpdated code:\n```python\ndef dequantize_helper(\n    g,\n    qtensor: _C.Value,\n    qdtype: Optional[torch.onnx.TensorProtoDataType] = None,\n) -> Tuple[_C.Value, _C.Value, _C.Value, Optional[_C.Value]]:\n    \n    \n    unpacked_qtensors = _unpack_quantized_tensor(qtensor)\n    tensor, scale, zero_point = unpacked_qtensors[:3]\n    axis = unpacked_qtensors[3] if len(unpacked_qtensors) >= 4 else None\n    axis_i = _get_const(axis, \"i\", \"axis\")\n    input_scalar_type = tensor.type().scalarType()\n    assert input_scalar_type is not None\n    input_qdtype = _type_utils.JitScalarType.from_name(tensor.type().scalarType())\n    if qdtype is None:\n        if input_qdtype is not None:\n            qdtype = input_qdtype.onnx_type()\n        else:\n            qdtype = _C_onnx.TensorProtoDataType.UINT8\n    value = g.op(\"Cast\", tensor, to_i=qdtype)\n    scale = g.op(\"Cast\", scale, to_i=_C_onnx.TensorProtoDataType.FLOAT)\n    zero_point = g.op(\"Cast\", zero_point, to_i=qdtype)\n\n    if axis_i is not None and GLOBALS.export_onnx_opset_version < 13:\n        _onnx_opset_unsupported_detailed(\n            \"DequantizeLinear\",\n            GLOBALS.export_onnx_opset_version,\n            13,\n            \"Attribute axis is not supported.\",\n        )\n\n    return (\n        g.op(\"DequantizeLinear\", value, scale, zero_point, axis_i=axis_i),\n        scale,\n        zero_point,\n        axis,\n    )\n```\n\nOriginal documentation:\n```python\nAppends to graph `g` ONNX nodes that dequantizes `qtensor` into `tensor`.\n\n    Args:\n        g: Graph, the ONNX IR graph that is under construction.\n        qtensor: torch._C.Value, either a tuple of (quantized_tensor, scale, zero_point) for per tensor quantization,\n          or (quantized_tensor, scale, zero_point, axis) for per channel quantization.\n          Representing the quantized tensor.\n        qdtype: torch.onnx.TensorProtoDataType default None, if not None, represents the data type of quantized tensor.\n          It must be either torch.onnx.TensorProtoDataType.UINT8 or torch.onnx.TensorProtoDataType.INT8.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Appends to graph `g` ONNX nodes that dequantizes `qtensor` into `tensor`.\n\n    Args:\n        g: Graph, the ONNX IR graph that is under construction.\n        qtensor: torch._C.Value, either a tuple of (quantized_tensor, scale, zero_point)\n            for per tensor quantization, or\n            (quantized_tensor, scale, zero_point, axis) for per channel quantization,\n            representing the quantized tensor.\n        qdtype: torch.onnx.TensorProtoDataType default None, if not None, represents the\n            data type of quantized tensor. It must be either\n            torch.onnx.TensorProtoDataType.UINT8 or torch.onnx.TensorProtoDataType.INT8.", "file": "torch_onnx_symbolic_helper.py", "function": "dequantize_helper", "project": "pytorch", "change_stats": {"code_changes": 4, "doc_changes": 14, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_output(script):\n    \n\n    \n    if six.PY2:\n        warn('Experimental instant mode is Python 3+ only')\n        return None, None\n\n    if 'THEFUCK_OUTPUT_LOG' not in os.environ:\n        warn(\"Output log isn't specified\")\n        return None, None\n\n    if const.USER_COMMAND_MARK not in os.environ.get('PS1', ''):\n        warn(\"PS1 doesn't contain user command mark, please ensure \"\n             \"that PS1 is not changed after The Fuck alias initialization\")\n        return None, None\n\n    try:\n        with open(os.environ['THEFUCK_OUTPUT_LOG'], 'rb') as log_file:\n            lines = _get_output_lines(script, log_file)\n            output = '\\n'.join(lines).strip()\n            return output, output\n    except OSError:\n        warn(\"Can't read output log\")\n        return None, None\n    except ScriptNotInLog:\n        warn(\"Script not found in output log\")\n        return None, None```\n\nUpdated code:\n```python\ndef get_output(script):\n    \n\n    \n    if six.PY2:\n        warn('Experimental instant mode is Python 3+ only')\n        return None\n\n    if 'THEFUCK_OUTPUT_LOG' not in os.environ:\n        warn(\"Output log isn't specified\")\n        return None\n\n    if const.USER_COMMAND_MARK not in os.environ.get('PS1', ''):\n        warn(\"PS1 doesn't contain user command mark, please ensure \"\n             \"that PS1 is not changed after The Fuck alias initialization\")\n        return None\n\n    try:\n        with open(os.environ['THEFUCK_OUTPUT_LOG'], 'rb') as log_file:\n            lines = _get_output_lines(script, log_file)\n            output = '\\n'.join(lines).strip()\n            debug(u'Received output: {}'.format(output))\n            return output\n    except OSError:\n        warn(\"Can't read output log\")\n        return None\n    except ScriptNotInLog:\n        warn(\"Script not found in output log\")\n        return None\n```\n\nOriginal documentation:\n```python\nReads script output from log.\n\n    :type script: str\n    :rtype: (str, str)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Reads script output from log.\n\n    :type script: str\n    :rtype: str | None", "file": "thefuck_output_readers_read_log.py", "function": "get_output", "project": "thefuck", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_document_topics(self, bow, minimum_probability=None):\n        \n        \n\n        \n        if minimum_probability is None:\n            minimum_probability = self.minimum_probability\n        minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output\n\n        # if the input vector is a corpus, return a transformed corpus\n        is_corpus, corpus = utils.is_corpus(bow)\n        if is_corpus:\n            return self._apply(corpus)\n\n        gamma, _ = self.inference([bow])\n        topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution\n        return [(topicid, topicvalue) for topicid, topicvalue in enumerate(topic_dist)\n                if topicvalue >= minimum_probability]```\n\nUpdated code:\n```python\ndef get_document_topics(self, bow, minimum_probability=None, minimum_phi_probability=None, per_word_topics=False):\n        \n        \n\n        \n        if minimum_probability is None:\n            minimum_probability = self.minimum_probability\n        minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output\n\n        if minimum_phi_probability is None:\n            minimum_phi_probability = self.minimum_probability\n        minimum_phi_probability = max(minimum_phi_probability, 1e-8)  # never allow zero values in sparse output\n\n        # if the input vector is a corpus, return a transformed corpus\n        is_corpus, corpus = utils.is_corpus(bow)\n        if is_corpus:\n            return self._apply(corpus)\n\n        gamma, phis = self.inference([bow], collect_sstats=True)\n        topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution\n\n        document_topics = [(topicid, topicvalue) for topicid, topicvalue in enumerate(topic_dist)\n                    if topicvalue >= minimum_probability]\n     \n        if not per_word_topics:\n            return document_topics\n        else:\n            word_topic = [] # contains word and corresponding topic\n            word_phi = [] # contains word and phi values\n            for word_type, weight in bow:\n                phi_values = [] # contains (phi_value, topic) pairing to later be sorted\n                phi_topic = [] # contains topic and corresponding phi value to be returned 'raw' to user\n                for topic_id in range(0, self.num_topics):\n                    if phis[topic_id][word_type] >= minimum_phi_probability:\n                        # appends phi values for each topic for that word\n                        # these phi values are scaled by feature length \n                        phi_values.append((phis[topic_id][word_type], topic_id))\n                        phi_topic.append((topic_id, phis[topic_id][word_type]))\n               \n                # list with ({word_id => [(topic_0, phi_value), (topic_1, phi_value) ...]).\n                word_phi.append((word_type, phi_topic))\n                # sorts the topics based on most likely topic\n                # returns a list like ({word_id => [topic_id_most_probable, topic_id_second_most_probable, ...]).\n                sorted_phi_values = sorted(phi_values, reverse=True)\n                topics_sorted = [x[1] for x in sorted_phi_values]\n                word_topic.append((word_type, topics_sorted))\n            return (document_topics, word_topic, word_phi)\n```\n\nOriginal documentation:\n```python\nReturn topic distribution for the given document `bow`, as a list of\n        (topic_id, topic_probability) 2-tuples.\n\n        Ignore topics with very low probability (below `minimum_probability`).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return topic distribution for the given document `bow`, as a list of\n        (topic_id, topic_probability) 2-tuples.\n\n        Ignore topics with very low probability (below `minimum_probability`).\n\n        If per_word_topics is True, it also returns a list of topics, sorted in descending order of most likely topics for that word. \n        It also returns a list of word_ids and each words corresponding topics' phi_values, multiplied by feature length (i.e, word count)", "file": "gensim_models_ldamodel.py", "function": "LdaModel.get_document_topics", "project": "gensim", "change_stats": {"code_changes": 39, "doc_changes": 5, "total_changes": 44}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate_detector_report(self, model: GraphModule) -> Tuple[str, Dict[str, Any]]:\n        \n        \n\n        # find the range of inputs\n        input_values: Dict[str, Dict] = self._extract_input_info(model)\n\n        # find the range of weights\n        weight_values: Dict[str, Dict] = self._extract_weight_info(model)\n\n        # calculate per_channel comparision statistic s_c\n        comp_stats: Dict[str, torch.Tensor] = self._generate_comparision_values(input_values, weight_values)\n\n        # generate the return dictionary\n        input_weight_equalization_info: Dict[str, Dict] = self._generate_dict_info(input_values, weight_values, comp_stats)\n\n        # now we can generate report based on this information\n        input_weight_string = \"Input-Weight Equalization suggestions: \\n\"\n\n        # some strings to be formatted depending on module we are adding\n        module_suggestion_str = \"For Module {} looked at with axis {}: \\n\"\n        channel_suggestion_str = \"\\tWe suggest {} input weight equalization because {}\\n\"\n        use_str = \"to use\"\n        no_use_str = \"to not use\"\n        input_weight_benefit_str = \"{}/{} channels would benefit and we expect significant reduction in quantization error.\"\n        input_weight_non_benefit_reasoning = \"{}/{} channels benefitting from input-weight equalization being applied.\"\n        input_weight_non_benefit_str = \"we don't expect much improvement from input-weight equalization based on {}\"\n\n        # added module check\n        added_module: bool = False\n\n        # compile the suggestion string\n        for module_fqn in input_weight_equalization_info:\n            # we added at least 1 module\n            added_module = True\n            # add the module level description\n            input_weight_string += module_suggestion_str.format(module_fqn, self.ch_axis)\n\n            mod_info: Dict[str, Any] = input_weight_equalization_info[module_fqn]\n\n            # gather info on how many channels would benefit from input weight and\n            recommendation_per_channel: torch.Tensor = mod_info[self.RECOMMENDED_KEY]\n            num_recs = sum(recommendation_per_channel)\n\n            if num_recs / len(recommendation_per_channel) >= self.DEFAULT_RECOMMEND_INPUT_WEIGHT_CHANNEL_RATIO:\n                input_benefit_formatted = input_weight_benefit_str.format(num_recs, len(recommendation_per_channel))\n                channel_str = channel_suggestion_str.format(use_str, input_benefit_formatted)\n                input_weight_string += channel_str\n            else:\n                non_benefit_reason_formatted = input_weight_non_benefit_reasoning.format(num_recs, len(recommendation_per_channel))\n                non_benefit_str = input_weight_non_benefit_str.format(non_benefit_reason_formatted)\n                channel_str = channel_suggestion_str.format(no_use_str, non_benefit_str)\n                input_weight_string += channel_str\n\n        # if no modules looked at, amend return string\n        if not added_module:\n            input_weight_string += \"No applicable layers for suggestions. Only linear and conv valid.\\n\"\n\n        # return a tuple with the string explanation and the compiled dict info\n        return (input_weight_string, input_weight_equalization_info)```\n\nUpdated code:\n```python\ndef generate_detector_report(self, model: GraphModule) -> Tuple[str, Dict[str, Any]]:\n        \n        \n\n        # find the range of inputs\n        input_values: Dict[str, Dict] = self._extract_input_info(model)\n\n        # find the range of weights\n        weight_values: Dict[str, Dict] = self._extract_weight_info(model)\n\n        # calculate per_channel comparison statistic s_c\n        comp_stats: Dict[str, torch.Tensor] = self._generate_comparison_values(input_values, weight_values)\n\n        # generate the return dictionary\n        input_weight_equalization_info: Dict[str, Dict] = self._generate_dict_info(input_values, weight_values, comp_stats)\n\n        # now we can generate report based on this information\n        input_weight_string = \"Input-Weight Equalization suggestions: \\n\"\n\n        # some strings to be formatted depending on module we are adding\n        module_suggestion_str = \"For Module {} looked at with axis {}: \\n\"\n        channel_suggestion_str = \"\\tWe suggest {} input weight equalization because {}\\n\"\n        use_str = \"to use\"\n        no_use_str = \"to not use\"\n        input_weight_benefit_str = \"{}/{} channels would benefit and we expect significant reduction in quantization error.\"\n        input_weight_non_benefit_reasoning = \"{}/{} channels benefitting from input-weight equalization being applied.\"\n        input_weight_non_benefit_str = \"we don't expect much improvement from input-weight equalization based on {}\"\n\n        # added module check\n        added_module: bool = False\n\n        # compile the suggestion string\n        for module_fqn in input_weight_equalization_info:\n            # we added at least 1 module\n            added_module = True\n            # add the module level description\n            input_weight_string += module_suggestion_str.format(module_fqn, self.ch_axis)\n\n            mod_info: Dict[str, Any] = input_weight_equalization_info[module_fqn]\n\n            # gather info on how many channels would benefit from input weight and\n            recommendation_per_channel: torch.Tensor = mod_info[self.RECOMMENDED_KEY]\n            num_recs = sum(recommendation_per_channel)\n\n            if num_recs / len(recommendation_per_channel) >= self.DEFAULT_RECOMMEND_INPUT_WEIGHT_CHANNEL_RATIO:\n                input_benefit_formatted = input_weight_benefit_str.format(num_recs, len(recommendation_per_channel))\n                channel_str = channel_suggestion_str.format(use_str, input_benefit_formatted)\n                input_weight_string += channel_str\n            else:\n                non_benefit_reason_formatted = input_weight_non_benefit_reasoning.format(num_recs, len(recommendation_per_channel))\n                non_benefit_str = input_weight_non_benefit_str.format(non_benefit_reason_formatted)\n                channel_str = channel_suggestion_str.format(no_use_str, non_benefit_str)\n                input_weight_string += channel_str\n\n        # if no modules looked at, amend return string\n        if not added_module:\n            input_weight_string += \"No applicable layers for suggestions. Only linear and conv valid.\\n\"\n\n        # return a tuple with the string explanation and the compiled dict info\n        return (input_weight_string, input_weight_equalization_info)\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Determines whether input weight equalization is appropriate for a given module.\n\n        Takes advantage of the ModelReport Observer which records per channel information of input range\n        It then uses the passed in weight info inconjunction to compute the desired ratio\n        Finally, it gives suggestions based on this information for each module of interest\n\n        Args:\n            model (GraphModule): The prepared and calibrated GraphModule with inserted ModelReportObservers\n            weight_info (Dict): Maps modules of interest to information on their weights to be analyzed\n\n        Returns a tuple with two elements:\n            String report of of whether input weight equalization is recommended for certain modules\n            Dictionary mapping modules of interest to:\n                whether input weight equalization is recommended\n                their s_c metric compared to the threshold\n                the threshold used to make the recommendation\n                the channel used for recording data\n                the input channel range info\n                the weight channel range info\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Determines whether input weight equalization is appropriate for a given module.\n\n        Takes advantage of the ModelReport Observer which records per channel information of input range\n        It then uses the passed in weight info inconjunction to compute the desired ratio\n        Finally, it gives suggestions based on this information for each module of interest\n\n        Args:\n            model (GraphModule): The prepared and calibrated GraphModule with inserted ModelReportObservers\n\n        Returns a tuple with two elements:\n            String report of of whether input weight equalization is recommended for certain modules\n            Dictionary mapping modules of interest to:\n                whether input weight equalization is recommended\n                their s_c metric compared to the threshold\n                the threshold used to make the recommendation\n                the channel used for recording data\n                the input channel range info\n                the weight channel range info", "file": "torch_ao_quantization_fx__model_report_detector.py", "function": "InputWeightEqualizationDetector.generate_detector_report", "project": "pytorch", "change_stats": {"code_changes": 6, "doc_changes": 3, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None,\n                            max_diff=80*8):\n        \n        \n        if seq_type is not None:\n            seq_type_name = seq_type.__name__\n            if not isinstance(seq1, seq_type):\n                raise self.failureException('First sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq1)))\n            if not isinstance(seq2, seq_type):\n                raise self.failureException('Second sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq2)))\n        else:\n            seq_type_name = \"sequence\"\n\n        differing = None\n        try:\n            len1 = len(seq1)\n        except (TypeError, NotImplementedError):\n            differing = 'First %s has no length.    Non-sequence?' % (\n                    seq_type_name)\n\n        if differing is None:\n            try:\n                len2 = len(seq2)\n            except (TypeError, NotImplementedError):\n                differing = 'Second %s has no length.    Non-sequence?' % (\n                        seq_type_name)\n\n        if differing is None:\n            if seq1 == seq2:\n                return\n\n            seq1_repr = safe_repr(seq1)\n            seq2_repr = safe_repr(seq2)\n            if len(seq1_repr) > 30:\n                seq1_repr = seq1_repr[:30] + '...'\n            if len(seq2_repr) > 30:\n                seq2_repr = seq2_repr[:30] + '...'\n            elements = (seq_type_name.capitalize(), seq1_repr, seq2_repr)\n            differing = '%ss differ: %s != %s\\n' % elements\n\n            for i in xrange(min(len1, len2)):\n                try:\n                    item1 = seq1[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of first %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                try:\n                    item2 = seq2[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of second %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                if item1 != item2:\n                    differing += ('\\nFirst differing element %d:\\n%s\\n%s\\n' %\n                                 (i, item1, item2))\n                    break\n            else:\n                if (len1 == len2 and seq_type is None and\n                    type(seq1) != type(seq2)):\n                    # The sequences are the same, but have differing types.\n                    return\n\n            if len1 > len2:\n                differing += ('\\nFirst %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len1 - len2))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len2, seq1[len2]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of first %s\\n' % (len2, seq_type_name))\n            elif len1 < len2:\n                differing += ('\\nSecond %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len2 - len1))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len1, seq2[len1]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of second %s\\n' % (len1, seq_type_name))\n        standardMsg = differing\n        diffMsg = '\\n' + '\\n'.join(\n            difflib.ndiff(pprint.pformat(seq1).splitlines(),\n                          pprint.pformat(seq2).splitlines()))\n        standardMsg = self._truncateMessage(standardMsg, diffMsg, max_diff)\n        msg = self._formatMessage(msg, standardMsg)\n        self.fail(msg)```\n\nUpdated code:\n```python\ndef assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None):\n        \n        \n        if seq_type is not None:\n            seq_type_name = seq_type.__name__\n            if not isinstance(seq1, seq_type):\n                raise self.failureException('First sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq1)))\n            if not isinstance(seq2, seq_type):\n                raise self.failureException('Second sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq2)))\n        else:\n            seq_type_name = \"sequence\"\n\n        differing = None\n        try:\n            len1 = len(seq1)\n        except (TypeError, NotImplementedError):\n            differing = 'First %s has no length.    Non-sequence?' % (\n                    seq_type_name)\n\n        if differing is None:\n            try:\n                len2 = len(seq2)\n            except (TypeError, NotImplementedError):\n                differing = 'Second %s has no length.    Non-sequence?' % (\n                        seq_type_name)\n\n        if differing is None:\n            if seq1 == seq2:\n                return\n\n            seq1_repr = safe_repr(seq1)\n            seq2_repr = safe_repr(seq2)\n            if len(seq1_repr) > 30:\n                seq1_repr = seq1_repr[:30] + '...'\n            if len(seq2_repr) > 30:\n                seq2_repr = seq2_repr[:30] + '...'\n            elements = (seq_type_name.capitalize(), seq1_repr, seq2_repr)\n            differing = '%ss differ: %s != %s\\n' % elements\n\n            for i in xrange(min(len1, len2)):\n                try:\n                    item1 = seq1[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of first %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                try:\n                    item2 = seq2[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of second %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                if item1 != item2:\n                    differing += ('\\nFirst differing element %d:\\n%s\\n%s\\n' %\n                                 (i, item1, item2))\n                    break\n            else:\n                if (len1 == len2 and seq_type is None and\n                    type(seq1) != type(seq2)):\n                    # The sequences are the same, but have differing types.\n                    return\n\n            if len1 > len2:\n                differing += ('\\nFirst %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len1 - len2))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len2, seq1[len2]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of first %s\\n' % (len2, seq_type_name))\n            elif len1 < len2:\n                differing += ('\\nSecond %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len2 - len1))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len1, seq2[len1]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of second %s\\n' % (len1, seq_type_name))\n        standardMsg = differing\n        diffMsg = '\\n' + '\\n'.join(\n            difflib.ndiff(pprint.pformat(seq1).splitlines(),\n                          pprint.pformat(seq2).splitlines()))\n        standardMsg = self._truncateMessage(standardMsg, diffMsg)\n        msg = self._formatMessage(msg, standardMsg)\n        self.fail(msg)\n```\n\nOriginal documentation:\n```python\nAn equality assertion for ordered sequences (like lists and tuples).\n\n        For the purposes of this function, a valid ordered sequence type is one\n        which can be indexed, has a length, and has an equality operator.\n\n        Args:\n            seq1: The first sequence to compare.\n            seq2: The second sequence to compare.\n            seq_type: The expected datatype of the sequences, or None if no\n                    datatype should be enforced.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n            max_diff: Maximum size off the diff, larger diffs are not shown\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "An equality assertion for ordered sequences (like lists and tuples).\n\n        For the purposes of this function, a valid ordered sequence type is one\n        which can be indexed, has a length, and has an equality operator.\n\n        Args:\n            seq1: The first sequence to compare.\n            seq2: The second sequence to compare.\n            seq_type: The expected datatype of the sequences, or None if no\n                    datatype should be enforced.\n            msg: Optional message to use on failure instead of a list of\n                    differences.", "file": "Lib_unittest_case.py", "function": "TestCase.assertSequenceEqual", "project": "cpython", "change_stats": {"code_changes": 7, "doc_changes": 3, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _async_recorder_ready(self):\n        \n        self.async_recorder_ready.set()```\n\nUpdated code:\n```python\ndef _async_recorder_ready(self):\n        \n        self._async_setup_periodic_tasks()\n        self.async_recorder_ready.set()\n```\n\nOriginal documentation:\n```python\nMark recorder ready.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Finish start and mark recorder ready.", "file": "homeassistant_components_recorder___init__.py", "function": "Recorder._async_recorder_ready", "project": "core", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef spy(self, Z, precision=0, marker=None, markersize=None,\n            aspect='equal', origin=\"upper\", **kwargs):\n        \n        \n        \n        if marker is None and markersize is None and hasattr(Z, 'tocoo'):\n            marker = 's'\n        if marker is None and markersize is None:\n            Z = np.asarray(Z)\n            mask = np.abs(Z) > precision\n\n            if 'cmap' not in kwargs:\n                kwargs['cmap'] = mcolors.ListedColormap(['w', 'k'],\n                                                        name='binary')\n            if 'interpolation' in kwargs:\n                raise TypeError(\n                    \"spy() got an unexpected keyword argument 'interpolation'\")\n            ret = self.imshow(mask, interpolation='nearest', aspect=aspect,\n                              origin=origin, **kwargs)\n        else:\n            if hasattr(Z, 'tocoo'):\n                c = Z.tocoo()\n                if precision == 'present':\n                    y = c.row\n                    x = c.col\n                else:\n                    nonzero = np.abs(c.data) > precision\n                    y = c.row[nonzero]\n                    x = c.col[nonzero]\n            else:\n                Z = np.asarray(Z)\n                nonzero = np.abs(Z) > precision\n                y, x = np.nonzero(nonzero)\n            if marker is None:\n                marker = 's'\n            if markersize is None:\n                markersize = 10\n            if 'linestyle' in kwargs:\n                raise TypeError(\n                    \"spy() got an unexpected keyword argument 'linestyle'\")\n            marks = mlines.Line2D(x, y, linestyle='None',\n                         marker=marker, markersize=markersize, **kwargs)\n            self.add_line(marks)\n            nr, nc = Z.shape\n            self.set_xlim(-0.5, nc - 0.5)\n            self.set_ylim(nr - 0.5, -0.5)\n            self.set_aspect(aspect)\n            ret = marks\n        self.title.set_y(1.05)\n        self.xaxis.tick_top()\n        self.xaxis.set_ticks_position('both')\n        self.xaxis.set_major_locator(mticker.MaxNLocator(nbins=9,\n                                                 steps=[1, 2, 5, 10],\n                                                 integer=True))\n        self.yaxis.set_major_locator(mticker.MaxNLocator(nbins=9,\n                                                 steps=[1, 2, 5, 10],\n                                                 integer=True))\n        return ret```\n\nUpdated code:\n```python\ndef spy(self, Z, precision=0, marker=None, markersize=None,\n            aspect='equal', origin=\"upper\", **kwargs):\n        \n        \n        \n        if marker is None and markersize is None and hasattr(Z, 'tocoo'):\n            marker = 's'\n        cbook._check_in_list([\"upper\", \"lower\"], origin=origin)\n        if marker is None and markersize is None:\n            Z = np.asarray(Z)\n            mask = np.abs(Z) > precision\n\n            if 'cmap' not in kwargs:\n                kwargs['cmap'] = mcolors.ListedColormap(['w', 'k'],\n                                                        name='binary')\n            if 'interpolation' in kwargs:\n                raise TypeError(\n                    \"spy() got an unexpected keyword argument 'interpolation'\")\n            ret = self.imshow(mask, interpolation='nearest', aspect=aspect,\n                              origin=origin, **kwargs)\n        else:\n            if hasattr(Z, 'tocoo'):\n                c = Z.tocoo()\n                if precision == 'present':\n                    y = c.row\n                    x = c.col\n                else:\n                    nonzero = np.abs(c.data) > precision\n                    y = c.row[nonzero]\n                    x = c.col[nonzero]\n            else:\n                Z = np.asarray(Z)\n                nonzero = np.abs(Z) > precision\n                y, x = np.nonzero(nonzero)\n            if marker is None:\n                marker = 's'\n            if markersize is None:\n                markersize = 10\n            if 'linestyle' in kwargs:\n                raise TypeError(\n                    \"spy() got an unexpected keyword argument 'linestyle'\")\n            ret = mlines.Line2D(\n                x, y, linestyle='None', marker=marker, markersize=markersize,\n                **kwargs)\n            self.add_line(ret)\n            nr, nc = Z.shape\n            self.set_xlim(-0.5, nc - 0.5)\n            if origin == \"upper\":\n                self.set_ylim(nr - 0.5, -0.5)\n            else:\n                self.set_ylim(-0.5, nr - 0.5)\n            self.set_aspect(aspect)\n        self.title.set_y(1.05)\n        if origin == \"upper\":\n            self.xaxis.tick_top()\n        else:\n            self.xaxis.tick_bottom()\n        self.xaxis.set_ticks_position('both')\n        self.xaxis.set_major_locator(\n            mticker.MaxNLocator(nbins=9, steps=[1, 2, 5, 10], integer=True))\n        self.yaxis.set_major_locator(\n            mticker.MaxNLocator(nbins=9, steps=[1, 2, 5, 10], integer=True))\n        return ret\n```\n\nOriginal documentation:\n```python\nPlot the sparsity pattern of a 2D array.\n\n        This visualizes the non-zero values of the array.\n\n        Two plotting styles are available: image and marker. Both\n        are available for full arrays, but only the marker style\n        works for `scipy.sparse.spmatrix` instances.\n\n        **Image style**\n\n        If *marker* and *markersize* are *None*, `~.Axes.imshow` is used. Any\n        extra remaining keyword arguments are passed to this method.\n\n        **Marker style**\n\n        If *Z* is a `scipy.sparse.spmatrix` or *marker* or *markersize* are\n        *None*, a `.Line2D` object will be returned with the value of marker\n        determining the marker type, and any remaining keyword arguments\n        passed to `~.Axes.plot`.\n\n        Parameters\n        ----------\n        Z : array-like (M, N)\n            The array to be plotted.\n\n        precision : float or 'present', default: 0\n            If *precision* is 0, any non-zero value will be plotted. Otherwise,\n            values of :math:`|Z| > precision` will be plotted.\n\n            For `scipy.sparse.spmatrix` instances, you can also\n            pass 'present'. In this case any value present in the array\n            will be plotted, even if it is identically zero.\n\n        origin : {'upper', 'lower'}, default: :rc:`image.origin`\n            Place the [0, 0] index of the array in the upper left or lower left\n            corner of the axes. The convention 'upper' is typically used for\n            matrices and images.\n\n        aspect : {'equal', 'auto', None} or float, default: 'equal'\n            The aspect ratio of the axes.  This parameter is particularly\n            relevant for images since it determines whether data pixels are\n            square.\n\n            This parameter is a shortcut for explicitly calling\n            `.Axes.set_aspect`. See there for further details.\n\n            - 'equal': Ensures an aspect ratio of 1. Pixels will be square.\n            - 'auto': The axes is kept fixed and the aspect is adjusted so\n              that the data fit in the axes. In general, this will result in\n              non-square pixels.\n            - *None*: Use :rc:`image.aspect`.\n\n        Returns\n        -------\n        ret : `~matplotlib.image.AxesImage` or `.Line2D`\n            The return type depends on the plotting style (see above).\n\n        Other Parameters\n        ----------------\n        **kwargs\n            The supported additional parameters depend on the plotting style.\n\n            For the image style, you can pass the following additional\n            parameters of `~.Axes.imshow`:\n\n            - *cmap*\n            - *alpha*\n            - *url*\n            - any `.Artist` properties (passed on to the `.AxesImage`)\n\n            For the marker style, you can pass any `.Line2D` property except\n            for *linestyle*:\n\n            %(_Line2D_docstr)s\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Plot the sparsity pattern of a 2D array.\n\n        This visualizes the non-zero values of the array.\n\n        Two plotting styles are available: image and marker. Both\n        are available for full arrays, but only the marker style\n        works for `scipy.sparse.spmatrix` instances.\n\n        **Image style**\n\n        If *marker* and *markersize* are *None*, `~.Axes.imshow` is used. Any\n        extra remaining keyword arguments are passed to this method.\n\n        **Marker style**\n\n        If *Z* is a `scipy.sparse.spmatrix` or *marker* or *markersize* are\n        *None*, a `.Line2D` object will be returned with the value of marker\n        determining the marker type, and any remaining keyword arguments\n        passed to `~.Axes.plot`.\n\n        Parameters\n        ----------\n        Z : array-like (M, N)\n            The array to be plotted.\n\n        precision : float or 'present', default: 0\n            If *precision* is 0, any non-zero value will be plotted. Otherwise,\n            values of :math:`|Z| > precision` will be plotted.\n\n            For `scipy.sparse.spmatrix` instances, you can also\n            pass 'present'. In this case any value present in the array\n            will be plotted, even if it is identically zero.\n\n        aspect : {'equal', 'auto', None} or float, default: 'equal'\n            The aspect ratio of the axes.  This parameter is particularly\n            relevant for images since it determines whether data pixels are\n            square.\n\n            This parameter is a shortcut for explicitly calling\n            `.Axes.set_aspect`. See there for further details.\n\n            - 'equal': Ensures an aspect ratio of 1. Pixels will be square.\n            - 'auto': The axes is kept fixed and the aspect is adjusted so\n              that the data fit in the axes. In general, this will result in\n              non-square pixels.\n            - *None*: Use :rc:`image.aspect`.\n\n        origin : {'upper', 'lower'}, default: :rc:`image.origin`\n            Place the [0, 0] index of the array in the upper left or lower left\n            corner of the axes. The convention 'upper' is typically used for\n            matrices and images.\n\n        Returns\n        -------\n        ret : `~matplotlib.image.AxesImage` or `.Line2D`\n            The return type depends on the plotting style (see above).\n\n        Other Parameters\n        ----------------\n        **kwargs\n            The supported additional parameters depend on the plotting style.\n\n            For the image style, you can pass the following additional\n            parameters of `~.Axes.imshow`:\n\n            - *cmap*\n            - *alpha*\n            - *url*\n            - any `.Artist` properties (passed on to the `.AxesImage`)\n\n            For the marker style, you can pass any `.Line2D` property except\n            for *linestyle*:\n\n            %(_Line2D_docstr)s", "file": "lib_matplotlib_axes__axes.py", "function": "Axes.spy", "project": "matplotlib", "change_stats": {"code_changes": 31, "doc_changes": 12, "total_changes": 43}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef header_decode(s):\n    \n    \n    s = s.replace('_', ' ')\n    return re.sub(r'=[a-fA-F0-9]{2}', _unquote_match, s, re.ASCII)```\n\nUpdated code:\n```python\ndef header_decode(s):\n    \n    \n    s = s.replace('_', ' ')\n    return re.sub(r'=[a-fA-F0-9]{2}', _unquote_match, s, flags=re.ASCII)\n```\n\nOriginal documentation:\n```python\nDecode a string encoded with RFC 2045 MIME header `Q' encoding.\n\n    This function does not parse a full MIME header value encoded with\n    quoted-printable (like =?iso-8895-1?q?Hello_World?=) -- please use\n    the high level email.Header class for that functionality.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Decode a string encoded with RFC 2045 MIME header `Q' encoding.\n\n    This function does not parse a full MIME header value encoded with\n    quoted-printable (like =?iso-8895-1?q?Hello_World?=) -- please use\n    the high level email.header class for that functionality.", "file": "Lib_email_quoprimime.py", "function": "header_decode", "project": "cpython", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load(self, file_path):\n        \n        \n        \n        file_name, file_ext = os.path.splitext(file_path)\n\n        if file_ext not in ALLOWED_EXTS:\n            raise Exception('Unsupported meta type %s, file %s. Allowed: %s' %\n                            (file_ext, file_path, ALLOWED_EXTS))\n\n        return self._load(PARSER_FUNCS[file_ext], file_path)```\n\nUpdated code:\n```python\ndef load(self, file_path, expected_type=None):\n        \n        \n        \n        file_name, file_ext = os.path.splitext(file_path)\n\n        if file_ext not in ALLOWED_EXTS:\n            raise Exception('Unsupported meta type %s, file %s. Allowed: %s' %\n                            (file_ext, file_path, ALLOWED_EXTS))\n\n        result = self._load(PARSER_FUNCS[file_ext], file_path)\n\n        if expected_type and not isinstance(result, expected_type):\n            actual_type = type(result).__name__\n            error = 'Expected \"%s\", got \"%s\"' % (expected_type.__name__, actual_type)\n            raise ValueError(error)\n\n        return result\n```\n\nOriginal documentation:\n```python\nLoads content from file_path if file_path's extension\n        is one of allowed ones (See ALLOWED_EXTS).\n\n        Throws UnsupportedMetaException on disallowed filetypes.\n        Throws ValueError on malformed meta.\n\n        :param file_path: Absolute path to the file to load content from.\n        :type file_path: ``str``\n\n        :rtype: ``dict``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Loads content from file_path if file_path's extension\n        is one of allowed ones (See ALLOWED_EXTS).\n\n        Throws UnsupportedMetaException on disallowed filetypes.\n        Throws ValueError on malformed meta.\n\n        :param file_path: Absolute path to the file to load content from.\n        :type file_path: ``str``\n\n        :param expected_type: Expected type for the loaded and parsed content (optional).\n        :type expected_type: ``object``\n\n        :rtype: ``dict``", "file": "st2common_st2common_content_loader.py", "function": "MetaLoader.load", "project": "st2", "change_stats": {"code_changes": 13, "doc_changes": 5, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ensure_path(path):\n    # type: (Union[Path, str]) -> Path\n    \n    \n    \n\n    if isinstance(path, Path):\n        return path\n    path = Path(os.path.expandvars(path))\n    return path.absolute()```\n\nUpdated code:\n```python\ndef ensure_path(path: Path | str) -> Path:\n    \n    \n    \n\n    if isinstance(path, Path):\n        return path\n    path = Path(os.path.expandvars(path))\n    return path.absolute()\n```\n\nOriginal documentation:\n```python\nGiven a path (either a string or a Path object), expand variables and return a Path object.\n\n    :param path: A string or a :class:`~pathlib.Path` object.\n    :type path: str or :class:`~pathlib.Path`\n    :return: A fully expanded Path object.\n    :rtype: :class:`~pathlib.Path`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Given a path (either a string or a Path object), expand variables and return a Path object.\n\n    :param path: A string or a :class:`~pathlib.Path` object.\n    :type path: str or :class:`~pathlib.Path`\n    :return: A fully expanded Path object.", "file": "pipenv_vendor_pythonfinder_utils.py", "function": "ensure_path", "project": "pipenv", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef trim_joins(self, target, join_list, last, trim):\n        \n        \n        \n        final = len(join_list)\n        penultimate = last.pop()\n        if penultimate == final:\n            penultimate = last.pop()\n        if trim and len(join_list) > 1:\n            extra = join_list[penultimate:]\n            join_list = join_list[:penultimate]\n            final = penultimate\n            penultimate = last.pop()\n            col = self.alias_map[extra[0]][LHS_JOIN_COL]\n            for alias in extra:\n                self.unref_alias(alias)\n        else:\n            col = target.column\n        alias = join_list[-1]\n        while final > 1:\n            join = self.alias_map[alias]\n            if col != join[RHS_JOIN_COL] or join[JOIN_TYPE] != self.INNER:\n                break\n            self.unref_alias(alias)\n            alias = join[LHS_ALIAS]\n            col = join[LHS_JOIN_COL]\n            join_list = join_list[:-1]\n            final -= 1\n            if final == penultimate:\n                penultimate = last.pop()\n        return col, alias, join_list```\n\nUpdated code:\n```python\ndef trim_joins(self, target, join_list, last, trim, nonnull_check=False):\n        \n        \n        \n        final = len(join_list)\n        penultimate = last.pop()\n        if penultimate == final:\n            penultimate = last.pop()\n        if trim and final > 1:\n            extra = join_list[penultimate:]\n            join_list = join_list[:penultimate]\n            final = penultimate\n            penultimate = last.pop()\n            col = self.alias_map[extra[0]][LHS_JOIN_COL]\n            for alias in extra:\n                self.unref_alias(alias)\n        else:\n            col = target.column\n        alias = join_list[-1]\n        while final > 1:\n            join = self.alias_map[alias]\n            if (col != join[RHS_JOIN_COL] or join[JOIN_TYPE] != self.INNER or\n                    nonnull_check):\n                break\n            self.unref_alias(alias)\n            alias = join[LHS_ALIAS]\n            col = join[LHS_JOIN_COL]\n            join_list.pop()\n            final -= 1\n            if final == penultimate:\n                penultimate = last.pop()\n        return col, alias, join_list\n```\n\nOriginal documentation:\n```python\nSometimes joins at the end of a multi-table sequence can be trimmed. If\n        the final join is against the same column as we are comparing against,\n        and is an inner join, we can go back one step in a join chain and\n        compare against the LHS of the join instead (and then repeat the\n        optimization). The result, potentially, involves fewer table joins.\n\n        The 'target' parameter is the final field being joined to, 'join_list'\n        is the full list of join aliases.\n\n        The 'last' list contains offsets into 'join_list', corresponding to\n        each component of the filter. Many-to-many relations, for example, add\n        two tables to the join list and we want to deal with both tables the\n        same way, so 'last' has an entry for the first of the two tables and\n        then the table immediately after the second table, in that case.\n\n        The 'trim' parameter forces the final piece of the join list to be\n        trimmed before anything. See the documentation of add_filter() for\n        details about this.\n\n        Returns the final active column and table alias and the new active\n        join_list.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sometimes joins at the end of a multi-table sequence can be trimmed. If\n        the final join is against the same column as we are comparing against,\n        and is an inner join, we can go back one step in a join chain and\n        compare against the LHS of the join instead (and then repeat the\n        optimization). The result, potentially, involves fewer table joins.\n\n        The 'target' parameter is the final field being joined to, 'join_list'\n        is the full list of join aliases.\n\n        The 'last' list contains offsets into 'join_list', corresponding to\n        each component of the filter. Many-to-many relations, for example, add\n        two tables to the join list and we want to deal with both tables the\n        same way, so 'last' has an entry for the first of the two tables and\n        then the table immediately after the second table, in that case.\n\n        The 'trim' parameter forces the final piece of the join list to be\n        trimmed before anything. See the documentation of add_filter() for\n        details about this.\n\n        The 'nonnull_check' parameter is True when we are using inner joins\n        between tables explicitly to exclude NULL entries. In that case, the\n        tables shouldn't be trimmed, because the very action of joining to them\n        alters the result set.\n\n        Returns the final active column and table alias and the new active\n        join_list.", "file": "django_db_models_sql_query.py", "function": "Query.trim_joins", "project": "django", "change_stats": {"code_changes": 11, "doc_changes": 7, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef wait(fs, timeout=None, return_when=ALL_COMPLETED):\n    \n    \n    with _AcquireFutures(fs):\n        done = set(f for f in fs\n                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])\n        not_done = set(fs) - done\n\n        if (return_when == FIRST_COMPLETED) and done:\n            return DoneAndNotDoneFutures(done, not_done)\n        elif (return_when == FIRST_EXCEPTION) and done:\n            if any(f for f in done\n                   if not f.cancelled() and f.exception() is not None):\n                return DoneAndNotDoneFutures(done, not_done)\n\n        if len(done) == len(fs):\n            return DoneAndNotDoneFutures(done, not_done)\n\n        waiter = _create_and_install_waiters(fs, return_when)\n\n    waiter.event.wait(timeout)\n    for f in fs:\n        with f._condition:\n            f._waiters.remove(waiter)\n\n    done.update(waiter.finished_futures)\n    return DoneAndNotDoneFutures(done, set(fs) - done)```\n\nUpdated code:\n```python\ndef wait(fs, timeout=None, return_when=ALL_COMPLETED):\n    \n    \n    fs = set(fs)\n    with _AcquireFutures(fs):\n        done = {f for f in fs\n                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}\n        not_done = fs - done\n        if (return_when == FIRST_COMPLETED) and done:\n            return DoneAndNotDoneFutures(done, not_done)\n        elif (return_when == FIRST_EXCEPTION) and done:\n            if any(f for f in done\n                   if not f.cancelled() and f.exception() is not None):\n                return DoneAndNotDoneFutures(done, not_done)\n\n        if len(done) == len(fs):\n            return DoneAndNotDoneFutures(done, not_done)\n\n        waiter = _create_and_install_waiters(fs, return_when)\n\n    waiter.event.wait(timeout)\n    for f in fs:\n        with f._condition:\n            f._waiters.remove(waiter)\n\n    done.update(waiter.finished_futures)\n    return DoneAndNotDoneFutures(done, fs - done)\n```\n\nOriginal documentation:\n```python\nWait for the futures in the given sequence to complete.\n\n    Args:\n        fs: The sequence of Futures (possibly created by different Executors) to\n            wait upon.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n        return_when: Indicates when this function should return. The options\n            are:\n\n            FIRST_COMPLETED - Return when any future finishes or is\n                              cancelled.\n            FIRST_EXCEPTION - Return when any future finishes by raising an\n                              exception. If no future raises an exception\n                              then it is equivalent to ALL_COMPLETED.\n            ALL_COMPLETED -   Return when all futures finish or are cancelled.\n\n    Returns:\n        A named 2-tuple of sets. The first set, named 'done', contains the\n        futures that completed (is finished or cancelled) before the wait\n        completed. The second set, named 'not_done', contains uncompleted\n        futures.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Wait for the futures in the given sequence to complete.\n\n    Args:\n        fs: The sequence of Futures (possibly created by different Executors) to\n            wait upon.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n        return_when: Indicates when this function should return. The options\n            are:\n\n            FIRST_COMPLETED - Return when any future finishes or is\n                              cancelled.\n            FIRST_EXCEPTION - Return when any future finishes by raising an\n                              exception. If no future raises an exception\n                              then it is equivalent to ALL_COMPLETED.\n            ALL_COMPLETED -   Return when all futures finish or are cancelled.\n\n    Returns:\n        A named 2-tuple of sets. The first set, named 'done', contains the\n        futures that completed (is finished or cancelled) before the wait\n        completed. The second set, named 'not_done', contains uncompleted\n        futures. Duplicate futures given to *fs* are removed and will be \n        returned only once.", "file": "Lib_concurrent_futures__base.py", "function": "wait", "project": "cpython", "change_stats": {"code_changes": 12, "doc_changes": 5, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef merge(self, info, mode='normal'):\n        \n        \n        \n        if info.is_fleet:\n            if self.is_sea:\n                self.is_fleet = True\n                if info.is_current_fleet:\n                    self.is_current_fleet = True\n                return True\n            else:\n                return False\n        if info.is_boss:\n            if not self.is_land and self.may_boss:\n                self.is_boss = True\n                return True\n            else:\n                return False\n        if info.is_siren:\n            if not self.is_land and self.may_siren:\n                self.is_siren = True\n                self.enemy_scale = 0\n                self.enemy_genre = info.enemy_genre\n                return True\n            else:\n                return False\n        if info.is_enemy:\n            if not self.is_land and (self.may_enemy or self.is_carrier):\n                self.is_enemy = True\n                if info.enemy_scale:\n                    self.enemy_scale = info.enemy_scale\n                if info.enemy_genre != 'Enemy':\n                    self.enemy_genre = info.enemy_genre\n                return True\n            elif mode == 'carrier' and not self.is_land and self.may_carrier:\n                self.is_enemy = True\n                self.is_carrier = True\n                if info.enemy_scale:\n                    self.enemy_scale = info.enemy_scale\n                if info.enemy_genre != 'Enemy':\n                    self.enemy_genre = info.enemy_genre\n                return True\n            else:\n                return False\n        if info.is_mystery:\n            if self.may_mystery:\n                self.is_mystery = info.is_mystery\n                return True\n            else:\n                return False\n        if info.is_ammo:\n            if self.may_ammo:\n                self.is_ammo = info.is_ammo\n                return True\n            else:\n                return False\n\n        return True```\n\nUpdated code:\n```python\ndef merge(self, info, mode='normal'):\n        \n        \n        \n        if info.is_fleet:\n            if self.is_sea:\n                self.is_fleet = True\n                if info.is_current_fleet:\n                    self.is_current_fleet = True\n                return True\n            else:\n                return False\n        if info.is_boss:\n            if not self.is_land and self.may_boss:\n                self.is_boss = True\n                return True\n            else:\n                return False\n        if info.is_siren:\n            if not self.is_land and self.may_siren:\n                self.is_siren = True\n                self.enemy_scale = 0\n                self.enemy_genre = info.enemy_genre\n                return True\n            elif mode == 'movable' and not self.is_land:\n                self.is_siren = True\n                self.enemy_scale = 0\n                self.enemy_genre = info.enemy_genre\n                return True\n            else:\n                return False\n        if info.is_enemy:\n            if not self.is_land and (self.may_enemy or self.is_carrier):\n                self.is_enemy = True\n                if info.enemy_scale:\n                    self.enemy_scale = info.enemy_scale\n                if info.enemy_genre != 'Enemy':\n                    self.enemy_genre = info.enemy_genre\n                return True\n            elif mode == 'carrier' and not self.is_land and self.may_carrier:\n                self.is_enemy = True\n                self.is_carrier = True\n                if info.enemy_scale:\n                    self.enemy_scale = info.enemy_scale\n                if info.enemy_genre != 'Enemy':\n                    self.enemy_genre = info.enemy_genre\n                return True\n            else:\n                return False\n        if info.is_mystery:\n            if self.may_mystery:\n                self.is_mystery = info.is_mystery\n                return True\n            else:\n                return False\n        if info.is_ammo:\n            if self.may_ammo:\n                self.is_ammo = info.is_ammo\n                return True\n            else:\n                return False\n\n        return True\n```\n\nOriginal documentation:\n```python\nArgs:\n            info (GridInfo):\n            mode (str): Scan mode, such as 'normal', 'carrier', 'move'\n\n        Returns:\n            bool: If success.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            info (GridInfo):\n            mode (str): Scan mode, such as 'normal', 'carrier', 'movable'\n\n        Returns:\n            bool: If success.", "file": "module_map_detection_grid_info.py", "function": "GridInfo.merge", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        coordinator: WeatherKitDataUpdateCoordinator,\n    ) -> None:\n        \n        super().__init__(coordinator)\n        config_data = coordinator.config_entry.data\n        self._attr_unique_id = (\n            f\"{config_data[CONF_LATITUDE]}-{config_data[CONF_LONGITUDE]}\"\n        )\n        self._attr_device_info = DeviceInfo(\n            entry_type=DeviceEntryType.SERVICE,\n            identifiers={(DOMAIN, self._attr_unique_id)},\n            manufacturer=\"Apple Weather\",\n        )```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        coordinator: WeatherKitDataUpdateCoordinator,\n    ) -> None:\n        \n        super().__init__(coordinator)\n        WeatherKitEntity.__init__(self, coordinator, unique_id_suffix=None)\n```\n\nOriginal documentation:\n```python\nInitialise the platform with a data instance and site.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize the platform with a coordinator.", "file": "homeassistant_components_weatherkit_weather.py", "function": "WeatherKitWeather.__init__", "project": "core", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_batch_states(self, batch_states, l_states, l_tokens):\n        \n\n        \n        if batch_states[0] is None:\n            return batch_states\n\n        max_len = max([len(t) for t in l_tokens])\n\n        for layer in range(len(self.decoders)):\n            batch_states[layer] = pad_batch_state(\n                [s[layer] for s in l_states], max_len, self.blank\n            )\n\n        return batch_states```\n\nUpdated code:\n```python\ndef create_batch_states(self, batch_states, l_states, check_list):\n        \n\n        \n        if l_states[0][0] is None:\n            return batch_states\n\n        max_len = max(len(elem) for elem in check_list) - 1\n\n        for layer in range(self.dlayers):\n            batch_states[layer] = check_batch_state(\n                [s[layer] for s in l_states], max_len, self.blank\n            )\n\n        return batch_states\n```\n\nOriginal documentation:\n```python\nCreate batch of decoder states.\n\n        Args:\n            batch_states (list): batch of decoder states\n                [L x (B, max_len, dec_dim)]\n            l_states (list): list of decoder states\n                [B x [L x (1, max_len, dec_dim)]]\n            l_tokens (list): list of token sequences for batch\n\n        Returns:\n            batch_states (list): batch of decoder states\n                [L x (B, max_len, dec_dim)]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Create batch of decoder states.\n\n        Args:\n            batch_states (list): batch of decoder states\n                [L x (B, max_len, dec_dim)]\n            l_states (list): list of decoder states\n                [B x [L x (1, max_len, dec_dim)]]\n            check_list (list): list of sequences for max_len\n\n        Returns:\n            batch_states (list): batch of decoder states\n                [L x (B, max_len, dec_dim)]", "file": "espnet_nets_pytorch_backend_transducer_custom_decoder.py", "function": "CustomDecoder.create_batch_states", "project": "espnet", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef async_render(\n        self,\n        variables: TemplateVarsType = None,\n        parse_result: bool = True,\n        **kwargs: Any,\n    ) -> Any:\n        \n        \n        if self.is_static:\n            if self.hass.config.legacy_templates or not parse_result:\n                return self.template\n            return self._parse_result(self.template)\n\n        compiled = self._compiled or self._ensure_compiled()\n\n        if variables is not None:\n            kwargs.update(variables)\n\n        try:\n            render_result = compiled.render(kwargs)\n        except Exception as err:  # pylint: disable=broad-except\n            raise TemplateError(err) from err\n\n        render_result = render_result.strip()\n\n        if self.hass.config.legacy_templates or not parse_result:\n            return render_result\n\n        return self._parse_result(render_result)```\n\nUpdated code:\n```python\ndef async_render(\n        self,\n        variables: TemplateVarsType = None,\n        parse_result: bool = True,\n        limited: bool = False,\n        **kwargs: Any,\n    ) -> Any:\n        \n        \n        if self.is_static:\n            if self.hass.config.legacy_templates or not parse_result:\n                return self.template\n            return self._parse_result(self.template)\n\n        compiled = self._compiled or self._ensure_compiled(limited)\n\n        if variables is not None:\n            kwargs.update(variables)\n\n        try:\n            render_result = compiled.render(kwargs)\n        except Exception as err:  # pylint: disable=broad-except\n            raise TemplateError(err) from err\n\n        render_result = render_result.strip()\n\n        if self.hass.config.legacy_templates or not parse_result:\n            return render_result\n\n        return self._parse_result(render_result)\n```\n\nOriginal documentation:\n```python\nRender given template.\n\n        This method must be run in the event loop.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Render given template.\n\n        This method must be run in the event loop.\n\n        If limited is True, the template is not allowed to access any function or filter depending on hass or the state machine.", "file": "homeassistant_helpers_template.py", "function": "Template.async_render", "project": "core", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef replace_range(self, start: Location, end: Location, text: str) -> EditResult:\n        \n        \n        self.edit_count += 1\n\n        top, bottom = sorted((start, end))\n        top_row, top_column = top\n        bottom_row, bottom_column = bottom\n\n        insert_lines = text.splitlines()\n        if text.endswith(tuple(VALID_NEWLINES)):\n            # Special case where a single newline character is inserted.\n            insert_lines.append(\"\")\n\n        lines = self._lines\n\n        replaced_text = self.get_text_range(top, bottom)\n        if bottom_row >= len(lines):\n            after_selection = \"\"\n        else:\n            after_selection = lines[bottom_row][bottom_column:]\n\n        if top_row >= len(lines):\n            before_selection = \"\"\n        else:\n            before_selection = lines[top_row][:top_column]\n\n        if insert_lines:\n            insert_lines[0] = before_selection + insert_lines[0]\n            destination_column = len(insert_lines[-1])\n            insert_lines[-1] = insert_lines[-1] + after_selection\n        else:\n            destination_column = len(before_selection)\n            insert_lines = [before_selection + after_selection]\n\n        lines[top_row : bottom_row + 1] = insert_lines\n        destination_row = top_row + len(insert_lines) - 1\n\n        end_location = (destination_row, destination_column)\n        return EditResult(end_location, replaced_text)```\n\nUpdated code:\n```python\ndef replace_range(self, start: Location, end: Location, text: str) -> EditResult:\n        \n        \n        top, bottom = sorted((start, end))\n        top_row, top_column = top\n        bottom_row, bottom_column = bottom\n\n        insert_lines = text.splitlines()\n        if text.endswith(tuple(VALID_NEWLINES)):\n            # Special case where a single newline character is inserted.\n            insert_lines.append(\"\")\n\n        lines = self._lines\n\n        replaced_text = self.get_text_range(top, bottom)\n        if bottom_row >= len(lines):\n            after_selection = \"\"\n        else:\n            after_selection = lines[bottom_row][bottom_column:]\n\n        if top_row >= len(lines):\n            before_selection = \"\"\n        else:\n            before_selection = lines[top_row][:top_column]\n\n        if insert_lines:\n            insert_lines[0] = before_selection + insert_lines[0]\n            destination_column = len(insert_lines[-1])\n            insert_lines[-1] = insert_lines[-1] + after_selection\n        else:\n            destination_column = len(before_selection)\n            insert_lines = [before_selection + after_selection]\n\n        lines[top_row : bottom_row + 1] = insert_lines\n        destination_row = top_row + len(insert_lines) - 1\n\n        end_location = (destination_row, destination_column)\n        return EditResult(end_location, replaced_text)\n```\n\nOriginal documentation:\n```python\nReplace text at the given range.\n\n        This is the only method by which a document may be updated.\n\n        Args:\n            start: A tuple (row, column) where the edit starts.\n            end: A tuple (row, column) where the edit ends.\n            text: The text to insert between start and end.\n\n        Returns:\n            The EditResult containing information about the completed\n                replace operation.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replace text at the given range.\n\n        Args:\n            start: A tuple (row, column) where the edit starts.\n            end: A tuple (row, column) where the edit ends.\n            text: The text to insert between start and end.\n\n        Returns:\n            The EditResult containing information about the completed\n                replace operation.", "file": "src_textual_document__document.py", "function": "Document.replace_range", "project": "textual", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef add_options(self, items: Iterable[NewOptionListContent]) -> Self:\n        \n        \n        # Only work if we have items to add; but don't make a fuss out of\n        # zero items to add, just carry on like nothing happened.\n        if items:\n            # Turn any incoming values into valid content for the list.\n            content = [self._make_content(item) for item in items]\n            self._contents.extend(content)\n            # Pull out the content that is genuine options and add them to the\n            # list of options.\n            self._options.extend([item for item in content if isinstance(item, Option)])\n            self._refresh_content_tracking(force=True)\n            self.refresh()\n        return self```\n\nUpdated code:\n```python\ndef add_options(self, items: Iterable[NewOptionListContent]) -> Self:\n        \n        \n        # Only work if we have items to add; but don't make a fuss out of\n        # zero items to add, just carry on like nothing happened.\n        if items:\n            # Turn any incoming values into valid content for the list.\n            content = [self._make_content(item) for item in items]\n            self._duplicate_id_check(content)\n            self._contents.extend(content)\n            # Pull out the content that is genuine options and add them to the\n            # list of options.\n            self._options.extend([item for item in content if isinstance(item, Option)])\n            self._refresh_content_tracking(force=True)\n            self.refresh()\n        return self\n```\n\nOriginal documentation:\n```python\nAdd new options to the end of the option list.\n\n        Args:\n            items: The new items to add.\n\n        Returns:\n            The `OptionList` instance.\n\n        Raises:\n            DuplicateID: If there is an attempt to use a duplicate ID.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add new options to the end of the option list.\n\n        Args:\n            items: The new items to add.\n\n        Returns:\n            The `OptionList` instance.\n\n        Raises:\n            DuplicateID: If there is an attempt to use a duplicate ID.\n\n        Note:\n            All options are checked for duplicate IDs *before* any option is\n            added. A duplicate ID will cause none of the passed items to be\n            added to the option list.", "file": "src_textual_widgets__option_list.py", "function": "OptionList.add_options", "project": "textual", "change_stats": {"code_changes": 3, "doc_changes": 7, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save_checkpoint(\n        self, path: _PATH, state: Dict[str, Union[Module, Optimizer, Any]], storage_options: Optional[Any] = None\n    ) -> None:\n        \n        \n        if not _TORCH_GREATER_EQUAL_2_0:\n            raise NotImplementedError(\n                \"Saving and loading checkpoints with the `FSDPStrategy` is not supported in PyTorch < 2.0.\"\n                \" Please upgrade `torch` or file an issue: `https://github.com/Lightning-AI/lightning/issues`.\"\n            )\n        if storage_options is not None:\n            raise TypeError(\n                \"`FSDPStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\n                \" `FSDPStrategy` does not use the `CheckpointIO`.\"\n            )\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\n        path = Path(self.broadcast(path))\n        if path.is_dir() and os.listdir(path):\n            raise FileExistsError(f\"The checkpoint directory already exists and is not empty: {path}\")\n\n        from torch.distributed.checkpoint import FileSystemWriter, save_state_dict\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\n        modules = [module for module in state.values() if isinstance(module, FSDP)]\n        if len(modules) == 0:\n            raise ValueError(\n                \"Could not find a FSDP model in the provided checkpoint state. Please provide the model as\"\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\n            )\n        if len(modules) > 1:\n            raise ValueError(\n                \"Found multiple FSDP modules in the given state. Saving checkpoints with FSDP is\"\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\n                \" save method for each model separately with a different path.\"\n            )\n\n        module = modules[0]\n\n        state_dict_type = _get_state_dict_type(module)\n\n        # replace the modules and optimizer objects in the state with their local state dict\n        # and separate the user's metadata\n        converted_state = {}\n        metadata = {}\n        with state_dict_type:\n            for key, obj in state.items():\n                if isinstance(obj, FSDP):\n                    converted_state[key] = obj.state_dict()\n                elif isinstance(obj, Optimizer):\n                    converted_state[key] = FSDP.optim_state_dict(module, obj)\n                else:  # everything not a module or optimizer is considered metadata\n                    metadata[key] = obj\n\n        # FSDP's FileSystemWriter streams the tensors to disk to minimize memory peaks\n        writer = FileSystemWriter(path=path, single_file_per_rank=True)\n        save_state_dict(converted_state, writer)\n\n        if self.global_rank == 0:\n            torch.save(metadata, path / \"meta.pt\")```\n\nUpdated code:\n```python\ndef save_checkpoint(\n        self, path: _PATH, state: Dict[str, Union[Module, Optimizer, Any]], storage_options: Optional[Any] = None\n    ) -> None:\n        \n        \n        if not _TORCH_GREATER_EQUAL_2_0:\n            raise NotImplementedError(\n                \"Saving and loading checkpoints with the `FSDPStrategy` is not supported in PyTorch < 2.0.\"\n                \" Please upgrade `torch` or file an issue: `https://github.com/Lightning-AI/lightning/issues`.\"\n            )\n        if storage_options is not None:\n            raise TypeError(\n                \"`FSDPStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\n                \" `FSDPStrategy` does not use the `CheckpointIO`.\"\n            )\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\n        path = Path(self.broadcast(path))\n        if path.is_dir() and os.listdir(path):\n            raise FileExistsError(f\"The checkpoint directory already exists and is not empty: {path}\")\n\n        from torch.distributed.checkpoint import FileSystemWriter, save_state_dict\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\n        modules = [module for module in state.values() if isinstance(module, FSDP)]\n        if len(modules) == 0:\n            raise ValueError(\n                \"Could not find a FSDP model in the provided checkpoint state. Please provide the model as\"\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\n            )\n        if len(modules) > 1:\n            raise ValueError(\n                \"Found multiple FSDP modules in the given state. Saving checkpoints with FSDP is\"\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\n                \" save method for each model separately with a different path.\"\n            )\n\n        module = modules[0]\n\n        if self._state_dict_type == \"sharded\":\n            path.mkdir(parents=True, exist_ok=True)\n            state_dict_ctx = _get_sharded_state_dict_context(module)\n\n            # replace the modules and optimizer objects in the state with their local state dict\n            # and separate the user's metadata\n            converted_state = {}\n            metadata = {}\n            with state_dict_ctx:\n                for key, obj in state.items():\n                    if isinstance(obj, FSDP):\n                        converted_state[key] = obj.state_dict()\n                    elif isinstance(obj, Optimizer):\n                        converted_state[key] = FSDP.optim_state_dict(module, obj)\n                    else:  # everything not a module or optimizer is considered metadata\n                        metadata[key] = obj\n\n            # FSDP's FileSystemWriter streams the tensors to disk to minimize memory peaks\n            writer = FileSystemWriter(path=path, single_file_per_rank=True)\n            save_state_dict(converted_state, writer)\n\n            if self.global_rank == 0:\n                torch.save(metadata, path / \"meta.pt\")\n\n        elif self._state_dict_type == \"full\":\n            state_dict_ctx = _get_full_state_dict_context(module)\n            full_state = {}\n            with state_dict_ctx:\n                for key, obj in state.items():\n                    if isinstance(obj, FSDP):\n                        full_state[key] = obj.state_dict()\n                    elif isinstance(obj, Optimizer):\n                        full_state[key] = FSDP.optim_state_dict(module, obj)\n                    else:  # everything not a module or optimizer is considered metadata\n                        full_state[key] = obj  # type: ignore[assignment]\n\n            if self.global_rank == 0:\n                torch.save(full_state, path)\n        else:\n            raise ValueError(f\"Unknown state_dict_type: {self._state_dict_type}\")\n```\n\nOriginal documentation:\n```python\nSave model, optimizer, and other state in a checkpoint directory.\n\n        The directory will contain one file per process, with model- and optimizer shards stored per file. Additionally,\n        it creates a metadata file `meta.pt` with the rest of the user's state (only saved from rank 0).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save model, optimizer, and other state to a checkpoint on disk.\n\n        If the state-dict-type is ``'full'``, the checkpoint will be written to a single file containing the weights,\n        optimizer state and other metadata. If the state-dict-type is ``'sharded'``, the checkpoint gets saved as a\n        directory containing one file per process, with model- and optimizer shards stored per file. Additionally, it\n        creates a metadata file `meta.pt` with the rest of the user's state (only saved from rank 0).", "file": "src_lightning_fabric_strategies_fsdp.py", "function": "FSDPStrategy.save_checkpoint", "project": "pytorch-lightning", "change_stats": {"code_changes": 57, "doc_changes": 10, "total_changes": 67}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate_rpkg(\n        pkg_data,\n        project_shortname,\n        export_string,\n        package_depends,\n        package_imports,\n        package_suggests\n):\n    \n    \n\n    \n    # Leverage package.json to import specifics which are also applicable\n    # to R package that we're generating here, use .get in case the key\n    # does not exist in package.json\n\n    package_name = snake_case_to_camel_case(project_shortname)\n    lib_name = pkg_data.get(\"name\")\n    package_description = pkg_data.get(\"description\", \"\")\n    package_version = pkg_data.get(\"version\", \"0.0.1\")\n\n    # remove leading and trailing commas\n    if package_depends:\n        package_depends = \", \" + package_depends.strip(\",\").lstrip()\n\n    if package_imports:\n        package_imports = package_imports.strip(\",\").lstrip()\n\n    if package_suggests:\n        package_suggests = package_suggests.strip(\",\").lstrip()\n\n    if \"bugs\" in pkg_data.keys():\n        package_issues = pkg_data[\"bugs\"].get(\"url\", \"\")\n    else:\n        package_issues = \"\"\n        print(\n            \"Warning: a URL for bug reports was \"\n            \"not provided. Empty string inserted.\",\n            file=sys.stderr,\n        )\n\n    if \"homepage\" in pkg_data.keys():\n        package_url = pkg_data.get(\"homepage\", \"\")\n    else:\n        package_url = \"\"\n        print(\n            \"Warning: a homepage URL was not provided. Empty string inserted.\",\n            file=sys.stderr,\n        )\n\n    package_author = pkg_data.get(\"author\")\n\n    package_author_no_email = package_author.split(\" <\")[0] + \" [aut]\"\n\n    maintainer = pkg_data.get(\"maintainer\", pkg_data.get(\"author\"))\n\n    if not (os.path.isfile(\"LICENSE\") or os.path.isfile(\"LICENSE.txt\")):\n        package_license = pkg_data.get(\"license\", \"\")\n    else:\n        package_license = pkg_data.get(\"license\", \"\") + \" + file LICENSE\"\n        # R requires that the LICENSE.txt file be named LICENSE\n        if not os.path.isfile(\"LICENSE\"):\n            os.symlink(\"LICENSE.txt\", \"LICENSE\")\n\n    import_string = \"# AUTO GENERATED FILE - DO NOT EDIT\\n\\n\"\n    packages_string = ''\n\n    rpackage_list = package_depends.split(', ') + package_imports.split(', ')\n    rpackage_list = filter(bool, rpackage_list)\n\n    if rpackage_list:\n        for rpackage in rpackage_list:\n            packages_string += \"\\nimport({})\\n\".format(rpackage)\n\n    pkghelp_stub_path = os.path.join(\"man\", package_name + \"-package.Rd\")\n\n    # generate the internal (not exported to the user) functions which\n    # supply the JavaScript dependencies to the dash package.\n    # this avoids having to generate an RData file from within Python.\n    write_js_metadata(pkg_data=pkg_data, project_shortname=project_shortname)\n\n    with open(\"NAMESPACE\", \"w\") as f:\n        f.write(import_string)\n        f.write(export_string)\n        f.write(packages_string)\n\n    with open(\".Rbuildignore\", \"w\") as f2:\n        f2.write(rbuild_ignore_string)\n\n    # Write package stub files for R online help, generate if\n    # dashHtmlComponents, dashTable, or dashCoreComponents; makes it easy\n    # for R users to bring up main package help page\n    pkg_help_header = \"\"\n\n    if package_name in [\"dashHtmlComponents\"]:\n        pkg_help_header = \"Vanilla HTML Components for Dash\"\n        pkg_help_desc = \"Dash is a web application framework that\\n\\\nprovides pure Python and R abstraction around HTML, CSS, and\\n\\\nJavaScript. Instead of writing HTML or using an HTML\\n\\\ntemplating engine, you compose your layout using R\\n\\\nfunctions within the dashHtmlComponents package. The\\n\\\nsource for this package is on GitHub:\\n\\\nplotly/dash-html-components.\"\n    if package_name in [\"dashCoreComponents\"]:\n        pkg_help_header = \"Core Interactive UI Components for Dash\"\n        pkg_help_desc = \"Dash ships with supercharged components for\\n\\\ninteractive user interfaces. A core set of components,\\n\\\nwritten and maintained by the Dash team, is available in\\n\\\nthe dashCoreComponents package. The source for this package\\n\\\nis on GitHub: plotly/dash-core-components.\"\n    if package_name in [\"dashTable\"]:\n        pkg_help_header = \"Core Interactive Table Component for Dash\"\n        pkg_help_desc = \"Dash DataTable is an interactive table component\\n\\\ndesigned for viewing, editing, and exploring large datasets. DataTable is\\n\\\nrendered with standard, semantic HTML <table/> markup, which makes it\\n\\\naccessible, responsive, and easy to style. This component was written\\n\\\nfrom scratch in React.js specifically for the Dash community. Its API\\n\\\nwas designed to be ergonomic and its behavior is completely customizable\\n\\\nthrough its properties.\"\n\n    description_string = description_template.format(\n        package_name=package_name,\n        package_description=package_description,\n        package_version=package_version,\n        package_author=package_author,\n        package_depends=package_depends,\n        package_imports=package_imports,\n        package_suggests=package_suggests,\n        package_license=package_license,\n        package_url=package_url,\n        package_issues=package_issues,\n        package_author_no_email=package_author_no_email,\n        maintainer=maintainer,\n    )\n\n    with open(\"DESCRIPTION\", \"w\") as f3:\n        f3.write(description_string)\n\n    if pkg_help_header != \"\":\n        pkghelp = pkghelp_stub.format(\n            package_name=package_name,\n            pkg_help_header=pkg_help_header,\n            pkg_help_desc=pkg_help_desc,\n            lib_name=lib_name,\n            package_author=package_author,\n        )\n        with open(pkghelp_stub_path, \"w\") as f4:\n            f4.write(pkghelp)```\n\nUpdated code:\n```python\ndef generate_rpkg(\n        pkg_data,\n        rpkg_data,\n        project_shortname,\n        export_string,\n        package_depends,\n        package_imports,\n        package_suggests\n):\n    \n    \n\n    \n    # Leverage package.json to import specifics which are also applicable\n    # to R package that we're generating here, use .get in case the key\n    # does not exist in package.json\n\n    package_name = snake_case_to_camel_case(project_shortname)\n    lib_name = pkg_data.get(\"name\")\n    package_description = pkg_data.get(\"description\", \"\")\n    package_version = pkg_data.get(\"version\", \"0.0.1\")\n\n    # remove leading and trailing commas\n    if package_depends:\n        package_depends = \", \" + package_depends.strip(\",\").lstrip()\n\n    if package_imports:\n        package_imports = package_imports.strip(\",\").lstrip()\n\n    if package_suggests:\n        package_suggests = package_suggests.strip(\",\").lstrip()\n\n    if \"bugs\" in pkg_data.keys():\n        package_issues = pkg_data[\"bugs\"].get(\"url\", \"\")\n    else:\n        package_issues = \"\"\n        print(\n            \"Warning: a URL for bug reports was \"\n            \"not provided. Empty string inserted.\",\n            file=sys.stderr,\n        )\n\n    if \"homepage\" in pkg_data.keys():\n        package_url = pkg_data.get(\"homepage\", \"\")\n    else:\n        package_url = \"\"\n        print(\n            \"Warning: a homepage URL was not provided. Empty string inserted.\",\n            file=sys.stderr,\n        )\n\n    package_author = pkg_data.get(\"author\")\n\n    package_author_no_email = package_author.split(\" <\")[0] + \" [aut]\"\n\n    maintainer = pkg_data.get(\"maintainer\", pkg_data.get(\"author\"))\n\n    if not (os.path.isfile(\"LICENSE\") or os.path.isfile(\"LICENSE.txt\")):\n        package_license = pkg_data.get(\"license\", \"\")\n    else:\n        package_license = pkg_data.get(\"license\", \"\") + \" + file LICENSE\"\n        # R requires that the LICENSE.txt file be named LICENSE\n        if not os.path.isfile(\"LICENSE\"):\n            os.symlink(\"LICENSE.txt\", \"LICENSE\")\n\n    import_string = \"# AUTO GENERATED FILE - DO NOT EDIT\\n\\n\"\n    packages_string = ''\n\n    rpackage_list = package_depends.split(', ') + package_imports.split(', ')\n    rpackage_list = filter(bool, rpackage_list)\n\n    if rpackage_list:\n        for rpackage in rpackage_list:\n            packages_string += \"\\nimport({})\\n\".format(rpackage)\n\n    pkghelp_stub_path = os.path.join(\"man\", package_name + \"-package.Rd\")\n\n    # generate the internal (not exported to the user) functions which\n    # supply the JavaScript dependencies to the dash package.\n    # this avoids having to generate an RData file from within Python.\n    write_js_metadata(pkg_data=pkg_data, project_shortname=project_shortname)\n\n    with open(\"NAMESPACE\", \"w\") as f:\n        f.write(import_string)\n        f.write(export_string)\n        f.write(packages_string)\n\n    with open(\".Rbuildignore\", \"w\") as f2:\n        f2.write(rbuild_ignore_string)\n\n    description_string = description_template.format(\n        package_name=package_name,\n        package_description=package_description,\n        package_version=package_version,\n        package_author=package_author,\n        package_depends=package_depends,\n        package_imports=package_imports,\n        package_suggests=package_suggests,\n        package_license=package_license,\n        package_url=package_url,\n        package_issues=package_issues,\n        package_author_no_email=package_author_no_email,\n        maintainer=maintainer,\n    )\n\n    with open(\"DESCRIPTION\", \"w\") as f3:\n        f3.write(description_string)\n\n    if pkg_help_header != \"\":\n        pkghelp = pkghelp_stub.format(\n            package_name=package_name,\n            pkg_help_title=rpkg_data.get(\"pkg_help_title\"),\n            pkg_help_description=rpkg_data.get(\"pkg_help_description\"),\n            lib_name=lib_name,\n            package_author=package_author,\n        )\n        with open(pkghelp_stub_path, \"w\") as f4:\n            f4.write(pkghelp)\n```\n\nOriginal documentation:\n```python\nGenerate documents for R package creation\n\n    Parameters\n    ----------\n    pkg_data\n    project_shortname\n    export_string\n\n    Returns\n    -------\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate documents for R package creation\n\n    Parameters\n    ----------\n    pkg_data\n    rpkg_data\n    project_shortname\n    export_string\n    package_depends\n    package_imports\n    package_suggests\n\n    Returns\n    -------", "file": "dash_development__r_components_generation.py", "function": "generate_rpkg", "project": "dash", "change_stats": {"code_changes": 38, "doc_changes": 6, "total_changes": 44}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None):\n  \n  \n  with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    tf.summary.image('image_with_bounding_boxes', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    tf.summary.image('images_with_distorted_bounding_box',\n                     image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method),\n        num_cases=num_resize_cases)\n\n    tf.summary.image('cropped_resized_image',\n                     tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    tf.summary.image('final_distorted_image',\n                     tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image```\n\nUpdated code:\n```python\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n  \n  \n  with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    if add_image_summaries:\n      tf.summary.image('image_with_bounding_boxes', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    if add_image_summaries:\n      tf.summary.image('images_with_distorted_bounding_box',\n                       image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method),\n        num_cases=num_resize_cases)\n\n    if add_image_summaries:\n      tf.summary.image('cropped_resized_image',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    if add_image_summaries:\n      tf.summary.image('final_distorted_image',\n                       tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n```\n\nOriginal documentation:\n```python\nDistort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n    add_image_summaries: Enable image summaries.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].", "file": "research_slim_preprocessing_inception_preprocessing.py", "function": "preprocess_for_train", "project": "models", "change_stats": {"code_changes": 23, "doc_changes": 3, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_state(self, entity_id: str, state: State) -> bool:\n        \n        \n        state_obj = self.get(entity_id)\n        return state_obj is not None and state_obj.state == state```\n\nUpdated code:\n```python\ndef is_state(self, entity_id: str, state: str) -> bool:\n        \n        \n        state_obj = self.get(entity_id)\n        return state_obj is not None and state_obj.state == state\n```\n\nOriginal documentation:\n```python\nTest if entity exists and is specified state.\n\n        Async friendly.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Test if entity exists and is in specified state.\n\n        Async friendly.", "file": "homeassistant_core.py", "function": "StateMachine.is_state", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef drop_path(input, drop_prob=0.0, training=False, scale_by_keep=True):\n    \n    \n    \n    if drop_prob == 0.0 or not training:\n        return input\n    keep_prob = 1 - drop_prob\n    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = input.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return input * random_tensor```\n\nUpdated code:\n```python\ndef drop_path(input, drop_prob=0.0, training=False, scale_by_keep=True):\n    \n    \n    \n    if drop_prob == 0.0 or not training:\n        return input\n    keep_prob = 1 - drop_prob\n    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n    random_tensor.floor_()  # binarize\n    output = input.div(keep_prob) * random_tensor\n    return output\n```\n\nOriginal documentation:\n```python\nDrop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n    argument.", "file": "src_transformers_models_swin_modeling_swin.py", "function": "drop_path", "project": "transformers", "change_stats": {"code_changes": 10, "doc_changes": 8, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef main(_):\n  \n  \n  # must create before flags are used\n  runner = runners.DirectRunner()\n\n  dirname = os.path.dirname(FLAGS.embedding_output_tfrecord)\n  tf.io.gfile.makedirs(dirname)\n  runner.run(\n      construct_pipeline(FLAGS.embedding_input_tfrecord,\n                         FLAGS.embedding_output_tfrecord,\n                         FLAGS.embedding_model_dir, FLAGS.top_k_embedding_count,\n                         FLAGS.bottom_k_embedding_count, FLAGS.num_shards))```\n\nUpdated code:\n```python\ndef main(argv=None, save_main_session=True):\n  \n  \n  args, pipeline_args = parse_args(argv)\n\n  pipeline_options = beam.options.pipeline_options.PipelineOptions(\n            pipeline_args)\n  pipeline_options.view_as(\n      beam.options.pipeline_options.SetupOptions).save_main_session = (\n          save_main_session)\n\n  dirname = os.path.dirname(args.embedding_output_tfrecord)\n  tf.io.gfile.makedirs(dirname)\n\n  p = beam.Pipeline(options=pipeline_options)\n\n  construct_pipeline(\n      p,\n      args.embedding_input_tfrecord,\n      args.embedding_output_tfrecord,\n      args.embedding_model_dir,\n      args.top_k_embedding_count,\n      args.bottom_k_embedding_count,\n      args.num_shards)\n\n  p.run()\n```\n\nOriginal documentation:\n```python\nRuns the Beam pipeline that performs inference.\n\n  Args:\n    _: unused\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Runs the Beam pipeline that performs inference.\n\n  Args:\n    argv: Command line arguments.\n    save_main_session: Whether to save the main session.", "file": "research_object_detection_dataset_tools_context_rcnn_generate_embedding_data.py", "function": "main", "project": "models", "change_stats": {"code_changes": 33, "doc_changes": 5, "total_changes": 38}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef level(self, name, no=None, color=None, icon=None):\n        \n        \n        if not isinstance(name, str):\n            raise TypeError(\n                \"Invalid level name, it should be a string, not: '%s'\" % type(name).__name__\n            )\n\n        if no is color is icon is None:\n            try:\n                return self._core.levels[name]\n            except KeyError:\n                raise ValueError(\"Level '%s' does not exist\" % name) from None\n\n        if name not in self._core.levels:\n            if no is None:\n                raise ValueError(\n                    \"Level '%s' does not exist, you have to create it by specifying a level no\"\n                    % name\n                )\n            else:\n                old_no, old_color, old_icon = None, \"\", \" \"\n        else:\n            _, old_no, old_color, old_icon = self.level(name)\n\n        if no is None:\n            no = old_no\n\n        if color is None:\n            color = old_color\n\n        if icon is None:\n            icon = old_icon\n\n        if not isinstance(no, int):\n            raise TypeError(\n                \"Invalid level no, it should be an integer, not: '%s'\" % type(no).__name__\n            )\n\n        if no < 0:\n            raise ValueError(\"Invalid level no, it should be a positive integer, not: %d\" % no)\n\n        ansi = ColoredString.ansify(color)\n        level = Level(name, no, color, icon)\n\n        with self._core.lock:\n            self._core.levels[name] = level\n            self._core.levels_ansi_codes[name] = ansi\n            for handler in self._core.handlers.values():\n                handler.update_format(name)\n\n        return level```\n\nUpdated code:\n```python\ndef level(self, name, no=None, color=None, icon=None):\n        \n        \n        if not isinstance(name, str):\n            raise TypeError(\n                \"Invalid level name, it should be a string, not: '%s'\" % type(name).__name__\n            )\n\n        if no is color is icon is None:\n            try:\n                return self._core.levels[name]\n            except KeyError:\n                raise ValueError(\"Level '%s' does not exist\" % name) from None\n\n        if name not in self._core.levels:\n            if no is None:\n                raise ValueError(\n                    \"Level '%s' does not exist, you have to create it by specifying a level no\"\n                    % name\n                )\n            else:\n                old_color, old_icon = \"\", \" \"\n        elif no is not None:\n            raise TypeError(\"Level '%s' already exists, you can't update its severity no\" % name)\n        else:\n            _, no, old_color, old_icon = self.level(name)\n\n        if color is None:\n            color = old_color\n\n        if icon is None:\n            icon = old_icon\n\n        if not isinstance(no, int):\n            raise TypeError(\n                \"Invalid level no, it should be an integer, not: '%s'\" % type(no).__name__\n            )\n\n        if no < 0:\n            raise ValueError(\"Invalid level no, it should be a positive integer, not: %d\" % no)\n\n        ansi = ColoredString.ansify(color)\n        level = Level(name, no, color, icon)\n\n        with self._core.lock:\n            self._core.levels[name] = level\n            self._core.levels_ansi_codes[name] = ansi\n            for handler in self._core.handlers.values():\n                handler.update_format(name)\n\n        return level\n```\n\nOriginal documentation:\n```python\nAdd, update or retrieve a logging level.\n\n        Logging levels are defined by their ``name`` to which a severity ``no``, an ansi ``color``\n        tag and an ``icon`` are associated and possibly modified at run-time. To |log| to a custom\n        level, you should necessarily use its name, the severity number is not linked back to levels\n        name (this implies that several levels can share the same severity).\n\n        To add a new level, its ``name`` and its ``no`` are required. A ``color`` and an ``icon``\n        can also be specified or will be empty by default.\n\n        To update an existing level, pass its ``name`` with the parameters to be changed.\n\n        To retrieve level information, the ``name`` solely suffices.\n\n        Parameters\n        ----------\n        name : |str|\n            The name of the logging level.\n        no : |int|\n            The severity of the level to be added or updated.\n        color : |str|\n            The color markup of the level to be added or updated.\n        icon : |str|\n            The icon of the level to be added or updated.\n\n        Returns\n        -------\n        ``Level``\n            A |namedtuple| containing information about the level.\n\n        Raises\n        ------\n        ValueError\n            If there is no level registered with such ``name``.\n\n        Examples\n        --------\n        >>> level = logger.level(\"ERROR\")\n        >>> print(level)\n        Level(name='ERROR', no=40, color='<red><bold>', icon='\u274c')\n        >>> logger.add(sys.stderr, format=\"{level.no} {level.icon} {message}\")\n        1\n        >>> logger.level(\"CUSTOM\", no=15, color=\"<blue>\", icon=\"@\")\n        Level(name='CUSTOM', no=15, color='<blue>', icon='@')\n        >>> logger.log(\"CUSTOM\", \"Logging...\")\n        15 @ Logging...\n        >>> logger.level(\"WARNING\", icon=r\"/!\\\\\")\n        Level(name='WARNING', no=30, color='<yellow><bold>', icon='/!\\\\\\\\')\n        >>> logger.warning(\"Updated!\")\n        30 /!\\\\ Updated!\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Add, update or retrieve a logging level.\n\n        Logging levels are defined by their ``name`` to which a severity ``no``, an ansi ``color``\n        tag and an ``icon`` are associated and possibly modified at run-time. To |log| to a custom\n        level, you should necessarily use its name, the severity number is not linked back to levels\n        name (this implies that several levels can share the same severity).\n\n        To add a new level, its ``name`` and its ``no`` are required. A ``color`` and an ``icon``\n        can also be specified or will be empty by default.\n\n        To update an existing level, pass its ``name`` with the parameters to be changed. It is not\n        possible to modify the ``no`` of a level once it has been added.\n\n        To retrieve level information, the ``name`` solely suffices.\n\n        Parameters\n        ----------\n        name : |str|\n            The name of the logging level.\n        no : |int|\n            The severity of the level to be added or updated.\n        color : |str|\n            The color markup of the level to be added or updated.\n        icon : |str|\n            The icon of the level to be added or updated.\n\n        Returns\n        -------\n        ``Level``\n            A |namedtuple| containing information about the level.\n\n        Raises\n        ------\n        ValueError\n            If there is no level registered with such ``name``.\n\n        Examples\n        --------\n        >>> level = logger.level(\"ERROR\")\n        >>> print(level)\n        Level(name='ERROR', no=40, color='<red><bold>', icon='\u274c')\n        >>> logger.add(sys.stderr, format=\"{level.no} {level.icon} {message}\")\n        1\n        >>> logger.level(\"CUSTOM\", no=15, color=\"<blue>\", icon=\"@\")\n        Level(name='CUSTOM', no=15, color='<blue>', icon='@')\n        >>> logger.log(\"CUSTOM\", \"Logging...\")\n        15 @ Logging...\n        >>> logger.level(\"WARNING\", icon=r\"/!\\\\\")\n        Level(name='WARNING', no=30, color='<yellow><bold>', icon='/!\\\\\\\\')\n        >>> logger.warning(\"Updated!\")\n        30 /!\\\\ Updated!", "file": "loguru__logger.py", "function": "Logger.level", "project": "loguru", "change_stats": {"code_changes": 11, "doc_changes": 5, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef compress_image(pil_img, path, quality, max_width=300, max_height=300):\n    '''\n        \n    '''\n    pil_img.thumbnail((max_width, max_height), Image.ANTIALIAS)\n    quality = int(round(quality))\n    if quality <= 0 or quality >= 100:\n        raise Exception(\"SNAPSHOT_QUALITY (\" + str(quality) + \") should be an integer in the range [1,99]\")\n    pil_img.save(path, quality=quality, optimize=True)```\n\nUpdated code:\n```python\ndef compress_image(pil_img, path, quality, max_size=None):\n    \n    \n    \n    if max_size:\n        # The picture will be saved in a size <= max_size*max_size\n        pil_img.thumbnail((max_size, max_size), Image.ANTIALIAS)\n    quality = int(round(quality))\n    if quality <= 0 or quality >= 100:\n        raise Exception(\"SNAPSHOT_QUALITY (\" + str(quality) + \") should be an integer in the range [1,99]\")\n    pil_img.save(path, quality=quality, optimize=True)\n```\n\nOriginal documentation:\n```python\n\u751f\u6210\u7f29\u7565\u56fe\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save the picture and compress\n\n    :param pil_img: PIL image\n    :param path: save path\n    :param quality: the image quality, integer in range [1, 99]\n    :param max_size: the maximum size of the picture, e.g 1200\n    :return:", "file": "airtest_aircv_utils.py", "function": "compress_image", "project": "Airtest", "change_stats": {"code_changes": 14, "doc_changes": 10, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef new_frame_seq(self):\n        ''''''\n        # Default implementation is just an iterator over self._framedata\n        return iter(self._framedata)```\n\nUpdated code:\n```python\ndef new_frame_seq(self):\n        \n        # Default implementation is just an iterator over self._framedata\n        return iter(self._framedata)\n```\n\nOriginal documentation:\n```python\nCreates a new sequence of frame information.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a new sequence of frame information.", "file": "lib_matplotlib_animation.py", "function": "Animation.new_frame_seq", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef forward_one_step(\n        self,\n        tgt: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        memory: torch.Tensor,\n        *,\n        cache: List[torch.Tensor] = None,\n        return_hs: bool = False,\n    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \n        \n        x = self.embed(tgt)\n        if cache is None:\n            cache = [None] * len(self.decoders)\n        new_cache = []\n        for c, decoder in zip(cache, self.decoders):\n            x, tgt_mask, memory, memory_mask = decoder(\n                x, tgt_mask, memory, None, cache=c\n            )\n            new_cache.append(x)\n\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if return_hs:\n            hidden = y\n        if self.output_layer is not None:\n            y = torch.log_softmax(self.output_layer(y), dim=-1)\n\n        if return_hs:\n            return (y, hidden), new_cache\n        return y, new_cache```\n\nUpdated code:\n```python\ndef forward_one_step(\n        self,\n        tgt: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        memory: torch.Tensor,\n        memory_mask: torch.Tensor = None,\n        *,\n        cache: List[torch.Tensor] = None,\n        return_hs: bool = False,\n    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \n        \n        x = self.embed(tgt)\n        if cache is None:\n            cache = [None] * len(self.decoders)\n        new_cache = []\n        for c, decoder in zip(cache, self.decoders):\n            x, tgt_mask, memory, memory_mask = decoder(\n                x, tgt_mask, memory, memory_mask, cache=c\n            )\n            new_cache.append(x)\n\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if return_hs:\n            hidden = y\n        if self.output_layer is not None:\n            y = torch.log_softmax(self.output_layer(y), dim=-1)\n\n        if return_hs:\n            return (y, hidden), new_cache\n        return y, new_cache\n```\n\nOriginal documentation:\n```python\nForward one step.\n\n        Args:\n            tgt: input token ids, int64 (batch, maxlen_out)\n            tgt_mask: input token mask,  (batch, maxlen_out)\n                      dtype=torch.uint8 in PyTorch 1.2-\n                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n            memory: encoded memory, float32  (batch, maxlen_in, feat)\n            cache: cached output list of (batch, max_time_out-1, size)\n            return_hs: dec hidden state corresponding to ys,\n                used for searchable hidden ints\n        Returns:\n            y, cache: NN output value and cache per `self.decoders`.\n            y.shape` is (batch, maxlen_out, token)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Forward one step.\n\n        Args:\n            tgt: input token ids, int64 (batch, maxlen_out)\n            tgt_mask: input token mask,  (batch, maxlen_out)\n                      dtype=torch.uint8 in PyTorch 1.2-\n                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n            memory: encoded memory, float32  (batch, maxlen_in, feat)\n            memory_mask: encoded memory mask (batch, maxlen_in)\n            cache: cached output list of (batch, max_time_out-1, size)\n            return_hs: dec hidden state corresponding to ys,\n                used for searchable hidden ints\n        Returns:\n            y, cache: NN output value and cache per `self.decoders`.\n            y.shape` is (batch, maxlen_out, token)", "file": "espnet2_asr_decoder_transformer_decoder.py", "function": "BaseTransformerDecoder.forward_one_step", "project": "espnet", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_sql(\n        self,\n        frame,\n        name,\n        if_exists=\"fail\",\n        index=True,\n        index_label=None,\n        schema=None,\n        chunksize=None,\n        dtype: DtypeArg | None = None,\n        method=None,\n    ):\n        \n        \n        \n        if dtype:\n            if not is_dict_like(dtype):\n                # error: Value expression in dictionary comprehension has incompatible\n                # type \"Union[ExtensionDtype, str, dtype[Any], Type[object],\n                # Dict[Hashable, Union[ExtensionDtype, Union[str, dtype[Any]],\n                # Type[str], Type[float], Type[int], Type[complex], Type[bool],\n                # Type[object]]]]\"; expected type \"Union[ExtensionDtype, str,\n                # dtype[Any], Type[object]]\"\n                dtype = {col_name: dtype for col_name in frame}  # type: ignore[misc]\n            else:\n                dtype = cast(dict, dtype)\n\n            from sqlalchemy.types import (\n                TypeEngine,\n                to_instance,\n            )\n\n            for col, my_type in dtype.items():\n                if not isinstance(to_instance(my_type), TypeEngine):\n                    raise ValueError(f\"The type of {col} is not a SQLAlchemy type\")\n\n        table = SQLTable(\n            name,\n            self,\n            frame=frame,\n            index=index,\n            if_exists=if_exists,\n            index_label=index_label,\n            schema=schema,\n            dtype=dtype,\n        )\n        table.create()\n\n        from sqlalchemy.exc import SQLAlchemyError\n\n        try:\n            table.insert(chunksize, method=method)\n        except SQLAlchemyError as err:\n            # GH 34431 36465\n            raise ValueError(\"inf cannot be used with MySQL\") from err\n\n        if not name.isdigit() and not name.islower():\n            # check for potentially case sensitivity issues (GH7815)\n            # Only check when name is not a number and name is not lower case\n            engine = self.connectable.engine\n            with self.connectable.connect() as conn:\n                if _gt14():\n                    from sqlalchemy import inspect\n\n                    insp = inspect(conn)\n                    table_names = insp.get_table_names(\n                        schema=schema or self.meta.schema\n                    )\n                else:\n                    table_names = engine.table_names(\n                        schema=schema or self.meta.schema, connection=conn\n                    )\n            if name not in table_names:\n                msg = (\n                    f\"The provided table name '{name}' is not found exactly as \"\n                    \"such in the database after writing the table, possibly \"\n                    \"due to case sensitivity issues. Consider using lower \"\n                    \"case table names.\"\n                )\n                warnings.warn(msg, UserWarning)```\n\nUpdated code:\n```python\ndef to_sql(\n        self,\n        frame,\n        name,\n        if_exists=\"fail\",\n        index=True,\n        index_label=None,\n        schema=None,\n        chunksize=None,\n        dtype: DtypeArg | None = None,\n        method=None,\n        engine=\"auto\",\n        **engine_kwargs,\n    ):\n        \n        \n        \n        sql_engine = get_engine(engine)\n\n        table = self.prep_table(\n            frame=frame,\n            name=name,\n            if_exists=if_exists,\n            index=index,\n            index_label=index_label,\n            schema=schema,\n            dtype=dtype,\n        )\n\n        sql_engine.insert_records(\n            table=table,\n            con=self.connectable,\n            frame=frame,\n            name=name,\n            index=index,\n            schema=schema,\n            chunksize=chunksize,\n            method=method,\n            **engine_kwargs,\n        )\n\n        self.check_case_sensitive(name=name, schema=schema)\n```\n\nOriginal documentation:\n```python\nWrite records stored in a DataFrame to a SQL database.\n\n        Parameters\n        ----------\n        frame : DataFrame\n        name : string\n            Name of SQL table.\n        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n            - fail: If table exists, do nothing.\n            - replace: If table exists, drop it, recreate it, and insert data.\n            - append: If table exists, insert data. Create if does not exist.\n        index : bool, default True\n            Write DataFrame index as a column.\n        index_label : string or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        schema : string, default None\n            Name of SQL schema in database to write to (if database flavor\n            supports this). If specified, this overwrites the default\n            schema of the SQLDatabase object.\n        chunksize : int, default None\n            If not None, then rows will be written in batches of this size at a\n            time.  If None, all rows will be written at once.\n        dtype : single type or dict of column name to SQL type, default None\n            Optional specifying the datatype for columns. The SQL type should\n            be a SQLAlchemy type. If all columns are of the same type, one\n            single value can be used.\n        method : {None', 'multi', callable}, default None\n            Controls the SQL insertion clause used:\n\n            * None : Uses standard SQL ``INSERT`` clause (one per row).\n            * 'multi': Pass multiple values in a single ``INSERT`` clause.\n            * callable with signature ``(pd_table, conn, keys, data_iter)``.\n\n            Details and a sample callable implementation can be found in the\n            section :ref:`insert method <io.sql.method>`.\n\n            .. versionadded:: 0.24.0\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write records stored in a DataFrame to a SQL database.\n\n        Parameters\n        ----------\n        frame : DataFrame\n        name : string\n            Name of SQL table.\n        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n            - fail: If table exists, do nothing.\n            - replace: If table exists, drop it, recreate it, and insert data.\n            - append: If table exists, insert data. Create if does not exist.\n        index : boolean, default True\n            Write DataFrame index as a column.\n        index_label : string or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        schema : string, default None\n            Name of SQL schema in database to write to (if database flavor\n            supports this). If specified, this overwrites the default\n            schema of the SQLDatabase object.\n        chunksize : int, default None\n            If not None, then rows will be written in batches of this size at a\n            time.  If None, all rows will be written at once.\n        dtype : single type or dict of column name to SQL type, default None\n            Optional specifying the datatype for columns. The SQL type should\n            be a SQLAlchemy type. If all columns are of the same type, one\n            single value can be used.\n        method : {None', 'multi', callable}, default None\n            Controls the SQL insertion clause used:\n\n            * None : Uses standard SQL ``INSERT`` clause (one per row).\n            * 'multi': Pass multiple values in a single ``INSERT`` clause.\n            * callable with signature ``(pd_table, conn, keys, data_iter)``.\n\n            Details and a sample callable implementation can be found in the\n            section :ref:`insert method <io.sql.method>`.\n\n            .. versionadded:: 0.24.0\n\n        engine : {'auto', 'sqlalchemy'}, default 'auto'\n            SQL engine library to use. If 'auto', then the option\n            ``io.sql.engine`` is used. The default ``io.sql.engine``\n            behavior is 'sqlalchemy'\n\n            .. versionadded:: 1.3.0\n\n        **engine_kwargs\n            Any additional kwargs are passed to the engine.", "file": "pandas_io_sql.py", "function": "SQLDatabase.to_sql", "project": "pandas", "change_stats": {"code_changes": 76, "doc_changes": 14, "total_changes": 90}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef newton_cg(func_grad_hess, func, grad, x0, args=(), eps=1e-4, tol=1e-4,\n              maxiter=100):\n    \n    \n    \n    x0 = np.asarray(x0).flatten()\n    xk = x0\n    k = 1\n    old_fval = func(x0, *args)\n    old_old_fval = None\n\n    # Outer loop: our Newton iteration\n    while k <= maxiter:\n\n        # Compute a search direction pk by applying the CG method to\n        #  del2 f(xk) p = - fgrad f(xk) starting from 0.\n        fval, fgrad, fhess_p = func_grad_hess(xk, *args)\n\n        absgrad = np.abs(fgrad)\n        if np.max(absgrad) < tol:\n            break\n\n        maggrad = np.sum(absgrad)\n        eta = min([0.5, np.sqrt(maggrad)])\n        termcond = eta * maggrad\n        xsupi = np.zeros(len(x0), dtype=x0.dtype)\n        ri = fgrad\n        psupi = -ri\n        i = 0\n        dri0 = np.dot(ri, ri)\n\n        # Inner loop: solve the Newton update by conjugate gradient, to\n        # avoid inverting the Hessian\n        while np.sum(np.abs(ri)) > termcond:\n            Ap = fhess_p(psupi)\n            # check curvature\n            curv = np.dot(psupi, Ap)\n            if 0 <= curv <= 3*np.finfo(np.float64).eps:\n                break\n            elif curv < 0:\n                if (i > 0):\n                    break\n                else:\n                    # fall back to steepest descent direction\n                    xsupi = xsupi + dri0 / curv * psupi\n                    break\n            alphai = dri0 / curv\n            xsupi = xsupi + alphai * psupi\n            ri = ri + alphai * Ap\n            dri1 = np.dot(ri, ri)\n            betai = dri1 / dri0\n            psupi = -ri + betai * psupi\n            i = i + 1\n            dri0 = dri1          # update np.dot(ri,ri) for next time.\n\n        try:\n            alphak, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n                _line_search_wolfe12(func, grad, xk, xsupi, fgrad,\n                                     old_fval, old_old_fval, args=args)\n        except _LineSearchError:\n            warnings.warn('Line Search failed')\n            break\n\n        update = alphak * xsupi\n        xk = xk + update        # upcast if necessary\n        k += 1\n\n    if k > maxiter:\n        warnings.warn(\"newton-cg failed to converge. Increase the \"\n                      \"number of iterations.\")\n    return xk```\n\nUpdated code:\n```python\ndef newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,\n              maxiter=100, maxinner=200, line_search=True, warn=True):\n    \n    \n    \n    x0 = np.asarray(x0).flatten()\n    xk = x0\n    k = 1\n\n    if line_search:\n        old_fval = func(x0, *args)\n        old_old_fval = None\n\n    # Outer loop: our Newton iteration\n    while k <= maxiter:\n        # Compute a search direction pk by applying the CG method to\n        #  del2 f(xk) p = - fgrad f(xk) starting from 0.\n        fgrad, fhess_p = grad_hess(xk, *args)\n\n        absgrad = np.abs(fgrad)\n        if np.max(absgrad) < tol:\n            break\n\n        maggrad = np.sum(absgrad)\n        eta = min([0.5, np.sqrt(maggrad)])\n        termcond = eta * maggrad\n\n        # Inner loop: solve the Newton update by conjugate gradient, to\n        # avoid inverting the Hessian\n        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)\n\n        alphak = 1.0\n\n        if line_search:\n            try:\n                alphak, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n                    _line_search_wolfe12(func, grad, xk, xsupi, fgrad,\n                                         old_fval, old_old_fval, args=args)\n            except _LineSearchError:\n                warnings.warn('Line Search failed')\n                break\n\n        xk = xk + alphak * xsupi        # upcast if necessary\n        k += 1\n\n    if warn and k > maxiter:\n        warnings.warn(\"newton-cg failed to converge. Increase the \"\n                      \"number of iterations.\")\n    return xk\n```\n\nOriginal documentation:\n```python\nMinimization of scalar function of one or more variables using the\n    Newton-CG algorithm.\n\n    Parameters\n    ----------\n    func_grad_hess : callable\n        Should return the value of the function, the gradient, and a\n        callable returning the matvec product of the Hessian.\n\n    func : callable\n        Should return the value of the function.\n\n    grad : callable\n        Should return the function value and the gradient. This is used\n        by the linesearch functions.\n\n    x0 : float\n        Initial guess.\n\n    args: tuple, optional\n        Arguments passed to func_grad_hess, func and grad.\n\n    tol : float\n        Stopping criterion. The iteration will stop when\n        ``max{|g_i | i = 1, ..., n} <= tol``\n        where ``g_i`` is the i-th component of the gradient.\n\n    eps : float, optional\n        If fhess is approximated, use this value for the step size.\n\n    maxiter : int\n        Number of iterations.\n\n    Returns\n    -------\n    xk : float\n        Estimated minimum.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Minimization of scalar function of one or more variables using the\n    Newton-CG algorithm.\n\n    Parameters\n    ----------\n    grad_hess : callable\n        Should return the gradient and a callable returning the matvec product\n        of the Hessian.\n\n    func : callable\n        Should return the value of the function.\n\n    grad : callable\n        Should return the function value and the gradient. This is used\n        by the linesearch functions.\n\n    x0 : array of float\n        Initial guess.\n\n    args: tuple, optional\n        Arguments passed to func_grad_hess, func and grad.\n\n    tol : float\n        Stopping criterion. The iteration will stop when\n        ``max{|g_i | i = 1, ..., n} <= tol``\n        where ``g_i`` is the i-th component of the gradient.\n\n    maxiter : int\n        Number of Newton iterations.\n\n    maxinner : int\n        Number of CG iterations.\n\n    line_search: boolean\n        Whether to use a line search or not.\n\n    warn: boolean\n        Whether to warn when didn't converge.\n\n    Returns\n    -------\n    xk : ndarray of float\n        Estimated minimum.", "file": "sklearn_utils_optimize.py", "function": "newton_cg", "project": "scikit-learn", "change_stats": {"code_changes": 64, "doc_changes": 26, "total_changes": 90}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_dataset_shard(\n    dataset_name: Optional[str] = None,\n) -> Optional[\"DatasetIterator\"]:\n    \n    \n    session = _get_session()\n    if not isinstance(session, _TrainSessionImpl):\n        raise RuntimeError(\n            \"`get_dataset_shard` can only be called for TrainSession! \"\n            \"Make sure you only use that in `train_loop_per_worker` function\"\n            \"that is passed into `DataParallelTrainer`.\"\n        )\n    return session.get_dataset_shard(dataset_name)```\n\nUpdated code:\n```python\ndef get_dataset_shard(\n    dataset_name: Optional[str] = None,\n) -> Optional[\"DataIterator\"]:\n    \n    \n    session = _get_session()\n    if not isinstance(session, _TrainSessionImpl):\n        raise RuntimeError(\n            \"`get_dataset_shard` can only be called for TrainSession! \"\n            \"Make sure you only use that in `train_loop_per_worker` function\"\n            \"that is passed into `DataParallelTrainer`.\"\n        )\n    return session.get_dataset_shard(dataset_name)\n```\n\nOriginal documentation:\n```python\nReturns the :class:`ray.data.DatasetIterator` shard for this worker.\n\n    Call :meth:`~ray.data.DatasetIterator.iter_torch_batches` or\n    :meth:`~ray.data.DatasetIterator.to_tf` on this shard to convert it to the\n    appropriate framework-specific data type.\n\n    .. code-block:: python\n\n        import ray\n        from ray import train\n        from ray.air import session\n        from ray.air.config import ScalingConfig\n\n        def train_loop_per_worker():\n            model = Net()\n            for iter in range(100):\n                # Trainer will automatically handle sharding.\n                data_shard = session.get_dataset_shard(\"train\")\n                for batch in data_shard.iter_torch_batches():\n                    # ...\n            return model\n\n        train_dataset = ray.data.from_items(\n            [{\"x\": x, \"y\": x + 1} for x in range(32)])\n        trainer = TorchTrainer(train_loop_per_worker,\n            scaling_config=ScalingConfig(num_workers=2),\n            datasets={\"train\": train_dataset})\n        trainer.fit()\n\n    Args:\n        dataset_name: If a Dictionary of Datasets was passed to ``Trainer``, then\n            specifies which dataset shard to return.\n\n    Returns:\n        The ``DatasetIterator`` shard to use for this worker.\n        If no dataset is passed into Trainer, then return None.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the :class:`ray.data.DataIterator` shard for this worker.\n\n    Call :meth:`~ray.data.DataIterator.iter_torch_batches` or\n    :meth:`~ray.data.DataIterator.to_tf` on this shard to convert it to the\n    appropriate framework-specific data type.\n\n    .. code-block:: python\n\n        import ray\n        from ray import train\n        from ray.air import session\n        from ray.air.config import ScalingConfig\n\n        def train_loop_per_worker():\n            model = Net()\n            for iter in range(100):\n                # Trainer will automatically handle sharding.\n                data_shard = session.get_dataset_shard(\"train\")\n                for batch in data_shard.iter_torch_batches():\n                    # ...\n            return model\n\n        train_dataset = ray.data.from_items(\n            [{\"x\": x, \"y\": x + 1} for x in range(32)])\n        trainer = TorchTrainer(train_loop_per_worker,\n            scaling_config=ScalingConfig(num_workers=2),\n            datasets={\"train\": train_dataset})\n        trainer.fit()\n\n    Args:\n        dataset_name: If a Dictionary of Datasets was passed to ``Trainer``, then\n            specifies which dataset shard to return.\n\n    Returns:\n        The ``DataIterator`` shard to use for this worker.\n        If no dataset is passed into Trainer, then return None.", "file": "python_ray_air_session.py", "function": "get_dataset_shard", "project": "ray", "change_stats": {"code_changes": 4, "doc_changes": 10, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _format_msg(\n    msg: str,\n    *args: Any,\n    _tags: Dict[str, Any] = None,\n    _numbered: Tuple[str, int, int] = None,\n    _no_format: bool = None,\n    **kwargs: Any\n):\n    \n    \n\n    if isinstance(msg, str) or isinstance(msg, ColorfulString):\n        tags_str = \"\"\n        if _tags is not None:\n            tags_list = []\n            for k, v in _tags.items():\n                if v is True:\n                    tags_list += [k]\n                    continue\n                if v is False:\n                    continue\n\n                tags_list += [k + \"=\" + v]\n            if tags_list:\n                tags_str = cf.reset(cf.dimmed(\" [{}]\".format(\", \".join(tags_list))))\n\n        numbering_str = \"\"\n        if _numbered is not None:\n            chars, i, n = _numbered\n            numbering_str = cf.dimmed(chars[0] + str(i) + \"/\" + str(n) + chars[1]) + \" \"\n\n        if _no_format:\n            # todo: throw if given args/kwargs?\n            return numbering_str + msg + tags_str\n        return numbering_str + msg.format(*args, **kwargs) + tags_str\n\n    if kwargs:\n        raise ValueError(\"We do not support printing kwargs yet.\")\n\n    res = [msg, *args]\n    res = [str(x) for x in res]\n    return \", \".join(res)```\n\nUpdated code:\n```python\ndef _format_msg(\n    msg: str,\n    *args: Any,\n    no_format: bool = None,\n    _tags: Dict[str, Any] = None,\n    _numbered: Tuple[str, int, int] = None,\n    **kwargs: Any\n):\n    \n    \n\n    if isinstance(msg, str) or isinstance(msg, ColorfulString):\n        tags_str = \"\"\n        if _tags is not None:\n            tags_list = []\n            for k, v in _tags.items():\n                if v is True:\n                    tags_list += [k]\n                    continue\n                if v is False:\n                    continue\n\n                tags_list += [k + \"=\" + v]\n            if tags_list:\n                tags_str = cf.reset(cf.dimmed(\" [{}]\".format(\", \".join(tags_list))))\n\n        numbering_str = \"\"\n        if _numbered is not None:\n            chars, i, n = _numbered\n            numbering_str = cf.dimmed(chars[0] + str(i) + \"/\" + str(n) + chars[1]) + \" \"\n\n        if no_format:\n            # todo: throw if given args/kwargs?\n            return numbering_str + msg + tags_str\n        return numbering_str + msg.format(*args, **kwargs) + tags_str\n\n    if kwargs:\n        raise ValueError(\"We do not support printing kwargs yet.\")\n\n    res = [msg, *args]\n    res = [str(x) for x in res]\n    return \", \".join(res)\n```\n\nOriginal documentation:\n```python\nFormats a message for printing.\n\n    Renders `msg` using the built-in `str.format` and the passed-in\n    `*args` and `**kwargs`.\n\n    Args:\n        *args (Any): `.format` arguments for `msg`.\n        _tags (Dict[str, Any]):\n            key-value pairs to display at the end of\n            the message in square brackets.\n\n            If a tag is set to `True`, it is printed without the value,\n            the presence of the tag treated as a \"flag\".\n\n            E.g. `_format_msg(\"hello\", _tags=dict(from=mom, signed=True))`\n                 `hello [from=Mom, signed]`\n        _numbered (Tuple[str, int, int]):\n            `(brackets, i, n)`\n\n            The `brackets` string is composed of two \"bracket\" characters,\n            `i` is the index, `n` is the total.\n\n            The string `{i}/{n}` surrounded by the \"brackets\" is\n            prepended to the message.\n\n            This is used to number steps in a procedure, with different\n            brackets specifying different major tasks.\n\n            E.g. `_format_msg(\"hello\", _numbered=(\"[]\", 0, 5))`\n                 `[0/5] hello`\n        _no_format (bool):\n            If `_no_format` is `True`,\n            `.format` will not be called on the message.\n\n            Useful if the output is user-provided or may otherwise\n            contain an unexpected formatting string (e.g. \"{}\").\n\n    Returns:\n        The formatted message.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Formats a message for printing.\n\n    Renders `msg` using the built-in `str.format` and the passed-in\n    `*args` and `**kwargs`.\n\n    Args:\n        *args (Any): `.format` arguments for `msg`.\n        no_format (bool):\n            If `no_format` is `True`,\n            `.format` will not be called on the message.\n\n            Useful if the output is user-provided or may otherwise\n            contain an unexpected formatting string (e.g. \"{}\").\n        _tags (Dict[str, Any]):\n            key-value pairs to display at the end of\n            the message in square brackets.\n\n            If a tag is set to `True`, it is printed without the value,\n            the presence of the tag treated as a \"flag\".\n\n            E.g. `_format_msg(\"hello\", _tags=dict(from=mom, signed=True))`\n                 `hello [from=Mom, signed]`\n        _numbered (Tuple[str, int, int]):\n            `(brackets, i, n)`\n\n            The `brackets` string is composed of two \"bracket\" characters,\n            `i` is the index, `n` is the total.\n\n            The string `{i}/{n}` surrounded by the \"brackets\" is\n            prepended to the message.\n\n            This is used to number steps in a procedure, with different\n            brackets specifying different major tasks.\n\n            E.g. `_format_msg(\"hello\", _numbered=(\"[]\", 0, 5))`\n                 `[0/5] hello`\n\n    Returns:\n        The formatted message.", "file": "python_ray_autoscaler__private_cli_logger.py", "function": "_format_msg", "project": "ray", "change_stats": {"code_changes": 6, "doc_changes": 14, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef working_dir(path: Union[str, Path]) -> None:\n    \n    \n    prev_cwd = Path.cwd()\n    os.chdir(str(path))\n    try:\n        yield\n    finally:\n        os.chdir(prev_cwd)```\n\nUpdated code:\n```python\ndef working_dir(path: Union[str, Path]) -> None:\n    \n    \n    prev_cwd = Path.cwd()\n    os.chdir(str(path))\n    try:\n        yield Path(path).resolve()\n    finally:\n        os.chdir(prev_cwd)\n```\n\nOriginal documentation:\n```python\nChange current working directory and returns to previous on exit.\n\n    path (str / Path): The directory to navigate to.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Change current working directory and returns to previous on exit.\n\n    path (str / Path): The directory to navigate to.\n    YIELDS (Path): The absolute path to the current working directory. This\n        should be used if the block needs to perform actions within the working\n        directory, to prevent mismatches with relative paths.", "file": "spacy_util.py", "function": "working_dir", "project": "spaCy", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef check_unknown_port(self, dport, proto, daddr, profileid, twid, uid, timestamp):\n         \n        portproto = f'{dport}/{proto}'\n        port_info = __database__.get_port_info(portproto)\n        if port_info:\n            # it's a known port\n            return False\n        # we don't have port info in our database\n        # is it a port that is known to be used by a specific organization\n        if self.port_belongs_to_an_org(daddr, portproto, profileid):\n            return False\n\n\n        if (not 'icmp' in proto\n            and not self.is_p2p(dport, proto, daddr)\n            and not __database__.is_ftp_port(dport)):\n            # we don't have info about this port\n            self.helper.set_evidence_unknown_port(daddr, dport, proto, timestamp, profileid, twid, uid)```\n\nUpdated code:\n```python\ndef check_unknown_port(self, dport, proto, daddr, profileid, twid, uid, timestamp):\n        \n        \n        portproto = f'{dport}/{proto}'\n        port_info = __database__.get_port_info(portproto)\n        if port_info:\n            # it's a known port\n            return False\n        # we don't have port info in our database\n        # is it a port that is known to be used by\n        # a specific organization\n        if self.port_belongs_to_an_org(daddr, portproto, profileid):\n            return False\n\n\n        if (not 'icmp' in proto\n            and not self.is_p2p(dport, proto, daddr)\n            and not __database__.is_ftp_port(dport)):\n            # we don't have info about this port\n            self.helper.set_evidence_unknown_port(daddr, dport, proto, timestamp, profileid, twid, uid)\n            return True\n```\n\nOriginal documentation:\n```python\nChecks dports that are not in our slips_files/ports_info/ files\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks dports that are not in our\n        slips_files/ports_info/services.csv", "file": "modules_flowalerts_flowalerts.py", "function": "Module.check_unknown_port", "project": "StratosphereLinuxIPS", "change_stats": {"code_changes": 9, "doc_changes": 5, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_redirect_path_with_status(\n        self, path, full_path=None, language=None, version_slug=None, forced_only=False\n    ):\n        \n        \n        \n        # Small optimization to skip executing the big query below.\n        if forced_only and not self.filter(force=True).exists():\n            return None, None\n\n        normalized_path = self._normalize_path(path)\n        normalized_full_path = self._normalize_path(full_path)\n\n        # add extra fields with the ``path`` and ``full_path`` to perform a\n        # filter at db level instead with Python.\n        queryset = self.annotate(\n            path=Value(\n                normalized_path,\n                output_field=CharField(),\n            ),\n            full_path=Value(\n                normalized_full_path,\n                output_field=CharField(),\n            ),\n        )\n        prefix = Q(\n            redirect_type=\"prefix\",\n            full_path__startswith=F(\"from_url\"),\n        )\n        page = Q(\n            redirect_type=\"page\",\n            path__exact=F(\"from_url\"),\n        )\n        exact = Q(\n            redirect_type=\"exact\",\n            from_url__endswith=\"$rest\",\n            full_path__startswith=F(\"from_url_without_rest\"),\n        ) | Q(\n            redirect_type=\"exact\",\n            full_path__exact=F(\"from_url\"),\n        )\n        sphinx_html = Q(\n            redirect_type=\"sphinx_html\",\n            path__endswith=\"/\",\n        ) | Q(\n            redirect_type=\"sphinx_html\",\n            path__endswith=\"/index.html\",\n        )\n        sphinx_htmldir = Q(\n            redirect_type=\"sphinx_htmldir\",\n            path__endswith=\".html\",\n        )\n\n        queryset = queryset.filter(prefix | page | exact | sphinx_html | sphinx_htmldir)\n        if forced_only:\n            queryset = queryset.filter(force=True)\n\n        # There should be one and only one redirect returned by this query. I\n        # can't think in a case where there can be more at this point. I'm\n        # leaving the loop just in case for now\n        for redirect in queryset.select_related(\"project\"):\n            new_path = redirect.get_redirect_path(\n                path=normalized_path,\n                full_path=normalized_full_path,\n                language=language,\n                version_slug=version_slug,\n            )\n            if new_path:\n                return new_path, redirect.http_status\n        return (None, None)```\n\nUpdated code:\n```python\ndef get_redirect_path_with_status(\n        self, filename, path=None, language=None, version_slug=None, forced_only=False\n    ):\n        \n        \n        \n        # Small optimization to skip executing the big query below.\n        # TODO: use filter(enabled=True) once we have removed the null option from the field.\n        if forced_only and not self.filter(force=True).exclude(enabled=False).exists():\n            return None, None\n\n        normalized_filename = self._normalize_path(filename)\n        normalized_path = self._normalize_path(path)\n\n        # Useful to allow redirects to match paths with or without trailling slash.\n        # For example, ``/docs`` will match ``/docs/`` and ``/docs``.\n        filename_without_trailling_slash = normalized_filename.rstrip(\"/\")\n        path_without_trailling_slash = normalized_path.rstrip(\"/\")\n\n        # Add extra fields with the ``filename`` and ``path`` to perform a\n        # filter at db level instead with Python.\n        queryset = self.annotate(\n            filename=Value(\n                filename,\n                output_field=CharField(),\n            ),\n            path=Value(\n                normalized_path,\n                output_field=CharField(),\n            ),\n            filename_without_trailling_slash=Value(\n                filename_without_trailling_slash,\n                output_field=CharField(),\n            ),\n            path_without_trailling_slash=Value(\n                path_without_trailling_slash,\n                output_field=CharField(),\n            ),\n        )\n        page = Q(\n            redirect_type=PAGE_REDIRECT,\n            from_url_without_rest__isnull=True,\n            filename_without_trailling_slash__exact=F(\"from_url\"),\n        ) | Q(\n            redirect_type=PAGE_REDIRECT,\n            from_url_without_rest__isnull=False,\n            filename__startswith=F(\"from_url_without_rest\"),\n        )\n        exact = Q(\n            redirect_type=EXACT_REDIRECT,\n            from_url_without_rest__isnull=True,\n            path_without_trailling_slash__exact=F(\"from_url\"),\n        ) | Q(\n            redirect_type=EXACT_REDIRECT,\n            from_url_without_rest__isnull=False,\n            path__startswith=F(\"from_url_without_rest\"),\n        )\n        clean_url_to_html = Q(redirect_type=CLEAN_URL_TO_HTML_REDIRECT)\n        html_to_clean_url = Q(redirect_type=HTML_TO_CLEAN_URL_REDIRECT)\n\n        if filename in [\"/index.html\", \"/\"]:\n            # If the filename is a root index file (``/index.html`` or ``/``), we only need to match page and exact redirects,\n            # since we don't have a filename to redirect to for clean_url_to_html and html_to_clean_url redirects.\n            queryset = queryset.filter(page | exact)\n        elif filename:\n            if filename.endswith((\"/index.html\", \"/\")):\n                queryset = queryset.filter(page | exact | clean_url_to_html)\n            elif filename.endswith(\".html\"):\n                queryset = queryset.filter(page | exact | html_to_clean_url)\n            else:\n                queryset = queryset.filter(page | exact)\n        else:\n            # If the filename is empty, we only need to match exact redirects.\n            # Since the other types of redirects are not valid without a filename.\n            queryset = queryset.filter(exact)\n\n        # TODO: use filter(enabled=True) once we have removed the null option from the field.\n        queryset = queryset.exclude(enabled=False)\n        if forced_only:\n            queryset = queryset.filter(force=True)\n\n        redirect = queryset.select_related(\"project\").first()\n        if redirect:\n            new_path = redirect.get_redirect_path(\n                filename=normalized_filename,\n                path=normalized_path,\n                language=language,\n                version_slug=version_slug,\n            )\n            return new_path, redirect.http_status\n        return None, None\n```\n\nOriginal documentation:\n```python\nGet the final redirect with its status code.\n\n        :param path: Is the path without the language and version parts.\n        :param full_path: Is the full path including the language and version parts.\n        :param forced_only: Include only forced redirects in the results.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the final redirect with its status code.\n\n        :param filename: The filename being served.\n        :param path: The whole path from the request.\n        :param forced_only: Include only forced redirects in the results.", "file": "readthedocs_redirects_querysets.py", "function": "RedirectQuerySet.get_redirect_path_with_status", "project": "readthedocs.org", "change_stats": {"code_changes": 97, "doc_changes": 6, "total_changes": 103}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_joint_position(self, position: float,\n                           disable_dynamics: bool = False) -> None:\n        \n        \n        if not disable_dynamics:\n            sim.simSetJointPosition(self._handle, position)\n            return\n\n        is_model = self.is_model()\n        if not is_model:\n            self.set_model(True)\n\n        prior = sim.simGetModelProperty(self.get_handle())\n        p = prior | sim.sim_modelproperty_not_dynamic\n        # Disable the dynamics\n        sim.simSetModelProperty(self._handle, p)\n        with utils.step_lock:\n            sim.simExtStep(True)  # Have to step for changes to take effect\n\n        sim.simSetJointPosition(self._handle, position)\n        self.set_joint_target_position(position)\n\n        # Re-enable the dynamics\n        sim.simSetModelProperty(self._handle, prior)\n        self.set_model(is_model)\n        with utils.step_lock:\n            sim.simExtStep(True)```\n\nUpdated code:\n```python\ndef set_joint_position(self, position: float,\n                           disable_dynamics: bool = False) -> None:\n        \n        \n        if not disable_dynamics:\n            sim.simSetJointPosition(self._handle, position)\n            return\n\n        is_model = self.is_model()\n        if not is_model:\n            self.set_model(True)\n\n        prior = sim.simGetModelProperty(self.get_handle())\n        p = prior | sim.sim_modelproperty_not_dynamic\n        # Disable the dynamics\n        sim.simSetModelProperty(self._handle, p)\n        with utils.step_lock:\n            sim.simExtStep(True)  # Have to step for changes to take effect\n\n        sim.simSetJointPosition(self._handle, position)\n        self.set_joint_target_position(position)\n\n        with utils.step_lock:\n            sim.simExtStep(True)  # Have to step for changes to take effect\n        # Re-enable the dynamics\n        sim.simSetModelProperty(self._handle, prior)\n        self.set_model(is_model)\n```\n\nOriginal documentation:\n```python\nSets the intrinsic position of the joint.\n\n        :param disable_dynamics: If True, then the position can be set even\n            when the joint mode is in Force mode. It will disable dynamics,\n            move the joint, and then re-enable dynamics.\n\n        :param positions: A list of positions of the joints (angular or linear\n            values depending on the joint type).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sets the intrinsic position of the joint.\n\n        :param disable_dynamics: If True, then the position can be set even\n            when the joint mode is in Force mode. It will disable dynamics,\n            move the joint, and then re-enable dynamics.\n\n        :param position: Position of a joint (angular or linear\n            values depending on the joint type).", "file": "pyrep_objects_joint.py", "function": "Joint.set_joint_position", "project": "PyRep", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _extract_account_syncid(data):\n        \n        \n        \n        sync_ids = (try_get(\n            data, (lambda x: x['responseContext']['mainAppWebResponseContext']['datasyncId'],\n                   lambda x: x['DATASYNC_ID']), compat_str) or '').split(\"||\")\n        if len(sync_ids) >= 2 and sync_ids[1]:\n            # datasyncid is of the form \"channel_syncid||user_syncid\" for secondary channel\n            # and just \"user_syncid||\" for primary channel. We only want the channel_syncid\n            return sync_ids[0]\n        # ytcfg includes channel_syncid if on secondary channel\n        return data.get('DELEGATED_SESSION_ID')```\n\nUpdated code:\n```python\ndef _extract_account_syncid(*args):\n        \n        \n        \n        for data in args:\n            # ytcfg includes channel_syncid if on secondary channel\n            delegated_sid = try_get(data, lambda x: x['DELEGATED_SESSION_ID'], compat_str)\n            if delegated_sid:\n                return delegated_sid\n            sync_ids = (try_get(\n                data, (lambda x: x['responseContext']['mainAppWebResponseContext']['datasyncId'],\n                       lambda x: x['DATASYNC_ID']), compat_str) or '').split(\"||\")\n            if len(sync_ids) >= 2 and sync_ids[1]:\n                # datasyncid is of the form \"channel_syncid||user_syncid\" for secondary channel\n                # and just \"user_syncid||\" for primary channel. We only want the channel_syncid\n                return sync_ids[0]\n```\n\nOriginal documentation:\n```python\nExtract syncId required to download private playlists of secondary channels\n        @param data Either response or ytcfg\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extract syncId required to download private playlists of secondary channels\n        @params response and/or ytcfg", "file": "yt_dlp_extractor_youtube.py", "function": "YoutubeBaseInfoExtractor._extract_account_syncid", "project": "yt-dlp", "change_stats": {"code_changes": 25, "doc_changes": 4, "total_changes": 29}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _copy_tick_props(self, src, dest):\n        \n        if src is None or dest is None:\n            return\n        dest.label1.update_from(src.label1)\n        dest.label2.update_from(src.label2)\n        dest.tick1line.update_from(src.tick1line)\n        dest.tick2line.update_from(src.tick2line)\n        dest.gridline.update_from(src.gridline)```\n\nUpdated code:\n```python\ndef _copy_tick_props(self, src, dest):\n        ''\n        if src is None or dest is None:\n            return\n        dest.label1.update_from(src.label1)\n        dest.label2.update_from(src.label2)\n        dest.tick1line.update_from(src.tick1line)\n        dest.tick2line.update_from(src.tick2line)\n        dest.gridline.update_from(src.gridline)\n```\n\nOriginal documentation:\n```python\nCopy the properties from *src* tick to *dest* tick.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Copy the props from src tick to dest tick", "file": "lib_matplotlib_axis.py", "function": "Axis._copy_tick_props", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _sort_mixed(values) -> np.ndarray:\n    \n    str_pos = np.array([isinstance(x, str) for x in values], dtype=bool)\n    none_pos = np.array([x is None for x in values], dtype=bool)\n    nums = np.sort(values[~str_pos & ~none_pos])\n    strs = np.sort(values[str_pos])\n    return np.concatenate(\n        [nums, np.asarray(strs, dtype=object), np.array(values[none_pos])]\n    )```\n\nUpdated code:\n```python\ndef _sort_mixed(values) -> np.ndarray:\n    \n    str_pos = np.array([isinstance(x, str) for x in values], dtype=bool)\n    null_pos = np.array([isna(x) for x in values], dtype=bool)\n    nums = np.sort(values[~str_pos & ~null_pos])\n    strs = np.sort(values[str_pos])\n    return np.concatenate(\n        [nums, np.asarray(strs, dtype=object), np.array(values[null_pos])]\n    )\n```\n\nOriginal documentation:\n```python\norder ints before strings in 1d arrays, safe in py3\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "order ints before strings before nulls in 1d arrays", "file": "pandas_core_algorithms.py", "function": "_sort_mixed", "project": "pandas", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef test_mm_sparse_first_NT(self, dtype, device, backend):\n        \n        \n        \n        SparseSemiStructuredTensor._FORCE_CUTLASS = (backend == \"cutlass\")\n\n        A = rand_sparse_semi_structured_mask(128, 128, dtype=dtype)\n        A_sparse = to_sparse_semi_structured(A)\n\n        B = torch.rand((128, 128), device=A_sparse.device).to(dtype)\n\n        # Currently we don't support int matmul on GPU, so evaluate on CPU and copy over\n        if dtype is torch.int8:\n            # This should fail\n            if backend == \"cutlass\":\n                with self.assertRaisesRegex(RuntimeError, \"two_four_sgemm_cutlass_dispatch_layouts\"):\n                    sparse_result = torch.mm(A_sparse, B)\n            else:\n                with self.assertRaisesRegex(RuntimeError,\n                                            \"CUDA error: operation not supported when calling `cusparseLtMatmulDescriptorInit\"):\n                    sparse_result = torch.mm(A_sparse, B)\n\n            # test transpose\n            # NOTE: CUTLASS and cuSPARSELt have slightly different int8 behavior.\n            # CUTLASS will output to an int32 tensor while cuSPARSELt will output to a int8 tensor\n            dense_result = torch.mm(A.cpu(), B.t().cpu()).to(device, dtype=torch.int32 if backend == \"cutlass\" else torch.int8)\n            sparse_result = torch.mm(A_sparse, B.t())\n            assert torch.allclose(dense_result, sparse_result, rtol=1e-3, atol=1e-3)\n        else:\n            dense_result = torch.mm(A, B)\n            sparse_result = torch.mm(A_sparse, B)\n            assert torch.allclose(dense_result, sparse_result, rtol=1e-3, atol=1e-3)\n            # test transpose\n            dense_result = torch.mm(A, B.t())\n            sparse_result = torch.mm(A_sparse, B.t())\n            assert torch.allclose(dense_result, sparse_result, rtol=1e-3, atol=1e-3)```\n\nUpdated code:\n```python\ndef test_mm_sparse_first_NT(self, dense_input_shape, dtype, device, backend):\n        \n        \n        \n        SparseSemiStructuredTensor._FORCE_CUTLASS = (backend == \"cutlass\")\n\n        A = rand_sparse_semi_structured_mask(256, 128, dtype=dtype)\n        A_sparse = to_sparse_semi_structured(A)\n\n        B = torch.rand(dense_input_shape, device=A_sparse.device).to(dtype)\n\n        # Currently we don't support int matmul on GPU, so evaluate on CPU and copy over\n        if dtype is torch.int8 and dense_input_shape in {(1, 128), (64, 128)}:\n            # padding with int8 throws an error because transposing B yields a contiguous output\n            # and row-row 2:4 sparse @ dense with NN is not supported by cuSPARSELt or CUTLASS.\n            if backend == \"cutlass\":\n                with self.assertRaisesRegex(RuntimeError, \"two_four_sgemm_cutlass_dispatch_layouts\"):\n                    sparse_result = torch.mm(A_sparse, B.t())\n            else:\n                with self.assertRaisesRegex(RuntimeError,\n                                            \"CUDA error: operation not supported when calling `cusparseLtMatmulDescriptorInit\"):\n                    sparse_result = torch.mm(A_sparse, B.t())\n        elif dtype is torch.int8:\n            # test transpose\n            # NOTE: CUTLASS and cuSPARSELt have slightly different int8 behavior.\n            # CUTLASS will output to an int32 tensor while cuSPARSELt will output to a int8 tensor\n            dense_result = torch.mm(A.cpu(), B.t().cpu()).to(device, dtype=torch.int32 if backend == \"cutlass\" else torch.int8)\n            sparse_result = torch.mm(A_sparse, B.t())\n            assert torch.allclose(dense_result, sparse_result, rtol=1e-3, atol=1e-3)\n        else:\n            # test transpose\n            dense_result = torch.mm(A, B.t())\n            sparse_result = torch.mm(A_sparse, B.t())\n            assert torch.allclose(dense_result, sparse_result, rtol=1e-3, atol=1e-3)\n```\n\nOriginal documentation:\n```python\nEnsure torch.mm(A_sparse, B) is correct for float16 and will throw error for int8\n        Ensure torch.mm(A_sparse, B.t()) is correct\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Ensure torch.mm(A_sparse, B.t()) is correct for float16/bfloat16\n        and will throw an error for int8 + padding", "file": "test_test_sparse_semi_structured.py", "function": "TestSparseSemiStructured.test_mm_sparse_first_NT", "project": "pytorch", "change_stats": {"code_changes": 22, "doc_changes": 6, "total_changes": 28}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update(\n        self,\n        task_id: TaskID,\n        *,\n        total: float = None,\n        completed: float = None,\n        advance: float = None,\n        visible: bool = None,\n        refresh: bool = False,\n        **fields: Any,\n    ) -> None:\n        \n        \n        current_time = self.get_time()\n        with self._lock:\n            task = self._tasks[task_id]\n            completed_start = task.completed\n\n            if total is not None:\n                task.total = total\n            if advance is not None:\n                task.completed += advance\n            if completed is not None:\n                task.completed = completed\n            if visible is not None:\n                task.visible = visible\n            task.fields.update(fields)\n\n            update_completed = task.completed - completed_start\n            old_sample_time = current_time - self.speed_estimate_period\n            _progress = task._progress\n\n            while _progress and _progress[0].timestamp < old_sample_time:\n                _progress.popleft()\n            task._progress.append(ProgressSample(current_time, update_completed))\n            if refresh:\n                self.refresh()```\n\nUpdated code:\n```python\ndef update(\n        self,\n        task_id: TaskID,\n        *,\n        total: float = None,\n        completed: float = None,\n        advance: float = None,\n        description: str = None,\n        visible: bool = None,\n        refresh: bool = False,\n        **fields: Any,\n    ) -> None:\n        \n        \n        current_time = self.get_time()\n        with self._lock:\n            task = self._tasks[task_id]\n            completed_start = task.completed\n\n            if total is not None:\n                task.total = total\n            if advance is not None:\n                task.completed += advance\n            if completed is not None:\n                task.completed = completed\n            if description is not None:\n                task.description = description\n            if visible is not None:\n                task.visible = visible\n            task.fields.update(fields)\n\n            update_completed = task.completed - completed_start\n            old_sample_time = current_time - self.speed_estimate_period\n            _progress = task._progress\n\n            while _progress and _progress[0].timestamp < old_sample_time:\n                _progress.popleft()\n            task._progress.append(ProgressSample(current_time, update_completed))\n            if refresh:\n                self.refresh()\n```\n\nOriginal documentation:\n```python\nUpdate information associated with a task.\n        \n        Args:\n            task_id (TaskID): Task id (returned by add_task).            \n            total (float, optional): Updates task.total if not None.\n            completed (float, optional): Updates task.completed if not None.\n            advance (float, optional): Add a value to task.completed if not None.\n            visible (bool, optional): Set visible flag if not None.\n            refresh (bool): Force a refresh of progress information. Default is False.\n            **fields (Any): Additional data fields required for rendering.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Update information associated with a task.\n        \n        Args:\n            task_id (TaskID): Task id (returned by add_task).            \n            total (float, optional): Updates task.total if not None.\n            completed (float, optional): Updates task.completed if not None.\n            advance (float, optional): Add a value to task.completed if not None.\n            description (str, optional): Change task description if not None.\n            visible (bool, optional): Set visible flag if not None.\n            refresh (bool): Force a refresh of progress information. Default is False.\n            **fields (Any): Additional data fields required for rendering.", "file": "rich_progress.py", "function": "Progress.update", "project": "rich", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef update_build(self, state=None):\n        \n        \n        \n        if not self.record or (self.successful and not self.report_build_success):\n            return None\n\n        self.build['project'] = self.project.pk\n        self.build['version'] = self.version.pk\n        self.build['builder'] = socket.gethostname()\n        self.build['state'] = state\n        if self.done:\n            self.build['success'] = self.successful\n\n            # TODO drop exit_code and provide a more meaningful UX for error\n            # reporting\n            if self.failure and isinstance(self.failure,\n                                           BuildEnvironmentException):\n                self.build['exit_code'] = self.failure.status_code\n            elif len(self.commands) > 0:\n                self.build['exit_code'] = max([cmd.exit_code\n                                               for cmd in self.commands])\n\n        self.build['setup'] = self.build['setup_error'] = \"\"\n        self.build['output'] = self.build['error'] = \"\"\n\n        if self.start_time:\n            build_length = (datetime.utcnow() - self.start_time)\n            self.build['length'] = build_length.total_seconds()\n\n        if self.failure is not None:\n            self.build['error'] = str(self.failure)\n\n        # Attempt to stop unicode errors on build reporting\n        for key, val in self.build.items():\n            if isinstance(val, basestring):\n                self.build[key] = val.decode('utf-8', 'ignore')\n\n        try:\n            resp = api_v2.build(self.build['id']).put(self.build)\n        except Exception:\n            log.error(\"Unable to post a new build\", exc_info=True)```\n\nUpdated code:\n```python\ndef update_build(self, state=None):\n        \n        \n        if not self.record or (state == BUILD_STATE_FINISHED and\n                               not self.report_build_success):\n            return None\n\n        self.build['project'] = self.project.pk\n        self.build['version'] = self.version.pk\n        self.build['builder'] = socket.gethostname()\n        self.build['state'] = state\n        if self.done:\n            self.build['success'] = self.successful\n\n            # TODO drop exit_code and provide a more meaningful UX for error\n            # reporting\n            if self.failure and isinstance(self.failure,\n                                           BuildEnvironmentException):\n                self.build['exit_code'] = self.failure.status_code\n            elif len(self.commands) > 0:\n                self.build['exit_code'] = max([cmd.exit_code\n                                               for cmd in self.commands])\n\n        self.build['setup'] = self.build['setup_error'] = \"\"\n        self.build['output'] = self.build['error'] = \"\"\n\n        if self.start_time:\n            build_length = (datetime.utcnow() - self.start_time)\n            self.build['length'] = build_length.total_seconds()\n\n        if self.failure is not None:\n            self.build['error'] = str(self.failure)\n\n        # Attempt to stop unicode errors on build reporting\n        for key, val in self.build.items():\n            if isinstance(val, basestring):\n                self.build[key] = val.decode('utf-8', 'ignore')\n\n        try:\n            resp = api_v2.build(self.build['id']).put(self.build)\n        except Exception:\n            log.error(\"Unable to post a new build\", exc_info=True)\n```\n\nOriginal documentation:\n```python\nRecord a build by hitting the API.\n\n        Returns nothing\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Record a build by hitting the API\n\n        This step is skipped if we aren't recording the build, or if we don't\n        want to record successful builds yet (if we are running setup commands\n        for the build)", "file": "readthedocs_doc_builder_environments.py", "function": "BuildEnvironment.update_build", "project": "readthedocs.org", "change_stats": {"code_changes": 6, "doc_changes": 8, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef raise_routing_exception(self, request: Request) -> \"te.NoReturn\":\n        \n        \n        if (\n            not self.debug\n            or not isinstance(request.routing_exception, RequestRedirect)\n            or request.method in (\"GET\", \"HEAD\", \"OPTIONS\")\n        ):\n            raise request.routing_exception  # type: ignore\n\n        from .debughelpers import FormDataRoutingRedirect\n\n        raise FormDataRoutingRedirect(request)```\n\nUpdated code:\n```python\ndef raise_routing_exception(self, request: Request) -> \"te.NoReturn\":\n        \n        \n        if (\n            not self.debug\n            or not isinstance(request.routing_exception, RequestRedirect)\n            or request.routing_exception.code in {307, 308}\n            or request.method in {\"GET\", \"HEAD\", \"OPTIONS\"}\n        ):\n            raise request.routing_exception  # type: ignore\n\n        from .debughelpers import FormDataRoutingRedirect\n\n        raise FormDataRoutingRedirect(request)\n```\n\nOriginal documentation:\n```python\nExceptions that are recording during routing are reraised with\n        this method.  During debug we are not reraising redirect requests\n        for non ``GET``, ``HEAD``, or ``OPTIONS`` requests and we're raising\n        a different error instead to help debug situations.\n\n        :internal:\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Intercept routing exceptions and possibly do something else.\n\n        In debug mode, intercept a routing redirect and replace it with\n        an error if the body will be discarded.\n\n        With modern Werkzeug this shouldn't occur, since it now uses a\n        308 status which tells the browser to resend the method and\n        body.\n\n        .. versionchanged:: 2.1\n            Don't intercept 307 and 308 redirects.\n\n        :meta private:\n        :internal:", "file": "src_flask_app.py", "function": "Flask.raise_routing_exception", "project": "flask", "change_stats": {"code_changes": 5, "doc_changes": 18, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef step(self):\n        \n        \n        \n        autograd_ctx_id = dist_autograd._current_context()._context_id()\n        rpc_futs = []\n        for optim in self.remote_optimizers:\n            rpc_futs.append(rpc.rpc_async(\n                optim.owner(),\n                _local_optimizer_step,\n                args=(optim, autograd_ctx_id),\n            ))\n        _wait_for_all(rpc_futs)```\n\nUpdated code:\n```python\ndef step(self, context_id):\n        \n        \n        \n        dist_autograd._is_valid_context(context_id)\n        rpc_futs = []\n        for optim in self.remote_optimizers:\n            rpc_futs.append(rpc.rpc_async(\n                optim.owner(),\n                _local_optimizer_step,\n                args=(optim, context_id),\n            ))\n        _wait_for_all(rpc_futs)\n```\n\nOriginal documentation:\n```python\nPerforms a single optimization step.\n\n        This will call :meth:`torch.optim.Optimizer.step` on each worker\n        containing parameters to be optimized, and will block until all workers\n        return. The current distributed autograd\n        :class:`~torch.distributed.autograd.context` will be used globally.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Performs a single optimization step.\n\n        This will call :meth:`torch.optim.Optimizer.step` on each worker\n        containing parameters to be optimized, and will block until all workers\n        return. The current distributed autograd\n        :class:`~torch.distributed.autograd.context` will be used globally.\n\n        Args:\n            context_id: the autograd context id for which we should run the\n                optimizer step.", "file": "torch_distributed_optim_optimizer.py", "function": "DistributedOptimizer.step", "project": "pytorch", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef rename(self, source_path, destination_path):\n        \n        \n        \n        self.copy(source_path, destination_path)\n        self.remove(source_path)```\n\nUpdated code:\n```python\ndef rename(self, *args, **kwargs):\n        \n        \n        \n        self.move(*args, **kwargs)\n```\n\nOriginal documentation:\n```python\nRename/move an object from one GCS location to another.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Alias for ``move()``", "file": "luigi_contrib_gcs.py", "function": "GCSClient.rename", "project": "luigi", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_target_bid(self, pair: str, ticker: Dict[str, float]) -> float:\n        \n        \n        \n        if ticker['ask'] < ticker['last']:\n            ticker_rate = ticker['ask']\n        else:\n            balance = self.config['bid_strategy']['ask_last_balance']\n            ticker_rate = ticker['ask'] + balance * (ticker['last'] - ticker['ask'])\n\n        used_rate = ticker_rate\n        config_bid_strategy = self.config.get('bid_strategy', {})\n        if 'use_order_book' in config_bid_strategy and\\\n                config_bid_strategy.get('use_order_book', False):\n            logger.info('Getting price from order book')\n            order_book_top = config_bid_strategy.get('order_book_top', 1)\n            order_book = self.exchange.get_order_book(pair, order_book_top)\n            logger.debug('order_book %s', order_book)\n            # top 1 = index 0\n            order_book_rate = order_book['bids'][order_book_top - 1][0]\n            # if ticker has lower rate, then use ticker ( usefull if down trending )\n            logger.info('...top %s order book buy rate %0.8f', order_book_top, order_book_rate)\n            if ticker_rate < order_book_rate:\n                logger.info('...using ticker rate instead %0.8f', ticker_rate)\n                used_rate = ticker_rate\n            else:\n                used_rate = order_book_rate\n        else:\n            logger.info('Using Last Ask / Last Price')\n            used_rate = ticker_rate\n\n        return used_rate```\n\nUpdated code:\n```python\ndef get_target_bid(self, pair: str) -> float:\n        \n        \n        \n        config_bid_strategy = self.config.get('bid_strategy', {})\n        if 'use_order_book' in config_bid_strategy and\\\n                config_bid_strategy.get('use_order_book', False):\n            logger.info('Getting price from order book')\n            order_book_top = config_bid_strategy.get('order_book_top', 1)\n            order_book = self.exchange.get_order_book(pair, order_book_top)\n            logger.debug('order_book %s', order_book)\n            # top 1 = index 0\n            order_book_rate = order_book['bids'][order_book_top - 1][0]\n            logger.info('...top %s order book buy rate %0.8f', order_book_top, order_book_rate)\n            used_rate = order_book_rate\n        else:\n            logger.info('Using Last Ask / Last Price')\n            ticker = self.exchange.get_ticker(pair)\n            if ticker['ask'] < ticker['last']:\n                ticker_rate = ticker['ask']\n            else:\n                balance = self.config['bid_strategy']['ask_last_balance']\n                ticker_rate = ticker['ask'] + balance * (ticker['last'] - ticker['ask'])\n            used_rate = ticker_rate\n\n        return used_rate\n```\n\nOriginal documentation:\n```python\nCalculates bid target between current ask price and last price\n        :param ticker: Ticker to use for getting Ask and Last Price\n        :return: float: Price\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculates bid target between current ask price and last price\n        :return: float: Price", "file": "freqtrade_freqtradebot.py", "function": "FreqtradeBot.get_target_bid", "project": "freqtrade", "change_stats": {"code_changes": 24, "doc_changes": 3, "total_changes": 27}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fit(self, X, Y):\n        \n        \n        super().fit(X, Y)\n        return self```\n\nUpdated code:\n```python\ndef fit(self, X, Y, **fit_params):\n        \n        \n        super().fit(X, Y, **fit_params)\n        return self\n```\n\nOriginal documentation:\n```python\nFit the model to data matrix X and targets Y.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The input data.\n        Y : array-like, shape (n_samples, n_classes)\n            The target values.\n\n        Returns\n        -------\n        self : object\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fit the model to data matrix X and targets Y.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The input data.\n        Y : array-like, shape (n_samples, n_classes)\n            The target values.\n\n        **fit_params : dict of string -> object\n            Parameters passed to the `fit` method at each step\n            of the regressor chain.\n\n        Returns\n        -------\n        self : object", "file": "sklearn_multioutput.py", "function": "RegressorChain.fit", "project": "scikit-learn", "change_stats": {"code_changes": 6, "doc_changes": 6, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, axis=None, use_rmin=True,\n                 _apply_theta_transforms=True):\n        \n        \n        \n        super().__init__()\n        self._axis = axis\n        self._use_rmin = use_rmin\n        self._apply_theta_transforms = _apply_theta_transforms```\n\nUpdated code:\n```python\ndef __init__(self, axis=None, use_rmin=True,\n                 *, apply_theta_transforms=True):\n        \n        \n        \n        super().__init__()\n        self._axis = axis\n        self._use_rmin = use_rmin\n        self._apply_theta_transforms = apply_theta_transforms\n        if apply_theta_transforms:\n            _apply_theta_transforms_warn()\n```\n\nOriginal documentation:\n```python\nParameters\n        ----------\n        axis : `~matplotlib.axis.Axis`, optional\n            Axis associated with this transform. This is used to get the\n            minimum radial limit.\n        use_rmin : `bool`, optional\n            If ``True`` add the minimum radial axis limit after\n            transforming from Cartesian coordinates. *axis* must also be\n            specified for this to take effect.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parameters\n        ----------\n        axis : `~matplotlib.axis.Axis`, optional\n            Axis associated with this transform. This is used to get the\n            minimum radial limit.\n        use_rmin : `bool`, optional\n            If ``True``, add the minimum radial axis limit after\n            transforming from Cartesian coordinates. *axis* must also be\n            specified for this to take effect.", "file": "lib_matplotlib_projections_polar.py", "function": "InvertedPolarTransform.__init__", "project": "matplotlib", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef project_state(self, nodes, at_end=True):\n        \n        \n        \n        if not isinstance(nodes[0], tuple):\n            nodes = [nodes]\n        plan = []\n        for node in nodes:\n            for migration in self.forwards_plan(node):\n                if migration not in plan:\n                    if not at_end and migration in nodes:\n                        continue\n                    plan.append(migration)\n        project_state = ProjectState()\n        for node in plan:\n            project_state = self.nodes[node].mutate_state(project_state)\n        return project_state```\n\nUpdated code:\n```python\ndef project_state(self, nodes=None, at_end=True):\n        \n        \n        \n        if nodes is None:\n            nodes = list(self.leaf_nodes())\n        if len(nodes) == 0:\n            return ProjectState()\n        if not isinstance(nodes[0], tuple):\n            nodes = [nodes]\n        plan = []\n        for node in nodes:\n            for migration in self.forwards_plan(node):\n                if migration not in plan:\n                    if not at_end and migration in nodes:\n                        continue\n                    plan.append(migration)\n        project_state = ProjectState()\n        for node in plan:\n            project_state = self.nodes[node].mutate_state(project_state)\n        return project_state\n```\n\nOriginal documentation:\n```python\nGiven a migration node or nodes, returns a complete ProjectState for it.\n        If at_end is False, returns the state before the migration has run.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Given a migration node or nodes, returns a complete ProjectState for it.\n        If at_end is False, returns the state before the migration has run.\n        If nodes is not provided, returns the overall most current project state.", "file": "django_db_migrations_graph.py", "function": "MigrationGraph.project_state", "project": "django", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef image_to_image(self, image_url, prompt=None, \n                       height=1024, width=1024, \n                       mask_image_url=None):\n        \n        \n        img = self._read_image_url(image_url)\n        mask_img = None if (mask_image_url is None) else self._read_image_url(mask_image_url)\n        answers = self.stability_api.generate(\n                        prompt=prompt,\n                        init_image=img,\n                        mask_image=mask_img,\n                        width=width,\n                        height=height\n                    )\n        \n        saved_images = self._process_artifacts(answers)\n\n        return saved_images```\n\nUpdated code:\n```python\ndef image_to_image(self, image_url, prompt=None,\n                       height=1024, width=1024,\n                       mask_image_url=None):\n        \n        \n        img = self._read_image_url(image_url)\n        mask_img = None if (mask_image_url is None) else self._read_image_url(mask_image_url)\n        if prompt is None:\n            prompt = \"\"\n        answers = self.stability_api.generate(prompt=prompt, init_image=img, mask_image=mask_img, width=width, height=height)\n\n        saved_images = self._process_artifacts(answers)\n\n        return saved_images\n```\n\nOriginal documentation:\n```python\nImage to Image inpainting + masking\n\n        Args:\n            image_url: The image url\n            prompt: The given text\n            height (int, optional): Height of the image. Defaults to 1024.\n            width (int, optional): Width of the image. Defaults to 1024.\n            mask_image_url (_type_, optional): The mask image url for masking. Defaults to None.\n\n        Returns:\n            The local saved paths of the generated images\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Image to Image inpainting + masking\n\n        Args:\n            image_url: The image url\n            prompt: The given text\n            height (int, optional): Height of the image. Defaults to 1024.\n            width (int, optional): Width of the image. Defaults to 1024.\n            mask_image_url (string, optional): The mask image url for masking. Defaults to None.\n\n        Returns:\n            The local saved paths of the generated images", "file": "mindsdb_integrations_handlers_stabilityai_handler_stabilityai.py", "function": "StabilityAPIClient.image_to_image", "project": "mindsdb", "change_stats": {"code_changes": 18, "doc_changes": 4, "total_changes": 22}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_query_constructor_chain(\n    llm: BaseLanguageModel,\n    document_contents: str,\n    attribute_info: List[AttributeInfo],\n    examples: Optional[List] = None,\n    allowed_comparators: Optional[Sequence[Comparator]] = None,\n    allowed_operators: Optional[Sequence[Operator]] = None,\n    enable_limit: bool = False,\n    **kwargs: Any,\n) -> LLMChain:\n    \n    \n    prompt = _get_prompt(\n        document_contents,\n        attribute_info,\n        examples=examples,\n        allowed_comparators=allowed_comparators,\n        allowed_operators=allowed_operators,\n        enable_limit=enable_limit,\n    )\n    return LLMChain(\n        llm=llm, prompt=prompt, output_parser=prompt.output_parser, **kwargs\n    )```\n\nUpdated code:\n```python\ndef load_query_constructor_chain(\n    llm: BaseLanguageModel,\n    document_contents: str,\n    attribute_info: Sequence[Union[AttributeInfo, dict]],\n    examples: Optional[List] = None,\n    allowed_comparators: Sequence[Comparator] = tuple(Comparator),\n    allowed_operators: Sequence[Operator] = tuple(Operator),\n    enable_limit: bool = False,\n    schema_prompt: Optional[BasePromptTemplate] = None,\n    **kwargs: Any,\n) -> LLMChain:\n    \n    \n    prompt = get_query_constructor_prompt(\n        document_contents,\n        attribute_info,\n        examples=examples,\n        allowed_comparators=allowed_comparators,\n        allowed_operators=allowed_operators,\n        enable_limit=enable_limit,\n        schema_prompt=schema_prompt,\n    )\n    allowed_attributes = []\n    for ainfo in attribute_info:\n        allowed_attributes.append(\n            ainfo.name if isinstance(ainfo, AttributeInfo) else ainfo[\"name\"]\n        )\n    output_parser = StructuredQueryOutputParser.from_components(\n        allowed_comparators=allowed_comparators,\n        allowed_operators=allowed_operators,\n        allowed_attributes=allowed_attributes,\n    )\n    # For backwards compatibility.\n    prompt.output_parser = output_parser\n    return LLMChain(llm=llm, prompt=prompt, output_parser=output_parser, **kwargs)\n```\n\nOriginal documentation:\n```python\nLoad a query constructor chain.\n\n    Args:\n        llm: BaseLanguageModel to use for the chain.\n        document_contents: The contents of the document to be queried.\n        attribute_info: A list of AttributeInfo objects describing\n            the attributes of the document.\n        examples: Optional list of examples to use for the chain.\n        allowed_comparators: An optional list of allowed comparators.\n        allowed_operators: An optional list of allowed operators.\n        enable_limit: Whether to enable the limit operator. Defaults to False.\n        **kwargs:\n\n    Returns:\n        A LLMChain that can be used to construct queries.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a query constructor chain.\n\n    Args:\n        llm: BaseLanguageModel to use for the chain.\n        document_contents: The contents of the document to be queried.\n        attribute_info: Sequence of attributes in the document.\n        examples: Optional list of examples to use for the chain.\n        allowed_comparators: Sequence of allowed comparators. Defaults to all\n            Comparators.\n        allowed_operators: Sequence of allowed operators. Defaults to all Operators.\n        enable_limit: Whether to enable the limit operator. Defaults to False.\n        schema_prompt: Prompt for describing query schema. Should have string input\n            variables allowed_comparators and allowed_operators.\n        **kwargs: Arbitrary named params to pass to LLMChain.\n\n    Returns:\n        A LLMChain that can be used to construct queries.", "file": "libs_langchain_langchain_chains_query_constructor_base.py", "function": "load_query_constructor_chain", "project": "langchain", "change_stats": {"code_changes": 26, "doc_changes": 14, "total_changes": 40}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_compressed_ids(labels, sizes) -> tuple[np.ndarray, np.ndarray]:\n    \n    \n    \n    ids = get_group_index(labels, sizes, sort=True, xnull=False)\n    return compress_group_index(ids, sort=True)```\n\nUpdated code:\n```python\ndef get_compressed_ids(labels, sizes: Shape) -> tuple[np.ndarray, np.ndarray]:\n    \n    \n    \n    ids = get_group_index(labels, sizes, sort=True, xnull=False)\n    return compress_group_index(ids, sort=True)\n```\n\nOriginal documentation:\n```python\nGroup_index is offsets into cartesian product of all possible labels. This\n    space can be huge, so this function compresses it, by computing offsets\n    (comp_ids) into the list of unique labels (obs_group_ids).\n\n    Parameters\n    ----------\n    labels : list of label arrays\n    sizes : list of size of the levels\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n        comp_ids\n    np.ndarray[np.int64]\n        obs_group_ids\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Group_index is offsets into cartesian product of all possible labels. This\n    space can be huge, so this function compresses it, by computing offsets\n    (comp_ids) into the list of unique labels (obs_group_ids).\n\n    Parameters\n    ----------\n    labels : list of label arrays\n    sizes : tuple[int] of size of the levels\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n        comp_ids\n    np.ndarray[np.int64]\n        obs_group_ids", "file": "pandas_core_sorting.py", "function": "get_compressed_ids", "project": "pandas", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef kill_locust_greenlets(self, greenlets):\n        \n        \n        \n        if self.options.stop_timeout:\n            dying = Group()\n            for g in greenlets:\n                locust = g.args[0]\n                if locust._state == LOCUST_STATE_WAITING:\n                    self.locusts.killone(g)\n                else:\n                    locust._state = LOCUST_STATE_STOPPING\n                    dying.add(g)\n            if not dying.join(timeout=self.options.stop_timeout):\n                logger.info(\"Not all locusts finished their tasks & terminated in %s seconds. Killing them...\" % self.options.stop_timeout)\n            dying.kill(block=True)\n        else:\n            for g in greenlets:\n                self.locusts.killone(g)```\n\nUpdated code:\n```python\ndef kill_locust_greenlets(self, greenlets):\n        \n        \n        \n        if self.environment.stop_timeout:\n            dying = Group()\n            for g in greenlets:\n                locust = g.args[0]\n                if locust._state == LOCUST_STATE_WAITING:\n                    self.locusts.killone(g)\n                else:\n                    locust._state = LOCUST_STATE_STOPPING\n                    dying.add(g)\n            if not dying.join(timeout=self.environment.stop_timeout):\n                logger.info(\"Not all locusts finished their tasks & terminated in %s seconds. Killing them...\" % self.environment.stop_timeout)\n            dying.kill(block=True)\n        else:\n            for g in greenlets:\n                self.locusts.killone(g)\n```\n\nOriginal documentation:\n```python\nKill running locust greenlets. If options.stop_timeout is set, we try to stop the \n        Locust users gracefully\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Kill running locust greenlets. If environment.stop_timeout is set, we try to stop the \n        Locust users gracefully", "file": "locust_runners.py", "function": "LocustRunner.kill_locust_greenlets", "project": "locust", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef assert_allclose(x, y, rtol=1e-7, atol=0, equal_nan=True, err_msg='', verbose=True):\n    \n    \n    x = _preprocess_input(x)\n    y = _preprocess_input(y)\n\n    numpy.testing.assert_allclose(x, y, rtol=rtol, atol=atol, equal_nan=equal_nan, err_msg=err_msg, verbose=verbose)```\n\nUpdated code:\n```python\ndef assert_allclose(\n        x, y, rtol=1e-7, atol=0, equal_nan=True, err_msg='', verbose=True):\n    \n    \n    x = _preprocess_input(x)\n    y = _preprocess_input(y)\n\n    numpy.testing.assert_allclose(\n        x, y, rtol=rtol, atol=atol, equal_nan=equal_nan, err_msg=err_msg,\n        verbose=verbose)\n```\n\nOriginal documentation:\n```python\nRaises an AssertionError if two array_like objects are not equal up to a tolerance.\n\n    Args:\n         x(numpy.ndarray or chainerx.ndarray): The actual object to check.\n         y(numpy.ndarray or chainerx.ndarray): The desired, expected object.\n         rtol(float): Relative tolerance.\n         atol(float): Absolute tolerance.\n         equal_nan(bool): Allow NaN values if True. Otherwise, fail the assertion if any NaN is found.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values\n             are appended to the error message.\n    .. seealso:: :func:`numpy.testing.assert_allclose`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Raises an AssertionError if two array_like objects are not equal up to a\n    tolerance.\n\n    Args:\n         x(numpy.ndarray or chainerx.ndarray): The actual object to check.\n         y(numpy.ndarray or chainerx.ndarray): The desired, expected object.\n         rtol(float): Relative tolerance.\n         atol(float): Absolute tolerance.\n         equal_nan(bool): Allow NaN values if True. Otherwise, fail the\n             assertion if any NaN is found.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values\n             are appended to the error message.\n    .. seealso:: :func:`numpy.testing.assert_allclose`", "file": "chainerx_testing_array.py", "function": "assert_allclose", "project": "chainer", "change_stats": {"code_changes": 9, "doc_changes": 8, "total_changes": 17}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef gotonext(self):\n        \n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else:\n                break```\n\nUpdated code:\n```python\ndef gotonext(self):\n        \n        wslist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                if self.field[self.pos] not in '\\n\\r':\n                    wslist.append(self.field[self.pos])\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else:\n                break\n        return EMPTYSTRING.join(wslist)\n```\n\nOriginal documentation:\n```python\nParse up to the start of the next address.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Skip white space and extract comments.", "file": "Lib_email__parseaddr.py", "function": "AddrlistClass.gotonext", "project": "cpython", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_batchsize(self, plugin_type, batchsize):\n         \n        \n        logger.debug(\"Overriding batchsize for plugin_type: %s to: %s\", plugin_type, batchsize)\n        plugin = getattr(self, f\"_{plugin_type}\")\n        plugin.batchsize = batchsize```\n\nUpdated code:\n```python\ndef set_batchsize(self,\n                      plugin_type: Literal[\"align\", \"detect\"],\n                      batchsize: int) -> None:\n         \n        \n        logger.debug(\"Overriding batchsize for plugin_type: %s to: %s\", plugin_type, batchsize)\n        plugin = getattr(self, f\"_{plugin_type}\")\n        plugin.batchsize = batchsize\n```\n\nOriginal documentation:\n```python\nSet the batch size of a given :attr:`plugin_type` to the given :attr:`batchsize`.\n\n        This should be set prior to :func:`launch` if the batch size is to be manually overridden\n\n        Parameters\n        ----------\n        plugin_type: {'aligner', 'detector'}\n            The plugin_type to be overridden\n        batchsize: int\n            The batch size to use for this plugin type\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the batch size of a given :attr:`plugin_type` to the given :attr:`batchsize`.\n\n        This should be set prior to :func:`launch` if the batch size is to be manually overridden\n\n        Parameters\n        ----------\n        plugin_type: {'align', 'detect'}\n            The plugin_type to be overridden\n        batchsize: int\n            The batch size to use for this plugin type", "file": "plugins_extract_pipeline.py", "function": "Extractor.set_batchsize", "project": "faceswap", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_zbound(self, lower=None, upper=None):\n        \n        \n        \n        if upper is None and np.iterable(lower):\n            lower, upper = lower\n\n        old_lower, old_upper = self.get_zbound()\n        if lower is None:\n            lower = old_lower\n        if upper is None:\n            upper = old_upper\n\n        self.set_zlim(sorted((lower, upper), reverse=self.zaxis_inverted()),\n                      auto=None)```\n\nUpdated code:\n```python\ndef set_zbound(self, lower=None, upper=None):\n        \n        \n        \n        if upper is None and np.iterable(lower):\n            lower, upper = lower\n        old_lower, old_upper = self.get_zbound()\n        if lower is None:\n            lower = old_lower\n        if upper is None:\n            upper = old_upper\n\n        if self.zaxis_inverted():\n            if lower < upper:\n                self.set_zlim(upper, lower, auto=None)\n            else:\n                self.set_zlim(lower, upper, auto=None)\n        else:\n            if lower < upper:\n                self.set_zlim(lower, upper, auto=None)\n            else:\n                self.set_zlim(upper, lower, auto=None)\n```\n\nOriginal documentation:\n```python\nSet the lower and upper numerical bounds of the z-axis.\n\n        This method will honor axes inversion regardless of parameter order.\n        It will not change the autoscaling setting (`.get_autoscalez_on()`).\n\n        .. versionadded :: 1.1.0\n            This function was added, but not tested. Please report any bugs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the lower and upper numerical bounds of the z-axis.\n        This method will honor axes inversion regardless of parameter order.\n        It will not change the :attr:`_autoscaleZon` attribute.\n\n        .. versionadded :: 1.1.0", "file": "lib_mpl_toolkits_mplot3d_axes3d.py", "function": "Axes3D.set_zbound", "project": "matplotlib", "change_stats": {"code_changes": 15, "doc_changes": 6, "total_changes": 21}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_leverage_tiers(self, symbols: Strings = None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = None\n        if symbols is not None:\n            market = self.market(symbols[0])\n            if market['spot']:\n                raise NotSupported(self.id + ' fetchLeverageTiers() is not supported for spot market')\n        subType = None\n        subType, params = self.handle_sub_type_and_params('fetchTickers', market, params, 'linear')\n        request = {\n            'category': subType,\n        }\n        response = self.publicGetV5MarketRiskLimit(self.extend(request, params))\n        result = self.safe_dict(response, 'result', {})\n        data = self.safe_list(result, 'list', [])\n        symbols = self.market_symbols(symbols)\n        return self.parse_leverage_tiers(data, symbols, 'symbol')```\n\nUpdated code:\n```python\ndef fetch_leverage_tiers(self, symbols: Strings = None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = None\n        symbol = None\n        if symbols is not None:\n            market = self.market(symbols[0])\n            if market['spot']:\n                raise NotSupported(self.id + ' fetchLeverageTiers() is not supported for spot market')\n            symbol = market['symbol']\n        data = self.get_leverage_tiers_paginated(symbol, self.extend({'paginate': True, 'paginationCalls': 20}, params))\n        symbols = self.market_symbols(symbols)\n        return self.parse_leverage_tiers(data, symbols, 'symbol')\n```\n\nOriginal documentation:\n```python\n:see: https://bybit-exchange.github.io/docs/v5/market/risk-limit\n        retrieve information on the maximum leverage, for different trade sizes\n        :param str[] [symbols]: a list of unified market symbols\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.subType]: market subType, ['linear', 'inverse'], default is 'linear'\n        :returns dict: a dictionary of `leverage tiers structures <https://docs.ccxt.com/#/?id=leverage-tiers-structure>`, indexed by market symbols\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": ":see: https://bybit-exchange.github.io/docs/v5/market/risk-limit\n        retrieve information on the maximum leverage, for different trade sizes\n        :param str[] [symbols]: a list of unified market symbols\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [params.subType]: market subType, ['linear', 'inverse'], default is 'linear'\n        :param boolean [params.paginate]: default False, when True will automatically paginate by calling self endpoint multiple times. See in the docs all the [available parameters](https://github.com/ccxt/ccxt/wiki/Manual#pagination-params)\n        :returns dict: a dictionary of `leverage tiers structures <https://docs.ccxt.com/#/?id=leverage-tiers-structure>`, indexed by market symbols", "file": "python_ccxt_bybit.py", "function": "bybit.fetch_leverage_tiers", "project": "ccxt", "change_stats": {"code_changes": 13, "doc_changes": 3, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef magic_loadpy(self, arg_s):\n        \n        \n        arg_s = unquote_filename(arg_s)\n        remote_url = arg_s.startswith(('http://', 'https://'))\n        local_url = not remote_url\n        if local_url and not arg_s.endswith('.py'):\n            # Local files must be .py; for remote URLs it's possible that the\n            # fetch URL doesn't have a .py in it (many servers have an opaque\n            # URL, such as scipy-central.org).\n            raise ValueError('%%loadpy only works with .py files: %s' % arg_s)\n        \n        # openpy takes care of finding the source encoding (per PEP 263)\n        if remote_url:\n            contents = openpy.read_py_url(arg_s, skip_encoding_cookie=True)\n        else:\n            contents = openpy.read_py_file(arg_s, skip_encoding_cookie=True)\n        \n        self.set_next_input(contents)```\n\nUpdated code:\n```python\ndef magic_loadpy(self, arg_s):\n        \n        \n        self.magic_load(arg_s)\n```\n\nOriginal documentation:\n```python\nLoad a .py python script into the GUI console.\n\n        This magic command can either take a local filename or a url::\n\n        %loadpy myscript.py\n        %loadpy http://www.example.com/myscript.py\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Alias of `%load`\n        \n        `%loadpy` has gained some flexibility and droped the requirement of a `.py`\n        extension. So it has been renamed simply into %load. You can look at\n        `%load`'s docstring for more info.", "file": "IPython_core_magic.py", "function": "Magic.magic_loadpy", "project": "ipython", "change_stats": {"code_changes": 19, "doc_changes": 13, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef futures_zh_realtime(symbol: str = \"\u767d\u7cd6\") -> pd.DataFrame:\n    \n    \n    \n    _futures_symbol_mark_df = futures_symbol_mark()\n    symbol_mark_map = dict(\n        zip(_futures_symbol_mark_df[\"symbol\"], _futures_symbol_mark_df[\"mark\"])\n    )\n    url = \"http://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/Market_Center.getHQFuturesData\"\n    params = {\n        \"page\": \"1\",\n        \"sort\": \"position\",\n        \"asc\": \"0\",\n        \"node\": symbol_mark_map[symbol],\n        \"base\": \"futures\",\n    }\n    r = requests.get(url, params=params)\n    data_json = r.json()\n    temp_df = pd.DataFrame(data_json)\n\n    temp_df[\"trade\"] = pd.to_numeric(temp_df[\"trade\"], errors=\"coerce\")\n    temp_df[\"settlement\"] = pd.to_numeric(temp_df[\"settlement\"], errors=\"coerce\")\n    temp_df[\"presettlement\"] = pd.to_numeric(temp_df[\"presettlement\"], errors=\"coerce\")\n    temp_df[\"open\"] = pd.to_numeric(temp_df[\"open\"], errors=\"coerce\")\n    temp_df[\"high\"] = pd.to_numeric(temp_df[\"high\"], errors=\"coerce\")\n    temp_df[\"low\"] = pd.to_numeric(temp_df[\"low\"], errors=\"coerce\")\n    temp_df[\"close\"] = pd.to_numeric(temp_df[\"close\"], errors=\"coerce\")\n    temp_df[\"bidprice1\"] = pd.to_numeric(temp_df[\"bidprice1\"], errors=\"coerce\")\n    temp_df[\"askprice1\"] = pd.to_numeric(temp_df[\"askprice1\"], errors=\"coerce\")\n    temp_df[\"bidvol1\"] = pd.to_numeric(temp_df[\"bidvol1\"], errors=\"coerce\")\n    temp_df[\"askvol1\"] = pd.to_numeric(temp_df[\"askvol1\"], errors=\"coerce\")\n    temp_df[\"volume\"] = pd.to_numeric(temp_df[\"volume\"], errors=\"coerce\")\n    temp_df[\"position\"] = pd.to_numeric(temp_df[\"position\"], errors=\"coerce\")\n    temp_df[\"preclose\"] = pd.to_numeric(temp_df[\"preclose\"], errors=\"coerce\")\n    temp_df[\"changepercent\"] = pd.to_numeric(temp_df[\"changepercent\"], errors=\"coerce\")\n    temp_df[\"bid\"] = pd.to_numeric(temp_df[\"bid\"], errors=\"coerce\")\n    temp_df[\"ask\"] = pd.to_numeric(temp_df[\"ask\"], errors=\"coerce\")\n    temp_df[\"prevsettlement\"] = pd.to_numeric(temp_df[\"prevsettlement\"], errors=\"coerce\")\n    return temp_df```\n\nUpdated code:\n```python\ndef futures_zh_realtime(symbol: str = \"\u767d\u7cd6\") -> pd.DataFrame:\n    \n    \n    \n    _futures_symbol_mark_df = futures_symbol_mark()\n    symbol_mark_map = dict(\n        zip(_futures_symbol_mark_df[\"symbol\"], _futures_symbol_mark_df[\"mark\"])\n    )\n    url = \"https://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/Market_Center.getHQFuturesData\"\n    params = {\n        \"page\": \"1\",\n        \"sort\": \"position\",\n        \"asc\": \"0\",\n        \"node\": symbol_mark_map[symbol],\n        \"base\": \"futures\",\n    }\n    r = requests.get(url, params=params)\n    data_json = r.json()\n    temp_df = pd.DataFrame(data_json)\n\n    temp_df[\"trade\"] = pd.to_numeric(temp_df[\"trade\"], errors=\"coerce\")\n    temp_df[\"settlement\"] = pd.to_numeric(temp_df[\"settlement\"], errors=\"coerce\")\n    temp_df[\"presettlement\"] = pd.to_numeric(temp_df[\"presettlement\"], errors=\"coerce\")\n    temp_df[\"open\"] = pd.to_numeric(temp_df[\"open\"], errors=\"coerce\")\n    temp_df[\"high\"] = pd.to_numeric(temp_df[\"high\"], errors=\"coerce\")\n    temp_df[\"low\"] = pd.to_numeric(temp_df[\"low\"], errors=\"coerce\")\n    temp_df[\"close\"] = pd.to_numeric(temp_df[\"close\"], errors=\"coerce\")\n    temp_df[\"bidprice1\"] = pd.to_numeric(temp_df[\"bidprice1\"], errors=\"coerce\")\n    temp_df[\"askprice1\"] = pd.to_numeric(temp_df[\"askprice1\"], errors=\"coerce\")\n    temp_df[\"bidvol1\"] = pd.to_numeric(temp_df[\"bidvol1\"], errors=\"coerce\")\n    temp_df[\"askvol1\"] = pd.to_numeric(temp_df[\"askvol1\"], errors=\"coerce\")\n    temp_df[\"volume\"] = pd.to_numeric(temp_df[\"volume\"], errors=\"coerce\")\n    temp_df[\"position\"] = pd.to_numeric(temp_df[\"position\"], errors=\"coerce\")\n    temp_df[\"preclose\"] = pd.to_numeric(temp_df[\"preclose\"], errors=\"coerce\")\n    temp_df[\"changepercent\"] = pd.to_numeric(temp_df[\"changepercent\"], errors=\"coerce\")\n    temp_df[\"bid\"] = pd.to_numeric(temp_df[\"bid\"], errors=\"coerce\")\n    temp_df[\"ask\"] = pd.to_numeric(temp_df[\"ask\"], errors=\"coerce\")\n    temp_df[\"prevsettlement\"] = pd.to_numeric(\n        temp_df[\"prevsettlement\"], errors=\"coerce\"\n    )\n    return temp_df\n```\n\nOriginal documentation:\n```python\n\u671f\u8d27\u54c1\u79cd\u5f53\u524d\u65f6\u523b\u6240\u6709\u53ef\u4ea4\u6613\u7684\u5408\u7ea6\u5b9e\u65f6\u6570\u636e\n    http://vip.stock.finance.sina.com.cn/quotes_service/view/qihuohangqing.html#titlePos_1\n    :param symbol: \u54c1\u79cd\u540d\u79f0\uff1b\u53ef\u4ee5\u901a\u8fc7 ak.futures_symbol_mark() \u83b7\u53d6\u6240\u6709\u54c1\u79cd\u547d\u540d\u8868\n    :type symbol: str\n    :return: \u671f\u8d27\u54c1\u79cd\u5f53\u524d\u65f6\u523b\u6240\u6709\u53ef\u4ea4\u6613\u7684\u5408\u7ea6\u5b9e\u65f6\u6570\u636e\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u671f\u8d27\u54c1\u79cd\u5f53\u524d\u65f6\u523b\u6240\u6709\u53ef\u4ea4\u6613\u7684\u5408\u7ea6\u5b9e\u65f6\u6570\u636e\n    https://vip.stock.finance.sina.com.cn/quotes_service/view/qihuohangqing.html#titlePos_1\n    :param symbol: \u54c1\u79cd\u540d\u79f0\uff1b\u53ef\u4ee5\u901a\u8fc7 ak.futures_symbol_mark() \u83b7\u53d6\u6240\u6709\u54c1\u79cd\u547d\u540d\u8868\n    :type symbol: str\n    :return: \u671f\u8d27\u54c1\u79cd\u5f53\u524d\u65f6\u523b\u6240\u6709\u53ef\u4ea4\u6613\u7684\u5408\u7ea6\u5b9e\u65f6\u6570\u636e\n    :rtype: pandas.DataFrame", "file": "akshare_futures_futures_zh_sina.py", "function": "futures_zh_realtime", "project": "akshare", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef collect(self, key: str) -> Dict[NodeOrEdgeType, Any]:\n        \n        \n        mapping = {}\n        for subtype, store in chain(self._node_store_dict.items(),\n                                    self._edge_store_dict.items()):\n            if hasattr(store, key):\n                mapping[subtype] = getattr(store, key)\n        return mapping```\n\nUpdated code:\n```python\ndef collect(\n        self,\n        key: str,\n        allow_empty: bool = False,\n    ) -> Dict[NodeOrEdgeType, Any]:\n        \n        \n        mapping = {}\n        for subtype, store in chain(self._node_store_dict.items(),\n                                    self._edge_store_dict.items()):\n            if hasattr(store, key):\n                mapping[subtype] = getattr(store, key)\n        if not allow_empty and len(mapping) == 0:\n            raise KeyError(f\"Tried to collect '{key}' but did not find any \"\n                           f\"occurrences of it in any node and/or edge type\")\n        return mapping\n```\n\nOriginal documentation:\n```python\nr\"\"\"Collects the attribute :attr:`key` from all node and edge types.\n\n        .. code-block:: python\n\n            data = HeteroData()\n            data['paper'].x = ...\n            data['author'].x = ...\n\n            print(data.collect('x'))\n            >>> { 'paper': ..., 'author': ...}\n\n        .. note::\n\n            This is equivalent to writing :obj:`data.x_dict`.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"Collects the attribute :attr:`key` from all node and edge types.\n\n        .. code-block:: python\n\n            data = HeteroData()\n            data['paper'].x = ...\n            data['author'].x = ...\n\n            print(data.collect('x'))\n            >>> { 'paper': ..., 'author': ...}\n\n        .. note::\n\n            This is equivalent to writing :obj:`data.x_dict`.\n\n        Args:\n            key (str): The attribute to collect from all node and ege types.\n            allow_empty (bool, optional): If set to :obj:`True`, will not raise\n                an error in case the attribute does not exit in any node or\n                edge type. (default: :obj:`False`)", "file": "torch_geometric_data_hetero_data.py", "function": "HeteroData.collect", "project": "pytorch_geometric", "change_stats": {"code_changes": 11, "doc_changes": 8, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _update_legacy_config(self) -> bool:\n         \n        \n        logger.debug(\"Checking for legacy state file update\")\n        priors = [\"dssim_loss\", \"mask_type\", \"mask_type\", \"l2_reg_term\"]\n        new_items = [\"loss_function\", \"learn_mask\", \"mask_type\", \"loss_function_2\"]\n        updated = False\n        for old, new in zip(priors, new_items):\n            if old not in self._config:\n                logger.debug(\"Legacy item '%s' not in config. Skipping update\", old)\n                continue\n\n            # dssim_loss > loss_function\n            if old == \"dssim_loss\":\n                self._config[new] = \"ssim\" if self._config[old] else \"mae\"\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy dssim format. New config loss \"\n                            \"function: '%s'\", self._config[new])\n                continue\n\n            # Add learn mask option and set to True if model has \"penalized_mask_loss\" specified\n            if old == \"mask_type\" and new == \"learn_mask\" and new not in self._config:\n                self._config[new] = self._config[\"mask_type\"] is not None\n                updated = True\n                logger.info(\"Added new 'learn_mask' config item for this model. Value set to: %s\",\n                            self._config[new])\n                continue\n\n            # Replace removed masks with most similar equivalent\n            if old == \"mask_type\" and new == \"mask_type\" and self._config[old] in (\"facehull\",\n                                                                                   \"dfl_full\"):\n                old_mask = self._config[old]\n                self._config[new] = \"components\"\n                updated = True\n                logger.info(\"Updated 'mask_type' from '%s' to '%s' for this model\",\n                            old_mask, self._config[new])\n\n            # Replace l2_reg_term with the correct loss_2_function and update the value of\n            # loss_2_weight\n            if old == \"l2_reg_term\":\n                self._config[new] = \"mse\"\n                self._config[\"loss_weight_2\"] = self._config[old]\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy 'l2_reg_term' to 'loss_function_2'\")\n\n        logger.debug(\"State file updated for legacy config: %s\", updated)\n        return updated```\n\nUpdated code:\n```python\ndef _update_legacy_config(self) -> bool:\n         \n        \n        logger.debug(\"Checking for legacy state file update\")\n        priors = [\"dssim_loss\", \"mask_type\", \"mask_type\", \"l2_reg_term\", \"clipnorm\"]\n        new_items = [\"loss_function\", \"learn_mask\", \"mask_type\", \"loss_function_2\",\n                     \"autoclip\"]\n        updated = False\n        for old, new in zip(priors, new_items):\n            if old not in self._config:\n                logger.debug(\"Legacy item '%s' not in config. Skipping update\", old)\n                continue\n\n            # dssim_loss > loss_function\n            if old == \"dssim_loss\":\n                self._config[new] = \"ssim\" if self._config[old] else \"mae\"\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy dssim format. New config loss \"\n                            \"function: '%s'\", self._config[new])\n                continue\n\n            # Add learn mask option and set to True if model has \"penalized_mask_loss\" specified\n            if old == \"mask_type\" and new == \"learn_mask\" and new not in self._config:\n                self._config[new] = self._config[\"mask_type\"] is not None\n                updated = True\n                logger.info(\"Added new 'learn_mask' config item for this model. Value set to: %s\",\n                            self._config[new])\n                continue\n\n            # Replace removed masks with most similar equivalent\n            if old == \"mask_type\" and new == \"mask_type\" and self._config[old] in (\"facehull\",\n                                                                                   \"dfl_full\"):\n                old_mask = self._config[old]\n                self._config[new] = \"components\"\n                updated = True\n                logger.info(\"Updated 'mask_type' from '%s' to '%s' for this model\",\n                            old_mask, self._config[new])\n\n            # Replace l2_reg_term with the correct loss_2_function and update the value of\n            # loss_2_weight\n            if old == \"l2_reg_term\":\n                self._config[new] = \"mse\"\n                self._config[\"loss_weight_2\"] = self._config[old]\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy 'l2_reg_term' to 'loss_function_2'\")\n\n            # Replace clipnorm with correct gradient clipping type and value\n            if old == \"clipnorm\":\n                self._config[new] = self._config[old]\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy '%s' to '%s'\", old, new)\n\n        logger.debug(\"State file updated for legacy config: %s\", updated)\n        return updated\n```\n\nOriginal documentation:\n```python\nLegacy updates for new config additions.\n\n        When new config items are added to the Faceswap code, existing model state files need to be\n        updated to handle these new items.\n\n        Current existing legacy update items:\n\n            * loss - If old `dssim_loss` is ``true`` set new `loss_function` to `ssim` otherwise\n            set it to `mae`. Remove old `dssim_loss` item\n\n            * l2_reg_term - If this exists, set loss_function_2 to ``mse`` and loss_weight_2 to\n            the value held in the old ``l2_reg_term`` item\n\n            * masks - If `learn_mask` does not exist then it is set to ``True`` if `mask_type` is\n            not ``None`` otherwise it is set to ``False``.\n\n            * masks type - Replace removed masks 'dfl_full' and 'facehull' with `components` mask\n\n        Returns\n        -------\n        bool\n            ``True`` if legacy items exist and state file has been updated, otherwise ``False``\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Legacy updates for new config additions.\n\n        When new config items are added to the Faceswap code, existing model state files need to be\n        updated to handle these new items.\n\n        Current existing legacy update items:\n\n            * loss - If old `dssim_loss` is ``true`` set new `loss_function` to `ssim` otherwise\n            set it to `mae`. Remove old `dssim_loss` item\n\n            * l2_reg_term - If this exists, set loss_function_2 to ``mse`` and loss_weight_2 to\n            the value held in the old ``l2_reg_term`` item\n\n            * masks - If `learn_mask` does not exist then it is set to ``True`` if `mask_type` is\n            not ``None`` otherwise it is set to ``False``.\n\n            * masks type - Replace removed masks 'dfl_full' and 'facehull' with `components` mask\n\n            * clipnorm - Only existed in 2 models (DFL-SAE + Unbalanced). Replaced with global\n            option autoclip\n\n        Returns\n        -------\n        bool\n            ``True`` if legacy items exist and state file has been updated, otherwise ``False``", "file": "plugins_train_model__base_model.py", "function": "State._update_legacy_config", "project": "faceswap", "change_stats": {"code_changes": 14, "doc_changes": 5, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef md_expand_links(text, cwd_rel_path, repo_url=\"\"):\n    \n    \n\n    \n    link_text, link, anchor = md_link_break_up(text)\n\n    #print(link_text, link, anchor)\n\n    # skip:\n    # - empty links (i.e. just local anchor to the same doc)\n    # - skip explicit .md links\n    # - external links like https://...\n    if len(link) == 0  or link.endswith(\".md\") or re.search(r'^\\w+://', link):\n        return text\n\n    link = Path(link)\n    try_link = link / \"README.md\"\n\n    full_path = cwd_rel_path / try_link\n    if full_path.exists():\n        link = str(try_link)\n    else:\n        link = str(link)\n\n        # leave the images local for pdf rendering, but for the rest of the file (scripts,\n        # reports, etc.)\n        # prepend the repo base url, while removing ./ relative prefix if any\n        if not re.search(re_link_images, link):\n            link = resolve_rel_link(link, cwd_rel_path)\n            link = repo_url + \"/\" + link\n\n    return md_link_build(link_text, link, anchor)```\n\nUpdated code:\n```python\ndef md_expand_links(text, cwd_rel_path, repo_url=\"\"):\n    \n    \n\n    \n    link_text, link, anchor = md_link_break_up(text)\n\n    #print(link_text, link, anchor)\n\n    # skip:\n    # - empty links (i.e. just local anchor to the same doc)\n    # - skip explicit .md links\n    # - external links like https://...\n    if len(link) == 0 or link.endswith(\".md\") or re.search(r'^\\w+://', link):\n        return text\n\n    link = Path(link)\n    try_link = link / \"README.md\"\n\n    full_path = cwd_rel_path / try_link\n    if full_path.exists():\n        link = str(try_link)\n    else:\n        link = str(link)\n\n        # leave the images local for pdf rendering, but for the rest of the file (scripts,\n        # reports, etc.)\n        # prepend the repo base url, while removing ./ relative prefix if any\n        if not re.search(re_link_images, link):\n            link = resolve_rel_link(link, cwd_rel_path)\n            link = repo_url + \"/\" + link\n\n    return md_link_build(link_text, link, anchor)\n```\n\nOriginal documentation:\n```python\nPerform link rewrites as following:\n    - if the link ends in .md or contains protocol :// return unmodified\n    - convert relative link shortcuts into full links, e.g. chapter/ => chapter/README.md\n    - if the link is not for .md or images, it's not going to be in the pdf, so resolve it and point\n      to its url at the the repo\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Perform link rewrites as following:\n    - return unmodified if the link:\n       * is empty (same doc internal anchor)\n       * ends in .md (well defined)\n       * is remote - i.e. contains protocol :// return unmodified\n    - convert relative link shortcuts into full links, e.g. s#chapter/?#chapter/README.md#\n    - if the local link is not for .md or images, it's not going to be in the pdf, so resolve it and point\n      to its url at the the repo", "file": "build_mdbook_utils_github_md_utils.py", "function": "md_expand_links", "project": "ml-engineering", "change_stats": {"code_changes": 4, "doc_changes": 11, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef status(self):\n        \n\n        \n        return self.backend.get_status(self.task_id)```\n\nUpdated code:\n```python\ndef status(self):\n        \n        return self.state\n```\n\nOriginal documentation:\n```python\nThe current status of the task.\n\n        Can be one of the following:\n\n            *PENDING*\n\n                The task is waiting for execution.\n\n            *STARTED*\n\n                The task has been started.\n\n            *RETRY*\n\n                The task is to be retried, possibly because of failure.\n\n            *FAILURE*\n\n                The task raised an exception, or has been retried more times\n                than its limit. The :attr:`result` attribute contains the\n                exception raised.\n\n            *SUCCESS*\n\n                The task executed successfully. The :attr:`result` attribute\n                contains the resulting value.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Deprecated alias of :attr:`state`.", "file": "celery_result.py", "function": "BaseAsyncResult.status", "project": "celery", "change_stats": {"code_changes": 6, "doc_changes": 29, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_box_model(\n        self, container_size: Size, parent_size: Size\n    ) -> tuple[Size, Spacing]:\n        \n        \n        width, height = container_size\n        has_rule = self.has_rule\n        if styles.width:\n            width = styles.width.resolve_dimension(container_size, parent_size)\n\n        if styles.min_width:\n            min_width = styles.min_width.resolve_dimension(container_size, parent_size)\n            width = max(width, min_width)\n\n        if styles.max_width:\n            max_width = styles.max_width.resolve_dimension(container_size, parent_size)\n            width = min(width, max_width)\n\n        if styles.height:\n            height = styles.height.resolve_dimension(container_size, parent_size)\n\n        if styles.min_height:\n            min_height = styles.min_height.resolve_dimension(\n                container_size, parent_size\n            )\n            height = max(height, min_height)\n\n        if styles.max_height:\n            max_height = styles.max_height.resolve_dimension(\n                container_size, parent_size\n            )\n            height = min(width, max_height)\n\n        # TODO: box sizing\n\n        size = Size(width, height)\n\n        spacing = Spacing(0, 0, 0, 0)\n        if has_rule(\"padding\"):\n            spacing += styles.padding\n        if has_rule(\"border\"):\n            spacing += styles.border.spacing\n        if has_rule(\"margin\"):\n            spacing += styles.margin\n\n        return size, spacing```\n\nUpdated code:\n```python\ndef get_box_model(\n        self, container_size: Size, parent_size: Size\n    ) -> tuple[Size, Spacing]:\n        \n        \n        has_rule = self.has_rule\n        width, height = container_size\n\n        if has_rule(\"width\"):\n            width = self.width.resolve_dimension(container_size, parent_size)\n        else:\n            width = max(0, width - self.margin.width)\n\n        if self.min_width:\n            min_width = self.min_width.resolve_dimension(container_size, parent_size)\n            width = max(width, min_width)\n\n        if self.max_width:\n            max_width = self.max_width.resolve_dimension(container_size, parent_size)\n            width = min(width, max_width)\n\n        if has_rule(\"height\"):\n            height = self.height.resolve_dimension(container_size, parent_size)\n        else:\n            height = max(0, height - self.margin.height)\n\n        if self.min_height:\n            min_height = self.min_height.resolve_dimension(container_size, parent_size)\n            height = max(height, min_height)\n\n        if self.max_height:\n            max_height = self.max_height.resolve_dimension(container_size, parent_size)\n            height = min(width, max_height)\n\n        # TODO: box sizing\n\n        size = Size(width, height)\n        margin = Spacing(0, 0, 0, 0)\n\n        if self.box_sizing == \"content-box\":\n\n            if has_rule(\"padding\"):\n                size += self.padding\n            if has_rule(\"border\"):\n                size += self.border.spacing.totals\n            if has_rule(\"margin\"):\n                margin = self.margin\n\n        else:  # border-box\n            if has_rule(\"padding\"):\n                size -= self.padding\n            if has_rule(\"border\"):\n                size -= self.border.spacing.totals\n            if has_rule(\"margin\"):\n                margin = self.margin\n\n        return size, margin\n```\n\nOriginal documentation:\n```python\nResolve the box model for this Styles.\n\n        Args:\n            parent_size (Size): The size of the widget container.\n            terminal_size (Size): The size of the terminal.\n\n        Returns:\n            tuple[Size, Spacing]: A tuple with the size of the renderable area, and the space around it.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Resolve the box model for this Styles.\n\n        Args:\n            parent_size (Size): The size of the widget container.\n            terminal_size (Size): The size of the terminal.\n\n        Returns:\n            tuple[Size, Spacing]: A tuple with the size of the content area and margin.", "file": "src_textual_css_styles.py", "function": "StylesBase.get_box_model", "project": "textual", "change_stats": {"code_changes": 63, "doc_changes": 4, "total_changes": 67}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_label_coords(self, distances, XX, YY, ysize, lw):\n         \n        \n\n        hysize = int(ysize/2)\n        adist = np.argsort(distances)\n\n        for ind in adist:\n            x, y = XX[ind][hysize], YY[ind][hysize]\n            if self.too_close(x,y, lw):\n                continue\n            else:\n                return x,y, ind\n\n        ind = adist[0]\n        x, y = XX[ind][hysize], YY[ind][hysize]\n        return x,y, ind```\n\nUpdated code:\n```python\ndef get_label_coords(self, distances, XX, YY, ysize, lw):\n        \n        \n        \n        hysize = int(ysize/2)\n        adist = np.argsort(distances)\n\n        for ind in adist:\n            x, y = XX[ind][hysize], YY[ind][hysize]\n            if self.too_close(x,y, lw):\n                continue\n            return x,y, ind\n\n        ind = adist[0]\n        x, y = XX[ind][hysize], YY[ind][hysize]\n        return x,y, ind\n```\n\nOriginal documentation:\n```python\nlabels are ploted at a location with the smallest\n        dispersion of the contour from a straight line\n        unless there's another label nearby, in which case\n        the second best place on the contour is picked up\n        if there's no good place a label isplotted at the\n        beginning of the contour\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return x, y, and the index of a label location.\n\n        Labels are plotted at a location with the smallest\n        deviation of the contour from a straight line\n        unless there is another label nearby, in which case\n        the next best place on the contour is picked up.\n        If all such candidates are rejected, the beginning\n        of the contour is chosen.", "file": "lib_matplotlib_contour.py", "function": "ContourLabeler.get_label_coords", "project": "matplotlib", "change_stats": {"code_changes": 9, "doc_changes": 16, "total_changes": 25}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __getitem__(self, bow, eps=None):\n        \n        \n\n        \n        return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)```\n\nUpdated code:\n```python\ndef __getitem__(self, bow, eps=None):\n        \n        \n        \n        return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)\n```\n\nOriginal documentation:\n```python\nReturn topic distribution for the given document `bow`, as a list of\n        (topic_id, topic_probability) 2-tuples.\n\n        Ignore topics with very low probability (below `eps`).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            bow (list): Bag-of-words representation of a document.\n            eps (float): Ignore topics with probability below `eps`.\n\n        Returns:\n            topic distribution for the given document `bow`, as a list of\n            `(topic_id, topic_probability)` 2-tuples.", "file": "gensim_models_ldamodel.py", "function": "LdaModel.__getitem__", "project": "gensim", "change_stats": {"code_changes": 3, "doc_changes": 11, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_mouse_pos(self, window_pos=None):\n    \n    window_pos = window_pos or pygame.mouse.get_pos()\n    # +0.5 to center the point on the middle of the pixel.\n    window_pt = point.Point(*window_pos) + 0.5\n    for surf in reversed(self.surfaces):\n      if (surf.surf_type != SurfType.CHROME and\n          surf.surf_rect.contains_point(window_pt)):\n        surf_rel_pt = window_pt - surf.surf_rect.tl\n        world_pt = surf.world_to_surf.back_pt(surf_rel_pt)\n        return MousePos(world_pt, surf.surf_type)```\n\nUpdated code:\n```python\ndef get_mouse_pos(self, window_pos=None):\n    \n    window_pos = window_pos or pygame.mouse.get_pos()\n    # +0.5 to center the point on the middle of the pixel.\n    window_pt = point.Point(*window_pos) + 0.5\n    for surf in reversed(self.surfaces):\n      if (surf.surf_type != SurfType.CHROME and\n          surf.surf_rect.contains_point(window_pt)):\n        surf_rel_pt = window_pt - surf.surf_rect.tl\n        world_pt = surf.world_to_surf.back_pt(surf_rel_pt)\n        return MousePos(world_pt, surf)\n```\n\nOriginal documentation:\n```python\nReturn a MousePos filled with the world and screen/minimap positions.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return a MousePos filled with the world position and surf it hit.", "file": "pysc2_lib_renderer_human.py", "function": "RendererHuman.get_mouse_pos", "project": "pysc2", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef scan_file_for_separating_barcodes(filepath: str) -> List[int]:\n    \n    \n    \n    separator_page_numbers = []\n    separator_barcode = str(settings.CONSUMER_BARCODE_STRING)\n    # use a temporary directory in case the file os too big to handle in memory\n    with tempfile.TemporaryDirectory() as path:\n        pages_from_path = convert_from_path(filepath, output_folder=path)\n        for current_page_number, page in enumerate(pages_from_path):\n            current_barcodes = barcode_reader(page)\n            if separator_barcode in current_barcodes:\n                separator_page_numbers.append(current_page_number)\n    return separator_page_numbers```\n\nUpdated code:\n```python\ndef scan_file_for_separating_barcodes(filepath: str) -> Tuple[Optional[str], List[int]]:\n    \n    \n    \n\n    separator_page_numbers = []\n    pdf_filepath = None\n\n    mime_type = get_file_mime_type(filepath)\n\n    if supported_file_type(mime_type):\n        pdf_filepath = filepath\n        if mime_type == \"image/tiff\":\n            pdf_filepath = convert_from_tiff_to_pdf(filepath)\n\n        pdf = Pdf.open(pdf_filepath)\n\n        for page_num, page in enumerate(pdf.pages):\n            for image_key in page.images:\n                pdfimage = PdfImage(page.images[image_key])\n                pillow_img = pdfimage.as_pil_image()\n\n                detected_barcodes = barcode_reader(pillow_img)\n\n                if settings.CONSUMER_BARCODE_STRING in detected_barcodes:\n                    separator_page_numbers.append(page_num)\n    else:\n        logger.warning(\n            f\"Unsupported file format for barcode reader: {str(mime_type)}\",\n        )\n    return pdf_filepath, separator_page_numbers\n```\n\nOriginal documentation:\n```python\nScan the provided pdf file for page separating barcodes\n    Returns a list of pagenumbers, which separate the file\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Scan the provided pdf file for page separating barcodes\n    Returns a the PDF filepath and a list of pagenumbers,\n    which separate the file into new files", "file": "src_documents_barcodes.py", "function": "scan_file_for_separating_barcodes", "project": "paperless-ngx", "change_stats": {"code_changes": 39, "doc_changes": 5, "total_changes": 44}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef call(self, x, axis=[1, 2]):\n    \n    \n    return gem(x, power=self.power, eps=self.eps, axis=axis)```\n\nUpdated code:\n```python\ndef call(self, x, axis=None):\n    \n    \n    if axis is None:\n      axis = [1, 2]\n    return gem(x, power=self.power, eps=self.eps, axis=axis)\n```\n\nOriginal documentation:\n```python\nInvokes the GeM instance.\n\n    Args:\n      x: [B, H, W, D] A float32 Tensor.\n      axis: Dimensions to reduce.\n\n    Returns:\n      output: [B, D] A float32 Tensor.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Invokes the GeM instance.\n\n    Args:\n      x: [B, H, W, D] A float32 Tensor.\n      axis: Dimensions to reduce. By default, dimensions [1, 2] are reduced.\n\n    Returns:\n      output: [B, D] A float32 Tensor.", "file": "research_delf_delf_python_pooling_layers_pooling.py", "function": "GeM.call", "project": "models", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_step_args(\n        self, step_id: StepID, workflows: List[Any], workflow_refs: List[WorkflowRef]\n    ) -> Tuple[List, Dict[str, Any]]:\n        \n        \n        with serialization_context.workflow_args_resolving_context(\n            workflows, workflow_refs\n        ):\n            flattened_args = self._get(self._key_step_args(step_id))\n            # dereference arguments like Ray remote functions\n            flattened_args = [\n                ray.get(a) if isinstance(a, ray.ObjectRef) else a\n                for a in flattened_args\n            ]\n            return signature.recover_args(flattened_args)```\n\nUpdated code:\n```python\ndef load_step_args(self, step_id: StepID) -> ray.ObjectRef:\n        \n        \n        with serialization_context.workflow_args_keeping_context():\n            x = self._get(self._key_step_args(step_id))\n        return ray.put(x)\n```\n\nOriginal documentation:\n```python\nLoad the input arguments of the workflow step. This must be\n        done under a serialization context, otherwise the arguments would\n        not be reconstructed successfully.\n\n        Args:\n            step_id: ID of the workflow step.\n            workflows: The workflows in the original arguments,\n                replaced by the actual workflow outputs.\n            object_refs: The object refs in the original arguments.\n\n        Returns:\n            Args and kwargs.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load the input arguments of the workflow step. This must be\n        done under a serialization context, otherwise the arguments would\n        not be reconstructed successfully.\n\n        Args:\n            step_id: ID of the workflow step.\n\n        Returns:\n            An object ref of the input args.", "file": "python_ray_workflow_workflow_storage.py", "function": "WorkflowStorage.load_step_args", "project": "ray", "change_stats": {"code_changes": 19, "doc_changes": 7, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef write_headers(self, start_line, headers, chunk=None, callback=None,\n                      has_body=True):\n        \n        \n        raise NotImplementedError()```\n\nUpdated code:\n```python\ndef write_headers(self, start_line, headers, chunk=None, callback=None):\n        \n        \n        raise NotImplementedError()\n```\n\nOriginal documentation:\n```python\nWrite an HTTP header block.\n\n        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`.\n        :arg headers: a `.HTTPHeaders` instance.\n        :arg chunk: the first (optional) chunk of data.  This is an optimization\n            so that small responses can be written in the same call as their\n            headers.\n        :arg callback: a callback to be run when the write is complete.\n        :arg bool has_body: may be false to indicate that this message\n            has no body (and so does not need either a Content-Length\n            or Transfer-Encoding)\n\n        Returns a `.Future` if no callback is given.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Write an HTTP header block.\n\n        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`.\n        :arg headers: a `.HTTPHeaders` instance.\n        :arg chunk: the first (optional) chunk of data.  This is an optimization\n            so that small responses can be written in the same call as their\n            headers.\n        :arg callback: a callback to be run when the write is complete.\n\n        Returns a `.Future` if no callback is given.", "file": "tornado_httputil.py", "function": "HTTPConnection.write_headers", "project": "tornado", "change_stats": {"code_changes": 5, "doc_changes": 5, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _check_all_gather(self, param: FlatParameter):\n        \n        \n        \n        # Only check all-gathers when rebuilding the full parameters in the\n        # forward pass or in the beginning of the backward pass\n        if self.training_state not in (\n            TrainingState_.FORWARD, TrainingState_.BACKWARD_PRE,\n        ):\n            return\n        eod = self._exec_order_data\n        param_index = eod.get_param_index(param)\n        if not eod.is_first_iter:\n            # Only issue warnings on the first deviating iteration and stop\n            # checking thereafter to avoid flooding the console\n            if eod.warn_status == _ExecOrderWarnStatus.WARNED:\n                return\n            # However, we may issue multiple warnings on the first deviating\n            # iteration to help debugging, where either:\n            # 1. This iteration sees more all-gathers than the first iteration\n            msg_prefix = all_gather_seq = None  # non-`None` means we warn\n            if eod.index >= len(eod.param_order):\n                msg_prefix = \"Expected no more all-gathers but got an all-gather for \"\n                all_gather_seq = eod.param_order + [param_index]\n            else:\n                expected_param_index = eod.param_order[eod.index]\n                # 2. This iteration sees the same number of all-gathers (so\n                # far) but the current parameter to all-gather differs\n                if param_index != expected_param_index:\n                    expected_param_names = eod.get_unflat_param_names(expected_param_index)\n                    assert len(expected_param_names) > 0, \\\n                        \"The expected parameter to all-gather should always be valid\"\n                    msg_prefix = \"Expected an all-gather for the FSDP module \" \\\n                        f\"wrapping {expected_param_names} but got an all-gather for \"\n                    all_gather_seq = eod.param_order[:eod.index - 1] + [param_index]\n            to_issue_warning = msg_prefix is not None\n            if to_issue_warning:\n                assert all_gather_seq is not None\n                param_names = eod.get_unflat_param_names(param_index)\n                is_added_param = len(param_names) == 0\n                if is_added_param:\n                    msg_suffix = \"a newly-added parameter since construction time\"\n                else:\n                    msg_suffix = f\"the FSDP module wrapping {param_names}\"\n                sub_msg = msg_prefix + msg_suffix\n                warnings.warn(\n                    \"All-gather order differs from that of the first iteration \"\n                    f\"on rank {self.rank} -- collectives are unchecked and may \"\n                    \"give incorrect results or hang\\n\" + sub_msg + \"\\n\" +\n                    f\"First iteration's all-gather sequence: {eod.param_order}\"\n                    \"\\nThis iteration's all-gather sequence (so far): \"\n                    f\"{all_gather_seq}\\nwhere indices follow the root FSDP \"\n                    \"module's `.parameters()` order\"\n                )\n                eod.warn_status = _ExecOrderWarnStatus.WARNING\n            eod.index += 1\n        else:\n            device = param.device\n            indices = torch.zeros(self.world_size, dtype=torch.int32, device=device)\n            index = torch.tensor([param_index], dtype=torch.int32, device=device)\n            dist._all_gather_base(indices, index, group=self.process_group)\n            # Check that all ranks plan to all-gather the same parameter index\n            for (r1, i1), (r2, i2) in itertools.combinations(\n                ((rank, indices[rank]) for rank in range(self.world_size)), 2,\n            ):\n                if not torch.equal(i1, i2):\n                    r1_param_names = eod.get_unflat_param_names(i1)\n                    r2_param_names = eod.get_unflat_param_names(i2)\n                    raise RuntimeError(\n                        f\"All-gather order differs across ranks: rank {r1} is \"\n                        f\"all-gathering the flattened parameter wrapping \"\n                        f\"{r1_param_names} while rank {r2} is all-gathering \"\n                        f\"the flattened parameter wrapping {r2_param_names}\"\n                    )\n            eod.param_order.append(param_index)```\n\nUpdated code:\n```python\ndef _check_all_gather(self, param: FlatParameter):\n        \n        \n        \n        # Only check all-gathers when rebuilding the full parameters in the\n        # forward pass\n        if self.training_state != TrainingState_.FORWARD:\n            return\n        eod = self._exec_order_data\n        param_index = eod.get_param_index(param)\n        if not eod.is_first_iter:\n            # Only issue warnings on the first deviating iteration and stop\n            # checking thereafter to avoid flooding the console\n            if eod.warn_status == _ExecOrderWarnStatus.WARNED:\n                return\n            # However, we may issue multiple warnings on the first deviating\n            # iteration to help debugging, where either:\n            # 1. This iteration sees more all-gathers than the first iteration\n            msg_prefix = all_gather_seq = None  # non-`None` means we warn\n            if eod.index >= len(eod.param_order):\n                msg_prefix = \"Expected no more all-gathers but got an all-gather for \"\n                all_gather_seq = eod.param_order + [param_index]\n            else:\n                expected_param_index = eod.param_order[eod.index]\n                # 2. This iteration sees the same number of all-gathers (so\n                # far) but the current parameter to all-gather differs\n                if param_index != expected_param_index:\n                    expected_param_names = eod.get_unflat_param_names(expected_param_index)\n                    assert len(expected_param_names) > 0, \\\n                        \"The expected parameter to all-gather should always be valid\"\n                    msg_prefix = \"Expected an all-gather for the FSDP module \" \\\n                        f\"wrapping {expected_param_names} but got an all-gather for \"\n                    all_gather_seq = eod.param_order[:eod.index - 1] + [param_index]\n            to_issue_warning = msg_prefix is not None\n            if to_issue_warning:\n                assert all_gather_seq is not None\n                param_names = eod.get_unflat_param_names(param_index)\n                is_added_param = len(param_names) == 0\n                if is_added_param:\n                    msg_suffix = \"a newly-added parameter since construction time\"\n                else:\n                    msg_suffix = f\"the FSDP module wrapping {param_names}\"\n                sub_msg = msg_prefix + msg_suffix\n                warnings.warn(\n                    \"All-gather order differs from that of the first iteration \"\n                    f\"on rank {self.rank} -- collectives are unchecked and may \"\n                    \"give incorrect results or hang\\n\" + sub_msg + \"\\n\" +\n                    f\"First iteration's all-gather sequence: {eod.param_order}\"\n                    \"\\nThis iteration's all-gather sequence (so far): \"\n                    f\"{all_gather_seq}\\nwhere indices follow the root FSDP \"\n                    \"module's `.parameters()` order\"\n                )\n                eod.warn_status = _ExecOrderWarnStatus.WARNING\n            eod.index += 1\n        else:\n            device = param.device\n            indices = torch.zeros(self.world_size, dtype=torch.int32, device=device)\n            index = torch.tensor([param_index], dtype=torch.int32, device=device)\n            dist._all_gather_base(indices, index, group=self.process_group)\n            # Check that all ranks plan to all-gather the same parameter index\n            for (r1, i1), (r2, i2) in itertools.combinations(\n                ((rank, indices[rank]) for rank in range(self.world_size)), 2,\n            ):\n                if not torch.equal(i1, i2):\n                    r1_param_names = eod.get_unflat_param_names(i1)\n                    r2_param_names = eod.get_unflat_param_names(i2)\n                    raise RuntimeError(\n                        f\"All-gather order differs across ranks: rank {r1} is \"\n                        f\"all-gathering the flattened parameter wrapping \"\n                        f\"{r1_param_names} while rank {r2} is all-gathering \"\n                        f\"the flattened parameter wrapping {r2_param_names}\"\n                    )\n            eod.param_order.append(param_index)\n```\n\nOriginal documentation:\n```python\nChecks the validity of an all-gather to rebuild the full parameter\n        ``param``. If on the first iteration, this uses an all-gather to check\n        that all ranks plan to all-gather the same parameter, erroring if not,\n        and on subsequent iterations, if the all-gather order differs from that\n        of the first iteration (meaning that we can no longer guarantee correct\n        execution), then we issue a warning to the user. This only issues\n        warnings on the first deviating iteration and stops checking\n        thereafter.\n\n        Executing in ``no_sync()`` does not affect this check for\n        ``FULL_SHARD`` and ``SHARD_GRAD_OP``: (1) Being in ``no_sync()`` in the\n        first iteration does not yield a different all-gather sequence, and (2)\n        being in ``no_sync()`` in a later iteration does not give false\n        positive warnings since the all-gather sequence still matches the first\n        iteration sequence (for ``FULL_SHARD``) or the first iteration\n        sequence's prefix (for ``SHARD_GRAD_OP``).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks the validity of an all-gather to rebuild the full parameter\n        ``param``. If on the first iteration, this uses an all-gather to check\n        that all ranks plan to all-gather the same parameter, erroring if not,\n        and on subsequent iterations, if the all-gather order differs from that\n        of the first iteration (meaning that we can no longer guarantee correct\n        execution), then we issue a warning to the user. This only issues\n        warnings on the first deviating iteration and stops checking\n        thereafter.\n\n        For now, only the all-gathers to rebuild full parameters in the forward\n        pass are checked since (1) a correct forward order should imply a\n        correct pre-backward order for typical cases and (2) there may be some\n        issues with pre-fetching that need to be looked into:\n        https://github.com/pytorch/pytorch/issues/76553\n\n        Executing in ``no_sync()`` does not affect this check for\n        ``FULL_SHARD`` and ``SHARD_GRAD_OP``: (1) Being in ``no_sync()`` in the\n        first iteration does not yield a different all-gather sequence, and (2)\n        being in ``no_sync()`` in a later iteration does not give false\n        positive warnings since the all-gather sequence still matches the first\n        iteration sequence (for ``FULL_SHARD``) or the first iteration\n        sequence's prefix (for ``SHARD_GRAD_OP``).", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel._check_all_gather", "project": "pytorch", "change_stats": {"code_changes": 8, "doc_changes": 8, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_ohlcv(self, symbol: str, timeframe='1m', since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'symbol': market['id'],\n        }\n        until = self.safe_integer_2(params, 'until', 'till')\n        if limit is None:\n            limit = 100\n        marketType = 'spot' if market['spot'] else 'swap'\n        timeframes = self.options['timeframes'][marketType]\n        selectedTimeframe = self.safe_string(timeframes, timeframe, timeframe)\n        duration = self.parse_timeframe(timeframe)\n        if market['spot']:\n            request['period'] = selectedTimeframe\n            request['limit'] = limit\n            if since is not None:\n                request['after'] = since\n                if until is None:\n                    request['before'] = self.sum(since, limit * duration * 1000)\n            if until is not None:\n                request['before'] = until\n        elif market['contract']:\n            request['granularity'] = selectedTimeframe\n            now = self.milliseconds()\n            if since is None:\n                request['startTime'] = now - (limit - 1) * (duration * 1000)\n                request['endTime'] = now\n            else:\n                request['startTime'] = self.sum(since, duration * 1000)\n                if until is not None:\n                    request['endTime'] = until\n                else:\n                    request['endTime'] = self.sum(since, limit * duration * 1000)\n        ommitted = self.omit(params, ['until', 'till'])\n        extended = self.extend(request, ommitted)\n        response = None\n        if market['spot']:\n            response = self.publicSpotGetMarketCandles(extended)\n        else:\n            response = self.publicMixGetMarketCandles(extended)\n        #  [[\"1645911960000\",\"39406\",\"39407\",\"39374.5\",\"39379\",\"35.526\",\"1399132.341\"]]\n        data = self.safe_value(response, 'data', response)\n        return self.parse_ohlcvs(data, market, timeframe, since, limit)```\n\nUpdated code:\n```python\ndef fetch_ohlcv(self, symbol: str, timeframe='1m', since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'symbol': market['id'],\n        }\n        until = self.safe_integer_2(params, 'until', 'till')\n        if limit is None:\n            limit = 1000\n        request['limit'] = limit\n        marketType = 'spot' if market['spot'] else 'swap'\n        timeframes = self.options['timeframes'][marketType]\n        selectedTimeframe = self.safe_string(timeframes, timeframe, timeframe)\n        duration = self.parse_timeframe(timeframe)\n        if market['spot']:\n            request['period'] = selectedTimeframe\n            request['limit'] = limit\n            if since is not None:\n                request['after'] = since\n                if until is None:\n                    request['before'] = self.sum(since, limit * duration * 1000)\n            if until is not None:\n                request['before'] = until\n        elif market['contract']:\n            request['granularity'] = selectedTimeframe\n            now = self.milliseconds()\n            if since is None:\n                request['startTime'] = now - limit * (duration * 1000)\n                request['endTime'] = now\n            else:\n                request['startTime'] = since\n                if until is not None:\n                    request['endTime'] = until\n                else:\n                    request['endTime'] = self.sum(since, limit * duration * 1000)\n        ommitted = self.omit(params, ['until', 'till'])\n        extended = self.extend(request, ommitted)\n        response = None\n        if market['spot']:\n            response = self.publicSpotGetMarketCandles(extended)\n        else:\n            response = self.publicMixGetMarketCandles(extended)\n        #  [[\"1645911960000\",\"39406\",\"39407\",\"39374.5\",\"39379\",\"35.526\",\"1399132.341\"]]\n        data = self.safe_value(response, 'data', response)\n        return self.parse_ohlcvs(data, market, timeframe, since, limit)\n```\n\nOriginal documentation:\n```python\nfetches historical candlestick data containing the open, high, low, and close price, and the volume of a market\n        see https://bitgetlimited.github.io/apidoc/en/mix/#get-candle-data\n        :param str symbol: unified symbol of the market to fetch OHLCV data for\n        :param str timeframe: the length of time each candle represents\n        :param int|None since: timestamp in ms of the earliest candle to fetch\n        :param int|None limit: the maximum amount of candles to fetch\n        :param dict params: extra parameters specific to the bitget api endpoint\n        :param int|None params['until']: timestamp in ms of the latest candle to fetch\n        :returns [[int]]: A list of candles ordered, open, high, low, close, volume\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetches historical candlestick data containing the open, high, low, and close price, and the volume of a market\n        see https://bitgetlimited.github.io/apidoc/en/mix/#get-candle-data\n        see https://bitgetlimited.github.io/apidoc/en/spot/#candlestick-line-timeframe\n        :param str symbol: unified symbol of the market to fetch OHLCV data for\n        :param str timeframe: the length of time each candle represents\n        :param int|None since: timestamp in ms of the earliest candle to fetch\n        :param int|None limit: the maximum amount of candles to fetch\n        :param dict params: extra parameters specific to the bitget api endpoint\n        :param int|None params['until']: timestamp in ms of the latest candle to fetch\n        :returns [[int]]: A list of candles ordered, open, high, low, close, volume", "file": "python_ccxt_bitget.py", "function": "bitget.fetch_ohlcv", "project": "ccxt", "change_stats": {"code_changes": 9, "doc_changes": 3, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef nlargest(self, n=5, take_last=False):\n        \n        \n        return select_n(self, n=n, take_last=take_last, method='nlargest')```\n\nUpdated code:\n```python\ndef nlargest(self, n=5, keep='first'):\n        \n        \n        return select_n(self, n=n, keep=keep, method='nlargest')\n```\n\nOriginal documentation:\n```python\nReturn the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int\n            Return this many descending sorted values\n        take_last : bool\n            Where there are duplicate values, take the last duplicate\n\n        Returns\n        -------\n        top_n : Series\n            The n largest values in the Series, in sorted order\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n` relative\n        to the size of the ``Series`` object.\n\n        See Also\n        --------\n        Series.nsmallest\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> import numpy as np\n        >>> s = pd.Series(np.random.randn(1e6))\n        >>> s.nlargest(10)  # only sorts up to the N requested\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int\n            Return this many descending sorted values\n        keep : {'first', 'last', False}, default 'first'\n            Where there are duplicate values:\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n        take_last : deprecated\n\n        Returns\n        -------\n        top_n : Series\n            The n largest values in the Series, in sorted order\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n` relative\n        to the size of the ``Series`` object.\n\n        See Also\n        --------\n        Series.nsmallest\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> import numpy as np\n        >>> s = pd.Series(np.random.randn(1e6))\n        >>> s.nlargest(10)  # only sorts up to the N requested", "file": "pandas_core_series.py", "function": "Series.nlargest", "project": "pandas", "change_stats": {"code_changes": 6, "doc_changes": 9, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_slots_sub_state(\n        tracker: \"DialogueStateTracker\", omit_unset_slots: bool = False,\n    ) -> Dict[Text, Union[Text, Tuple[float]]]:\n        \n        \n        slots = {}\n        for slot_name, slot in tracker.slots.items():\n            if slot is not None and slot.as_feature():\n                if omit_unset_slots and not slot.has_been_set:\n                    continue\n                if slot.value == rasa.shared.core.constants.SHOULD_NOT_BE_SET:\n                    slots[slot_name] = rasa.shared.core.constants.SHOULD_NOT_BE_SET\n                elif any(slot.as_feature()):\n                    # only add slot if some of the features are not zero\n                    slots[slot_name] = tuple(slot.as_feature())\n\n        return slots```\n\nUpdated code:\n```python\ndef _get_slots_sub_state(\n        tracker: \"DialogueStateTracker\", omit_unset_slots: bool = False,\n    ) -> Dict[Text, Union[Text, Tuple[float]]]:\n        \n        \n        slots = {}\n        for slot_name, slot in tracker.slots.items():\n            # If the slot doesn't influence conversations, slot.as_feature() will return\n            # a result that evaluates to False, meaning that the slot shouldn't be\n            # included in featurised sub-states.\n            # Note that this condition checks if the slot itself is None. An unset slot\n            # will be a Slot object and its `value` attribute will be None.\n            if slot is not None and slot.as_feature():\n                if omit_unset_slots and not slot.has_been_set:\n                    continue\n                if slot.value == rasa.shared.core.constants.SHOULD_NOT_BE_SET:\n                    slots[slot_name] = rasa.shared.core.constants.SHOULD_NOT_BE_SET\n                elif any(slot.as_feature()):\n                    # Only include slot in featurised sub-state if the slot is not\n                    # unset, i.e. is set to some actual valid value, has been\n                    # successfully featurized, and so has at least one non-zero feature.\n                    slots[slot_name] = tuple(slot.as_feature())\n        return slots\n```\n\nOriginal documentation:\n```python\nSets all set slots with the featurization of the stored value.\n\n        Args:\n            tracker: dialog state tracker containing the dialog so far\n            omit_unset_slots: If `True` do not include the initial values of slots.\n\n        Returns:\n            a dictionary mapping slot names to their featurization\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Sets all set slots with the featurization of the stored value.\n\n        Args:\n            tracker: dialog state tracker containing the dialog so far\n            omit_unset_slots: If `True` do not include the initial values of slots.\n\n        Returns:\n            a mapping of slot names to their featurization", "file": "rasa_shared_core_domain.py", "function": "Domain._get_slots_sub_state", "project": "rasa", "change_stats": {"code_changes": 12, "doc_changes": 4, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef patch(self, patcher):\n        \n        \n        *options, _, extra = self._options\n        return Logger(self._core, *options, patcher, extra)```\n\nUpdated code:\n```python\ndef patch(self, patcher):\n        \n        \n        *options, patchers, extra = self._options\n        return Logger(self._core, *options, patchers + [patcher], extra)\n```\n\nOriginal documentation:\n```python\nAttach a function to modify the record dict created by each logging call.\n\n        The ``patcher`` may be used to update the record on-the-fly before it's propagated to the\n        handlers. This allows the \"extra\" dict to be populated with dynamic values and also permits\n        advanced modifications of the record emitted while logging a message. The function is called\n        once before sending the log message to the different handlers.\n\n        It is recommended to apply modification on the ``record[\"extra\"]`` dict rather than on the\n        ``record`` dict itself, as some values are used internally by `Loguru`, and modify them may\n        produce unexpected results.\n\n        Parameters\n        ----------\n        patcher: |callable|_\n            The function to which the record dict will be passed as the sole argument. This function\n            is in charge of updating the record in-place, the function does not need to return any\n            value, the modified record object will be re-used.\n\n        Returns\n        -------\n        :class:`~Logger`\n            A logger wrapping the core logger, but which records are passed through the ``patcher``\n            function before being sent to the added handlers.\n\n        Examples\n        --------\n        >>> logger.add(sys.stderr, format=\"{extra[utc]} {message}\")\n        >>> logger = logger.patch(lambda record: record[\"extra\"].update(utc=datetime.utcnow())\n        >>> logger.info(\"That's way, you can log messages with time displayed in UTC\")\n\n        >>> def wrapper(func):\n        ...     @functools.wraps(func)\n        ...     def wrapped(*args, **kwargs):\n        ...         logger.patch(lambda r: r.update(function=func.__name__)).info(\"Wrapped!\")\n        ...         return func(*args, **kwargs)\n        ...     return wrapped\n\n        >>> def recv_record_from_network(pipe):\n        ...     record = pickle.loads(pipe.read())\n        ...     level, message = record[\"level\"], record[\"message\"]\n        ...     logger.patch(lambda r: r.update(record)).log(level, message)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Attach a function to modify the record dict created by each logging call.\n\n        The ``patcher`` may be used to update the record on-the-fly before it's propagated to the\n        handlers. This allows the \"extra\" dict to be populated with dynamic values and also permits\n        advanced modifications of the record emitted while logging a message. The function is called\n        once before sending the log message to the different handlers.\n\n        It is recommended to apply modification on the ``record[\"extra\"]`` dict rather than on the\n        ``record`` dict itself, as some values are used internally by `Loguru`, and modify them may\n        produce unexpected results.\n\n        The logger can be patched multiple times. In this case, the functions are called in the\n        same order as they are added.\n\n        Parameters\n        ----------\n        patcher: |callable|_\n            The function to which the record dict will be passed as the sole argument. This function\n            is in charge of updating the record in-place, the function does not need to return any\n            value, the modified record object will be re-used.\n\n        Returns\n        -------\n        :class:`~Logger`\n            A logger wrapping the core logger, but which records are passed through the ``patcher``\n            function before being sent to the added handlers.\n\n        Examples\n        --------\n        >>> logger.add(sys.stderr, format=\"{extra[utc]} {message}\")\n        >>> logger = logger.patch(lambda record: record[\"extra\"].update(utc=datetime.utcnow())\n        >>> logger.info(\"That's way, you can log messages with time displayed in UTC\")\n\n        >>> def wrapper(func):\n        ...     @functools.wraps(func)\n        ...     def wrapped(*args, **kwargs):\n        ...         logger.patch(lambda r: r.update(function=func.__name__)).info(\"Wrapped!\")\n        ...         return func(*args, **kwargs)\n        ...     return wrapped\n\n        >>> def recv_record_from_network(pipe):\n        ...     record = pickle.loads(pipe.read())\n        ...     level, message = record[\"level\"], record[\"message\"]\n        ...     logger.patch(lambda r: r.update(record)).log(level, message)", "file": "loguru__logger.py", "function": "Logger.patch", "project": "loguru", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _ensure_nanosecond_dtype(dtype: DtypeObj) -> DtypeObj:\n    \n    \n      # noqa:E501\n    msg = (\n        f\"The '{dtype.name}' dtype has no unit. \"\n        f\"Please pass in '{dtype.name}[ns]' instead.\"\n    )\n\n    # unpack e.g. SparseDtype\n    dtype = getattr(dtype, \"subtype\", dtype)\n\n    if not isinstance(dtype, np.dtype):\n        # i.e. datetime64tz\n        pass\n\n    elif dtype.kind in [\"m\", \"M\"]:\n        reso = get_unit_from_dtype(dtype)\n        if not is_supported_unit(reso):\n            # pre-2.0 we would silently swap in nanos for lower-resolutions,\n            #  raise for above-nano resolutions\n            if dtype.name in [\"datetime64\", \"timedelta64\"]:\n                raise ValueError(msg)\n            # TODO: ValueError or TypeError? existing test\n            #  test_constructor_generic_timestamp_bad_frequency expects TypeError\n            raise TypeError(\n                f\"dtype={dtype} is not supported. Supported resolutions are 's', \"\n                \"'ms', 'us', and 'ns'\"\n            )\n    return dtype```\n\nUpdated code:\n```python\ndef _ensure_nanosecond_dtype(dtype: DtypeObj) -> None:\n    \n    \n      # noqa:E501\n    msg = (\n        f\"The '{dtype.name}' dtype has no unit. \"\n        f\"Please pass in '{dtype.name}[ns]' instead.\"\n    )\n\n    # unpack e.g. SparseDtype\n    dtype = getattr(dtype, \"subtype\", dtype)\n\n    if not isinstance(dtype, np.dtype):\n        # i.e. datetime64tz\n        pass\n\n    elif dtype.kind in [\"m\", \"M\"]:\n        reso = get_unit_from_dtype(dtype)\n        if not is_supported_unit(reso):\n            # pre-2.0 we would silently swap in nanos for lower-resolutions,\n            #  raise for above-nano resolutions\n            if dtype.name in [\"datetime64\", \"timedelta64\"]:\n                raise ValueError(msg)\n            # TODO: ValueError or TypeError? existing test\n            #  test_constructor_generic_timestamp_bad_frequency expects TypeError\n            raise TypeError(\n                f\"dtype={dtype} is not supported. Supported resolutions are 's', \"\n                \"'ms', 'us', and 'ns'\"\n            )\n```\n\nOriginal documentation:\n```python\nConvert dtypes with granularity less than nanosecond to nanosecond\n\n    >>> _ensure_nanosecond_dtype(np.dtype(\"M8[us]\"))\n    dtype('<M8[us]')\n\n    >>> _ensure_nanosecond_dtype(np.dtype(\"M8[D]\"))\n    Traceback (most recent call last):\n        ...\n    TypeError: dtype=datetime64[D] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'\n\n    >>> _ensure_nanosecond_dtype(np.dtype(\"m8[ps]\"))\n    Traceback (most recent call last):\n        ...\n    TypeError: dtype=timedelta64[ps] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert dtypes with granularity less than nanosecond to nanosecond\n\n    >>> _ensure_nanosecond_dtype(np.dtype(\"M8[us]\"))\n\n    >>> _ensure_nanosecond_dtype(np.dtype(\"M8[D]\"))\n    Traceback (most recent call last):\n        ...\n    TypeError: dtype=datetime64[D] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'\n\n    >>> _ensure_nanosecond_dtype(np.dtype(\"m8[ps]\"))\n    Traceback (most recent call last):\n        ...\n    TypeError: dtype=timedelta64[ps] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'", "file": "pandas_core_dtypes_cast.py", "function": "_ensure_nanosecond_dtype", "project": "pandas", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef has_fit_parameter(estimator, parameter):\n     \n    \n    if (parameter in getargspec(estimator.fit)[0]):\n        return True\n    else:\n        return False```\n\nUpdated code:\n```python\ndef has_fit_parameter(estimator, parameter):\n     \n    \n    return parameter in getargspec(estimator.fit)[0]\n```\n\nOriginal documentation:\n```python\nChecks whether the base estimator supports the given parameter.\n    Returns True if the base estimator supports parameter\n    Retuns False otherwise\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks whether the estimator's fit method supports the given parameter.\n\n    Parameters\n    ----------\n    estimator: Estimator\n\n    parameter: string\n    Checking is done on this parameter\n\n    Returns\n    -------\n    Boolean", "file": "sklearn_utils_validation.py", "function": "has_fit_parameter", "project": "scikit-learn", "change_stats": {"code_changes": 7, "doc_changes": 17, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef market(\n        self,\n        symbol: Annotated[\n            Union[str, List[str]],\n            OpenBBCustomParameter(\n                description=\"Symbol to get data for. Multiple items allowed: yfinance.\"\n            ),\n        ],\n        start_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"Start date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        end_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"End date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        provider: Optional[Literal[\"fmp\", \"intrinio\", \"polygon\", \"yfinance\"]] = None,\n        **kwargs\n    ) -> OBBject:\n        \n          # noqa: E501\n\n        simplefilter(\"always\", DeprecationWarning)\n        warn(\n            \"This endpoint is deprecated; use `/index/price/historical` instead. Deprecated in OpenBB Platform V4.1 to be removed in V4.3.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n\n        return self._run(\n            \"/index/market\",\n            **filter_inputs(\n                provider_choices={\n                    \"provider\": self._get_provider(\n                        provider,\n                        \"/index/market\",\n                        (\"fmp\", \"intrinio\", \"polygon\", \"yfinance\"),\n                    )\n                },\n                standard_params={\n                    \"symbol\": symbol,\n                    \"start_date\": start_date,\n                    \"end_date\": end_date,\n                },\n                extra_params=kwargs,\n                extra_info={\"symbol\": {\"multiple_items_allowed\": [\"yfinance\"]}},\n            )\n        )```\n\nUpdated code:\n```python\ndef market(\n        self,\n        symbol: Annotated[\n            Union[str, List[str]],\n            OpenBBCustomParameter(\n                description=\"Symbol to get data for. Multiple items allowed for provider(s): yfinance.\"\n            ),\n        ],\n        start_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"Start date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        end_date: Annotated[\n            Union[datetime.date, None, str],\n            OpenBBCustomParameter(\n                description=\"End date of the data, in YYYY-MM-DD format.\"\n            ),\n        ] = None,\n        provider: Optional[Literal[\"fmp\", \"intrinio\", \"polygon\", \"yfinance\"]] = None,\n        **kwargs\n    ) -> OBBject:\n        \n          # noqa: E501\n\n        simplefilter(\"always\", DeprecationWarning)\n        warn(\n            \"This endpoint is deprecated; use `/index/price/historical` instead. Deprecated in OpenBB Platform V4.1 to be removed in V4.3.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n\n        return self._run(\n            \"/index/market\",\n            **filter_inputs(\n                provider_choices={\n                    \"provider\": self._get_provider(\n                        provider,\n                        \"/index/market\",\n                        (\"fmp\", \"intrinio\", \"polygon\", \"yfinance\"),\n                    )\n                },\n                standard_params={\n                    \"symbol\": symbol,\n                    \"start_date\": start_date,\n                    \"end_date\": end_date,\n                },\n                extra_params=kwargs,\n                extra_info={\"symbol\": {\"multiple_items_allowed\": [\"yfinance\"]}},\n            )\n        )\n```\n\nOriginal documentation:\n```python\nHistorical Market Indices.\n\n        Parameters\n        ----------\n        symbol : Union[str, List[str]]\n            Symbol to get data for. Multiple items allowed: yfinance.\n        start_date : Union[datetime.date, None, str]\n            Start date of the data, in YYYY-MM-DD format.\n        end_date : Union[datetime.date, None, str]\n            End date of the data, in YYYY-MM-DD format.\n        provider : Optional[Literal['fmp', 'intrinio', 'polygon', 'yfinance']]\n            The provider to use for the query, by default None.\n            If None, the provider specified in defaults is selected or 'fmp' if there is\n            no default.\n        timeseries : Optional[Annotated[int, Ge(ge=0)]]\n            Number of days to look back. (provider: fmp)\n        interval : Optional[Union[Literal['1min', '5min', '15min', '30min', '1hour', '4hour', '1day'], Literal['1m', '2m', '5m', '15m', '30m', '60m', '90m', '1h', '1d', '5d', '1wk', '1mo', '3mo']]]\n            Data granularity. (provider: fmp, yfinance)\n        sort : Literal['asc', 'desc']\n            Sort the data in ascending or descending order. (provider: fmp);\n            Sort order. (provider: intrinio);\n            Sort order of the data. (provider: polygon)\n        tag : Optional[str]\n            Index tag. (provider: intrinio)\n        type : Optional[str]\n            Index type. (provider: intrinio)\n        limit : int\n            The number of data entries to return. (provider: intrinio, polygon)\n        timespan : Literal['minute', 'hour', 'day', 'week', 'month', 'quarter', 'year']\n            Timespan of the data. (provider: polygon)\n        adjusted : bool\n            Whether the data is adjusted. (provider: polygon)\n        multiplier : int\n            Multiplier of the timespan. (provider: polygon)\n        period : Optional[Literal['1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max']]\n            Time period of the data to return. (provider: yfinance)\n        prepost : bool\n            Include Pre and Post market data. (provider: yfinance)\n        rounding : bool\n            Round prices to two decimals? (provider: yfinance)\n\n        Returns\n        -------\n        OBBject\n            results : List[MarketIndices]\n                Serializable results.\n            provider : Optional[Literal['fmp', 'intrinio', 'polygon', 'yfinance']]\n                Provider name.\n            warnings : Optional[List[Warning_]]\n                List of warnings.\n            chart : Optional[Chart]\n                Chart object.\n            extra: Dict[str, Any]\n                Extra info.\n\n        MarketIndices\n        -------------\n        date : datetime\n            The date of the data.\n        open : Optional[Annotated[float, Strict(strict=True)]]\n            The open price.\n        high : Optional[Annotated[float, Strict(strict=True)]]\n            The high price.\n        low : Optional[Annotated[float, Strict(strict=True)]]\n            The low price.\n        close : Optional[Annotated[float, Strict(strict=True)]]\n            The close price.\n        volume : Optional[int]\n            The trading volume.\n        adj_close : Optional[float]\n            The adjusted close price. (provider: fmp)\n        unadjusted_volume : Optional[float]\n            Unadjusted volume of the symbol. (provider: fmp)\n        change : Optional[float]\n            Change in the price of the symbol from the previous day. (provider: fmp)\n        change_percent : Optional[float]\n            Change % in the price of the symbol. (provider: fmp)\n        label : Optional[str]\n            Human readable format of the date. (provider: fmp)\n        change_over_time : Optional[float]\n            Change % in the price of the symbol over a period of time. (provider: fmp)\n        transactions : Optional[Annotated[int, Gt(gt=0)]]\n            Number of transactions for the symbol in the time period. (provider: polygon)\n\n        Example\n        -------\n        >>> from openbb import obb\n        >>> obb.index.market(symbol=\"SPX\")\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Historical Market Indices.\n\n        Parameters\n        ----------\n        symbol : Union[str, List[str]]\n            Symbol to get data for. Multiple items allowed for provider(s): yfinance.\n        start_date : Union[datetime.date, None, str]\n            Start date of the data, in YYYY-MM-DD format.\n        end_date : Union[datetime.date, None, str]\n            End date of the data, in YYYY-MM-DD format.\n        provider : Optional[Literal['fmp', 'intrinio', 'polygon', 'yfinance']]\n            The provider to use for the query, by default None.\n            If None, the provider specified in defaults is selected or 'fmp' if there is\n            no default.\n        timeseries : Optional[Annotated[int, Ge(ge=0)]]\n            Number of days to look back. (provider: fmp)\n        interval : Optional[Union[Literal['1min', '5min', '15min', '30min', '1hour', '4hour', '1day'], Literal['1m', '2m', '5m', '15m', '30m', '60m', '90m', '1h', '1d', '5d', '1wk', '1mo', '3mo']]]\n            Data granularity. (provider: fmp, yfinance)\n        sort : Literal['asc', 'desc']\n            Sort the data in ascending or descending order. (provider: fmp);\n            Sort order. (provider: intrinio);\n            Sort order of the data. (provider: polygon)\n        tag : Optional[str]\n            Index tag. (provider: intrinio)\n        type : Optional[str]\n            Index type. (provider: intrinio)\n        limit : int\n            The number of data entries to return. (provider: intrinio, polygon)\n        timespan : Literal['minute', 'hour', 'day', 'week', 'month', 'quarter', 'year']\n            Timespan of the data. (provider: polygon)\n        adjusted : bool\n            Whether the data is adjusted. (provider: polygon)\n        multiplier : int\n            Multiplier of the timespan. (provider: polygon)\n        period : Optional[Literal['1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max']]\n            Time period of the data to return. (provider: yfinance)\n        prepost : bool\n            Include Pre and Post market data. (provider: yfinance)\n        rounding : bool\n            Round prices to two decimals? (provider: yfinance)\n\n        Returns\n        -------\n        OBBject\n            results : List[MarketIndices]\n                Serializable results.\n            provider : Optional[Literal['fmp', 'intrinio', 'polygon', 'yfinance']]\n                Provider name.\n            warnings : Optional[List[Warning_]]\n                List of warnings.\n            chart : Optional[Chart]\n                Chart object.\n            extra: Dict[str, Any]\n                Extra info.\n\n        MarketIndices\n        -------------\n        date : datetime\n            The date of the data.\n        open : Optional[Annotated[float, Strict(strict=True)]]\n            The open price.\n        high : Optional[Annotated[float, Strict(strict=True)]]\n            The high price.\n        low : Optional[Annotated[float, Strict(strict=True)]]\n            The low price.\n        close : Optional[Annotated[float, Strict(strict=True)]]\n            The close price.\n        volume : Optional[int]\n            The trading volume.\n        adj_close : Optional[float]\n            The adjusted close price. (provider: fmp)\n        unadjusted_volume : Optional[float]\n            Unadjusted volume of the symbol. (provider: fmp)\n        change : Optional[float]\n            Change in the price of the symbol from the previous day. (provider: fmp)\n        change_percent : Optional[float]\n            Change % in the price of the symbol. (provider: fmp)\n        label : Optional[str]\n            Human readable format of the date. (provider: fmp)\n        change_over_time : Optional[float]\n            Change % in the price of the symbol over a period of time. (provider: fmp)\n        transactions : Optional[Annotated[int, Gt(gt=0)]]\n            Number of transactions for the symbol in the time period. (provider: polygon)\n\n        Example\n        -------\n        >>> from openbb import obb\n        >>> obb.index.market(symbol=\"SPX\")", "file": "openbb_platform_openbb_package_index.py", "function": "ROUTER_index.market", "project": "OpenBBTerminal", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef parse_request_range(range_header):\n    \n    \n    unit, _, value = range_header.partition(\"=\")\n    unit, value = unit.strip(), value.strip()\n    if unit != \"bytes\":\n        return None\n    start_b, _, end_b = value.partition(\"-\")\n    try:\n        start = _int_or_none(start_b)\n        end = _int_or_none(end_b)\n    except ValueError:\n        return None\n    if end is not None:\n        end += 1\n    return slice(start, end)```\n\nUpdated code:\n```python\ndef parse_request_range(range_header, ):\n    \n    \n    unit, _, value = range_header.partition(\"=\")\n    unit, value = unit.strip(), value.strip()\n    if unit != \"bytes\":\n        return None\n    start_b, _, end_b = value.partition(\"-\")\n    try:\n        start = _int_or_none(start_b)\n        end = _int_or_none(end_b)\n    except ValueError:\n        return None\n    if end is not None:\n        if start is None:\n            start = -end\n            end = None\n        else:\n            end += 1\n    return slice(start, end)\n```\n\nOriginal documentation:\n```python\nParses a Range header.\n\n    Returns either ``None`` or an instance of ``slice``::\n\n        >>> rh = parse_request_range(\"bytes=1-2\")\n        >>> rh\n        slice(1, 3, None)\n        >>> [0, 1, 2, 3, 4][rh]\n        [1, 2]\n        >>> parse_request_range(\"bytes=6-\")\n        slice(6, None, None)\n        >>> parse_request_range(\"bytes=\")\n        slice(None, None, None)\n        >>> parse_request_range(\"foo=42\")\n        >>> parse_request_range(\"bytes=1-2,6-10\")\n\n    Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Parses a Range header.\n\n    Returns either ``None`` or an instance of ``slice``::\n\n        >>> rh = parse_request_range(\"bytes=1-2\")\n        >>> rh\n        slice(1, 3, None)\n        >>> [0, 1, 2, 3, 4][rh]\n        [1, 2]\n        >>> parse_request_range(\"bytes=6-\")\n        slice(6, None, None)\n        >>> parse_request_range(\"bytes=-6\")\n        slice(-6, None, None)\n        >>> parse_request_range(\"bytes=\")\n        slice(None, None, None)\n        >>> parse_request_range(\"foo=42\")\n        >>> parse_request_range(\"bytes=1-2,6-10\")\n\n    Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).\n\n    See [0] for the details of the range header.\n\n    [0]: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html#byte.ranges", "file": "tornado_httputil.py", "function": "parse_request_range", "project": "tornado", "change_stats": {"code_changes": 10, "doc_changes": 8, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _non_max_suppression_as_is(boxes: tf.Tensor,\n                               scores: tf.Tensor,\n                               output_size: int,\n                               iou_threshold: float = 0.5) -> tf.Tensor:\n  \n  \n  batch_shape = boxes.shape[:-2]\n  batch_size = np.prod(batch_shape, dtype=np.int32)\n  boxes_size = boxes.shape[-2]\n  if boxes.shape[-1] != 4:\n    raise ValueError(f'Boxes shape ({boxes.shape}) last dimension must be 4 '\n                     'to represent [y1, x1, y2, x2] boxes coordinates')\n  if scores.shape != boxes.shape[:-1]:\n    raise ValueError(f'Boxes shape ({boxes.shape}) and scores shape '\n                     f'({scores.shape}) do not match.')\n  order = tf.range(boxes_size, dtype=tf.float32)\n  relative_order = _tensor_sum_vectors(order, -order)\n  relative_scores = _tensor_sum_vectors(scores, -scores)\n  similar = _greater(_tensor_product_iou(boxes) - iou_threshold)\n  worse = _greater(relative_scores)\n  same_later = _and(_same(relative_scores), _greater(relative_order))\n  similar_worse_or_same_later = _and(similar, _or(worse, same_later))\n  prunable = _reduce_or(similar_worse_or_same_later, axis=-1)\n  remaining = tf.constant(1.) - prunable\n  scores = tf.reshape(tf.exp(scores), [1, 1, batch_size, boxes_size])\n  remaining = tf.reshape(remaining, [1, 1, batch_size, boxes_size])\n  # top_k runs on TPU cores, let it happen, TPU tiles implementation is slower.\n  top_k = tf.math.top_k(scores * remaining, output_size)\n  indices = (\n      tf.cast(top_k.indices, top_k.values.dtype) * _greater(top_k.values) -\n      _same(top_k.values))\n  return tf.reshape(indices, batch_shape + [output_size])```\n\nUpdated code:\n```python\ndef _non_max_suppression_as_is(boxes: tf.Tensor,\n                               scores: tf.Tensor,\n                               output_size: int,\n                               iou_threshold: float = 0.5,\n                               refinements: int = 0) -> tf.Tensor:\n  \n  \n  batch_shape = boxes.shape[:-2]\n  batch_size = np.prod(batch_shape, dtype=np.int32)\n  boxes_size = boxes.shape[-2]\n  if boxes.shape[-1] != 4:\n    raise ValueError(f'Boxes shape ({boxes.shape}) last dimension must be 4 '\n                     'to represent [y1, x1, y2, x2] boxes coordinates')\n  if scores.shape != boxes.shape[:-1]:\n    raise ValueError(f'Boxes shape ({boxes.shape}) and scores shape '\n                     f'({scores.shape}) do not match.')\n  order = tf.range(boxes_size, dtype=tf.float32)\n  relative_order = _tensor_sum_vectors(order, -order)\n  relative_scores = _tensor_sum_vectors(scores, -scores)\n  similar = _greater(_tensor_product_iou(boxes) - iou_threshold)\n  worse = _greater(relative_scores)\n  same_later = _and(_same(relative_scores), _greater(relative_order))\n  similar_worse_or_same_later = _and(similar, _or(worse, same_later))\n  for _ in range(refinements):\n    similar_worse_or_same_later = _refine_nms_graph_to_original_algorithm(\n        similar_worse_or_same_later)\n  prunable = _reduce_or(similar_worse_or_same_later, axis=-1)\n  remaining = tf.constant(1.) - prunable\n  scores = tf.reshape(tf.exp(scores), [1, 1, batch_size, boxes_size])\n  remaining = tf.reshape(remaining, [1, 1, batch_size, boxes_size])\n  # top_k runs on TPU cores, let it happen, TPU tiles implementation is slower.\n  top_k = tf.math.top_k(scores * remaining, output_size)\n  indices = (\n      tf.cast(top_k.indices, top_k.values.dtype) * _greater(top_k.values) -\n      _same(top_k.values))\n  return tf.reshape(indices, batch_shape + [output_size])\n```\n\nOriginal documentation:\n```python\nSelects a subset of boxes which have highest score among IOU-similar boxes.\n\n  Args:\n    boxes: A 2-D+ float `Tensor` of shape `[...batch_dims, num_boxes, 4]`.\n    scores: A 1-D+ float `Tensor` of shape `[...batch_dims, num_boxes]`\n      representing a single score corresponding to each box (each row of boxes).\n    output_size: A scalar integer `Tensor` representing the maximum number of\n      boxes to be selected by non-max suppression.\n    iou_threshold: A 0-D float tensor representing the threshold for deciding\n      whether boxes overlap too much with respect to IOU.\n\n  Returns:\n    A 1-D+ integer `Tensor` of shape `[...batch_dims, output_size]` representing\n    the selected indices from the boxes tensor and `-1` values for the padding.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Selects a subset of boxes which have highest score among IOU-similar boxes.\n\n  Args:\n    boxes: A 2-D+ float `Tensor` of shape `[...batch_dims, num_boxes, 4]`.\n    scores: A 1-D+ float `Tensor` of shape `[...batch_dims, num_boxes]`\n      representing a single score corresponding to each box (each row of boxes).\n    output_size: A scalar integer `Tensor` representing the maximum number of\n      boxes to be selected by non-max suppression.\n    iou_threshold: A 0-D float tensor representing the threshold for deciding\n      whether boxes overlap too much with respect to IOU.\n    refinements: A number of extra refinement steps to make result closer to\n      original sequencial NMS.\n\n  Returns:\n    A 1-D+ integer `Tensor` of shape `[...batch_dims, output_size]` representing\n    the selected indices from the boxes tensor and `-1` values for the padding.", "file": "official_vision_modeling_layers_edgetpu.py", "function": "_non_max_suppression_as_is", "project": "models", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(\n        self,\n        embedding: Embeddings,\n        *,\n        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,\n        table_name: str = \"embeddings\",\n        content_field: str = \"content\",\n        metadata_field: str = \"metadata\",\n        vector_field: str = \"vector\",\n        use_vector_index: bool = False,\n        vector_index_name: str = \"\",\n        vector_index_options: Optional[dict] = None,\n        vector_size: int = 1536,\n        pool_size: int = 5,\n        max_overflow: int = 10,\n        timeout: float = 30,\n        **kwargs: Any,\n    ):\n        \n        \n\n        self.embedding = embedding\n        self.distance_strategy = distance_strategy\n        self.table_name = self._sanitize_input(table_name)\n        self.content_field = self._sanitize_input(content_field)\n        self.metadata_field = self._sanitize_input(metadata_field)\n        self.vector_field = self._sanitize_input(vector_field)\n\n        self.use_vector_index = bool(use_vector_index)\n        self.vector_index_name = self._sanitize_input(vector_index_name)\n        self.vector_index_options = dict(vector_index_options or {})\n        self.vector_index_options[\"metric_type\"] = self.distance_strategy\n        self.vector_size = int(vector_size)\n\n        # Pass the rest of the kwargs to the connection.\n        self.connection_kwargs = kwargs\n\n        # Add program name and version to connection attributes.\n        if \"conn_attrs\" not in self.connection_kwargs:\n            self.connection_kwargs[\"conn_attrs\"] = dict()\n\n        self.connection_kwargs[\"conn_attrs\"][\"_connector_name\"] = \"langchain python sdk\"\n        self.connection_kwargs[\"conn_attrs\"][\"_connector_version\"] = \"1.0.2\"\n\n        # Create connection pool.\n        self.connection_pool = QueuePool(\n            self._get_connection,\n            max_overflow=max_overflow,\n            pool_size=pool_size,\n            timeout=timeout,\n        )\n        self._create_table()```\n\nUpdated code:\n```python\ndef __init__(\n        self,\n        embedding: Embeddings,\n        *,\n        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,\n        table_name: str = \"embeddings\",\n        content_field: str = \"content\",\n        metadata_field: str = \"metadata\",\n        vector_field: str = \"vector\",\n        id_field: str = \"id\",\n        use_vector_index: bool = False,\n        vector_index_name: str = \"\",\n        vector_index_options: Optional[dict] = None,\n        vector_size: int = 1536,\n        use_full_text_search: bool = False,\n        pool_size: int = 5,\n        max_overflow: int = 10,\n        timeout: float = 30,\n        **kwargs: Any,\n    ):\n        \n        \n\n        self.embedding = embedding\n        self.distance_strategy = distance_strategy\n        self.table_name = self._sanitize_input(table_name)\n        self.content_field = self._sanitize_input(content_field)\n        self.metadata_field = self._sanitize_input(metadata_field)\n        self.vector_field = self._sanitize_input(vector_field)\n        self.id_field = self._sanitize_input(id_field)\n\n        self.use_vector_index = bool(use_vector_index)\n        self.vector_index_name = self._sanitize_input(vector_index_name)\n        self.vector_index_options = dict(vector_index_options or {})\n        self.vector_index_options[\"metric_type\"] = self.distance_strategy\n        self.vector_size = int(vector_size)\n\n        self.use_full_text_search = bool(use_full_text_search)\n\n        # Pass the rest of the kwargs to the connection.\n        self.connection_kwargs = kwargs\n\n        # Add program name and version to connection attributes.\n        if \"conn_attrs\" not in self.connection_kwargs:\n            self.connection_kwargs[\"conn_attrs\"] = dict()\n\n        self.connection_kwargs[\"conn_attrs\"][\"_connector_name\"] = \"langchain python sdk\"\n        self.connection_kwargs[\"conn_attrs\"][\"_connector_version\"] = \"2.0.0\"\n\n        # Create connection pool.\n        self.connection_pool = QueuePool(\n            self._get_connection,\n            max_overflow=max_overflow,\n            pool_size=pool_size,\n            timeout=timeout,\n        )\n        self._create_table()\n```\n\nOriginal documentation:\n```python\nInitialize with necessary components.\n\n        Args:\n            embedding (Embeddings): A text embedding model.\n\n            distance_strategy (DistanceStrategy, optional):\n                Determines the strategy employed for calculating\n                the distance between vectors in the embedding space.\n                Defaults to DOT_PRODUCT.\n                Available options are:\n                - DOT_PRODUCT: Computes the scalar product of two vectors.\n                    This is the default behavior\n                - EUCLIDEAN_DISTANCE: Computes the Euclidean distance between\n                    two vectors. This metric considers the geometric distance in\n                    the vector space, and might be more suitable for embeddings\n                    that rely on spatial relationships.\n\n            table_name (str, optional): Specifies the name of the table in use.\n                Defaults to \"embeddings\".\n            content_field (str, optional): Specifies the field to store the content.\n                Defaults to \"content\".\n            metadata_field (str, optional): Specifies the field to store metadata.\n                Defaults to \"metadata\".\n            vector_field (str, optional): Specifies the field to store the vector.\n                Defaults to \"vector\".\n\n            use_vector_index (bool, optional): Toggles the use of a vector index.\n                Works only with SingleStoreDB 8.5 or later. Defaults to False.\n                If set to True, vector_size parameter is required to be set to\n                a proper value.\n\n            vector_index_name (str, optional): Specifies the name of the vector index.\n                Defaults to empty. Will be ignored if use_vector_index is set to False.\n\n            vector_index_options (dict, optional): Specifies the options for\n                the vector index. Defaults to {}.\n                Will be ignored if use_vector_index is set to False. The options are:\n                index_type (str, optional): Specifies the type of the index.\n                    Defaults to IVF_PQFS.\n                For more options, please refer to the SingleStoreDB documentation:\n                https://docs.singlestore.com/cloud/reference/sql-reference/vector-functions/vector-indexing/\n\n            vector_size (int, optional): Specifies the size of the vector.\n                Defaults to 1536. Required if use_vector_index is set to True.\n                Should be set to the same value as the size of the vectors\n                stored in the vector_field.\n\n            Following arguments pertain to the connection pool:\n\n            pool_size (int, optional): Determines the number of active connections in\n                the pool. Defaults to 5.\n            max_overflow (int, optional): Determines the maximum number of connections\n                allowed beyond the pool_size. Defaults to 10.\n            timeout (float, optional): Specifies the maximum wait time in seconds for\n                establishing a connection. Defaults to 30.\n\n            Following arguments pertain to the database connection:\n\n            host (str, optional): Specifies the hostname, IP address, or URL for the\n                database connection. The default scheme is \"mysql\".\n            user (str, optional): Database username.\n            password (str, optional): Database password.\n            port (int, optional): Database port. Defaults to 3306 for non-HTTP\n                connections, 80 for HTTP connections, and 443 for HTTPS connections.\n            database (str, optional): Database name.\n\n            Additional optional arguments provide further customization over the\n            database connection:\n\n            pure_python (bool, optional): Toggles the connector mode. If True,\n                operates in pure Python mode.\n            local_infile (bool, optional): Allows local file uploads.\n            charset (str, optional): Specifies the character set for string values.\n            ssl_key (str, optional): Specifies the path of the file containing the SSL\n                key.\n            ssl_cert (str, optional): Specifies the path of the file containing the SSL\n                certificate.\n            ssl_ca (str, optional): Specifies the path of the file containing the SSL\n                certificate authority.\n            ssl_cipher (str, optional): Sets the SSL cipher list.\n            ssl_disabled (bool, optional): Disables SSL usage.\n            ssl_verify_cert (bool, optional): Verifies the server's certificate.\n                Automatically enabled if ``ssl_ca`` is specified.\n            ssl_verify_identity (bool, optional): Verifies the server's identity.\n            conv (dict[int, Callable], optional): A dictionary of data conversion\n                functions.\n            credential_type (str, optional): Specifies the type of authentication to\n                use: auth.PASSWORD, auth.JWT, or auth.BROWSER_SSO.\n            autocommit (bool, optional): Enables autocommits.\n            results_type (str, optional): Determines the structure of the query results:\n                tuples, namedtuples, dicts.\n            results_format (str, optional): Deprecated. This option has been renamed to\n                results_type.\n\n        Examples:\n            Basic Usage:\n\n            .. code-block:: python\n\n                from langchain_community.embeddings import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                vectorstore = SingleStoreDB(\n                    OpenAIEmbeddings(),\n                    host=\"https://user:password@127.0.0.1:3306/database\"\n                )\n\n            Advanced Usage:\n\n            .. code-block:: python\n\n                from langchain_community.embeddings import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                vectorstore = SingleStoreDB(\n                    OpenAIEmbeddings(),\n                    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n                    host=\"127.0.0.1\",\n                    port=3306,\n                    user=\"user\",\n                    password=\"password\",\n                    database=\"db\",\n                    table_name=\"my_custom_table\",\n                    pool_size=10,\n                    timeout=60,\n                )\n\n            Using environment variables:\n\n            .. code-block:: python\n\n                from langchain_community.embeddings import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                os.environ['SINGLESTOREDB_URL'] = 'me:p455w0rd@s2-host.com/my_db'\n                vectorstore = SingleStoreDB(OpenAIEmbeddings())\n\n            Using vector index:\n\n            .. code-block:: python\n\n                from langchain_community.embeddings import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                os.environ['SINGLESTOREDB_URL'] = 'me:p455w0rd@s2-host.com/my_db'\n                vectorstore = SingleStoreDB(\n                    OpenAIEmbeddings(),\n                    use_vector_index=True,\n                )\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialize with necessary components.\n\n        Args:\n            embedding (Embeddings): A text embedding model.\n\n            distance_strategy (DistanceStrategy, optional):\n                Determines the strategy employed for calculating\n                the distance between vectors in the embedding space.\n                Defaults to DOT_PRODUCT.\n                Available options are:\n                - DOT_PRODUCT: Computes the scalar product of two vectors.\n                    This is the default behavior\n                - EUCLIDEAN_DISTANCE: Computes the Euclidean distance between\n                    two vectors. This metric considers the geometric distance in\n                    the vector space, and might be more suitable for embeddings\n                    that rely on spatial relationships. This metric is not\n                    compatible with the WEIGHTED_SUM search strategy.\n\n            table_name (str, optional): Specifies the name of the table in use.\n                Defaults to \"embeddings\".\n            content_field (str, optional): Specifies the field to store the content.\n                Defaults to \"content\".\n            metadata_field (str, optional): Specifies the field to store metadata.\n                Defaults to \"metadata\".\n            vector_field (str, optional): Specifies the field to store the vector.\n                Defaults to \"vector\".\n            id_field (str, optional): Specifies the field to store the id.\n                Defaults to \"id\".\n\n            use_vector_index (bool, optional): Toggles the use of a vector index.\n                Works only with SingleStoreDB 8.5 or later. Defaults to False.\n                If set to True, vector_size parameter is required to be set to\n                a proper value.\n\n            vector_index_name (str, optional): Specifies the name of the vector index.\n                Defaults to empty. Will be ignored if use_vector_index is set to False.\n\n            vector_index_options (dict, optional): Specifies the options for\n                the vector index. Defaults to {}.\n                Will be ignored if use_vector_index is set to False. The options are:\n                index_type (str, optional): Specifies the type of the index.\n                    Defaults to IVF_PQFS.\n                For more options, please refer to the SingleStoreDB documentation:\n                https://docs.singlestore.com/cloud/reference/sql-reference/vector-functions/vector-indexing/\n\n            vector_size (int, optional): Specifies the size of the vector.\n                Defaults to 1536. Required if use_vector_index is set to True.\n                Should be set to the same value as the size of the vectors\n                stored in the vector_field.\n\n            use_full_text_search (bool, optional): Toggles the use a full-text index\n                on the document content. Defaults to False. If set to True, the table\n                will be created with a full-text index on the content field,\n                and the simularity_search method will all using TEXT_ONLY,\n                FILTER_BY_TEXT, FILTER_BY_VECTOR, and WIGHTED_SUM search strategies.\n                If set to False, the simularity_search method will only allow\n                VECTOR_ONLY search strategy.\n\n            Following arguments pertain to the connection pool:\n\n            pool_size (int, optional): Determines the number of active connections in\n                the pool. Defaults to 5.\n            max_overflow (int, optional): Determines the maximum number of connections\n                allowed beyond the pool_size. Defaults to 10.\n            timeout (float, optional): Specifies the maximum wait time in seconds for\n                establishing a connection. Defaults to 30.\n\n            Following arguments pertain to the database connection:\n\n            host (str, optional): Specifies the hostname, IP address, or URL for the\n                database connection. The default scheme is \"mysql\".\n            user (str, optional): Database username.\n            password (str, optional): Database password.\n            port (int, optional): Database port. Defaults to 3306 for non-HTTP\n                connections, 80 for HTTP connections, and 443 for HTTPS connections.\n            database (str, optional): Database name.\n\n            Additional optional arguments provide further customization over the\n            database connection:\n\n            pure_python (bool, optional): Toggles the connector mode. If True,\n                operates in pure Python mode.\n            local_infile (bool, optional): Allows local file uploads.\n            charset (str, optional): Specifies the character set for string values.\n            ssl_key (str, optional): Specifies the path of the file containing the SSL\n                key.\n            ssl_cert (str, optional): Specifies the path of the file containing the SSL\n                certificate.\n            ssl_ca (str, optional): Specifies the path of the file containing the SSL\n                certificate authority.\n            ssl_cipher (str, optional): Sets the SSL cipher list.\n            ssl_disabled (bool, optional): Disables SSL usage.\n            ssl_verify_cert (bool, optional): Verifies the server's certificate.\n                Automatically enabled if ``ssl_ca`` is specified.\n            ssl_verify_identity (bool, optional): Verifies the server's identity.\n            conv (dict[int, Callable], optional): A dictionary of data conversion\n                functions.\n            credential_type (str, optional): Specifies the type of authentication to\n                use: auth.PASSWORD, auth.JWT, or auth.BROWSER_SSO.\n            autocommit (bool, optional): Enables autocommits.\n            results_type (str, optional): Determines the structure of the query results:\n                tuples, namedtuples, dicts.\n            results_format (str, optional): Deprecated. This option has been renamed to\n                results_type.\n\n        Examples:\n            Basic Usage:\n\n            .. code-block:: python\n\n                from langchain_openai import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                vectorstore = SingleStoreDB(\n                    OpenAIEmbeddings(),\n                    host=\"https://user:password@127.0.0.1:3306/database\"\n                )\n\n            Advanced Usage:\n\n            .. code-block:: python\n\n                from langchain_openai import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                vectorstore = SingleStoreDB(\n                    OpenAIEmbeddings(),\n                    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n                    host=\"127.0.0.1\",\n                    port=3306,\n                    user=\"user\",\n                    password=\"password\",\n                    database=\"db\",\n                    table_name=\"my_custom_table\",\n                    pool_size=10,\n                    timeout=60,\n                )\n\n            Using environment variables:\n\n            .. code-block:: python\n\n                from langchain_openai import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                os.environ['SINGLESTOREDB_URL'] = 'me:p455w0rd@s2-host.com/my_db'\n                vectorstore = SingleStoreDB(OpenAIEmbeddings())\n\n            Using vector index:\n\n            .. code-block:: python\n\n                from langchain_openai import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                os.environ['SINGLESTOREDB_URL'] = 'me:p455w0rd@s2-host.com/my_db'\n                vectorstore = SingleStoreDB(\n                    OpenAIEmbeddings(),\n                    use_vector_index=True,\n                )\n\n            Using full-text index:\n\n            .. code-block:: python\n                from langchain_openai import OpenAIEmbeddings\n                from langchain_community.vectorstores import SingleStoreDB\n\n                os.environ['SINGLESTOREDB_URL'] = 'me:p455w0rd@s2-host.com/my_db'\n                vectorstore = SingleStoreDB(\n                    OpenAIEmbeddings(),\n                    use_full_text_search=True,\n                )", "file": "libs_community_langchain_community_vectorstores_singlestoredb.py", "function": "SingleStoreDB.__init__", "project": "langchain", "change_stats": {"code_changes": 9, "doc_changes": 35, "total_changes": 44}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = None\n        request = {}\n        if symbol is not None:\n            market = self.market(symbol)\n            request['symbol'] = market['id']\n        if since is not None:\n            request['startTime'] = self.iso8601(since)\n        if limit is not None:\n            request['count'] = limit\n        request = self.deep_extend(request, params)\n        # why the hassle? urlencode in python is kinda broken for nested dicts.\n        # E.g. self.urlencode({\"filter\": {\"open\": True}}) will return \"filter={'open':+True}\"\n        # Bitmex doesn't like that. Hence resorting to self hack.\n        if 'filter' in request:\n            request['filter'] = self.json(request['filter'])\n        response = self.privateGetExecutionTradeHistory(request)\n        #\n        #     [\n        #         {\n        #             \"execID\": \"string\",\n        #             \"orderID\": \"string\",\n        #             \"clOrdID\": \"string\",\n        #             \"clOrdLinkID\": \"string\",\n        #             \"account\": 0,\n        #             \"symbol\": \"string\",\n        #             \"side\": \"string\",\n        #             \"lastQty\": 0,\n        #             \"lastPx\": 0,\n        #             \"underlyingLastPx\": 0,\n        #             \"lastMkt\": \"string\",\n        #             \"lastLiquidityInd\": \"string\",\n        #             \"simpleOrderQty\": 0,\n        #             \"orderQty\": 0,\n        #             \"price\": 0,\n        #             \"displayQty\": 0,\n        #             \"stopPx\": 0,\n        #             \"pegOffsetValue\": 0,\n        #             \"pegPriceType\": \"string\",\n        #             \"currency\": \"string\",\n        #             \"settlCurrency\": \"string\",\n        #             \"execType\": \"string\",\n        #             \"ordType\": \"string\",\n        #             \"timeInForce\": \"string\",\n        #             \"execInst\": \"string\",\n        #             \"contingencyType\": \"string\",\n        #             \"exDestination\": \"string\",\n        #             \"ordStatus\": \"string\",\n        #             \"triggered\": \"string\",\n        #             \"workingIndicator\": True,\n        #             \"ordRejReason\": \"string\",\n        #             \"simpleLeavesQty\": 0,\n        #             \"leavesQty\": 0,\n        #             \"simpleCumQty\": 0,\n        #             \"cumQty\": 0,\n        #             \"avgPx\": 0,\n        #             \"commission\": 0,\n        #             \"tradePublishIndicator\": \"string\",\n        #             \"multiLegReportingType\": \"string\",\n        #             \"text\": \"string\",\n        #             \"trdMatchID\": \"string\",\n        #             \"execCost\": 0,\n        #             \"execComm\": 0,\n        #             \"homeNotional\": 0,\n        #             \"foreignNotional\": 0,\n        #             \"transactTime\": \"2019-03-05T12:47:02.762Z\",\n        #             \"timestamp\": \"2019-03-05T12:47:02.762Z\"\n        #         }\n        #     ]\n        #\n        return self.parse_trades(response, market, since, limit)```\n\nUpdated code:\n```python\ndef fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):\n        \n        \n        \n        self.load_markets()\n        paginate = False\n        paginate, params = self.handle_option_and_params(params, 'fetchMyTrades', 'paginate')\n        if paginate:\n            return self.fetch_paginated_call_dynamic('fetchMyTrades', symbol, since, limit, params, 100)\n        market = None\n        request = {}\n        if symbol is not None:\n            market = self.market(symbol)\n            request['symbol'] = market['id']\n        if since is not None:\n            request['startTime'] = self.iso8601(since)\n        if limit is not None:\n            request['count'] = limit\n        until = self.safe_integer_2(params, 'until', 'endTime')\n        if until is not None:\n            params = self.omit(params, ['until'])\n            request['endTime'] = self.iso8601(until)\n        request = self.deep_extend(request, params)\n        # why the hassle? urlencode in python is kinda broken for nested dicts.\n        # E.g. self.urlencode({\"filter\": {\"open\": True}}) will return \"filter={'open':+True}\"\n        # Bitmex doesn't like that. Hence resorting to self hack.\n        if 'filter' in request:\n            request['filter'] = self.json(request['filter'])\n        response = self.privateGetExecutionTradeHistory(request)\n        #\n        #     [\n        #         {\n        #             \"execID\": \"string\",\n        #             \"orderID\": \"string\",\n        #             \"clOrdID\": \"string\",\n        #             \"clOrdLinkID\": \"string\",\n        #             \"account\": 0,\n        #             \"symbol\": \"string\",\n        #             \"side\": \"string\",\n        #             \"lastQty\": 0,\n        #             \"lastPx\": 0,\n        #             \"underlyingLastPx\": 0,\n        #             \"lastMkt\": \"string\",\n        #             \"lastLiquidityInd\": \"string\",\n        #             \"simpleOrderQty\": 0,\n        #             \"orderQty\": 0,\n        #             \"price\": 0,\n        #             \"displayQty\": 0,\n        #             \"stopPx\": 0,\n        #             \"pegOffsetValue\": 0,\n        #             \"pegPriceType\": \"string\",\n        #             \"currency\": \"string\",\n        #             \"settlCurrency\": \"string\",\n        #             \"execType\": \"string\",\n        #             \"ordType\": \"string\",\n        #             \"timeInForce\": \"string\",\n        #             \"execInst\": \"string\",\n        #             \"contingencyType\": \"string\",\n        #             \"exDestination\": \"string\",\n        #             \"ordStatus\": \"string\",\n        #             \"triggered\": \"string\",\n        #             \"workingIndicator\": True,\n        #             \"ordRejReason\": \"string\",\n        #             \"simpleLeavesQty\": 0,\n        #             \"leavesQty\": 0,\n        #             \"simpleCumQty\": 0,\n        #             \"cumQty\": 0,\n        #             \"avgPx\": 0,\n        #             \"commission\": 0,\n        #             \"tradePublishIndicator\": \"string\",\n        #             \"multiLegReportingType\": \"string\",\n        #             \"text\": \"string\",\n        #             \"trdMatchID\": \"string\",\n        #             \"execCost\": 0,\n        #             \"execComm\": 0,\n        #             \"homeNotional\": 0,\n        #             \"foreignNotional\": 0,\n        #             \"transactTime\": \"2019-03-05T12:47:02.762Z\",\n        #             \"timestamp\": \"2019-03-05T12:47:02.762Z\"\n        #         }\n        #     ]\n        #\n        return self.parse_trades(response, market, since, limit)\n```\n\nOriginal documentation:\n```python\nfetch all trades made by the user\n        :param str symbol: unified market symbol\n        :param int [since]: the earliest time in ms to fetch trades for\n        :param int [limit]: the maximum number of trades structures to retrieve\n        :param dict [params]: extra parameters specific to the bitmex api endpoint\n        :returns Trade[]: a list of `trade structures <https://github.com/ccxt/ccxt/wiki/Manual#trade-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "see https://www.bitmex.com/api/explorer/#not /Execution/Execution_getTradeHistory\n        fetch all trades made by the user\n        :param str symbol: unified market symbol\n        :param int [since]: the earliest time in ms to fetch trades for\n        :param int [limit]: the maximum number of trades structures to retrieve\n        :param dict [params]: extra parameters specific to the bitmex api endpoint\n        :param boolean [params.paginate]: default False, when True will automatically paginate by calling self endpoint multiple times. See in the docs all the [availble parameters]  (ttps://github.com/ccxt/ccxt/wiki/Manual#pagination-params)\n        :returns Trade[]: a list of `trade structures <https://github.com/ccxt/ccxt/wiki/Manual#trade-structure>`", "file": "python_ccxt_bitmex.py", "function": "bitmex.fetch_my_trades", "project": "ccxt", "change_stats": {"code_changes": 10, "doc_changes": 6, "total_changes": 16}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef distributed_circuit_2_propagation_impedance( distributed_admittance,\\\n        distributed_impedance):\n    '''\n    \n    '''\n    propagation_constant = \\\n            sqrt(distributed_impedance*distributed_admittance)\n    characteristic_impedance = \\\n            sqrt(distributed_impedance/distributed_admittance)\n    return (propagation_constant, characteristic_impedance)```\n\nUpdated code:\n```python\ndef distributed_circuit_2_propagation_impedance(distributed_admittance,\n        distributed_impedance):\n    \n    \n    propagation_constant = \\\n            sqrt(distributed_impedance*distributed_admittance)\n    characteristic_impedance = \\\n            sqrt(distributed_impedance/distributed_admittance)\n    return (propagation_constant, characteristic_impedance)\n```\n\nOriginal documentation:\n```python\nConverts distributed circuit values to wave quantities.\n\n    This converts complex distributed impedance and admittance to\n    propagation constant and characteristic impedance. The relation is\n\n    .. math::\n            Z_0 = \\\\sqrt{ \\\\frac{Z^{'}}{Y^{'}}}\n            \\\\quad\\\\quad\n            \\\\gamma = \\\\sqrt{ Z^{'}  Y^{'}}\n\n    Parameters\n    ------------\n    distributed_admittance : number, array-like\n            distributed admittance\n    distributed_impedance :  number, array-like\n            distributed impedance\n\n    Returns\n    ----------\n    propagation_constant : number, array-like\n            distributed impedance\n    characteristic_impedance : number, array-like\n            distributed impedance\n\n    See Also\n    ----------\n            propagation_impedance_2_distributed_circuit : opposite conversion\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n    Convert distrubuted circuit values to wave quantities.\n\n    This converts complex distributed impedance and admittance to\n    propagation constant and characteristic impedance. The relation is\n\n    .. math::\n            Z_0 = \\\\sqrt{ \\\\frac{Z^{'}}{Y^{'}}}\n            \\\\quad\\\\quad\n            \\\\gamma = \\\\sqrt{ Z^{'}  Y^{'}}\n\n    Parameters\n    ----------\n    distributed_admittance : number, array-like\n            distributed admittance\n    distributed_impedance :  number, array-like\n            distributed impedance\n\n    Returns\n    -------\n    propagation_constant : number, array-like\n            distributed impedance\n    characteristic_impedance : number, array-like\n            distributed impedance\n\n    See Also\n    --------\n            propagation_impedance_2_distributed_circuit : opposite conversion", "file": "skrf_tlineFunctions.py", "function": "distributed_circuit_2_propagation_impedance", "project": "scikit-rf", "change_stats": {"code_changes": 7, "doc_changes": 11, "total_changes": 18}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef register_post_execute(self, func):\n        \n        \n        warn(\"ip.register_post_execute is deprecated, use \"\n             \"ip.callbacks.register('post_run_cell', func) instead.\")\n        self.events.register('post_run_cell', func)```\n\nUpdated code:\n```python\ndef register_post_execute(self, func):\n        \n        \n        warn(\"ip.register_post_execute is deprecated, use \"\n             \"ip.events.register('post_run_cell', func) instead.\")\n        self.events.register('post_run_cell', func)\n```\n\nOriginal documentation:\n```python\nDEPRECATED: Use ip.callbacks.register('post_run_cell', func)\n        \n        Register a function for calling after code execution.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "DEPRECATED: Use ip.events.register('post_run_cell', func)\n        \n        Register a function for calling after code execution.", "file": "IPython_core_interactiveshell.py", "function": "InteractiveShell.register_post_execute", "project": "ipython", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef compute_transition_scores(\n        self,\n        sequences: torch.Tensor,\n        scores: Tuple[torch.Tensor],\n        beam_indices: Optional[torch.Tensor] = None,\n        normalize_logits: bool = False,\n    ) -> torch.Tensor:\n        \n        \n        # 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent\n        # to a beam search approach were the first (and only) beam is always selected\n        if beam_indices is None:\n            beam_indices = torch.arange(scores[0].shape[0]).view(-1, 1).to(sequences.device)\n            beam_indices = beam_indices.expand(-1, len(scores))\n\n        # 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being\n        # seq_len - input_length\n        scores = torch.stack(scores).reshape(len(scores), -1).transpose(0, 1)\n\n        # 3. Optionally normalize the logits (across the vocab dimension)\n        if normalize_logits:\n            scores = scores.reshape(-1, self.config.vocab_size, scores.shape[-1])\n            scores = torch.nn.functional.log_softmax(scores, dim=1)\n            scores = scores.reshape(-1, scores.shape[-1])\n\n        # 4. cut beam_indices to longest beam length\n        beam_indices_mask = beam_indices < 0\n        max_beam_length = (1 - beam_indices_mask.long()).sum(-1).max()\n        beam_indices = beam_indices[:, :max_beam_length]\n        beam_indices_mask = beam_indices_mask[:, :max_beam_length]\n\n        # 5. Set indices of beams that finished early to 0\n        # such indices will be masked correctly afterwards\n        beam_indices[beam_indices_mask] = 0\n\n        # 6. multiply beam_indices with vocab size to gather correctly from scores\n        beam_sequence_indices = beam_indices * self.config.vocab_size\n\n        # 7. Define which indices contributed to scores\n        cut_idx = sequences.shape[-1] - max_beam_length\n        indices = sequences[:, cut_idx:] + beam_sequence_indices\n\n        # 8. Compute scores\n        transition_scores = scores.gather(0, indices)\n\n        # 9. Mask out transition_scores of beams that stopped early\n        transition_scores[beam_indices_mask] = 0\n\n        return transition_scores```\n\nUpdated code:\n```python\ndef compute_transition_scores(\n        self,\n        sequences: torch.Tensor,\n        scores: Tuple[torch.Tensor],\n        beam_indices: Optional[torch.Tensor] = None,\n        normalize_logits: bool = False,\n    ) -> torch.Tensor:\n        \n        \n        # 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent\n        # to a beam search approach were the first (and only) beam is always selected\n        if beam_indices is None:\n            beam_indices = torch.arange(scores[0].shape[0]).view(-1, 1).to(sequences.device)\n            beam_indices = beam_indices.expand(-1, len(scores))\n\n        # 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being\n        # seq_len - input_length\n        scores = torch.stack(scores).reshape(len(scores), -1).transpose(0, 1)\n\n        # 3. Optionally normalize the logits (across the vocab dimension)\n        if normalize_logits:\n            scores = scores.reshape(-1, self.config.vocab_size, scores.shape[-1])\n            scores = torch.nn.functional.log_softmax(scores, dim=1)\n            scores = scores.reshape(-1, scores.shape[-1])\n\n        # 4. cut beam_indices to longest beam length\n        beam_indices_mask = beam_indices < 0\n        max_beam_length = (1 - beam_indices_mask.long()).sum(-1).max()\n        beam_indices = beam_indices.clone()[:, :max_beam_length]\n        beam_indices_mask = beam_indices_mask[:, :max_beam_length]\n\n        # 5. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards\n        beam_indices[beam_indices_mask] = 0\n\n        # 6. multiply beam_indices with vocab size to gather correctly from scores\n        beam_sequence_indices = beam_indices * self.config.vocab_size\n\n        # 7. Define which indices contributed to scores\n        cut_idx = sequences.shape[-1] - max_beam_length\n        indices = sequences[:, cut_idx:] + beam_sequence_indices\n\n        # 8. Compute scores\n        transition_scores = scores.gather(0, indices)\n\n        # 9. Mask out transition_scores of beams that stopped early\n        transition_scores[beam_indices_mask] = 0\n\n        return transition_scores\n```\n\nOriginal documentation:\n```python\nComputes the transition scores of sequences given the generation scores (and beam indices, if beam search was\n        used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.\n\n        Parameters:\n            sequences (`torch.LongTensor`):\n                The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or\n                shorter if all batches finished early due to the `eos_token_id`.\n            scores (`tuple(torch.FloatTensor)`):\n                Transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n                of log probabilities of tokens conditioned on log softmax of previously generated tokens Tuple of\n                `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token), with\n                each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n            beam_indices (`tuple(tuple(torch.LongTensor))`, *optional*):\n                Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n                `(batch_size*num_return_sequences, input_ids.shape[-1])`. Only required if a `num_beams>1` at\n                generate-time.\n            normalize_logits (`bool`, *optional*, defaults to `False`):\n                Whether to normalize the logits (which, for legacy reasons, may be unnormalized).\n\n        Return:\n            `torch.Tensor`: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)` containing\n                the transition scores (logits)\n\n        Examples:\n\n        ```python\n        >>> from transformers import GPT2Tokenizer, AutoModelForCausalLM\n        >>> import numpy as np\n\n        >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        >>> tokenizer.pad_token_id = tokenizer.eos_token_id\n        >>> inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n\n        >>> # Example 1: Print the scores for each token generated with Greedy Search\n        >>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n        >>> transition_scores = model.compute_transition_scores(\n        ...     outputs.sequences, outputs.scores, normalize_logits=True\n        ... )\n        >>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n        >>> # encoder-decoder models, like BART or T5.\n        >>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n        >>> generated_tokens = outputs.sequences[:, input_length:]\n        >>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n        ...     # | token | token string | logits | probability\n        ...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n        |   262 |  the     | -1.414 | 24.33%\n        |  1110 |  day     | -2.609 | 7.36%\n        |   618 |  when    | -2.010 | 13.40%\n        |   356 |  we      | -1.859 | 15.58%\n        |   460 |  can     | -2.508 | 8.14%\n\n        >>> # Example 2: Reconstruct the sequence scores from Beam Search\n        >>> outputs = model.generate(\n        ...     **inputs,\n        ...     max_new_tokens=5,\n        ...     num_beams=4,\n        ...     num_return_sequences=4,\n        ...     return_dict_in_generate=True,\n        ...     output_scores=True,\n        ... )\n        >>> transition_scores = model.compute_transition_scores(\n        ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n        ... )\n        >>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n        >>> # Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n        >>> # use case, you might want to recompute it with `normalize_logits=True`.\n        >>> output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n        >>> length_penalty = model.generation_config.length_penalty\n        >>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n        >>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n        True\n        ```\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Computes the transition scores of sequences given the generation scores (and beam indices, if beam search was\n        used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.\n\n        Parameters:\n            sequences (`torch.LongTensor`):\n                The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or\n                shorter if all batches finished early due to the `eos_token_id`.\n            scores (`tuple(torch.FloatTensor)`):\n                Transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n                of log probabilities of tokens conditioned on log softmax of previously generated tokens Tuple of\n                `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token), with\n                each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n            beam_indices (`torch.LongTensor`, *optional*):\n                Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n                `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams>1` at\n                generate-time.\n            normalize_logits (`bool`, *optional*, defaults to `False`):\n                Whether to normalize the logits (which, for legacy reasons, may be unnormalized).\n\n        Return:\n            `torch.Tensor`: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)` containing\n                the transition scores (logits)\n\n        Examples:\n\n        ```python\n        >>> from transformers import GPT2Tokenizer, AutoModelForCausalLM\n        >>> import numpy as np\n\n        >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        >>> tokenizer.pad_token_id = tokenizer.eos_token_id\n        >>> inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n\n        >>> # Example 1: Print the scores for each token generated with Greedy Search\n        >>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n        >>> transition_scores = model.compute_transition_scores(\n        ...     outputs.sequences, outputs.scores, normalize_logits=True\n        ... )\n        >>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n        >>> # encoder-decoder models, like BART or T5.\n        >>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n        >>> generated_tokens = outputs.sequences[:, input_length:]\n        >>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n        ...     # | token | token string | logits | probability\n        ...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n        |   262 |  the     | -1.414 | 24.33%\n        |  1110 |  day     | -2.609 | 7.36%\n        |   618 |  when    | -2.010 | 13.40%\n        |   356 |  we      | -1.859 | 15.58%\n        |   460 |  can     | -2.508 | 8.14%\n\n        >>> # Example 2: Reconstruct the sequence scores from Beam Search\n        >>> outputs = model.generate(\n        ...     **inputs,\n        ...     max_new_tokens=5,\n        ...     num_beams=4,\n        ...     num_return_sequences=4,\n        ...     return_dict_in_generate=True,\n        ...     output_scores=True,\n        ... )\n        >>> transition_scores = model.compute_transition_scores(\n        ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n        ... )\n        >>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n        >>> # Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n        >>> # use case, you might want to recompute it with `normalize_logits=True`.\n        >>> output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n        >>> length_penalty = model.generation_config.length_penalty\n        >>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n        >>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n        True\n        ```", "file": "src_transformers_generation_utils.py", "function": "GenerationMixin.compute_transition_scores", "project": "transformers", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef detect(self, image):\n        \n        \n        \n        start_time = time.time()\n        image = np.array(image)\n        self.image = image\n\n        # Image initialization\n        image = rgb2gray(crop(image, self.config.DETECTING_AREA))\n\n        # Perspective transform\n        image_trans = cv2.warpPerspective(image, self.homo_data, self.homo_size)\n\n        # Edge detection\n        image_edge = cv2.Canny(image_trans, *self.config.HOMO_CANNY_THRESHOLD)\n        image_edge = cv2.bitwise_and(image_edge, self.ui_mask_homo_stroke)\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n        image_edge = cv2.morphologyEx(image_edge, cv2.MORPH_CLOSE, kernel)\n        # Image.fromarray(image_edge, mode='L').show()\n\n        # Find free tile\n        if self.search_tile_center(image_edge, threshold_good=self.config.HOMO_CENTER_GOOD_THRESHOLD,\n                                   threshold=self.config.HOMO_CENTER_THRESHOLD):\n            pass\n        elif self.search_tile_corner(image_edge, threshold=self.config.HOMO_CORNER_THRESHOLD):\n            pass\n        elif self.search_tile_rectangle(image_edge, threshold=self.config.HOMO_RECTANGLE_THRESHOLD):\n            pass\n        else:\n            raise MapDetectionError('Failed to find a free tile')\n\n        self.homo_loca %= self.config.HOMO_TILE\n\n        # Detect map edges\n        self.lower_edge, self.upper_edge, self.left_edge, self.right_edge = False, False, False, False\n        self._map_edge_count = (0, 0)\n        if self.config.HOMO_EDGE_DETECT:\n            image_edge = cv2.bitwise_and(cv2.dilate(image_edge, kernel),\n                                         cv2.inRange(image_trans, *self.config.HOMO_EDGE_COLOR_RANGE))\n            image_edge = cv2.bitwise_and(image_edge, self.ui_mask_homo_stroke)\n            self.detect_edges(image_edge, hough_th=self.config.HOMO_EDGE_HOUGHLINES_THRESHOLD)\n\n        # Log\n        time_cost = round(time.time() - start_time, 3)\n        logger.info('%ss  %s   edge_lines: %s hori, %s vert' % (\n            float2str(time_cost), '_' if self.lower_edge else ' ',\n            self._map_edge_count[1], self._map_edge_count[0])\n                    )\n        logger.info('Edges: %s%s%s   homo_loca: %s' % (\n            '/' if self.left_edge else ' ', '_' if self.upper_edge else ' ', '\\\\' if self.right_edge else ' ',\n            point2str(*self.homo_loca, length=3))\n                    )```\n\nUpdated code:\n```python\ndef detect(self, image):\n        \n        \n        \n        start_time = time.time()\n        self.image = image\n\n        # Image initialization\n        image = rgb2gray(crop(image, self.config.DETECTING_AREA))\n\n        # Perspective transform\n        image_trans = cv2.warpPerspective(image, self.homo_data, self.homo_size)\n\n        # Edge detection\n        image_edge = cv2.Canny(image_trans, *self.config.HOMO_CANNY_THRESHOLD)\n        image_edge = cv2.bitwise_and(image_edge, self.ui_mask_homo_stroke)\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n        image_edge = cv2.morphologyEx(image_edge, cv2.MORPH_CLOSE, kernel)\n        # Image.fromarray(image_edge, mode='L').show()\n\n        # Find free tile\n        if self.search_tile_center(image_edge, threshold_good=self.config.HOMO_CENTER_GOOD_THRESHOLD,\n                                   threshold=self.config.HOMO_CENTER_THRESHOLD):\n            pass\n        elif self.search_tile_corner(image_edge, threshold=self.config.HOMO_CORNER_THRESHOLD):\n            pass\n        elif self.search_tile_rectangle(image_edge, threshold=self.config.HOMO_RECTANGLE_THRESHOLD):\n            pass\n        else:\n            raise MapDetectionError('Failed to find a free tile')\n\n        self.homo_loca %= self.config.HOMO_TILE\n\n        # Detect map edges\n        self.lower_edge, self.upper_edge, self.left_edge, self.right_edge = False, False, False, False\n        self._map_edge_count = (0, 0)\n        if self.config.HOMO_EDGE_DETECT:\n            image_edge = cv2.bitwise_and(cv2.dilate(image_edge, kernel),\n                                         cv2.inRange(image_trans, *self.config.HOMO_EDGE_COLOR_RANGE))\n            image_edge = cv2.bitwise_and(image_edge, self.ui_mask_homo_stroke)\n            self.detect_edges(image_edge, hough_th=self.config.HOMO_EDGE_HOUGHLINES_THRESHOLD)\n\n        # Log\n        time_cost = round(time.time() - start_time, 3)\n        logger.info('%ss  %s   edge_lines: %s hori, %s vert' % (\n            float2str(time_cost), '_' if self.lower_edge else ' ',\n            self._map_edge_count[1], self._map_edge_count[0])\n                    )\n        logger.info('Edges: %s%s%s   homo_loca: %s' % (\n            '/' if self.left_edge else ' ', '_' if self.upper_edge else ' ', '\\\\' if self.right_edge else ' ',\n            point2str(*self.homo_loca, length=3))\n                    )\n```\n\nOriginal documentation:\n```python\nArgs:\n            image: Screenshot.\n\n        Returns:\n            bool: If success.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            image (np.ndarray): Screenshot.\n\n        Returns:\n            bool: If success.", "file": "module_map_detection_homography.py", "function": "Homography.detect", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 3, "doc_changes": 4, "total_changes": 7}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _converse(self, utterances, lang, message):\n        \n        \n\n        # check for conversation time-out\n        self.active_skills = [skill for skill in self.active_skills\n                              if time.time() - skill[\n                                  1] <= self.converse_timeout * 60]\n\n        # check if any skill wants to handle utterance\n        for skill in copy(self.active_skills):\n            if self.do_converse(utterances, skill[0], lang, message):\n                # update timestamp, or there will be a timeout where\n                # intent stops conversing whether its being used or not\n                self.add_active_skill(skill[0])\n                return True\n        return False```\n\nUpdated code:\n```python\ndef _converse(self, utterances, lang, message):\n        \n        \n        # check for conversation time-out\n        self.active_skills = [skill for skill in self.active_skills\n                              if time.time() - skill[\n                                  1] <= self.converse_timeout * 60]\n\n        # check if any skill wants to handle utterance\n        for skill in copy(self.active_skills):\n            if self.do_converse(utterances, skill[0], lang, message):\n                # update timestamp, or there will be a timeout where\n                # intent stops conversing whether its being used or not\n                return IntentMatch('Converse', None, None, skill[0])\n        return None\n```\n\nOriginal documentation:\n```python\nGive active skills a chance at the utterance\n\n        Args:\n            utterances (list):  list of utterances\n            lang (string):      4 letter ISO language code\n            message (Message):  message to use to generate reply\n\n        Returns:\n            bool: True if converse handled it, False if  no skill processes it\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Give active skills a chance at the utterance\n\n        Arguments:\n            utterances (list):  list of utterances\n            lang (string):      4 letter ISO language code\n            message (Message):  message to use to generate reply\n\n        Returns:\n            IntentMatch if handled otherwise None.", "file": "mycroft_skills_intent_service.py", "function": "IntentService._converse", "project": "mycroft-core", "change_stats": {"code_changes": 8, "doc_changes": 6, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, dag, pipeline_name='DAGPipeline'):\n    \n    \n    # Expand DAG shorthand.\n    self.dag = dict(self._expand_dag_shorthands(dag))\n\n    # Make sure DAG is valid.\n    # DagInput types match output types. Nothing depends on outputs.\n    # Things that require input get input. DAG is composed of correct types.\n    for unit, dependency in self.dag.items():\n      if not isinstance(unit, (pipeline.Pipeline, DagOutput)):\n        raise InvalidDAGException(\n            'Dependency {%s: %s} is invalid. Left hand side value %s must '\n            'either be a Pipeline or DagOutput object'\n            % (unit, dependency, unit))\n      if isinstance(dependency, dict):\n        if not all([isinstance(name, six.string_types) for name in dependency]):\n          raise InvalidDAGException(\n              'Dependency {%s: %s} is invalid. Right hand side keys %s must be '\n              'strings' % (unit, dependency, dependency.keys()))\n        values = dependency.values()\n      else:\n        values = [dependency]\n      for subordinate in values:\n        if not (isinstance(subordinate, (pipeline.Pipeline, DagInput)) or\n                (isinstance(subordinate, pipeline.PipelineKey) and\n                 isinstance(subordinate.unit, pipeline.Pipeline))):\n          raise InvalidDAGException(\n              'Dependency {%s: %s} is invalid. Right hand side subordinate %s '\n              'must be either a Pipeline, PipelineKey, or DagInput object'\n              % (unit, dependency, subordinate))\n\n      # Check that all input types match output types.\n      if isinstance(unit, DagOutput):\n        # DagOutput objects don't know their types.\n        continue\n      if unit.input_type != self._get_type_signature_for_dependency(dependency):\n        raise TypeMismatchException(\n            'Invalid dependency {%s: %s}. Required `input_type` of left hand '\n            'side is %s. DagOutput type of right hand side is %s.'\n            % (unit, dependency, unit.input_type,\n               self._get_type_signature_for_dependency(dependency)))\n\n    # Make sure all Pipeline names are unique, so that Statistic objects don't\n    # clash.\n    sorted_unit_names = sorted(\n        [(unit, unit.name) for unit in self.dag],\n        key=lambda t: t[1])\n    for index, (unit, name) in enumerate(sorted_unit_names[:-1]):\n      if name == sorted_unit_names[index + 1][1]:\n        other_unit = sorted_unit_names[index + 1][0]\n        raise DuplicateNameException(\n            'Pipelines %s and %s both have name \"%s\". Each Pipeline must have '\n            'a unique name.' % (unit, other_unit, name))\n\n    # Find DagInput and DagOutput objects and make sure they are being used\n    # correctly.\n    self.outputs = [unit for unit in self.dag if isinstance(unit, DagOutput)]\n    self.output_names = dict((output.name, output) for output in self.outputs)\n    for output in self.outputs:\n      output.input_type = output.output_type = (\n          self._get_type_signature_for_dependency(self.dag[output]))\n    inputs = set()\n    for deps in self.dag.values():\n      units = self._get_units(deps)\n      for unit in units:\n        if isinstance(unit, DagInput):\n          inputs.add(unit)\n    if len(inputs) != 1:\n      if not inputs:\n        raise BadInputOrOutputException(\n            'No DagInput object found. DagInput is the start of the pipeline.')\n      else:\n        raise BadInputOrOutputException(\n            'Multiple DagInput objects found. Only one input is supported.')\n    if not self.outputs:\n      raise BadInputOrOutputException(\n          'No DagOutput objects found. DagOutput is the end of the pipeline.')\n    self.input = inputs.pop()\n\n    # Compute output_type for self and call super constructor.\n    output_signature = dict((output.name, output.output_type)\n                            for output in self.outputs)\n    super(DAGPipeline, self).__init__(\n        input_type=self.input.output_type,\n        output_type=output_signature,\n        name=pipeline_name)\n\n    # Make sure all Pipeline objects have DAG vertices that feed into them,\n    # and feed their output into other DAG vertices.\n    all_subordinates = (\n        set(dep_unit for unit in self.dag\n            for dep_unit in self._get_units(self.dag[unit]))\n        .difference(set([self.input])))\n    all_destinations = set(self.dag.keys()).difference(set(self.outputs))\n    if all_subordinates != all_destinations:\n      units_with_no_input = all_subordinates.difference(all_destinations)\n      units_with_no_output = all_destinations.difference(all_subordinates)\n      if units_with_no_input:\n        raise NotConnectedException(\n            '%s is given as a dependency in the DAG but has nothing connected '\n            'to it. Nothing in the DAG feeds into it.'\n            % units_with_no_input.pop())\n      else:\n        raise NotConnectedException(\n            '%s is given as a destination in the DAG but does not output '\n            'anywhere. It is a deadend.' % units_with_no_output.pop())\n\n    # Construct topological ordering to determine the execution order of the\n    # pipelines.\n    # https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm\n\n    # `graph` maps a pipeline to the pipelines it depends on. Each dict value\n    # is a list with the dependency pipelines in the 0th position, and a count\n    # of forward connections to the key pipeline (how many pipelines use this\n    # pipeline as a dependency).\n    graph = dict((unit, [self._get_units(self.dag[unit]), 0])\n                 for unit in self.dag)\n    graph[self.input] = [[], 0]\n    for unit, (forward_connections, _) in graph.items():\n      for to_unit in forward_connections:\n        graph[to_unit][1] += 1\n    self.call_list = call_list = []  # Topologically sorted elements go here.\n    nodes = set(self.outputs)\n    while nodes:\n      n = nodes.pop()\n      call_list.append(n)\n      for m in graph[n][0]:\n        graph[m][1] -= 1\n        if graph[m][1] == 0:\n          nodes.add(m)\n        elif graph[m][1] < 0:\n          raise Exception(\n              'Congratulations, you found a bug! Please report this issue at '\n              'https://github.com/tensorflow/magenta/issues and copy/paste the '\n              'following: dag=%s, graph=%s, call_list=%s' % (self.dag, graph,\n                                                             call_list))\n    # Check for cycles by checking if any edges remain.\n    for unit in graph:\n      if graph[unit][1] != 0:\n        raise BadTopologyException('Dependency loop found on %s' % unit)\n\n    # Note: this exception should never be raised. Disconnected graphs will be\n    # caught where NotConnectedException is raised. If this exception goes off\n    # there is likely a bug.\n    if set(call_list) != set(\n        list(all_subordinates) + self.outputs + [self.input]):\n      raise BadTopologyException('Not all pipelines feed into an output or '\n                                 'there is a dependency loop.')\n\n    call_list.reverse()\n    assert call_list[0] == self.input```\n\nUpdated code:\n```python\ndef __init__(self, dag, pipeline_name='DAGPipeline'):\n    \n    \n    # Expand DAG shorthand.\n    self.dag = dict(self._expand_dag_shorthands(dag))\n\n    # Make sure DAG is valid.\n    # DagInput types match output types. Nothing depends on outputs.\n    # Things that require input get input. DAG is composed of correct types.\n    for unit, dependency in self.dag.items():\n      if not isinstance(unit, (pipeline.Pipeline, DagOutput)):\n        raise InvalidDAGError(\n            'Dependency {%s: %s} is invalid. Left hand side value %s must '\n            'either be a Pipeline or DagOutput object'\n            % (unit, dependency, unit))\n      if isinstance(dependency, dict):\n        if not all([isinstance(name, six.string_types) for name in dependency]):\n          raise InvalidDAGError(\n              'Dependency {%s: %s} is invalid. Right hand side keys %s must be '\n              'strings' % (unit, dependency, dependency.keys()))\n        values = dependency.values()\n      else:\n        values = [dependency]\n      for subordinate in values:\n        if not (isinstance(subordinate, (pipeline.Pipeline, DagInput)) or\n                (isinstance(subordinate, pipeline.PipelineKey) and\n                 isinstance(subordinate.unit, pipeline.Pipeline))):\n          raise InvalidDAGError(\n              'Dependency {%s: %s} is invalid. Right hand side subordinate %s '\n              'must be either a Pipeline, PipelineKey, or DagInput object'\n              % (unit, dependency, subordinate))\n\n      # Check that all input types match output types.\n      if isinstance(unit, DagOutput):\n        # DagOutput objects don't know their types.\n        continue\n      if unit.input_type != self._get_type_signature_for_dependency(dependency):\n        raise TypeMismatchError(\n            'Invalid dependency {%s: %s}. Required `input_type` of left hand '\n            'side is %s. DagOutput type of right hand side is %s.'\n            % (unit, dependency, unit.input_type,\n               self._get_type_signature_for_dependency(dependency)))\n\n    # Make sure all Pipeline names are unique, so that Statistic objects don't\n    # clash.\n    sorted_unit_names = sorted(\n        [(unit, unit.name) for unit in self.dag],\n        key=lambda t: t[1])\n    for index, (unit, name) in enumerate(sorted_unit_names[:-1]):\n      if name == sorted_unit_names[index + 1][1]:\n        other_unit = sorted_unit_names[index + 1][0]\n        raise DuplicateNameError(\n            'Pipelines %s and %s both have name \"%s\". Each Pipeline must have '\n            'a unique name.' % (unit, other_unit, name))\n\n    # Find DagInput and DagOutput objects and make sure they are being used\n    # correctly.\n    self.outputs = [unit for unit in self.dag if isinstance(unit, DagOutput)]\n    self.output_names = dict((output.name, output) for output in self.outputs)\n    for output in self.outputs:\n      output.input_type = output.output_type = (\n          self._get_type_signature_for_dependency(self.dag[output]))\n    inputs = set()\n    for deps in self.dag.values():\n      units = self._get_units(deps)\n      for unit in units:\n        if isinstance(unit, DagInput):\n          inputs.add(unit)\n    if len(inputs) != 1:\n      if not inputs:\n        raise BadInputOrOutputError(\n            'No DagInput object found. DagInput is the start of the pipeline.')\n      else:\n        raise BadInputOrOutputError(\n            'Multiple DagInput objects found. Only one input is supported.')\n    if not self.outputs:\n      raise BadInputOrOutputError(\n          'No DagOutput objects found. DagOutput is the end of the pipeline.')\n    self.input = inputs.pop()\n\n    # Compute output_type for self and call super constructor.\n    output_signature = dict((output.name, output.output_type)\n                            for output in self.outputs)\n    super(DAGPipeline, self).__init__(\n        input_type=self.input.output_type,\n        output_type=output_signature,\n        name=pipeline_name)\n\n    # Make sure all Pipeline objects have DAG vertices that feed into them,\n    # and feed their output into other DAG vertices.\n    all_subordinates = (\n        set(dep_unit for unit in self.dag\n            for dep_unit in self._get_units(self.dag[unit]))\n        .difference(set([self.input])))\n    all_destinations = set(self.dag.keys()).difference(set(self.outputs))\n    if all_subordinates != all_destinations:\n      units_with_no_input = all_subordinates.difference(all_destinations)\n      units_with_no_output = all_destinations.difference(all_subordinates)\n      if units_with_no_input:\n        raise NotConnectedError(\n            '%s is given as a dependency in the DAG but has nothing connected '\n            'to it. Nothing in the DAG feeds into it.'\n            % units_with_no_input.pop())\n      else:\n        raise NotConnectedError(\n            '%s is given as a destination in the DAG but does not output '\n            'anywhere. It is a deadend.' % units_with_no_output.pop())\n\n    # Construct topological ordering to determine the execution order of the\n    # pipelines.\n    # https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm\n\n    # `graph` maps a pipeline to the pipelines it depends on. Each dict value\n    # is a list with the dependency pipelines in the 0th position, and a count\n    # of forward connections to the key pipeline (how many pipelines use this\n    # pipeline as a dependency).\n    graph = dict((unit, [self._get_units(self.dag[unit]), 0])\n                 for unit in self.dag)\n    graph[self.input] = [[], 0]\n    for unit, (forward_connections, _) in graph.items():\n      for to_unit in forward_connections:\n        graph[to_unit][1] += 1\n    self.call_list = call_list = []  # Topologically sorted elements go here.\n    nodes = set(self.outputs)\n    while nodes:\n      n = nodes.pop()\n      call_list.append(n)\n      for m in graph[n][0]:\n        graph[m][1] -= 1\n        if graph[m][1] == 0:\n          nodes.add(m)\n        elif graph[m][1] < 0:\n          raise Exception(\n              'Congratulations, you found a bug! Please report this issue at '\n              'https://github.com/tensorflow/magenta/issues and copy/paste the '\n              'following: dag=%s, graph=%s, call_list=%s' % (self.dag, graph,\n                                                             call_list))\n    # Check for cycles by checking if any edges remain.\n    for unit in graph:\n      if graph[unit][1] != 0:\n        raise BadTopologyError('Dependency loop found on %s' % unit)\n\n    # Note: this exception should never be raised. Disconnected graphs will be\n    # caught where NotConnectedError is raised. If this exception goes off\n    # there is likely a bug.\n    if set(call_list) != set(\n        list(all_subordinates) + self.outputs + [self.input]):\n      raise BadTopologyError('Not all pipelines feed into an output or '\n                             'there is a dependency loop.')\n\n    call_list.reverse()\n    assert call_list[0] == self.input\n```\n\nOriginal documentation:\n```python\nConstructs a DAGPipeline.\n\n    A DAG (direct acyclic graph) is given which fully specifies what the\n    DAGPipeline runs.\n\n    Args:\n      dag: A dictionary mapping `Pipeline` or `DagOutput` instances to any of\n         `Pipeline`, `PipelineKey`, `DagInput`. `dag` defines a directed acyclic\n         graph.\n      pipeline_name: String name of this Pipeline object.\n\n    Raises:\n      InvalidDAGException: If each key value pair in the `dag` dictionary is\n          not of the form\n          (Pipeline or DagOutput): (Pipeline, PipelineKey, or DagInput).\n      TypeMismatchException: The type signature of each key and value in `dag`\n          must match, otherwise this will be thrown.\n      DuplicateNameException: If two `Pipeline` instances in `dag` have the\n          same string name.\n      BadInputOrOutputException: If there are no `DagOutput` instaces in `dag`\n          or not exactly one `DagInput` plus type combination in `dag`.\n      InvalidDictionaryOutput: If `DagOutput()` is not connected to a\n          dictionary, or `DagOutput(name)` is not connected to a Pipeline,\n          PipelineKey, or DagInput instance.\n      NotConnectedException: If a `Pipeline` used in a dependency has nothing\n          feeding into it, or a `Pipeline` used as a destination does not feed\n          anywhere.\n      BadTopologyException: If there there is a directed cycle in `dag`.\n      Exception: Misc. exceptions.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Constructs a DAGPipeline.\n\n    A DAG (direct acyclic graph) is given which fully specifies what the\n    DAGPipeline runs.\n\n    Args:\n      dag: A dictionary mapping `Pipeline` or `DagOutput` instances to any of\n         `Pipeline`, `PipelineKey`, `DagInput`. `dag` defines a directed acyclic\n         graph.\n      pipeline_name: String name of this Pipeline object.\n\n    Raises:\n      InvalidDAGError: If each key value pair in the `dag` dictionary is\n          not of the form\n          (Pipeline or DagOutput): (Pipeline, PipelineKey, or DagInput).\n      TypeMismatchError: The type signature of each key and value in `dag`\n          must match, otherwise this will be thrown.\n      DuplicateNameError: If two `Pipeline` instances in `dag` have the\n          same string name.\n      BadInputOrOutputError: If there are no `DagOutput` instaces in `dag`\n          or not exactly one `DagInput` plus type combination in `dag`.\n      InvalidDictionaryOutputError: If `DagOutput()` is not connected to a\n          dictionary, or `DagOutput(name)` is not connected to a Pipeline,\n          PipelineKey, or DagInput instance.\n      NotConnectedError: If a `Pipeline` used in a dependency has nothing\n          feeding into it, or a `Pipeline` used as a destination does not feed\n          anywhere.\n      BadTopologyError: If there there is a directed cycle in `dag`.\n      Exception: Misc. exceptions.", "file": "magenta_pipelines_dag_pipeline.py", "function": "DAGPipeline.__init__", "project": "magenta", "change_stats": {"code_changes": 30, "doc_changes": 16, "total_changes": 46}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_ticklabels(self, ticklabels, update_ticks=True, *, minor=False,\n                       **kwargs):\n        \n        \n        \n        self._long_axis().set_ticklabels(ticklabels, minor=minor, **kwargs)```\n\nUpdated code:\n```python\ndef set_ticklabels(self, ticklabels, update_ticks=True):\n        \n        \n        \n        if isinstance(self.locator, ticker.FixedLocator):\n            self.formatter = ticker.FixedFormatter(ticklabels)\n        else:\n            _api._warn_external(\"set_ticks() must have been called.\")\n        self.stale = True\n```\n\nOriginal documentation:\n```python\nSet tick labels.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged, because of the dependency\n            on tick positions. In most cases, you'll want to use\n            ``set_ticks(positions, labels=labels)`` instead.\n\n            If you are using this method, you should always fix the tick\n            positions before, e.g. by using `.Colorbar.set_ticks` or by\n            explicitly setting a `~.ticker.FixedLocator` on the long axis\n            of the colorbar. Otherwise, ticks are free to move and the\n            labels may end up in unexpected positions.\n\n        Parameters\n        ----------\n        ticklabels : sequence of str or of `.Text`\n            Texts for labeling each tick location in the sequence set by\n            `.Colorbar.set_ticks`; the number of labels must match the number\n            of locations.\n\n        update_ticks : bool, default: True\n            This keyword argument is ignored and will be be removed.\n            Deprecated\n\n         minor : bool\n            If True, set minor ticks instead of major ticks.\n\n        **kwargs\n            `.Text` properties for the labels.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set tick labels.\n\n        Parameters\n        ----------\n        ticklabels : sequence of str or of `.Text`\n            Texts for labeling each tick location in the sequence set by\n            `.Axis.set_ticks`; the number of labels must match the number of\n            locations.\n\n        update_ticks : bool, default: True\n            This keyword argument is ignored and will be be removed.\n            Deprecated", "file": "lib_matplotlib_colorbar.py", "function": "Colorbar.set_ticklabels", "project": "matplotlib", "change_stats": {"code_changes": 11, "doc_changes": 24, "total_changes": 35}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _fill_zeros(result, y, fill):\n     \n    \n\n    if fill is not None:\n        if not isinstance(y, np.ndarray):\n            dtype, value = _infer_dtype_from_scalar(y)\n            y = pa.empty(result.shape, dtype=dtype)\n            y.fill(value)\n\n        if is_integer_dtype(y):\n\n            mask = y.ravel() == 0\n            if mask.any():\n                shape = result.shape\n                result, changed = _maybe_upcast_putmask(\n                    result.ravel(), mask, fill)\n                result = result.reshape(shape)\n\n    return result```\n\nUpdated code:\n```python\ndef _fill_zeros(result, x, y, name, fill):\n    \n    \n    \n\n    if fill is not None:\n\n        if name.startswith('r'):\n            x,y = y,x\n\n\n        if not isinstance(y, np.ndarray):\n            dtype, value = _infer_dtype_from_scalar(y)\n            y = pa.empty(result.shape, dtype=dtype)\n            y.fill(value)\n\n        if is_integer_dtype(y):\n\n            mask = y.ravel() == 0\n            if mask.any():\n                shape = result.shape\n                result = result.ravel().astype('float64')\n\n                signs = np.sign(result)\n                nans = np.isnan(x.ravel())\n                np.putmask(result, mask & ~nans, fill)\n\n                # if we have a fill of inf, then sign it\n                # correctly\n                # GH 6178\n                if np.isinf(fill):\n                    np.putmask(result,signs<0 & mask & ~nans,-fill)\n\n                result = result.reshape(shape)\n\n    return result\n```\n\nOriginal documentation:\n```python\nif we have an integer value (or array in y)\n    and we have 0's, fill them with the fill,\n    return the result\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "if this is a reversed op, then flip x,y\n\n    if we have an integer value (or array in y)\n    and we have 0's, fill them with the fill,\n    return the result\n\n    mask the nan's from x", "file": "pandas_core_common.py", "function": "_fill_zeros", "project": "pandas", "change_stats": {"code_changes": 26, "doc_changes": 8, "total_changes": 34}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef generate_model_report(\n        self, remove_inserted_observers: bool\n    ) -> Dict[str, Tuple[str, Dict]]:\n        \n        \n        # if we already removed the observers, we cannot generate report\n        if self._removed_observers:\n            raise Exception(\"Cannot generate report on model you already removed observers from\")\n\n        # keep track of all the reports of interest and their outputs\n        reports_of_interest = {}\n\n        for detector in self._desired_report_detectors:\n            # generate the individual report for the detector\n            report_output = detector.generate_detector_report(self._model)\n            reports_of_interest[detector.get_detector_name()] = report_output\n\n        # if user wishes to remove inserted observers, go ahead and remove\n        if remove_inserted_observers:\n            self._removed_observers = True\n            # get the set of all Observers inserted by this instance of ModelReport\n            all_observers_of_interest: Set[str] = set([])\n            for desired_report in self._detector_name_to_observer_fqns:\n                observers_of_interest = self._detector_name_to_observer_fqns[desired_report]\n                all_observers_of_interest.update(observers_of_interest)\n\n            # go through all_observers_of_interest and remove them from the graph and model\n            for observer_fqn in all_observers_of_interest:\n                # remove the observer from the model\n                self._model.delete_submodule(observer_fqn)\n\n                # remove the observer from the graph structure\n                node_obj = self._get_node_from_fqn(observer_fqn)\n\n                if node_obj:\n                    self._model.graph.erase_node(node_obj)\n                else:\n                    raise ValueError(\"Node no longer exists in GraphModule structure\")\n\n            # remember to recompile the model\n            self._model.recompile()\n\n        # return the reports of interest\n        return reports_of_interest```\n\nUpdated code:\n```python\ndef generate_model_report(\n        self, remove_inserted_observers: bool\n    ) -> Dict[str, Tuple[str, Dict]]:\n        \n        \n        # if we haven't prepped model for callibration, then we shouldn't generate report yet\n        if not self._prepared_flag:\n            raise Exception(\"Cannot generate report without preparing model for callibration\")\n\n        # if we already removed the observers, we cannot generate report\n        if self._removed_observers:\n            raise Exception(\"Cannot generate report on model you already removed observers from\")\n\n        # keep track of all the reports of interest and their outputs\n        reports_of_interest = {}\n\n        for detector in self._desired_report_detectors:\n            # generate the individual report for the detector\n            report_output = detector.generate_detector_report(self._model)\n            reports_of_interest[detector.get_detector_name()] = report_output\n\n        # if user wishes to remove inserted observers, go ahead and remove\n        if remove_inserted_observers:\n            self._removed_observers = True\n            # get the set of all Observers inserted by this instance of ModelReport\n            all_observers_of_interest: Set[str] = set([])\n            for desired_report in self._detector_name_to_observer_fqns:\n                observers_of_interest = self._detector_name_to_observer_fqns[desired_report]\n                all_observers_of_interest.update(observers_of_interest)\n\n            # go through all_observers_of_interest and remove them from the graph and model\n            for observer_fqn in all_observers_of_interest:\n                # remove the observer from the model\n                self._model.delete_submodule(observer_fqn)\n\n                # remove the observer from the graph structure\n                node_obj = self._get_node_from_fqn(observer_fqn)\n\n                if node_obj:\n                    self._model.graph.erase_node(node_obj)\n                else:\n                    raise ValueError(\"Node no longer exists in GraphModule structure\")\n\n            # remember to recompile the model\n            self._model.recompile()\n\n        # save the generated reports for visualization purposes\n        saved_reports: Dict[str, Dict] = {\n            report_name : report_tuple[1] for report_name, report_tuple in reports_of_interest.items()\n        }\n\n        self._generated_reports = saved_reports\n\n        # return the reports of interest\n        return reports_of_interest\n```\n\nOriginal documentation:\n```python\nr\"\"\"\n        Generates all the requested reports.\n\n        Note:\n            You should have callibrated the model with relavent data before calling this\n\n        The reports generated are specified by the desired_reports specified in desired_reports\n\n        Can optionally remove all the observers inserted by the ModelReport instance\n\n        Args:\n            remove_inserted_observers (bool): True to remove the observers inserted by this ModelReport instance\n\n        Returns a mapping of each desired report name to a tuple with:\n            The textual summary of that report information\n            A dictionary containing relavent statistics or information for that report\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "r\"\"\"\n        Generates all the requested reports.\n\n        Note:\n            You should have callibrated the model with relavent data before calling this\n\n        The reports generated are specified by the desired_reports specified in desired_reports\n\n        Can optionally remove all the observers inserted by the ModelReport instance\n\n        Args:\n            remove_inserted_observers (bool): True to remove the observers inserted by this ModelReport instance\n\n        Returns a mapping of each desired report name to a tuple with:\n            The textual summary of that report information\n            A dictionary containing relavent statistics or information for that report\n\n        Note:\n            Throws exception if we try to generate report on model we already removed observers from\n            Throws exception if we try to generate report without preparing for callibration", "file": "torch_ao_quantization_fx__model_report_model_report.py", "function": "ModelReport.generate_model_report", "project": "pytorch", "change_stats": {"code_changes": 13, "doc_changes": 6, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef native_value(self) -> str:\n        \n        return self.addon_info[self.attribute_name]```\n\nUpdated code:\n```python\ndef native_value(self) -> str:\n        \n        return self.coordinator.data[DATA_KEY_ADDONS][self._addon_slug][\n            self.entity_description.key\n        ]\n```\n\nOriginal documentation:\n```python\nReturn state of entity.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return native value of entity.", "file": "homeassistant_components_hassio_sensor.py", "function": "HassioAddonSensor.native_value", "project": "core", "change_stats": {"code_changes": 6, "doc_changes": 4, "total_changes": 10}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef distance(self, lat, lon):\n        \n        return location.distance(self.latitude, self.longitude, lat, lon)```\n\nUpdated code:\n```python\ndef distance(self: object, lat: float, lon: float) -> float:\n        \n        return self.units.length(\n            location.distance(self.latitude, self.longitude, lat, lon), 'm')\n```\n\nOriginal documentation:\n```python\nCalculate distance from Home Assistant in meters.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Calculate distance from Home Assistant.", "file": "homeassistant_core.py", "function": "Config.distance", "project": "core", "change_stats": {"code_changes": 7, "doc_changes": 4, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_number_of_zones(self) -> int:\n        \n        \n        return len(self.device.color_zones) if self.device.color_zones else 0```\n\nUpdated code:\n```python\ndef get_number_of_zones(self) -> int:\n        \n        \n        return len(self.device.color_zones) if self.device.color_zones else 1\n```\n\nOriginal documentation:\n```python\nReturn the number of zones.\n\n        If the number of zones is not yet populated, return 0\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the number of zones.\n\n        If the number of zones is not yet populated, return 1 since\n        the device will have a least one zone.", "file": "homeassistant_components_lifx_coordinator.py", "function": "LIFXUpdateCoordinator.get_number_of_zones", "project": "core", "change_stats": {"code_changes": 4, "doc_changes": 5, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef predict_tag(image):\n        \n        \n        \n        return ''```\n\nUpdated code:\n```python\ndef predict_tag(image):\n        \n        \n        \n        return None\n```\n\nOriginal documentation:\n```python\nArgs:\n            image: Pillow image. tag_area of the item.\n            Replace this method to predict tags.\n\n        Returns:\n            str: Tags are like `catchup`, `bonus`. Default to ''\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Args:\n            image: Pillow image. tag_area of the item.\n            Replace this method to predict tags.\n\n        Returns:\n            str: Tags are like `catchup`, `bonus`. Default to None", "file": "module_statistics_item.py", "function": "ItemGrid.predict_tag", "project": "AzurLaneAutoScript", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef convert_12term_2_8term(coefs_12term, redundant_k = False):\n    \n    \n    \n\n    # Nomenclature taken from Roger Marks\n    Edf = coefs_12term['forward directivity']\n    Esf = coefs_12term['forward source match']\n    Erf = coefs_12term['forward reflection tracking']\n    Etf = coefs_12term['forward transmission tracking']\n    Elf = coefs_12term['forward load match']\n    Eif = coefs_12term.get('forward isolation',0)\n\n    Edr = coefs_12term['reverse directivity']\n    Esr = coefs_12term['reverse source match']\n    Err = coefs_12term['reverse reflection tracking']\n    Elr = coefs_12term['reverse load match']\n    Etr = coefs_12term['reverse transmission tracking']\n    Eir = coefs_12term.get('reverse isolation',0)\n\n    # these are given in eq (30) - (33) in Roger Mark's paper listed in\n    # the docstring\n    # NOTE: k = e10/e23 = alpha/beta\n    #   the 'k' nomenclature is from Soares Speciale\n    gamma_f = (Elf - Esr)/(Err + Edr*(Elf - Esr))\n    gamma_r = (Elr - Esf)/(Erf + Edf*(Elr - Esf))\n\n    k_first  =   Etf/(Err + Edr*(Elf  - Esr) )\n    k_second =1/(Etr/(Erf + Edf *(Elr - Esf)))\n    k = (k_first + k_second)/2.\n    coefs_8term = {}\n    for l in ['forward directivity','forward source match',\n        'forward reflection tracking','reverse directivity',\n        'reverse reflection tracking','reverse source match',\n        'forward isolation', 'reverse isolation']:\n        coefs_8term[l] = coefs_12term[l].copy()\n\n    coefs_8term['forward switch term'] = gamma_f\n    coefs_8term['reverse switch term'] = gamma_r\n    coefs_8term['k'] = k\n    if redundant_k:\n        coefs_8term['k first'] = k_first\n        coefs_8term['k second'] = k_second\n    return coefs_8term```\n\nUpdated code:\n```python\ndef convert_12term_2_8term(coefs_12term, redundant_k = False):\n    \n    \n\n    \n\n    # Nomenclature taken from Roger Marks\n    Edf = coefs_12term['forward directivity']\n    Esf = coefs_12term['forward source match']\n    Erf = coefs_12term['forward reflection tracking']\n    Etf = coefs_12term['forward transmission tracking']\n    Elf = coefs_12term['forward load match']\n    Eif = coefs_12term.get('forward isolation',0)\n\n    Edr = coefs_12term['reverse directivity']\n    Esr = coefs_12term['reverse source match']\n    Err = coefs_12term['reverse reflection tracking']\n    Elr = coefs_12term['reverse load match']\n    Etr = coefs_12term['reverse transmission tracking']\n    Eir = coefs_12term.get('reverse isolation',0)\n\n    # these are given in eq (30) - (33) in Roger Mark's paper listed in\n    # the docstring\n    # NOTE: k = e10/e23 = alpha/beta\n    #   the 'k' nomenclature is from Soares Speciale\n    gamma_f = (Elf - Esr)/(Err + Edr*(Elf - Esr))\n    gamma_r = (Elr - Esf)/(Erf + Edf*(Elr - Esf))\n\n    k_first  =   Etf/(Err + Edr*(Elf  - Esr) )\n    k_second =1/(Etr/(Erf + Edf *(Elr - Esf)))\n    k = (k_first + k_second)/2.\n    coefs_8term = {}\n    for l in ['forward directivity','forward source match',\n        'forward reflection tracking','reverse directivity',\n        'reverse reflection tracking','reverse source match',\n        'forward isolation', 'reverse isolation']:\n        coefs_8term[l] = coefs_12term[l].copy()\n\n    coefs_8term['forward switch term'] = gamma_f\n    coefs_8term['reverse switch term'] = gamma_r\n    coefs_8term['k'] = k\n    if redundant_k:\n        coefs_8term['k first'] = k_first\n        coefs_8term['k second'] = k_second\n    return coefs_8term\n```\n\nOriginal documentation:\n```python\nConvert the 12-term and 8-term error coefficients.\n\n\n    Derivation of this conversion can be found in [#]_ .\n\n    References\n    ----------\n\n    .. [#] Marks, Roger B.; , \"Formulations of the Basic Vector Network Analyzer Error Model including Switch-Terms,\" ARFTG Conference Digest-Fall, 50th , vol.32, no., pp.115-126, Dec. 1997. URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4119948&isnumber=4119931\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert the 12-term and 8-term error coefficients.\n\n\n    Derivation of this conversion can be found in [#]_ .\n\n    References\n    ----------\n\n    .. [#] Marks, Roger B.; , \"Formulations of the Basic Vector Network Analyzer Error Model including Switch-Terms,\"\n        ARFTG Conference Digest-Fall, 50th , vol.32, no., pp.115-126, Dec. 1997.\n        URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4119948&isnumber=4119931", "file": "skrf_calibration_calibration.py", "function": "convert_12term_2_8term", "project": "scikit-rf", "change_stats": {"code_changes": 3, "doc_changes": 6, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_observation_pairs(\n    study: Study, param_name: str\n) -> Tuple[List[Optional[float]], List[Tuple[float, float]]]:\n    \n    \n\n    sign = 1\n    if study.direction == StudyDirection.MAXIMIZE:\n        sign = -1\n\n    values = []\n    scores = []\n    for trial in study._storage.get_all_trials(study._study_id, deepcopy=False):\n        if trial.state is TrialState.COMPLETE and trial.value is not None:\n            score = (-float(\"inf\"), sign * trial.value)\n        elif trial.state is TrialState.PRUNED:\n            if len(trial.intermediate_values) > 0:\n                step, intermediate_value = max(trial.intermediate_values.items())\n                if math.isnan(intermediate_value):\n                    score = (-step, float(\"inf\"))\n                else:\n                    score = (-step, sign * intermediate_value)\n            else:\n                score = (float(\"inf\"), 0.0)\n        else:\n            continue\n\n        param_value = None  # type: Optional[float]\n        if param_name in trial.params:\n            distribution = trial.distributions[param_name]\n            param_value = distribution.to_internal_repr(trial.params[param_name])\n\n        values.append(param_value)\n        scores.append(score)\n\n    return values, scores```\n\nUpdated code:\n```python\ndef _get_observation_pairs(\n    study: Study, param_name: str, trial: FrozenTrial\n) -> Tuple[List[Optional[float]], List[Tuple[float, float]]]:\n\n    \n    \n\n    sign = 1\n    if study.direction == StudyDirection.MAXIMIZE:\n        sign = -1\n\n    values = []\n    scores = []\n    for trial in study._storage.get_all_trials(study._study_id, deepcopy=False):\n        if trial.state is TrialState.COMPLETE and trial.value is not None:\n            score = (-float(\"inf\"), sign * trial.value)\n        elif trial.state is TrialState.PRUNED:\n            if len(trial.intermediate_values) > 0:\n                step, intermediate_value = max(trial.intermediate_values.items())\n                if math.isnan(intermediate_value):\n                    score = (-step, float(\"inf\"))\n                else:\n                    score = (-step, sign * intermediate_value)\n            else:\n                score = (float(\"inf\"), 0.0)\n        else:\n            continue\n\n        param_value: Optional[float] = None\n        if param_name in trial.params:\n            distribution = trial.distributions[param_name]\n            param_value = distribution.to_internal_repr(trial.params[param_name])\n\n        values.append(param_value)\n        scores.append(score)\n\n    return values, scores\n```\n\nOriginal documentation:\n```python\nGet observation pairs from the study.\n\n    This function collects observation pairs from the complete or pruned trials of the study.\n    The values for trials that don't contain the parameter named ``param_name`` are set to None.\n\n    An observation pair fundamentally consists of a parameter value and an objective value.\n    However, due to the pruning mechanism of Optuna, final objective values are not always\n    available. Therefore, this function uses intermediate values in addition to the final\n    ones, and reports the value with its step count as ``(-step, value)``.\n    Consequently, the structure of the observation pair is as follows:\n    ``(param_value, (-step, value))``.\n\n    The second element of an observation pair is used to rank observations in\n    ``_split_observation_pairs`` method (i.e., observations are sorted lexicographically by\n    ``(-step, value)``).\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get observation pairs from the study.\n\n       This function collects observation pairs from the complete or pruned trials of the study.\n       The values for trials that don't contain the parameter named ``param_name`` are set to None.\n\n       An observation pair fundamentally consists of a parameter value and an objective value.\n       However, due to the pruning mechanism of Optuna, final objective values are not always\n       available. Therefore, this function uses intermediate values in addition to the final\n       ones, and reports the value with its step count as ``(-step, value)``.\n       Consequently, the structure of the observation pair is as follows:\n       ``(param_value, (-step, value))``.\n\n       The second element of an observation pair is used to rank observations in\n       ``_split_observation_pairs`` method (i.e., observations are sorted lexicographically by\n       ``(-step, value)``).", "file": "optuna_samplers__tpe_sampler.py", "function": "_get_observation_pairs", "project": "optuna", "change_stats": {"code_changes": 7, "doc_changes": 24, "total_changes": 31}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef ohlcv(self, pair: str, tick_interval: str = None, copy: bool = True) -> DataFrame:\n        \n        \n        \n        if self.runmode in (RunMode.DRY_RUN, RunMode.LIVE):\n            if tick_interval:\n                pairtick = (pair, tick_interval)\n            else:\n                pairtick = (pair, self._config['ticker_interval'])\n\n            return self._exchange.klines(pairtick, copy=copy)\n        else:\n            return DataFrame()```\n\nUpdated code:\n```python\ndef ohlcv(self, pair: str, ticker_interval: str = None, copy: bool = True) -> DataFrame:\n        \n        \n        \n        if self.runmode in (RunMode.DRY_RUN, RunMode.LIVE):\n            if ticker_interval:\n                pairtick = (pair, ticker_interval)\n            else:\n                pairtick = (pair, self._config['ticker_interval'])\n\n            return self._exchange.klines(pairtick, copy=copy)\n        else:\n            return DataFrame()\n```\n\nOriginal documentation:\n```python\nget ohlcv data for the given pair as DataFrame\n        Please check `available_pairs` to verify which pairs are currently cached.\n        :param pair: pair to get the data for\n        :param tick_interval: ticker_interval to get pair for\n        :param copy: copy dataframe before returning.\n                     Use false only for RO operations (where the dataframe is not modified)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "get ohlcv data for the given pair as DataFrame\n        Please check `available_pairs` to verify which pairs are currently cached.\n        :param pair: pair to get the data for\n        :param ticker_interval: ticker_interval to get pair for\n        :param copy: copy dataframe before returning.\n                     Use false only for RO operations (where the dataframe is not modified)", "file": "freqtrade_data_dataprovider.py", "function": "DataProvider.ohlcv", "project": "freqtrade", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef nice_duration(duration, lang=None, speech=True):\n     \n    \n    if type(duration) is datetime.timedelta:\n        duration = duration.total_seconds()\n\n    # Do traditional rounding: 2.5->3, 3.5->4, plus this\n    # helps in a few cases of where calculations generate\n    # times like 2:59:59.9 instead of 3:00.\n    duration += 0.5\n\n    days = int(duration // 86400)\n    hours = int(duration // 3600 % 24)\n    minutes = int(duration // 60 % 60)\n    seconds = int(duration % 60)\n\n    if speech:\n        out = \"\"\n        if days > 0:\n            out += pronounce_number(days, lang) + \" \"\n            if days == 1:\n                out += _translate_word(\"day\", lang)\n            else:\n                out += _translate_word(\"days\", lang)\n            out += \" \"\n        if hours > 0:\n            if out:\n                out += \" \"\n            out += pronounce_number(hours, lang) + \" \"\n            if hours == 1:\n                out += _translate_word(\"hour\", lang)\n            else:\n                out += _translate_word(\"hours\", lang)\n        if minutes > 0:\n            if out:\n                out += \" \"\n            out += pronounce_number(minutes, lang) + \" \"\n            if minutes == 1:\n                out += _translate_word(\"minute\", lang)\n            else:\n                out += _translate_word(\"minutes\", lang)\n        if seconds > 0:\n            if out:\n                out += \" \"\n            out += pronounce_number(seconds, lang) + \" \"\n            if seconds == 1:\n                out += _translate_word(\"second\", lang)\n            else:\n                out += _translate_word(\"seconds\", lang)\n    else:\n        # M:SS, MM:SS, H:MM:SS, Dd H:MM:SS format\n        out = \"\"\n        if days > 0:\n            out = str(days) + \"d \"\n        if hours > 0 or days > 0:\n            out += str(hours) + \":\"\n        if minutes < 10 and (hours > 0 or days > 0):\n            out += \"0\"\n        out += str(minutes)+\":\"\n        if seconds < 10:\n            out += \"0\"\n        out += str(seconds)\n\n    return out```\n\nUpdated code:\n```python\ndef nice_duration(duration, lang=None, speech=True, use_years=True,\n                  resolution=TimeResolution.SECONDS):\n     \n    \n    return _duration_handler(duration, lang=lang, speech=speech,\n                             use_years=use_years, resolution=resolution)\n```\n\nOriginal documentation:\n```python\nConvert duration in seconds to a nice spoken timespan\n\n    Examples:\n       duration = 60  ->  \"1:00\" or \"one minute\"\n       duration = 163  ->  \"2:43\" or \"two minutes forty three seconds\"\n\n    Args:\n        duration: time, in seconds\n        lang (str, optional): a BCP-47 language code, None for default\n        speech (bool): format for speech (True) or display (False)\n    Returns:\n        str: timespan as a string\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Convert duration in seconds to a nice spoken timespan\n\n    Accepts:\n        time, in seconds, or datetime.timedelta\n\n    Examples:\n       duration = 60  ->  \"1:00\" or \"one minute\"\n       duration = 163  ->  \"2:43\" or \"two minutes forty three seconds\"\n       duration = timedelta(seconds=120)  ->  \"2:00\" or \"two minutes\"\n\n    Args:\n        duration (int/float/datetime.timedelta)\n        lang (str, optional): a BCP-47 language code, None for default\n        speech (bool, opt): format output for speech (True) or display (False)\n        use_years (bool, opt): rtn years and days if True, total days if False\n        resolution (mycroft.util.format.TimeResolution, optional): lower bound\n\n            mycroft.util.format.TimeResolution values:\n                TimeResolution.YEARS\n                TimeResolution.DAYS\n                TimeResolution.HOURS\n                TimeResolution.MINUTES\n                TimeResolution.SECONDS\n                TimeResolution.MILLISECONDS\n            NOTE: nice_duration will not produce milliseconds\n            unless that resolution is passed.\n\n    Returns:\n        str: timespan as a string", "file": "mycroft_util_format.py", "function": "nice_duration", "project": "mycroft-core", "change_stats": {"code_changes": 68, "doc_changes": 23, "total_changes": 91}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _main_shell_completion(\n        self,\n        ctx_args: t.Dict[str, t.Any],\n        prog_name: str,\n        complete_var: t.Optional[str] = None,\n    ) -> None:\n        \n        \n        if complete_var is None:\n            complete_var = f\"_{prog_name}_COMPLETE\".replace(\"-\", \"_\").upper()\n\n        instruction = os.environ.get(complete_var)\n\n        if not instruction:\n            return\n\n        from .shell_completion import shell_complete\n\n        rv = shell_complete(self, ctx_args, prog_name, complete_var, instruction)\n        sys.exit(rv)```\n\nUpdated code:\n```python\ndef _main_shell_completion(\n        self,\n        ctx_args: t.MutableMapping[str, t.Any],\n        prog_name: str,\n        complete_var: t.Optional[str] = None,\n    ) -> None:\n        \n        \n        if complete_var is None:\n            complete_name = prog_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n            complete_var = f\"_{complete_name}_COMPLETE\".upper()\n\n        instruction = os.environ.get(complete_var)\n\n        if not instruction:\n            return\n\n        from .shell_completion import shell_complete\n\n        rv = shell_complete(self, ctx_args, prog_name, complete_var, instruction)\n        sys.exit(rv)\n```\n\nOriginal documentation:\n```python\nCheck if the shell is asking for tab completion, process\n        that, then exit early. Called from :meth:`main` before the\n        program is invoked.\n\n        :param prog_name: Name of the executable in the shell.\n        :param complete_var: Name of the environment variable that holds\n            the completion instruction. Defaults to\n            ``_{PROG_NAME}_COMPLETE``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if the shell is asking for tab completion, process\n        that, then exit early. Called from :meth:`main` before the\n        program is invoked.\n\n        :param prog_name: Name of the executable in the shell.\n        :param complete_var: Name of the environment variable that holds\n            the completion instruction. Defaults to\n            ``_{PROG_NAME}_COMPLETE``.\n\n        .. versionchanged:: 8.2.0\n            Dots (``.``) in ``prog_name`` are replaced with underscores (``_``).", "file": "pipenv_vendor_click_core.py", "function": "BaseCommand._main_shell_completion", "project": "pipenv", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef lexsort_indexer(\n    keys,\n    orders=None,\n    na_position: str = \"last\",\n    key: Callable | None = None,\n    codes_given: bool = False,\n) -> npt.NDArray[np.intp]:\n    \n    \n    \n    from pandas.core.arrays import Categorical\n\n    labels = []\n    shape = []\n    if isinstance(orders, bool):\n        orders = [orders] * len(keys)\n    elif orders is None:\n        orders = [True] * len(keys)\n\n    keys = [ensure_key_mapped(k, key) for k in keys]\n\n    for k, order in zip(keys, orders):\n        if na_position not in [\"last\", \"first\"]:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        if codes_given:\n            mask = k == -1\n            codes = k.copy()\n            n = len(codes)\n            mask_n = n\n            if mask.any():\n                n -= 1\n\n        else:\n            cat = Categorical(k, ordered=True)\n            n = len(cat.categories)\n            codes = cat.codes.copy()\n            mask = cat.codes == -1\n            if mask.any():\n                mask_n = n + 1\n            else:\n                mask_n = n\n\n        if order:  # ascending\n            if na_position == \"last\":\n                codes = np.where(mask, n, codes)\n            elif na_position == \"first\":\n                codes += 1\n        else:  # not order means descending\n            if na_position == \"last\":\n                codes = np.where(mask, n, n - codes - 1)\n            elif na_position == \"first\":\n                codes = np.where(mask, 0, n - codes)\n\n        shape.append(mask_n)\n        labels.append(codes)\n\n    return indexer_from_factorized(labels, tuple(shape))```\n\nUpdated code:\n```python\ndef lexsort_indexer(\n    keys: list[ArrayLike] | list[Series],\n    orders=None,\n    na_position: str = \"last\",\n    key: Callable | None = None,\n    codes_given: bool = False,\n) -> npt.NDArray[np.intp]:\n    \n    \n    \n    from pandas.core.arrays import Categorical\n\n    labels = []\n    shape = []\n    if isinstance(orders, bool):\n        orders = [orders] * len(keys)\n    elif orders is None:\n        orders = [True] * len(keys)\n\n    # error: Incompatible types in assignment (expression has type\n    # \"List[Union[ExtensionArray, ndarray[Any, Any], Index, Series]]\", variable\n    # has type \"Union[List[Union[ExtensionArray, ndarray[Any, Any]]], List[Series]]\")\n    keys = [ensure_key_mapped(k, key) for k in keys]  # type: ignore[assignment]\n\n    for k, order in zip(keys, orders):\n        if na_position not in [\"last\", \"first\"]:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        if codes_given:\n            mask = k == -1\n            codes = k.copy()\n            n = len(codes)\n            mask_n = n\n            # error: Item \"ExtensionArray\" of \"Union[Any, ExtensionArray,\n            # ndarray[Any, Any]]\" has no attribute \"any\"\n            if mask.any():  # type: ignore[union-attr]\n                n -= 1\n\n        else:\n            cat = Categorical(k, ordered=True)\n            n = len(cat.categories)\n            codes = cat.codes.copy()\n            mask = cat.codes == -1\n            if mask.any():\n                mask_n = n + 1\n            else:\n                mask_n = n\n\n        if order:  # ascending\n            if na_position == \"last\":\n                # error: Argument 1 to \"where\" has incompatible type \"Union[Any,\n                # ExtensionArray, ndarray[Any, Any]]\"; expected\n                # \"Union[_SupportsArray[dtype[Any]],\n                # _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float,\n                # complex, str, bytes, _NestedSequence[Union[bool, int, float,\n                # complex, str, bytes]]]\"\n                codes = np.where(mask, n, codes)  # type: ignore[arg-type]\n            elif na_position == \"first\":\n                # error: Incompatible types in assignment (expression has type\n                # \"Union[Any, int, ndarray[Any, dtype[signedinteger[Any]]]]\",\n                # variable has type \"Union[Series, ExtensionArray, ndarray[Any, Any]]\")\n                # error: Unsupported operand types for + (\"ExtensionArray\" and \"int\")\n                codes += 1  # type: ignore[operator,assignment]\n        else:  # not order means descending\n            if na_position == \"last\":\n                # error: Unsupported operand types for - (\"int\" and \"ExtensionArray\")\n                # error: Argument 1 to \"where\" has incompatible type \"Union[Any,\n                # ExtensionArray, ndarray[Any, Any]]\"; expected\n                # \"Union[_SupportsArray[dtype[Any]],\n                # _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float,\n                # complex, str, bytes, _NestedSequence[Union[bool, int, float,\n                # complex, str, bytes]]]\"\n                codes = np.where(\n                    mask, n, n - codes - 1  # type: ignore[operator,arg-type]\n                )\n            elif na_position == \"first\":\n                # error: Unsupported operand types for - (\"int\" and \"ExtensionArray\")\n                # error: Argument 1 to \"where\" has incompatible type \"Union[Any,\n                # ExtensionArray, ndarray[Any, Any]]\"; expected\n                # \"Union[_SupportsArray[dtype[Any]],\n                # _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float,\n                # complex, str, bytes, _NestedSequence[Union[bool, int, float,\n                # complex, str, bytes]]]\"\n                codes = np.where(mask, 0, n - codes)  # type: ignore[operator,arg-type]\n\n        shape.append(mask_n)\n        labels.append(codes)\n\n    return indexer_from_factorized(labels, tuple(shape))\n```\n\nOriginal documentation:\n```python\nPerforms lexical sorting on a set of keys\n\n    Parameters\n    ----------\n    keys : sequence of arrays\n        Sequence of ndarrays to be sorted by the indexer\n    orders : bool or list of booleans, optional\n        Determines the sorting order for each element in keys. If a list,\n        it must be the same length as keys. This determines whether the\n        corresponding element in keys should be sorted in ascending\n        (True) or descending (False) order. if bool, applied to all\n        elements as above. if None, defaults to True.\n    na_position : {'first', 'last'}, default 'last'\n        Determines placement of NA elements in the sorted list (\"last\" or \"first\")\n    key : Callable, optional\n        Callable key function applied to every element in keys before sorting\n    codes_given: bool, False\n        Avoid categorical materialization if codes are already provided.\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Performs lexical sorting on a set of keys\n\n    Parameters\n    ----------\n    keys : list[ArrayLike] | list[Series]\n        Sequence of ndarrays to be sorted by the indexer\n        list[Series] is only if key is not None.\n    orders : bool or list of booleans, optional\n        Determines the sorting order for each element in keys. If a list,\n        it must be the same length as keys. This determines whether the\n        corresponding element in keys should be sorted in ascending\n        (True) or descending (False) order. if bool, applied to all\n        elements as above. if None, defaults to True.\n    na_position : {'first', 'last'}, default 'last'\n        Determines placement of NA elements in the sorted list (\"last\" or \"first\")\n    key : Callable, optional\n        Callable key function applied to every element in keys before sorting\n    codes_given: bool, False\n        Avoid categorical materialization if codes are already provided.\n\n    Returns\n    -------\n    np.ndarray[np.intp]", "file": "pandas_core_sorting.py", "function": "lexsort_indexer", "project": "pandas", "change_stats": {"code_changes": 47, "doc_changes": 5, "total_changes": 52}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_ncf_input_data(params, producer=None, input_meta_data=None):\n  \n  \n  if params[\"train_dataset_path\"]:\n    assert params[\"eval_dataset_path\"]\n\n    train_dataset = create_dataset_from_tf_record_files(\n        params[\"train_dataset_path\"],\n        input_meta_data[\"train_prebatch_size\"],\n        params[\"batch_size\"],\n        is_training=True)\n    eval_dataset = create_dataset_from_tf_record_files(\n        params[\"eval_dataset_path\"],\n        input_meta_data[\"eval_prebatch_size\"],\n        params[\"eval_batch_size\"],\n        is_training=False)\n\n    num_train_steps = int(input_meta_data[\"num_train_steps\"])\n    num_eval_steps = int(input_meta_data[\"num_eval_steps\"])\n  else:\n    if params[\"use_tpu\"]:\n      raise ValueError(\"TPU training does not support data producer yet. \"\n                       \"Use pre-processed data.\")\n\n    assert producer\n    # Start retrieving data from producer.\n    train_dataset, eval_dataset = create_dataset_from_data_producer(\n        producer, params)\n    num_train_steps = producer.train_batches_per_epoch\n    num_eval_steps = producer.eval_batches_per_epoch\n\n  return train_dataset, eval_dataset, num_train_steps, num_eval_steps```\n\nUpdated code:\n```python\ndef create_ncf_input_data(params,\n                          producer=None,\n                          input_meta_data=None,\n                          strategy=None):\n  \n  \n  # NCF evaluation metric calculation logic assumes that evaluation data\n  # sample size are in multiples of (1 + number of negative samples in\n  # evaluation) for each device. As so, evaluation batch size must be a\n  # multiple of (number of replicas * (1 + number of negative samples)).\n  num_devices = strategy.num_replicas_in_sync if strategy else 1\n  if (params[\"eval_batch_size\"] % (num_devices *\n                                   (1 + rconst.NUM_EVAL_NEGATIVES))):\n    raise ValueError(\"Evaluation batch size must be divisible by {} \"\n                     \"times {}\".format(num_devices,\n                                       (1 + rconst.NUM_EVAL_NEGATIVES)))\n\n  if params[\"train_dataset_path\"]:\n    assert params[\"eval_dataset_path\"]\n\n    train_dataset = create_dataset_from_tf_record_files(\n        params[\"train_dataset_path\"],\n        input_meta_data[\"train_prebatch_size\"],\n        params[\"batch_size\"],\n        is_training=True)\n    eval_dataset = create_dataset_from_tf_record_files(\n        params[\"eval_dataset_path\"],\n        input_meta_data[\"eval_prebatch_size\"],\n        params[\"eval_batch_size\"],\n        is_training=False)\n\n    num_train_steps = int(input_meta_data[\"num_train_steps\"])\n    num_eval_steps = int(input_meta_data[\"num_eval_steps\"])\n  else:\n    if params[\"use_tpu\"]:\n      raise ValueError(\"TPU training does not support data producer yet. \"\n                       \"Use pre-processed data.\")\n\n    assert producer\n    # Start retrieving data from producer.\n    train_dataset, eval_dataset = create_dataset_from_data_producer(\n        producer, params)\n    num_train_steps = producer.train_batches_per_epoch\n    num_eval_steps = producer.eval_batches_per_epoch\n\n  return train_dataset, eval_dataset, num_train_steps, num_eval_steps\n```\n\nOriginal documentation:\n```python\nCreates NCF training/evaluation dataset.\n\n  Args:\n    params: Dictionary containing parameters for train/evaluation data.\n    producer: Instance of BaseDataConstructor that generates data online. Must\n      not be None when params['train_dataset_path'] or\n      params['eval_dataset_path'] is not specified.\n    input_meta_data: A dictionary of input metadata to be used when reading data\n      from tf record files. Must be specified when params[\"train_input_dataset\"]\n      is specified.\n\n  Returns:\n    (training dataset, evaluation dataset, train steps per epoch,\n    eval steps per epoch)\n\n  Raises:\n    ValueError: If data is being generated online for when using TPU's.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Creates NCF training/evaluation dataset.\n\n  Args:\n    params: Dictionary containing parameters for train/evaluation data.\n    producer: Instance of BaseDataConstructor that generates data online. Must\n      not be None when params['train_dataset_path'] or\n      params['eval_dataset_path'] is not specified.\n    input_meta_data: A dictionary of input metadata to be used when reading data\n      from tf record files. Must be specified when params[\"train_input_dataset\"]\n      is specified.\n    strategy: Distribution strategy used for distributed training. If specified,\n      used to assert that evaluation batch size is correctly a multiple of\n      total number of devices used.\n\n  Returns:\n    (training dataset, evaluation dataset, train steps per epoch,\n    eval steps per epoch)\n\n  Raises:\n    ValueError: If data is being generated online for when using TPU's.", "file": "official_recommendation_ncf_input_pipeline.py", "function": "create_ncf_input_data", "project": "models", "change_stats": {"code_changes": 18, "doc_changes": 5, "total_changes": 23}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_order(row, col):\n    \n    \n    if _is_c_order(row, col):\n        return 'C'\n    if _is_c_order(col, row):\n        return 'F'\n    return None```\n\nUpdated code:\n```python\ndef get_order(row, col):\n    \n    \n    if _is_c_order(row, col):\n        return 'C'\n    if _is_c_order(col, row):\n        return 'F'\n    return 'other'\n```\n\nOriginal documentation:\n```python\nCheck if a coo matrix with given row and col is C or F order.\n\n    Args:\n        row (numpy.ndarray or cupy.ndarray): The row indices of the matrix\n            entries.\n        col (numpy.ndarray or cupy.ndarray): The column indices of the matrix\n            entries.\n\n    Returns:\n        Returns ``'C'`` when a coo matrix with given row and column indices is\n        C order, in other words, the row indices are sorted. Returns ``'F'``\n        when it is F order, in other words, the column indices are sorted.\n        Returns ``None`` otherwise.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Check if a coo matrix with given row and col is C or F order.\n\n    Args:\n        row (numpy.ndarray or cupy.ndarray): The row indices of the matrix\n            entries.\n        col (numpy.ndarray or cupy.ndarray): The column indices of the matrix\n            entries.\n\n    Returns:\n        Returns ``'C'`` when a coo matrix with given row and column indices is\n        C order, in other words, the row indices are sorted. Returns ``'F'``\n        when it is F order, in other words, the column indices are sorted.\n        Returns ``'other'`` otherwise.", "file": "chainer_utils_sparse.py", "function": "get_order", "project": "chainer", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef deploy_certificate(self, cert_name, domains, privkey_path,\n                           cert_path, chain_path, fullchain_path):\n        \n\n        \n        if self.installer is None:\n            logger.error(\"No installer specified, client is unable to deploy\"\n                           \"the certificate\")\n            raise errors.Error(\"No installer available\")\n\n        chain_path = None if chain_path is None else os.path.abspath(chain_path)\n\n        display_util.notify(\"Deploying certificate\")\n\n        msg = f\"Failed to install the certificate (installer: {self.config.installer}).\"\n        if cert_name:\n            msg += (\" Try again after fixing errors by running:\\n\\n\"\n                    f\"  {cli.cli_constants.cli_command} install --cert-name {cert_name}\\n\")\n\n        with error_handler.ErrorHandler(self._recovery_routine_with_msg, msg):\n            for dom in domains:\n                self.installer.deploy_cert(\n                    domain=dom, cert_path=os.path.abspath(cert_path),\n                    key_path=os.path.abspath(privkey_path),\n                    chain_path=chain_path,\n                    fullchain_path=fullchain_path)\n                self.installer.save()  # needed by the Apache plugin\n\n            self.installer.save(\"Deployed ACME Certificate\")\n\n        msg = (\"We were unable to install your certificate, \"\n               \"however, we successfully restored your \"\n               \"server to its prior configuration.\")\n        with error_handler.ErrorHandler(self._rollback_and_restart, msg):\n            # sites may have been enabled / final cleanup\n            self.installer.restart()```\n\nUpdated code:\n```python\ndef deploy_certificate(self, domains, privkey_path, cert_path, chain_path, fullchain_path):\n        \n\n        \n        if self.installer is None:\n            logger.error(\"No installer specified, client is unable to deploy\"\n                           \"the certificate\")\n            raise errors.Error(\"No installer available\")\n\n        chain_path = None if chain_path is None else os.path.abspath(chain_path)\n\n        display_util.notify(\"Deploying certificate\")\n\n        msg = \"Could not install certificate\"\n        with error_handler.ErrorHandler(self._recovery_routine_with_msg, msg):\n            for dom in domains:\n                self.installer.deploy_cert(\n                    domain=dom, cert_path=os.path.abspath(cert_path),\n                    key_path=os.path.abspath(privkey_path),\n                    chain_path=chain_path,\n                    fullchain_path=fullchain_path)\n                self.installer.save()  # needed by the Apache plugin\n\n            self.installer.save(\"Deployed ACME Certificate\")\n\n        msg = (\"We were unable to install your certificate, \"\n               \"however, we successfully restored your \"\n               \"server to its prior configuration.\")\n        with error_handler.ErrorHandler(self._rollback_and_restart, msg):\n            # sites may have been enabled / final cleanup\n            self.installer.restart()\n```\n\nOriginal documentation:\n```python\nInstall certificate\n\n        :param str cert_name: name of the certificate lineage (optional)\n        :param list domains: list of domains to install the certificate\n        :param str privkey_path: path to certificate private key\n        :param str cert_path: certificate file path (optional)\n        :param str chain_path: chain file path\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Install certificate\n\n        :param list domains: list of domains to install the certificate\n        :param str privkey_path: path to certificate private key\n        :param str cert_path: certificate file path (optional)\n        :param str chain_path: chain file path", "file": "certbot_certbot__internal_client.py", "function": "Client.deploy_certificate", "project": "certbot", "change_stats": {"code_changes": 11, "doc_changes": 3, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _remove_resource_field(\n        self,\n        resource_logical_id: str,\n        resource_type: str,\n        resource_dict: Dict,\n        linked_resources: Set[str] = set(),\n        built_resource_dict: Optional[Dict] = None,\n    ) -> Optional[str]:\n        \n        \n        \n        processed_logical_id = None\n\n        if resource_type == AWS_LAMBDA_FUNCTION:\n            for field in LAMBDA_FUNCTION_REMOVAL_MAP.get(resource_type, {}).get(\"Code\", []):\n                # We sanitize only if the provided resource is local\n                # Lambda function's Code property accepts dictionary values\n                if (\n                    built_resource_dict\n                    and isinstance(built_resource_dict.get(\"Properties\", {}).get(\"Code\"), dict)\n                    and is_local_path(built_resource_dict.get(\"Properties\", {}).get(\"Code\", {}).get(field, None))\n                ) or resource_logical_id in linked_resources:\n                    resource_dict.get(\"Properties\", {}).get(\"Code\", {}).pop(field, None)\n                    processed_logical_id = resource_logical_id\n                # SAM templates also accepts local paths for AWS::Lambda::Function's Code property\n                # Which will be transformed into a dict containing S3Bucket and S3Key after packaging\n                if (\n                    built_resource_dict\n                    and isinstance(built_resource_dict.get(\"Properties\", {}).get(\"Code\"), str)\n                    and is_local_path(built_resource_dict.get(\"Properties\", {}).get(\"Code\"))\n                ):\n                    resource_dict.get(\"Properties\", {}).get(\"Code\", {}).pop(\"S3Bucket\", None)\n                    resource_dict.get(\"Properties\", {}).get(\"Code\", {}).pop(\"S3Key\", None)\n                    processed_logical_id = resource_logical_id\n        else:\n            for field in GENERAL_REMOVAL_MAP.get(resource_type, []):\n                if resource_type in SYNCABLE_STACK_RESOURCES:\n                    if not isinstance(resource_dict.get(\"Properties\", {}).get(field, None), dict):\n                        resource_dict.get(\"Properties\", {}).pop(field, None)\n                        processed_logical_id = resource_logical_id\n                elif (\n                    built_resource_dict and is_local_path(built_resource_dict.get(\"Properties\", {}).get(field, None))\n                ) or resource_logical_id in linked_resources:\n                    resource_dict.get(\"Properties\", {}).pop(field, None)\n                    processed_logical_id = resource_logical_id\n\n        return processed_logical_id```\n\nUpdated code:\n```python\ndef _remove_resource_field(\n        self,\n        resource_logical_id: str,\n        resource_type: str,\n        resource_dict: Dict,\n        linked_resources: Optional[Set[str]] = None,\n        built_resource_dict: Optional[Dict] = None,\n    ) -> Optional[str]:\n        \n        \n        \n        linked_resources = linked_resources or set()\n        processed_logical_id = None\n\n        if resource_type == AWS_LAMBDA_FUNCTION:\n            for field in LAMBDA_FUNCTION_REMOVAL_MAP.get(resource_type, {}).get(\"Code\", []):\n                # We sanitize only if the provided resource is local\n                # Lambda function's Code property accepts dictionary values\n                if (\n                    built_resource_dict\n                    and isinstance(built_resource_dict.get(\"Properties\", {}).get(\"Code\"), dict)\n                    and is_local_path(built_resource_dict.get(\"Properties\", {}).get(\"Code\", {}).get(field, None))\n                ) or resource_logical_id in linked_resources:\n                    resource_dict.get(\"Properties\", {}).get(\"Code\", {}).pop(field, None)\n                    processed_logical_id = resource_logical_id\n                # SAM templates also accepts local paths for AWS::Lambda::Function's Code property\n                # Which will be transformed into a dict containing S3Bucket and S3Key after packaging\n                if (\n                    built_resource_dict\n                    and isinstance(built_resource_dict.get(\"Properties\", {}).get(\"Code\"), str)\n                    and is_local_path(built_resource_dict.get(\"Properties\", {}).get(\"Code\"))\n                ):\n                    resource_dict.get(\"Properties\", {}).get(\"Code\", {}).pop(\"S3Bucket\", None)\n                    resource_dict.get(\"Properties\", {}).get(\"Code\", {}).pop(\"S3Key\", None)\n                    processed_logical_id = resource_logical_id\n        else:\n            for field in GENERAL_REMOVAL_MAP.get(resource_type, []):\n                if resource_type in SYNCABLE_STACK_RESOURCES:\n                    if not isinstance(resource_dict.get(\"Properties\", {}).get(field, None), dict):\n                        resource_dict.get(\"Properties\", {}).pop(field, None)\n                        processed_logical_id = resource_logical_id\n                elif (\n                    built_resource_dict and is_local_path(built_resource_dict.get(\"Properties\", {}).get(field, None))\n                ) or resource_logical_id in linked_resources:\n                    resource_dict.get(\"Properties\", {}).pop(field, None)\n                    processed_logical_id = resource_logical_id\n\n        return processed_logical_id\n```\n\nOriginal documentation:\n```python\nHelper method to process resource dict\n\n        Parameters\n        ----------\n        resource_logical_id: str\n            Logical ID of the resource\n        resource_type: str\n            Resource type\n        resource_dict: Dict\n            The resource level dict containing Properties field\n        linked_resources: Set[str]\n            The corresponding resources in the other template that got processed\n        built_resource_dict: Optional[Dict]\n            Only passed in for current template sanitization to determine if local\n\n        Returns\n        -------\n        Optional[str]\n            The processed resource ID\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Helper method to process resource dict\n\n        Parameters\n        ----------\n        resource_logical_id: str\n            Logical ID of the resource\n        resource_type: str\n            Resource type\n        resource_dict: Dict\n            The resource level dict containing Properties field\n        linked_resources: Optional[Set[str]]\n            The corresponding resources in the other template that got processed\n        built_resource_dict: Optional[Dict]\n            Only passed in for current template sanitization to determine if local\n\n        Returns\n        -------\n        Optional[str]\n            The processed resource ID", "file": "samcli_lib_sync_infra_sync_executor.py", "function": "InfraSyncExecutor._remove_resource_field", "project": "aws-sam-cli", "change_stats": {"code_changes": 5, "doc_changes": 4, "total_changes": 9}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __query(self, query):\n        \n        \n        \n        if self.encrypted:\n            document = self.decrypted_document\n        else:\n            document = self.document\n        return OneLogin_Saml2_XML.query(document, query)```\n\nUpdated code:\n```python\ndef __query(self, query, tagid=None):\n        \n        \n        \n        if self.encrypted:\n            document = self.decrypted_document\n        else:\n            document = self.document\n        return OneLogin_Saml2_XML.query(document, query, None, tagid)\n```\n\nOriginal documentation:\n```python\nExtracts nodes that match the query from the Response\n\n        :param query: Xpath Expresion\n        :type query: String\n\n        :returns: The queried nodes\n        :rtype: list\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Extracts nodes that match the query from the Response\n\n        :param query: Xpath Expresion\n        :type query: String\n\n        :param tagid: Tag ID\n        :type query: String\n\n        :returns: The queried nodes\n        :rtype: list", "file": "src_onelogin_saml2_response.py", "function": "OneLogin_Saml2_Response.__query", "project": "python3-saml", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef bond_conv_adj_logs_jsl(symbol: str = \"128013\") -> pd.DataFrame:\n    \n    \n    \n    url = f\"https://www.jisilu.cn/data/cbnew/adj_logs/?bond_id={symbol}\"\n    r = requests.get(url)\n    data_text = r.text\n    if '</table>' not in data_text:\n        # 1. \u8be5\u53ef\u8f6c\u503a\u6ca1\u6709\u8f6c\u80a1\u4ef7\u8c03\u6574\u8bb0\u5f55\uff0c\u670d\u52a1\u7aef\u8fd4\u56de\u6587\u672c '\u6682\u65e0\u6570\u636e'\n        # 2. \u65e0\u6548\u53ef\u8f6c\u503a\u4ee3\u7801\uff0c\u670d\u52a1\u7aef\u8fd4\u56de {\"timestamp\":1639565628,\"isError\":1,\"msg\":\"\u65e0\u6548\u4ee3\u7801\u683c\u5f0f\"}\n        # \u4ee5\u4e0a\u4e24\u79cd\u60c5\u51b5\uff0c\u8fd4\u56de\u7a7a\u7684 DataFrame\n        return pd.DataFrame()\n    else:\n        temp_df = pd.read_html(data_text, parse_dates=True)[0]\n        temp_df['\u80a1\u4e1c\u5927\u4f1a\u65e5'] = pd.to_datetime(temp_df['\u80a1\u4e1c\u5927\u4f1a\u65e5']).dt.date\n        temp_df['\u4e0b\u4fee\u524d\u8f6c\u80a1\u4ef7'] = pd.to_numeric(temp_df['\u4e0b\u4fee\u524d\u8f6c\u80a1\u4ef7'])\n        temp_df['\u4e0b\u4fee\u540e\u8f6c\u80a1\u4ef7'] = pd.to_numeric(temp_df['\u4e0b\u4fee\u540e\u8f6c\u80a1\u4ef7'])\n        temp_df['\u65b0\u8f6c\u80a1\u4ef7\u751f\u6548\u65e5\u671f'] = pd.to_datetime(temp_df['\u65b0\u8f6c\u80a1\u4ef7\u751f\u6548\u65e5\u671f']).dt.date\n        temp_df['\u4e0b\u4fee\u5e95\u4ef7'] = pd.to_numeric(temp_df['\u4e0b\u4fee\u5e95\u4ef7'])\n        return temp_df```\n\nUpdated code:\n```python\ndef bond_conv_adj_logs_jsl(symbol: str = \"128013\") -> pd.DataFrame:\n    \n    \n    \n    url = f\"https://www.jisilu.cn/data/cbnew/adj_logs/?bond_id={symbol}\"\n    r = requests.get(url)\n    data_text = r.text\n    if '</table>' not in data_text:\n        # 1. \u8be5\u53ef\u8f6c\u503a\u6ca1\u6709\u8f6c\u80a1\u4ef7\u8c03\u6574\u8bb0\u5f55\uff0c\u670d\u52a1\u7aef\u8fd4\u56de\u6587\u672c '\u6682\u65e0\u6570\u636e'\n        # 2. \u65e0\u6548\u53ef\u8f6c\u503a\u4ee3\u7801\uff0c\u670d\u52a1\u7aef\u8fd4\u56de {\"timestamp\":1639565628,\"isError\":1,\"msg\":\"\u65e0\u6548\u4ee3\u7801\u683c\u5f0f\"}\n        # \u4ee5\u4e0a\u4e24\u79cd\u60c5\u51b5\uff0c\u8fd4\u56de\u7a7a\u7684 DataFrame\n        return\n    else:\n        temp_df = pd.read_html(data_text, parse_dates=True)[0]\n        temp_df['\u80a1\u4e1c\u5927\u4f1a\u65e5'] = pd.to_datetime(temp_df['\u80a1\u4e1c\u5927\u4f1a\u65e5']).dt.date\n        temp_df['\u4e0b\u4fee\u524d\u8f6c\u80a1\u4ef7'] = pd.to_numeric(temp_df['\u4e0b\u4fee\u524d\u8f6c\u80a1\u4ef7'])\n        temp_df['\u4e0b\u4fee\u540e\u8f6c\u80a1\u4ef7'] = pd.to_numeric(temp_df['\u4e0b\u4fee\u540e\u8f6c\u80a1\u4ef7'])\n        temp_df['\u65b0\u8f6c\u80a1\u4ef7\u751f\u6548\u65e5\u671f'] = pd.to_datetime(temp_df['\u65b0\u8f6c\u80a1\u4ef7\u751f\u6548\u65e5\u671f']).dt.date\n        temp_df['\u4e0b\u4fee\u5e95\u4ef7'] = pd.to_numeric(temp_df['\u4e0b\u4fee\u5e95\u4ef7'])\n        return temp_df\n```\n\nOriginal documentation:\n```python\n\u96c6\u601d\u5f55\u53ef\u8f6c\u503a\u8f6c\u80a1\u4ef7\u8c03\u6574\u8bb0\u5f55\n    https://app.jisilu.cn/data/cbnew/#cb\n    :param symbol: \u53ef\u8f6c\u503a\u4ee3\u7801\n    :type symbol: str\n    :return: \u8f6c\u80a1\u4ef7\u8c03\u6574\u8bb0\u5f55\n    :rtype: pandas.DataFrame\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "\u96c6\u601d\u5f55-\u53ef\u8f6c\u503a\u8f6c\u80a1\u4ef7-\u8c03\u6574\u8bb0\u5f55\n    https://app.jisilu.cn/data/cbnew/#cb\n    :param symbol: \u53ef\u8f6c\u503a\u4ee3\u7801\n    :type symbol: str\n    :return: \u8f6c\u80a1\u4ef7\u8c03\u6574\u8bb0\u5f55\n    :rtype: pandas.DataFrame", "file": "akshare_bond_bond_convert.py", "function": "bond_conv_adj_logs_jsl", "project": "akshare", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef connect(self) -> ClientContext:\n        \n        \n        \n        connection_dict = ray.init(\n            address=self.address,\n            job_config=self._job_config,\n            **self._init_args_dict,\n            **self._internal_config)\n        return ClientContext(\n            dashboard_url=connection_dict[\"webui_url\"],\n            python_version=\"{}.{}.{}\".format(\n                sys.version_info[0], sys.version_info[1], sys.version_info[2]),\n            ray_version=ray.__version__,\n            ray_commit=ray.__commit__,\n            protocol_version=None,\n            _num_clients=1)```\n\nUpdated code:\n```python\ndef connect(self) -> ClientContext:\n        \n        \n        \n        connection_dict = ray.init(\n            address=self.address, job_config=self._job_config)\n        return ClientContext(\n            dashboard_url=connection_dict[\"webui_url\"],\n            python_version=\"{}.{}.{}\".format(\n                sys.version_info[0], sys.version_info[1], sys.version_info[2]),\n            ray_version=ray.__version__,\n            ray_commit=ray.__commit__,\n            protocol_version=None,\n            _num_clients=1)\n```\n\nOriginal documentation:\n```python\nBegin a connection to the address passed in via ray.client(...) or\n        ray.init(\"local://...\")\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Begin a connection to the address passed in via ray.client(...)", "file": "python_ray_client_builder.py", "function": "_LocalClientBuilder.connect", "project": "ray", "change_stats": {"code_changes": 7, "doc_changes": 5, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef offset_line(x, xerr):\n    \n    \n    \n    if np.isscalar(xerr) or len(xerr) == len(x):\n        xmin = x - xerr\n        xmax = x + xerr\n    elif len(xerr) == 2:\n        xmin, xmax = x - xerr[0], x + xerr[1]\n    return xmin, xmax```\n\nUpdated code:\n```python\ndef offset_line(y, yerr):\n    \n    \n    \n    if np.isscalar(yerr) or len(yerr) == len(y):\n        ymin = y - yerr\n        ymax = y + yerr\n    elif len(yerr) == 2:\n        ymin, ymax = y - yerr[0], y + yerr[1]\n    return ymin, ymax\n```\n\nOriginal documentation:\n```python\nOffsets an array *x* by +/- an error and returns a tuple (x - err, x + err).\n\n    The error term can be:\n\n    o A scalar. In this case, the returned tuple is obvious.\n    o A vector of the same length as *x*. The quantities x +/- err are computed\n      component-wise.\n    o A tuple of length 2. In this case, xerr[0] is the error below *x* and\n      xerr[1] is error above *x*.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Offsets an array *y* by +/- an error and returns a tuple (y - err, y + err).\n\n    The error term can be:\n\n    o A scalar. In this case, the returned tuple is obvious.\n    o A vector of the same length as *y*. The quantities y +/- err are computed\n      component-wise.\n    o A tuple of length 2. In this case, yerr[0] is the error below *y* and\n      yerr[1] is error above *y*.", "file": "lib_matplotlib_mlab.py", "function": "offset_line", "project": "matplotlib", "change_stats": {"code_changes": 16, "doc_changes": 10, "total_changes": 26}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_permitted(self):\n\t\t\n\t\tfrom frappe.utils import has_common\n\n\t\tallowed = [d.role for d in frappe.get_all(\"Page Role\", fields=[\"role\"],\n\t\t\tfilters={\"parent\": self.name})]\n\n\t\tif not allowed:\n\t\t\treturn True\n\n\t\troles = frappe.get_roles()\n\n\t\tif has_common(roles, allowed):\n\t\t\treturn True```\n\nUpdated code:\n```python\ndef is_permitted(self):\n\t\t\n\t\tfrom frappe.utils import has_common\n\n\t\tallowed = [d.role for d in frappe.get_all(\"Has Role\", fields=[\"role\"],\n\t\t\tfilters={\"parent\": self.name})]\n\n\t\tif not allowed:\n\t\t\treturn True\n\n\t\troles = frappe.get_roles()\n\n\t\tif has_common(roles, allowed):\n\t\t\treturn True\n```\n\nOriginal documentation:\n```python\nReturns true if Page Role is not set or the user is allowed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns true if Has Role is not set or the user is allowed.", "file": "frappe_core_doctype_page_page.py", "function": "Page.is_permitted", "project": "frappe", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_funding_rate_history(self, symbol=None, since=None, limit=None, params={}):\n        \n        \n        \n        self.load_markets()\n        request = {}\n        market = None\n        if symbol in self.currencies:\n            code = self.currency(symbol)\n            request['symbol'] = code['id']\n        elif symbol is not None:\n            splitSymbol = symbol.split(':')\n            splitSymbolLength = len(splitSymbol)\n            timeframes = ['nearest', 'daily', 'weekly', 'monthly', 'quarterly', 'biquarterly', 'perpetual']\n            if (splitSymbolLength > 1) and self.in_array(splitSymbol[1], timeframes):\n                code = self.currency(splitSymbol[0])\n                symbol = code['id'] + ':' + splitSymbol[1]\n                request['symbol'] = symbol\n            else:\n                market = self.market(symbol)\n                request['symbol'] = market['id']\n        if since is not None:\n            request['startTime'] = self.iso8601(since)\n        if limit is not None:\n            request['count'] = limit\n        till = self.safe_integer(params, 'till')\n        params = self.omit(params, ['till'])\n        if till is not None:\n            request['endTime'] = self.iso8601(till)\n        response = self.publicGetFunding(self.extend(request, params))\n        #\n        #    [\n        #        {\n        #            \"timestamp\": \"2016-05-07T12:00:00.000Z\",\n        #            \"symbol\": \"ETHXBT\",\n        #            \"fundingInterval\": \"2000-01-02T00:00:00.000Z\",\n        #            \"fundingRate\": 0.0010890000000000001,\n        #            \"fundingRateDaily\": 0.0010890000000000001\n        #        }\n        #    ]\n        #\n        return self.parse_funding_rate_histories(response, market, since, limit)```\n\nUpdated code:\n```python\ndef fetch_funding_rate_history(self, symbol=None, since=None, limit=None, params={}):\n        \n        \n        \n        self.load_markets()\n        request = {}\n        market = None\n        if symbol in self.currencies:\n            code = self.currency(symbol)\n            request['symbol'] = code['id']\n        elif symbol is not None:\n            splitSymbol = symbol.split(':')\n            splitSymbolLength = len(splitSymbol)\n            timeframes = ['nearest', 'daily', 'weekly', 'monthly', 'quarterly', 'biquarterly', 'perpetual']\n            if (splitSymbolLength > 1) and self.in_array(splitSymbol[1], timeframes):\n                code = self.currency(splitSymbol[0])\n                symbol = code['id'] + ':' + splitSymbol[1]\n                request['symbol'] = symbol\n            else:\n                market = self.market(symbol)\n                request['symbol'] = market['id']\n        if since is not None:\n            request['startTime'] = self.iso8601(since)\n        if limit is not None:\n            request['count'] = limit\n        until = self.safe_integer_2(params, 'until', 'till')\n        params = self.omit(params, ['until', 'till'])\n        if until is not None:\n            request['endTime'] = self.iso8601(until)\n        response = self.publicGetFunding(self.extend(request, params))\n        #\n        #    [\n        #        {\n        #            \"timestamp\": \"2016-05-07T12:00:00.000Z\",\n        #            \"symbol\": \"ETHXBT\",\n        #            \"fundingInterval\": \"2000-01-02T00:00:00.000Z\",\n        #            \"fundingRate\": 0.0010890000000000001,\n        #            \"fundingRateDaily\": 0.0010890000000000001\n        #        }\n        #    ]\n        #\n        return self.parse_funding_rate_histories(response, market, since, limit)\n```\n\nOriginal documentation:\n```python\nFetches the history of funding rates\n        :param str|None symbol: unified symbol of the market to fetch the funding rate history for\n        :param int|None since: timestamp in ms of the earliest funding rate to fetch\n        :param int|None limit: the maximum amount of `funding rate structures <https://docs.ccxt.com/en/latest/manual.html?#funding-rate-history-structure>` to fetch\n        :param dict params: extra parameters specific to the bitmex api endpoint\n        :param int|None params['till']: timestamp in ms for ending date filter\n        :param bool|None params['reverse']: if True, will sort results newest first\n        :param int|None params['start']: starting point for results\n        :param str|None params['columns']: array of column names to fetch in info, if omitted, will return all columns\n        :param str|None params['filter']: generic table filter, send json key/value pairs, such as {\"key\": \"value\"}, you can key on individual fields, and do more advanced querying on timestamps, see the `timestamp docs <https://www.bitmex.com/app/restAPI#Timestamp-Filters>` for more details\n        :returns [dict]: a list of `funding rate structures <https://docs.ccxt.com/en/latest/manual.html?#funding-rate-history-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Fetches the history of funding rates\n        :param str|None symbol: unified symbol of the market to fetch the funding rate history for\n        :param int|None since: timestamp in ms of the earliest funding rate to fetch\n        :param int|None limit: the maximum amount of `funding rate structures <https://docs.ccxt.com/en/latest/manual.html?#funding-rate-history-structure>` to fetch\n        :param dict params: extra parameters specific to the bitmex api endpoint\n        :param int|None params['until']: timestamp in ms for ending date filter\n        :param bool|None params['reverse']: if True, will sort results newest first\n        :param int|None params['start']: starting point for results\n        :param str|None params['columns']: array of column names to fetch in info, if omitted, will return all columns\n        :param str|None params['filter']: generic table filter, send json key/value pairs, such as {\"key\": \"value\"}, you can key on individual fields, and do more advanced querying on timestamps, see the `timestamp docs <https://www.bitmex.com/app/restAPI#Timestamp-Filters>` for more details\n        :returns [dict]: a list of `funding rate structures <https://docs.ccxt.com/en/latest/manual.html?#funding-rate-history-structure>`", "file": "python_ccxt_bitmex.py", "function": "bitmex.fetch_funding_rate_history", "project": "ccxt", "change_stats": {"code_changes": 10, "doc_changes": 4, "total_changes": 14}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __init__(self, model_dir, top_k_embedding_count,\n               bottom_k_embedding_count):\n    \n    \n    self._model_dir = model_dir\n    self._session = None\n    self._num_examples_processed = beam.metrics.Metrics.counter(\n        'embedding_data_generation', 'num_tf_examples_processed')\n    self._top_k_embedding_count = top_k_embedding_count\n    self._bottom_k_embedding_count = bottom_k_embedding_count```\n\nUpdated code:\n```python\ndef __init__(self, model_dir, top_k_embedding_count,\n               bottom_k_embedding_count, embedding_type='final_box_features'):\n    \n    \n    self._model_dir = model_dir\n    self._session = None\n    self._num_examples_processed = beam.metrics.Metrics.counter(\n        'embedding_data_generation', 'num_tf_examples_processed')\n    self._top_k_embedding_count = top_k_embedding_count\n    self._bottom_k_embedding_count = bottom_k_embedding_count\n    self._embedding_type = embedding_type\n```\n\nOriginal documentation:\n```python\nInitialization function.\n\n    Args:\n      model_dir: A directory containing saved model.\n      top_k_embedding_count: the number of high-confidence embeddings to store\n      bottom_k_embedding_count: the number of low-confidence embeddings to store\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Initialization function.\n\n    Args:\n      model_dir: A directory containing saved model.\n      top_k_embedding_count: the number of high-confidence embeddings to store\n      bottom_k_embedding_count: the number of low-confidence embeddings to store\n      embedding_type: One of 'final_box_features', 'rpn_box_features'", "file": "research_object_detection_dataset_tools_context_rcnn_generate_embedding_data.py", "function": "GenerateEmbeddingDataFn.__init__", "project": "models", "change_stats": {"code_changes": 5, "doc_changes": 3, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef completed(self) -> bool:\n        \n        \n        return (\n            self._inputs_complete\n            and self.num_active_tasks() == 0\n            and not self.has_next()\n        ) or self._dependents_complete```\n\nUpdated code:\n```python\ndef completed(self) -> bool:\n        \n        \n        if not self._execution_completed:\n            if self._inputs_complete and self.num_active_tasks() == 0:\n                # If all inputs are complete and there are no active tasks,\n                # then the operator has completed execution.\n                self._execution_completed = True\n        return self._execution_completed and not self.has_next()\n```\n\nOriginal documentation:\n```python\nReturn True when this operator is completed.\n\n        An operator is completed if any of the following conditions are met:\n        - All upstream operators are completed and all outputs are taken.\n        - All downstream operators are completed.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return True when this operator is completed.\n\n        An operator is completed the operator has stopped execution and all\n        outputs are taken.", "file": "python_ray_data__internal_execution_interfaces_physical_operator.py", "function": "PhysicalOperator.completed", "project": "ray", "change_stats": {"code_changes": 13, "doc_changes": 7, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):\n        \n        \n        \n        assert len(w.shape) == 1\n        w = w * 1000.0\n\n        half_dim = embedding_dim // 2\n        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)\n        emb = w.to(dtype)[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n        if embedding_dim % 2 == 1:  # zero pad\n            emb = torch.nn.functional.pad(emb, (0, 1))\n        assert emb.shape == (w.shape[0], embedding_dim)\n        return emb```\n\nUpdated code:\n```python\ndef get_guidance_scale_embedding(\n        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32\n    ) -> torch.FloatTensor:\n        \n        \n        \n        assert len(w.shape) == 1\n        w = w * 1000.0\n\n        half_dim = embedding_dim // 2\n        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)\n        emb = w.to(dtype)[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n        if embedding_dim % 2 == 1:  # zero pad\n            emb = torch.nn.functional.pad(emb, (0, 1))\n        assert emb.shape == (w.shape[0], embedding_dim)\n        return emb\n```\n\nOriginal documentation:\n```python\nSee https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298\n\n        Args:\n            timesteps (`torch.Tensor`):\n                generate embedding vectors at these timesteps\n            embedding_dim (`int`, *optional*, defaults to 512):\n                dimension of the embeddings to generate\n            dtype:\n                data type of the generated embeddings\n\n        Returns:\n            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298\n\n        Args:\n            w (`torch.Tensor`):\n                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.\n            embedding_dim (`int`, *optional*, defaults to 512):\n                Dimension of the embeddings to generate.\n            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n                Data type of the generated embeddings.\n\n        Returns:\n            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.", "file": "src_diffusers_pipelines_stable_diffusion_pipeline_stable_diffusion.py", "function": "StableDiffusionPipeline.get_guidance_scale_embedding", "project": "diffusers", "change_stats": {"code_changes": 6, "doc_changes": 14, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_ignored_params(\n    root_module: torch.nn.Module,\n    ignored_modules: Set[torch.nn.Module],\n) -> Set[torch.nn.Parameter]:\n    \n    \n    \n    return set(\n        p\n        for m in ignored_modules\n        for p in m.parameters()\n        if not _is_fsdp_flattened(p)\n    )```\n\nUpdated code:\n```python\ndef _get_ignored_params(\n    root_module: torch.nn.Module,\n    ignored_modules: Set[torch.nn.Module],\n    ignored_parameters: Optional[Iterable[torch.nn.Parameter]] = None,\n) -> Set[torch.nn.Parameter]:\n    \n    \n    \n    all_ignored_params: Set[torch.nn.Parameter] = set()\n\n    params_in_ignored_modules = set(\n        p\n        for m in ignored_modules\n        for p in m.parameters()\n        if not _is_fsdp_flattened(p)\n    )\n\n    all_ignored_params.update(params_in_ignored_modules)\n\n    if ignored_parameters is not None:\n        params_in_ignored_parameters = set(\n            p\n            for p in ignored_parameters\n            if not _is_fsdp_flattened(p)\n        )\n        all_ignored_params.update(params_in_ignored_parameters)\n\n        # Include nested FSDP modules' ignored parameters\n        for submodule in root_module.modules():\n            optional_fsdp_state = _get_module_fsdp_state(submodule)\n            if optional_fsdp_state is not None:\n                assert hasattr(optional_fsdp_state, \"_ignored_params\")\n                all_ignored_params.update(optional_fsdp_state._ignored_params)\n\n    return all_ignored_params\n```\n\nOriginal documentation:\n```python\nReturns the parameters of the modules in ``ignored_modules``,\n    excluding any :class:`FlatParameter` s.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns the parameters of the modules in ``ignored_modules`` and\n    the parameters in ``ignored_parameters``, excluding any :class:`FlatParameter` s.", "file": "torch_distributed_fsdp__init_utils.py", "function": "_get_ignored_params", "project": "pytorch", "change_stats": {"code_changes": 26, "doc_changes": 6, "total_changes": 32}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = self.market(symbol)\n        orderType = self.capitalize(type)\n        reduceOnly = self.safe_value(params, 'reduceOnly')\n        if reduceOnly is not None:\n            if (market['type'] != 'swap') and (market['type'] != 'future'):\n                raise InvalidOrder(self.id + ' createOrder() does not support reduceOnly for ' + market['type'] + ' orders, reduceOnly orders are supported for swap and future markets only')\n        brokerId = self.safe_string(self.options, 'brokerId', 'CCXT')\n        qty = self.parse_to_int(self.amount_to_precision(symbol, amount))\n        request = {\n            'symbol': market['id'],\n            'side': self.capitalize(side),\n            'orderQty': qty,  # lot size multiplied by the number of contracts\n            'ordType': orderType,\n            'text': brokerId,\n        }\n        customTriggerType = (orderType == 'Stop') or (orderType == 'StopLimit') or (orderType == 'MarketIfTouched') or (orderType == 'LimitIfTouched')\n        # support for unified trigger format\n        triggerPrice = self.safe_number_n(params, ['triggerPrice', 'stopPx', 'stopPrice'])\n        if (triggerPrice is not None) and not customTriggerType:\n            request['stopPx'] = float(self.price_to_precision(symbol, triggerPrice))\n            triggerDirection = self.safe_string(params, 'triggerDirection')\n            params = self.omit(params, ['triggerPrice', 'stopPrice', 'stopPx', 'triggerDirection'])\n            triggerAbove = (triggerDirection == 'above')\n            self.check_required_argument('createOrder', triggerDirection, 'triggerDirection', ['above', 'below'])\n            self.check_required_argument('createOrder', side, 'side', ['buy', 'sell'])\n            if type == 'limit':\n                request['price'] = float(self.price_to_precision(symbol, price))\n                if side == 'buy':\n                    request['ordType'] = 'StopLimit' if triggerAbove else 'LimitIfTouched'\n                else:\n                    request['ordType'] = 'LimitIfTouched' if triggerAbove else 'StopLimit'\n            elif type == 'market':\n                if side == 'buy':\n                    request['ordType'] = 'Stop' if triggerAbove else 'MarketIfTouched'\n                else:\n                    request['ordType'] = 'MarketIfTouched' if triggerAbove else 'Stop'\n        elif customTriggerType:\n            if triggerPrice is None:\n                # if exchange specific trigger types were provided\n                raise ArgumentsRequired(self.id + ' createOrder() requires a triggerPrice(stopPx|stopPrice) parameter for the ' + orderType + ' order type')\n            params = self.omit(params, ['triggerPrice', 'stopPrice', 'stopPx'])\n            request['stopPx'] = float(self.price_to_precision(symbol, triggerPrice))\n        if (orderType == 'Limit') or (orderType == 'StopLimit') or (orderType == 'LimitIfTouched'):\n            request['price'] = float(self.price_to_precision(symbol, price))\n        clientOrderId = self.safe_string_2(params, 'clOrdID', 'clientOrderId')\n        if clientOrderId is not None:\n            request['clOrdID'] = clientOrderId\n            params = self.omit(params, ['clOrdID', 'clientOrderId'])\n        response = self.privatePostOrder(self.extend(request, params))\n        return self.parse_order(response, market)```\n\nUpdated code:\n```python\ndef create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):\n        \n        \n        \n        self.load_markets()\n        market = self.market(symbol)\n        orderType = self.capitalize(type)\n        reduceOnly = self.safe_value(params, 'reduceOnly')\n        if reduceOnly is not None:\n            if (not market['swap']) and (not market['future']):\n                raise InvalidOrder(self.id + ' createOrder() does not support reduceOnly for ' + market['type'] + ' orders, reduceOnly orders are supported for swap and future markets only')\n        brokerId = self.safe_string(self.options, 'brokerId', 'CCXT')\n        qty = self.parse_to_int(self.amount_to_precision(symbol, amount))\n        request = {\n            'symbol': market['id'],\n            'side': self.capitalize(side),\n            'orderQty': qty,  # lot size multiplied by the number of contracts\n            'ordType': orderType,\n            'text': brokerId,\n        }\n        # support for unified trigger format\n        triggerPrice = self.safe_number_n(params, ['triggerPrice', 'stopPx', 'stopPrice'])\n        trailingAmount = self.safe_string_2(params, 'trailingAmount', 'pegOffsetValue')\n        isTriggerOrder = triggerPrice is not None\n        isTrailingAmountOrder = trailingAmount is not None\n        if isTriggerOrder or isTrailingAmountOrder:\n            triggerDirection = self.safe_string(params, 'triggerDirection')\n            triggerAbove = (triggerDirection == 'above')\n            if (type == 'limit') or (type == 'market'):\n                self.check_required_argument('createOrder', triggerDirection, 'triggerDirection', ['above', 'below'])\n            if type == 'limit':\n                if side == 'buy':\n                    orderType = 'StopLimit' if triggerAbove else 'LimitIfTouched'\n                else:\n                    orderType = 'LimitIfTouched' if triggerAbove else 'StopLimit'\n            elif type == 'market':\n                if side == 'buy':\n                    orderType = 'Stop' if triggerAbove else 'MarketIfTouched'\n                else:\n                    orderType = 'MarketIfTouched' if triggerAbove else 'Stop'\n            if isTrailingAmountOrder:\n                isStopSellOrder = (side == 'sell') and ((orderType == 'Stop') or (orderType == 'StopLimit'))\n                isBuyIfTouchedOrder = (side == 'buy') and ((orderType == 'MarketIfTouched') or (orderType == 'LimitIfTouched'))\n                if isStopSellOrder or isBuyIfTouchedOrder:\n                    trailingAmount = '-' + trailingAmount\n                request['pegOffsetValue'] = self.parse_to_numeric(trailingAmount)\n                request['pegPriceType'] = 'TrailingStopPeg'\n            else:\n                if triggerPrice is None:\n                    # if exchange specific trigger types were provided\n                    raise ArgumentsRequired(self.id + ' createOrder() requires a triggerPrice(stopPx|stopPrice) parameter for the ' + orderType + ' order type')\n                request['stopPx'] = self.parse_to_numeric(self.price_to_precision(symbol, triggerPrice))\n            request['ordType'] = orderType\n            params = self.omit(params, ['triggerPrice', 'stopPrice', 'stopPx', 'triggerDirection', 'trailingAmount'])\n        if (orderType == 'Limit') or (orderType == 'StopLimit') or (orderType == 'LimitIfTouched'):\n            request['price'] = self.parse_to_numeric(self.price_to_precision(symbol, price))\n        clientOrderId = self.safe_string_2(params, 'clOrdID', 'clientOrderId')\n        if clientOrderId is not None:\n            request['clOrdID'] = clientOrderId\n            params = self.omit(params, ['clOrdID', 'clientOrderId'])\n        response = self.privatePostOrder(self.extend(request, params))\n        return self.parse_order(response, market)\n```\n\nOriginal documentation:\n```python\ncreate a trade order\n        :see: https://www.bitmex.com/api/explorer/#not /Order/Order_new\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'market' or 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much of currency you want to trade in units of base currency\n        :param float [price]: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param dict [params.triggerPrice]: the price at which a trigger order is triggered at\n        :param dict [params.triggerDirection]: the direction whenever the trigger happens with relation to price - 'above' or 'below'\n        :returns dict: an `order structure <https://github.com/ccxt/ccxt/wiki/Manual#order-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "create a trade order\n        :see: https://www.bitmex.com/api/explorer/#not /Order/Order_new\n        :param str symbol: unified symbol of the market to create an order in\n        :param str type: 'market' or 'limit'\n        :param str side: 'buy' or 'sell'\n        :param float amount: how much of currency you want to trade in units of base currency\n        :param float [price]: the price at which the order is to be fullfilled, in units of the quote currency, ignored in market orders\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param dict [params.triggerPrice]: the price at which a trigger order is triggered at\n        :param dict [params.triggerDirection]: the direction whenever the trigger happens with relation to price - 'above' or 'below'\n        :param float [params.trailingAmount]: the quote amount to trail away from the current market price\n        :returns dict: an `order structure <https://github.com/ccxt/ccxt/wiki/Manual#order-structure>`", "file": "python_ccxt_bitmex.py", "function": "bitmex.create_order", "project": "ccxt", "change_stats": {"code_changes": 47, "doc_changes": 3, "total_changes": 50}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_entry_meta(\n        self,\n        entry: str,\n        working_dir: Path,\n        **kwargs,\n    ) -> Dict[str, str]:\n        \n        \n        # TODO: add tests for this\n        with tempfile.TemporaryDirectory() as temp_dir:\n            flow_file = Path(temp_dir) / \"flow.dag.yaml\"\n            flow_file.write_text(json.dumps({\"entry\": entry}))\n\n            # TODO: enable cache?\n            command = [\n                \"dotnet\",\n                EXECUTOR_SERVICE_DLL,\n                \"--flow_meta\",\n                \"--yaml_path\",\n                flow_file.absolute().as_posix(),\n                \"--assembly_folder\",\n                \".\",\n            ]\n            try:\n                subprocess.check_output(\n                    command,\n                    cwd=working_dir,\n                )\n            except subprocess.CalledProcessError as e:\n                raise UnexpectedError(\n                    message_format=\"Failed to generate flow meta for csharp flow.\\n\"\n                    \"Command: {command}\\n\"\n                    \"Working directory: {working_directory}\\n\"\n                    \"Return code: {return_code}\\n\"\n                    \"Output: {output}\",\n                    command=\" \".join(command),\n                    working_directory=working_dir.as_posix(),\n                    return_code=e.returncode,\n                    output=e.output,\n                )\n        return json.loads((working_dir / PROMPT_FLOW_DIR_NAME / \"flow.json\").read_text())```\n\nUpdated code:\n```python\ndef get_entry_meta(\n        self,\n        entry: str,\n        working_dir: Path,\n        **kwargs,\n    ) -> Dict[str, str]:\n        \n        target_path = working_dir / PROMPT_FLOW_DIR_NAME / FLOW_META_JSON\n\n        if target_path.is_file():\n            entry_meta = read_json_content(target_path, \"flow metadata\")\n            for key in [\"inputs\", \"outputs\", \"init\"]:\n                if key not in entry_meta:\n                    continue\n                for port_name, port in entry_meta[key].items():\n                    if \"type\" in port and isinstance(port[\"type\"], list) and len(port[\"type\"]) == 1:\n                        port[\"type\"] = port[\"type\"][0]\n            entry_meta.pop(\"framework\", None)\n            return entry_meta\n        raise UserErrorException(\"Flow metadata not found.\")\n```\n\nOriginal documentation:\n```python\nIn csharp, we need to generate metadata based on a dotnet command for now and the metadata will\n        always be dumped.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "In csharp, the metadata will always be dumped at the beginning of each local run.", "file": "src_promptflow-devkit_promptflow__proxy__csharp_inspector_proxy.py", "function": "CSharpInspectorProxy.get_entry_meta", "project": "promptflow", "change_stats": {"code_changes": 47, "doc_changes": 5, "total_changes": 52}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef render_to_response(self, context, **response_kwargs):\n        \n        \n        \n\n        content_types = {\n            Document.TYPE_PDF: \"application/pdf\",\n            Document.TYPE_PNG: \"image/png\",\n            Document.TYPE_JPG: \"image/jpeg\",\n            Document.TYPE_GIF: \"image/gif\",\n            Document.TYPE_TIF: \"image/tiff\",\n        }\n\n        response = HttpResponse(\n            GnuPG.decrypted(self.object.source_file),\n            content_type=content_types[self.object.file_type]\n        )\n        response[\"Content-Disposition\"] = 'attachment; filename=\"{}\"'.format(\n            self.object.file_name)\n\n        return response```\n\nUpdated code:\n```python\ndef render_to_response(self, context, **response_kwargs):\n        \n        \n        \n\n        content_types = {\n            Document.TYPE_PDF: \"application/pdf\",\n            Document.TYPE_PNG: \"image/png\",\n            Document.TYPE_JPG: \"image/jpeg\",\n            Document.TYPE_GIF: \"image/gif\",\n            Document.TYPE_TIF: \"image/tiff\",\n        }\n\n        if self.kwargs[\"kind\"] == \"thumb\":\n            return HttpResponse(\n                GnuPG.decrypted(self.object.thumb_file),\n                content_type=content_types[Document.TYPE_JPG]\n            )\n\n        response = HttpResponse(\n            GnuPG.decrypted(self.object.source_file),\n            content_type=content_types[self.object.file_type]\n        )\n        response[\"Content-Disposition\"] = 'attachment; filename=\"{}\"'.format(\n            self.object.file_name)\n\n        return response\n```\n\nOriginal documentation:\n```python\nOverride the default to return the unencrypted PDF as raw data.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Override the default to return the unencrypted image/PDF as raw data.", "file": "src_documents_views.py", "function": "FetchView.render_to_response", "project": "paperless-ngx", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _process_rpc_error(self, e: grpc.RpcError):\n        \n        \n        \n        if e.code() == grpc.StatusCode.CANCELLED:\n            # Graceful shutdown. We've cancelled our own connection.\n            logger.info(\"Cancelling logs channel\")\n        elif e.code() in (grpc.StatusCode.UNAVAILABLE,\n                          grpc.StatusCode.RESOURCE_EXHAUSTED):\n            # TODO(barakmich): The server may have\n            # dropped. In theory, we can retry, as per\n            # https://grpc.github.io/grpc/core/md_doc_statuscodes.html but\n            # in practice we may need to think about the correct semantics\n            # here.\n            logger.info(\"Server disconnected from logs channel\")\n        else:\n            # Some other, unhandled, gRPC error\n            logger.exception(\n                f\"Got Error from logger channel -- shutting down: {e}\")```\n\nUpdated code:\n```python\ndef _process_rpc_error(self, e: grpc.RpcError) -> bool:\n        \n        \n        \n        if self.client_worker._can_reconnect(e):\n            if log_once(\"lost_reconnect_logs\"):\n                logger.warning(\n                    \"Log channel is reconnecting. Logs produced while \"\n                    \"the connection was down can be found on the head \"\n                    \"node of the cluster in \"\n                    \"`ray_client_server_[port].out`\")\n            logger.info(\"Log channel dropped, retrying.\")\n            time.sleep(.5)\n            return True\n        logger.info(\"Shutting down log channel.\")\n        if not self.client_worker._in_shutdown:\n            logger.exception(\"Unexpected exception:\")\n        return False\n```\n\nOriginal documentation:\n```python\nProcesses RPC errors that occur while reading from data stream.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Processes RPC errors that occur while reading from data stream.\n        Returns True if the error can be recovered from, False otherwise.", "file": "python_ray_util_client_logsclient.py", "function": "LogstreamClient._process_rpc_error", "project": "ray", "change_stats": {"code_changes": 33, "doc_changes": 3, "total_changes": 36}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef fetch_positions(self, symbols: Strings = None, params={}):\n        \n        \n        \n        self.load_markets()\n        symbols = self.market_symbols(symbols)\n        subType = None\n        code = self.safe_string(params, 'currency')\n        settle = None\n        market = None\n        firstSymbol = self.safe_string(symbols, 0)\n        if firstSymbol is not None:\n            market = self.market(firstSymbol)\n            settle = market['settle']\n            code = market['settle']\n        else:\n            settle, params = self.handle_option_and_params(params, 'fetchPositions', 'settle', 'USD')\n        subType, params = self.handle_sub_type_and_params('fetchPositions', market, params)\n        isUSDTSettled = settle == 'USDT'\n        if isUSDTSettled:\n            code = 'USDT'\n        elif code is None:\n            code = 'USD' if (subType == 'linear') else 'BTC'\n        else:\n            params = self.omit(params, 'code')\n        currency = self.currency(code)\n        request = {\n            'currency': currency['id'],\n        }\n        response = None\n        if isUSDTSettled:\n            response = self.privateGetGAccountsAccountPositions(self.extend(request, params))\n        else:\n            response = self.privateGetAccountsAccountPositions(self.extend(request, params))\n        #\n        #     {\n        #         \"code\":0,\"msg\":\"\",\n        #         \"data\":{\n        #             \"account\":{\n        #                 \"accountId\":6192120001,\n        #                 \"currency\":\"BTC\",\n        #                 \"accountBalanceEv\":1254744,\n        #                 \"totalUsedBalanceEv\":0,\n        #                 \"bonusBalanceEv\":1254744\n        #             },\n        #             \"positions\":[\n        #                 {\n        #                     \"accountID\":6192120001,\n        #                     \"symbol\":\"BTCUSD\",\n        #                     \"currency\":\"BTC\",\n        #                     \"side\":\"None\",\n        #                     \"positionStatus\":\"Normal\",\n        #                     \"crossMargin\":false,\n        #                     \"leverageEr\":100000000,\n        #                     \"leverage\":1.00000000,\n        #                     \"initMarginReqEr\":100000000,\n        #                     \"initMarginReq\":1.00000000,\n        #                     \"maintMarginReqEr\":500000,\n        #                     \"maintMarginReq\":0.00500000,\n        #                     \"riskLimitEv\":10000000000,\n        #                     \"riskLimit\":100.00000000,\n        #                     \"size\":0,\n        #                     \"value\":0E-8,\n        #                     \"valueEv\":0,\n        #                     \"avgEntryPriceEp\":0,\n        #                     \"avgEntryPrice\":0E-8,\n        #                     \"posCostEv\":0,\n        #                     \"posCost\":0E-8,\n        #                     \"assignedPosBalanceEv\":0,\n        #                     \"assignedPosBalance\":0E-8,\n        #                     \"bankruptCommEv\":0,\n        #                     \"bankruptComm\":0E-8,\n        #                     \"bankruptPriceEp\":0,\n        #                     \"bankruptPrice\":0E-8,\n        #                     \"positionMarginEv\":0,\n        #                     \"positionMargin\":0E-8,\n        #                     \"liquidationPriceEp\":0,\n        #                     \"liquidationPrice\":0E-8,\n        #                     \"deleveragePercentileEr\":0,\n        #                     \"deleveragePercentile\":0E-8,\n        #                     \"buyValueToCostEr\":100225000,\n        #                     \"buyValueToCost\":1.00225000,\n        #                     \"sellValueToCostEr\":100075000,\n        #                     \"sellValueToCost\":1.00075000,\n        #                     \"markPriceEp\":135736070,\n        #                     \"markPrice\":13573.60700000,\n        #                     \"markValueEv\":0,\n        #                     \"markValue\":null,\n        #                     \"unRealisedPosLossEv\":0,\n        #                     \"unRealisedPosLoss\":null,\n        #                     \"estimatedOrdLossEv\":0,\n        #                     \"estimatedOrdLoss\":0E-8,\n        #                     \"usedBalanceEv\":0,\n        #                     \"usedBalance\":0E-8,\n        #                     \"takeProfitEp\":0,\n        #                     \"takeProfit\":null,\n        #                     \"stopLossEp\":0,\n        #                     \"stopLoss\":null,\n        #                     \"cumClosedPnlEv\":0,\n        #                     \"cumFundingFeeEv\":0,\n        #                     \"cumTransactFeeEv\":0,\n        #                     \"realisedPnlEv\":0,\n        #                     \"realisedPnl\":null,\n        #                     \"cumRealisedPnlEv\":0,\n        #                     \"cumRealisedPnl\":null\n        #                 }\n        #             ]\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data', {})\n        positions = self.safe_value(data, 'positions', [])\n        result = []\n        for i in range(0, len(positions)):\n            position = positions[i]\n            result.append(self.parse_position(position))\n        return self.filter_by_array_positions(result, 'symbol', symbols, False)```\n\nUpdated code:\n```python\ndef fetch_positions(self, symbols: Strings = None, params={}):\n        \n        \n        \n        self.load_markets()\n        symbols = self.market_symbols(symbols)\n        subType = None\n        code = self.safe_string(params, 'currency')\n        settle = None\n        market = None\n        firstSymbol = self.safe_string(symbols, 0)\n        if firstSymbol is not None:\n            market = self.market(firstSymbol)\n            settle = market['settle']\n            code = market['settle']\n        else:\n            settle, params = self.handle_option_and_params(params, 'fetchPositions', 'settle', 'USD')\n        subType, params = self.handle_sub_type_and_params('fetchPositions', market, params)\n        isUSDTSettled = settle == 'USDT'\n        if isUSDTSettled:\n            code = 'USDT'\n        elif code is None:\n            code = 'USD' if (subType == 'linear') else 'BTC'\n        else:\n            params = self.omit(params, 'code')\n        currency = self.currency(code)\n        request = {\n            'currency': currency['id'],\n        }\n        response = None\n        if isUSDTSettled:\n            method = None\n            method, params = self.handle_option_and_params(params, 'fetchPositions', 'method', 'privateGetGAccountsAccountPositions')\n            if method == 'privateGetGAccountsAccountPositions':\n                response = self.privateGetGAccountsAccountPositions(self.extend(request, params))\n            else:\n                response = self.privateGetAccountsPositions(self.extend(request, params))\n        else:\n            response = self.privateGetAccountsAccountPositions(self.extend(request, params))\n        #\n        #     {\n        #         \"code\":0,\"msg\":\"\",\n        #         \"data\":{\n        #             \"account\":{\n        #                 \"accountId\":6192120001,\n        #                 \"currency\":\"BTC\",\n        #                 \"accountBalanceEv\":1254744,\n        #                 \"totalUsedBalanceEv\":0,\n        #                 \"bonusBalanceEv\":1254744\n        #             },\n        #             \"positions\":[\n        #                 {\n        #                     \"accountID\":6192120001,\n        #                     \"symbol\":\"BTCUSD\",\n        #                     \"currency\":\"BTC\",\n        #                     \"side\":\"None\",\n        #                     \"positionStatus\":\"Normal\",\n        #                     \"crossMargin\":false,\n        #                     \"leverageEr\":100000000,\n        #                     \"leverage\":1.00000000,\n        #                     \"initMarginReqEr\":100000000,\n        #                     \"initMarginReq\":1.00000000,\n        #                     \"maintMarginReqEr\":500000,\n        #                     \"maintMarginReq\":0.00500000,\n        #                     \"riskLimitEv\":10000000000,\n        #                     \"riskLimit\":100.00000000,\n        #                     \"size\":0,\n        #                     \"value\":0E-8,\n        #                     \"valueEv\":0,\n        #                     \"avgEntryPriceEp\":0,\n        #                     \"avgEntryPrice\":0E-8,\n        #                     \"posCostEv\":0,\n        #                     \"posCost\":0E-8,\n        #                     \"assignedPosBalanceEv\":0,\n        #                     \"assignedPosBalance\":0E-8,\n        #                     \"bankruptCommEv\":0,\n        #                     \"bankruptComm\":0E-8,\n        #                     \"bankruptPriceEp\":0,\n        #                     \"bankruptPrice\":0E-8,\n        #                     \"positionMarginEv\":0,\n        #                     \"positionMargin\":0E-8,\n        #                     \"liquidationPriceEp\":0,\n        #                     \"liquidationPrice\":0E-8,\n        #                     \"deleveragePercentileEr\":0,\n        #                     \"deleveragePercentile\":0E-8,\n        #                     \"buyValueToCostEr\":100225000,\n        #                     \"buyValueToCost\":1.00225000,\n        #                     \"sellValueToCostEr\":100075000,\n        #                     \"sellValueToCost\":1.00075000,\n        #                     \"markPriceEp\":135736070,\n        #                     \"markPrice\":13573.60700000,\n        #                     \"markValueEv\":0,\n        #                     \"markValue\":null,\n        #                     \"unRealisedPosLossEv\":0,\n        #                     \"unRealisedPosLoss\":null,\n        #                     \"estimatedOrdLossEv\":0,\n        #                     \"estimatedOrdLoss\":0E-8,\n        #                     \"usedBalanceEv\":0,\n        #                     \"usedBalance\":0E-8,\n        #                     \"takeProfitEp\":0,\n        #                     \"takeProfit\":null,\n        #                     \"stopLossEp\":0,\n        #                     \"stopLoss\":null,\n        #                     \"cumClosedPnlEv\":0,\n        #                     \"cumFundingFeeEv\":0,\n        #                     \"cumTransactFeeEv\":0,\n        #                     \"realisedPnlEv\":0,\n        #                     \"realisedPnl\":null,\n        #                     \"cumRealisedPnlEv\":0,\n        #                     \"cumRealisedPnl\":null\n        #                 }\n        #             ]\n        #         }\n        #     }\n        #\n        data = self.safe_value(response, 'data', {})\n        positions = self.safe_value(data, 'positions', [])\n        result = []\n        for i in range(0, len(positions)):\n            position = positions[i]\n            result.append(self.parse_position(position))\n        return self.filter_by_array_positions(result, 'symbol', symbols, False)\n```\n\nOriginal documentation:\n```python\nfetch all open positions\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Contract-API-en.md#query-trading-account-and-positions\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Hedged-Perpetual-API.md#query-account-positions\n        :param str[]|None symbols: list of unified market symbols\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :returns dict[]: a list of `position structure <https://docs.ccxt.com/#/?id=position-structure>`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "fetch all open positions\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Contract-API-en.md#query-trading-account-and-positions\n        :see: https://github.com/phemex/phemex-api-docs/blob/master/Public-Hedged-Perpetual-API.md#query-account-positions\n        :see: https://phemex-docs.github.io/#query-account-positions-with-unrealized-pnl\n        :param str[] [symbols]: list of unified market symbols\n        :param dict [params]: extra parameters specific to the exchange API endpoint\n        :param str [param.method]: *USDT contracts only* 'privateGetGAccountsAccountPositions' or 'privateGetAccountsPositions' default is 'privateGetGAccountsAccountPositions'\n        :returns dict[]: a list of `position structure <https://docs.ccxt.com/#/?id=position-structure>`", "file": "python_ccxt_phemex.py", "function": "phemex.fetch_positions", "project": "ccxt", "change_stats": {"code_changes": 9, "doc_changes": 6, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef warning_advice(self, *args, **kwargs):\n    \n    \n    \n    no_advisory_warnings = os.getenv(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\", False)\n    if no_advisory_warnings:\n        return\n    self.warning(*args, **kwargs)```\n\nUpdated code:\n```python\ndef warning_advice(self, *args, **kwargs):\n    \n    \n    \n    no_advisory_warnings = os.getenv(\"DIFFUSERS_NO_ADVISORY_WARNINGS\", False)\n    if no_advisory_warnings:\n        return\n    self.warning(*args, **kwargs)\n```\n\nOriginal documentation:\n```python\nThis method is identical to `logger.warninging()`, but if env var TRANSFORMERS_NO_ADVISORY_WARNINGS=1 is set, this\n    warning will not be printed\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "This method is identical to `logger.warninging()`, but if env var DIFFUSERS_NO_ADVISORY_WARNINGS=1 is set, this\n    warning will not be printed", "file": "src_diffusers_utils_logging.py", "function": "warning_advice", "project": "diffusers", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _parse_path_variables(pathname, path_template):\n    \n    \n    \n    path_segments = pathname.split(\"/\")\n    template_segments = path_template.split(\"/\")\n\n    if len(path_segments) != len(template_segments):\n        return None\n\n    path_vars = {}\n    for path_segment, template_segment in zip(path_segments, template_segments):\n        if template_segment.startswith(\"<\"):\n            path_vars[template_segment[1:-1]] = path_segment\n        elif template_segment != path_segment:\n            return None\n    return path_vars```\n\nUpdated code:\n```python\ndef _parse_path_variables(pathname, path_template, separator=\"/\"):\n    \n    \n    \n    path_segments = pathname.split(separator)\n    template_segments = path_template.split(separator)\n\n    if len(path_segments) != len(template_segments):\n        return None\n\n    path_vars = {}\n    for path_segment, template_segment in zip(path_segments, template_segments):\n        if template_segment.startswith(\"<\"):\n            path_vars[template_segment[1:-1]] = path_segment\n        elif template_segment != path_segment:\n            return None\n    return path_vars\n```\n\nOriginal documentation:\n```python\ncreates the dict of path variables passed to the layout\n    e.g. path_template= \"/asset/<asset_id>\"\n         if pathname provided by the browser is \"/assets/a100\"\n         returns **{\"asset_id\": \"a100\"}\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "creates the dict of path variables passed to the layout\n    e.g. path_template= \"/asset/<asset_id>\"\n         if pathname provided by the browser is \"/assets/a100\"\n         returns **{\"asset_id\": \"a100\"}\n     `separator` allows custom separator for parsing the pathname into segments. (Used in snapshot engine).", "file": "dash__pages.py", "function": "_parse_path_variables", "project": "dash", "change_stats": {"code_changes": 8, "doc_changes": 3, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef warp(image, inverse_map=None, map_args={}, output_shape=None, order=1,\n         mode='constant', cval=0., reverse_map=None):\n    \n\n    \n    # Backward API compatibility\n    if reverse_map is not None:\n        inverse_map = reverse_map\n\n    if image.ndim < 2:\n        raise ValueError(\"Input must have more than 1 dimension.\")\n\n    image = np.atleast_3d(img_as_float(image))\n    ishape = np.array(image.shape)\n    bands = ishape[2]\n\n    if output_shape is None:\n        output_shape = ishape\n\n    coords = np.empty(np.r_[3, output_shape], dtype=float)\n\n    ## Construct transformed coordinates\n\n    rows, cols = output_shape[:2]\n\n    # Reshape grid coordinates into a (P, 2) array of (x, y) pairs\n    tf_coords = np.indices((cols, rows), dtype=float).reshape(2, -1).T\n\n    # Map each (x, y) pair to the source image according to\n    # the user-provided mapping\n    if callable(getattr(inverse_map, 'inverse', None)):\n        inverse_map = inverse_map.inverse\n    tf_coords = inverse_map(tf_coords, **map_args)\n\n    # Reshape back to a (2, M, N) coordinate grid\n    tf_coords = tf_coords.T.reshape((-1, cols, rows)).swapaxes(1, 2)\n\n    # Place the y-coordinate mapping\n    _stackcopy(coords[1, ...], tf_coords[0, ...])\n\n    # Place the x-coordinate mapping\n    _stackcopy(coords[0, ...], tf_coords[1, ...])\n\n    # colour-coordinate mapping\n    coords[2, ...] = range(bands)\n\n    # Prefilter not necessary for order 1 interpolation\n    prefilter = order > 1\n    mapped = ndimage.map_coordinates(image, coords, prefilter=prefilter,\n                                     mode=mode, order=order, cval=cval)\n\n    # The spline filters sometimes return results outside [0, 1],\n    # so clip to ensure valid data\n    return np.clip(mapped.squeeze(), 0, 1)```\n\nUpdated code:\n```python\ndef warp(image, inverse_map=None, map_args={}, output_shape=None, order=1,\n         mode='constant', cval=0., reverse_map=None):\n    \n\n    \n    # Backward API compatibility\n    if reverse_map is not None:\n        inverse_map = reverse_map\n\n    if image.ndim < 2:\n        raise ValueError(\"Input must have more than 1 dimension.\")\n\n    image = np.atleast_3d(img_as_float(image))\n    ishape = np.array(image.shape)\n    bands = ishape[2]\n\n    if output_shape is None:\n        output_shape = ishape\n\n    coords = np.empty(np.r_[3, output_shape], dtype=float)\n\n    ## Construct transformed coordinates\n\n    rows, cols = output_shape[:2]\n\n    # Reshape grid coordinates into a (P, 2) array of (x, y) pairs\n    tf_coords = np.indices((cols, rows), dtype=float).reshape(2, -1).T\n\n    # Map each (x, y) pair to the source image according to\n    # the user-provided mapping\n    tf_coords = inverse_map(tf_coords, **map_args)\n\n    # Reshape back to a (2, M, N) coordinate grid\n    tf_coords = tf_coords.T.reshape((-1, cols, rows)).swapaxes(1, 2)\n\n    # Place the y-coordinate mapping\n    _stackcopy(coords[1, ...], tf_coords[0, ...])\n\n    # Place the x-coordinate mapping\n    _stackcopy(coords[0, ...], tf_coords[1, ...])\n\n    # colour-coordinate mapping\n    coords[2, ...] = range(bands)\n\n    # Prefilter not necessary for order 1 interpolation\n    prefilter = order > 1\n    mapped = ndimage.map_coordinates(image, coords, prefilter=prefilter,\n                                     mode=mode, order=order, cval=cval)\n\n    # The spline filters sometimes return results outside [0, 1],\n    # so clip to ensure valid data\n    return np.clip(mapped.squeeze(), 0, 1)\n```\n\nOriginal documentation:\n```python\nWarp an image according to a given coordinate transformation.\n\n    Parameters\n    ----------\n    image : 2-D array\n        Input image.\n    inverse_map : transformation object, callable xy = f(xy, **kwargs)\n        Inverse coordinate map. A function that transforms a (N, 2) array of\n        ``(x, y)`` coordinates in the *output image* into their corresponding\n        coordinates in the *source image*. In case of a transformation object\n        its `inverse` method will be used as transformation function. Also see\n        examples below.\n    map_args : dict, optional\n        Keyword arguments passed to `inverse_map`.\n    output_shape : tuple (rows, cols)\n        Shape of the output image generated.\n    order : int\n        Order of splines used in interpolation. See\n        `scipy.ndimage.map_coordinates` for detail.\n    mode : string\n        How to handle values outside the image borders.  See\n        `scipy.ndimage.map_coordinates` for detail.\n    cval : string\n        Used in conjunction with mode 'constant', the value outside\n        the image boundaries.\n\n    Examples\n    --------\n    Shift an image to the right:\n\n    >>> from skimage import data\n    >>> image = data.camera()\n    >>>\n    >>> def shift_right(xy):\n    ...     xy[:, 0] -= 10\n    ...     return xy\n    >>>\n    >>> warp(image, shift_right)\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Warp an image according to a given coordinate transformation.\n\n    Parameters\n    ----------\n    image : 2-D array\n        Input image.\n    inverse_map : transformation object, callable xy = f(xy, **kwargs)\n        Inverse coordinate map. A function that transforms a (N, 2) array of\n        ``(x, y)`` coordinates in the *output image* into their corresponding\n        coordinates in the *source image* (e.g. a transformation object or its\n        inverse).\n    map_args : dict, optional\n        Keyword arguments passed to `inverse_map`.\n    output_shape : tuple (rows, cols)\n        Shape of the output image generated.\n    order : int\n        Order of splines used in interpolation. See\n        `scipy.ndimage.map_coordinates` for detail.\n    mode : string\n        How to handle values outside the image borders.  See\n        `scipy.ndimage.map_coordinates` for detail.\n    cval : string\n        Used in conjunction with mode 'constant', the value outside\n        the image boundaries.\n\n    Examples\n    --------\n    Shift an image to the right:\n\n    >>> from skimage import data\n    >>> image = data.camera()\n    >>>\n    >>> def shift_right(xy):\n    ...     xy[:, 0] -= 10\n    ...     return xy\n    >>>\n    >>> warp(image, shift_right)", "file": "skimage_transform__geometric.py", "function": "warp", "project": "scikit-image", "change_stats": {"code_changes": 4, "doc_changes": 7, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef get_co_vertices(self):\n        \n        \n        \n        x0 = self._center[0] - self._height / 2 * np.sin(np.deg2rad(self._angle))\n        y0 = self._center[1] + self._height / 2 * np.cos(np.deg2rad(self._angle))\n        x1 = self._center[0] + self._height / 2 * np.sin(np.deg2rad(self._angle))\n        y1 = self._center[1] - self._height / 2 * np.cos(np.deg2rad(self._angle))\n        return [(x0, y0), (x1, y1)]```\n\nUpdated code:\n```python\ndef get_co_vertices(self):\n        \n        \n        \n        return self._calculate_vertices_coordinates(return_major=False)\n```\n\nOriginal documentation:\n```python\nReturn the left and right co-vertex coordinates of the ellipse.\n\n        The definition can be found `here <https://en.wikipedia.org/wiki/Ellipse>`_\n\n        .. versionadded:: 3.8\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the co-vertices coordinates of the ellipse.\n\n        The definition can be found `here <https://en.wikipedia.org/wiki/Ellipse>`_\n\n        .. versionadded:: 3.8", "file": "lib_matplotlib_patches.py", "function": "Ellipse.get_co_vertices", "project": "matplotlib", "change_stats": {"code_changes": 8, "doc_changes": 4, "total_changes": 12}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef inference(self, x, spemb=None, *args, **kwargs):\n        \n\n        \n        # setup batch axis\n        ilens = torch.tensor([x.shape[0]], dtype=torch.long, device=x.device)\n        xs = x.unsqueeze(0)\n        if spemb is not None:\n            spembs = spemb.unsqueeze(0)\n        else:\n            spembs = None\n\n        # inference\n        outs = self._forward(xs, ilens, spembs=spembs, is_inference=True)[0]  # (L, odim)\n\n        return outs, None, None```\n\nUpdated code:\n```python\ndef inference(self, x, inference_args, spemb=None, *args, **kwargs):\n        \n\n        \n        # setup batch axis\n        ilens = torch.tensor([x.shape[0]], dtype=torch.long, device=x.device)\n        xs = x.unsqueeze(0)\n        if spemb is not None:\n            spembs = spemb.unsqueeze(0)\n        else:\n            spembs = None\n\n        # inference\n        outs = self._forward(xs, ilens, spembs=spembs, is_inference=True)[0]  # (L, odim)\n\n        return outs, None, None\n```\n\nOriginal documentation:\n```python\nGenerate the sequence of features given the sequences of characters.\n\n        Args:\n            x (Tensor): Input sequence of characters (T,).\n\n        Returns:\n            Tensor: Output sequence of features (1, L, odim).\n            None: Dummy for compatibility.\n            None: Dummy for compatibility.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Generate the sequence of features given the sequences of characters.\n\n        Args:\n            x (Tensor): Input sequence of characters (T,).\n            inference_args (Namespace): Dummy for compatibility.\n            spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim).\n\n        Returns:\n            Tensor: Output sequence of features (1, L, odim).\n            None: Dummy for compatibility.\n            None: Dummy for compatibility.", "file": "espnet_nets_pytorch_backend_e2e_tts_fastspeech.py", "function": "FeedForwardTransformer.inference", "project": "espnet", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef to_rgba_array(self, c, alpha=None):\n        \n        \n        \n        try:\n            if c.lower() == 'none':\n                return np.zeros((0,4), dtype=np.float_)\n        except AttributeError:\n            pass\n        if len(c) == 0:\n            return np.zeros((0,4), dtype=np.float_)\n        try:\n            result = np.array([self.to_rgba(c, alpha)], dtype=np.float_)\n        except ValueError:\n            # If c is a list it must be maintained as the same list\n            # with modified items so that items can be appended to\n            # it. This is needed for examples/dynamic_collections.py.\n            if isinstance(c, np.ndarray):\n                if len(c.shape) != 2:\n                    raise ValueError(\"Color array must be two-dimensional\")\n\n            result = np.zeros((len(c), 4))\n            for i, cc in enumerate(c):\n                result[i] = self.to_rgba(cc, alpha)  # change in place\n        return np.asarray(result, np.float_)```\n\nUpdated code:\n```python\ndef to_rgba_array(self, c, alpha=None):\n        \n        \n        \n        try:\n            if c.lower() == 'none':\n                return np.zeros((0,4), dtype=np.float_)\n        except AttributeError:\n            pass\n        if len(c) == 0:\n            return np.zeros((0,4), dtype=np.float_)\n        try:\n            result = np.array([self.to_rgba(c, alpha)], dtype=np.float_)\n        except ValueError:\n            if isinstance(c, np.ndarray):\n                if c.ndim != 2 and c.dtype.kind not in 'SU':\n                    raise ValueError(\"Color array must be two-dimensional\")\n\n            result = np.zeros((len(c), 4))\n            for i, cc in enumerate(c):\n                result[i] = self.to_rgba(cc, alpha)  # change in place\n        return np.asarray(result, np.float_)\n```\n\nOriginal documentation:\n```python\nReturns an Numpy array of *RGBA* tuples.\n\n        Accepts a single mpl color spec or a sequence of specs.\n        If the sequence is a list or array, the items are changed in place,\n        but an array copy is still returned.\n\n        Special case to handle \"no color\": if *c* is \"none\" (case-insensitive),\n        then an empty array will be returned.  Same for an empty list.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Returns a numpy array of *RGBA* tuples.\n\n        Accepts a single mpl color spec or a sequence of specs.\n\n        Special case to handle \"no color\": if *c* is \"none\" (case-insensitive),\n        then an empty array will be returned.  Same for an empty list.", "file": "lib_matplotlib_colors.py", "function": "ColorConverter.to_rgba_array", "project": "matplotlib", "change_stats": {"code_changes": 7, "doc_changes": 6, "total_changes": 13}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef set_interpolation(self, s):\n        \n        \n        \n        if s is None:\n            s = rcParams['image.interpolation']\n        s = s.lower()\n        cbook._check_in_list(_interpd_, interpolation=s)\n        self._interpolation = s\n        self.stale = True```\n\nUpdated code:\n```python\ndef set_interpolation(self, s):\n        \n        \n        \n        if s is None:\n            s = mpl.rcParams['image.interpolation']\n        s = s.lower()\n        cbook._check_in_list(_interpd_, interpolation=s)\n        self._interpolation = s\n        self.stale = True\n```\n\nOriginal documentation:\n```python\nSet the interpolation method the image uses when resizing.\n\n        If None, use :rc:`image.interpolation`. If 'none', the image is\n        shown as is without interpolating. 'none' is only supported in\n        agg, ps and pdf backends and will fall back to 'nearest' mode\n        for other backends.\n\n        Parameters\n        ----------\n        s : {'antialiased', 'nearest', 'bilinear', 'bicubic', 'spline16',\n'spline36', 'hanning', 'hamming', 'hermite', 'kaiser', 'quadric', 'catrom', \\\n'gaussian', 'bessel', 'mitchell', 'sinc', 'lanczos', 'none'} or None\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Set the interpolation method the image uses when resizing.\n\n        If None, use :rc:`image.interpolation`. If 'none', the image is\n        shown as is without interpolating. 'none' is only supported in\n        agg, ps and pdf backends and will fall back to 'nearest' mode\n        for other backends.\n\n        Parameters\n        ----------\n        s : {'antialiased', 'nearest', 'bilinear', 'bicubic', 'spline16', \\\n'spline36', 'hanning', 'hamming', 'hermite', 'kaiser', 'quadric', 'catrom', \\\n'gaussian', 'bessel', 'mitchell', 'sinc', 'lanczos', 'none'} or None", "file": "lib_matplotlib_image.py", "function": "_ImageBase.set_interpolation", "project": "matplotlib", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef __hash__(self):\n        \n        return hash(self.task_id)```\n\nUpdated code:\n```python\ndef __hash__(self):\n        \n        return hash(self.id)\n```\n\nOriginal documentation:\n```python\n`hash(self) -> hash(self.task_id)`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "`hash(self) -> hash(self.id)`", "file": "celery_result.py", "function": "AsyncResult.__hash__", "project": "celery", "change_stats": {"code_changes": 4, "doc_changes": 4, "total_changes": 8}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _get_timezone_name(timezone):\n    \n    return str(timezone)```\n\nUpdated code:\n```python\ndef _get_timezone_name(timezone):\n    \n    \n    \n    return timezone.tzname(None) or str(timezone)\n```\n\nOriginal documentation:\n```python\nReturn the name of ``timezone``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Return the offset for fixed offset timezones, or the name of timezone if\n    not set.", "file": "django_utils_timezone.py", "function": "_get_timezone_name", "project": "django", "change_stats": {"code_changes": 6, "doc_changes": 5, "total_changes": 11}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef take(self, indexer, axis: int = 1, verify: bool = True, convert: bool = True):\n        \n        \n        \n        indexer = (\n            np.arange(indexer.start, indexer.stop, indexer.step, dtype=\"int64\")\n            if isinstance(indexer, slice)\n            else np.asanyarray(indexer, dtype=\"int64\")\n        )\n\n        n = self.shape[axis]\n        if convert:\n            indexer = maybe_convert_indices(indexer, n)\n\n        if verify:\n            if ((indexer == -1) | (indexer >= n)).any():\n                raise Exception(\"Indices must be nonzero and less than the axis length\")\n\n        new_labels = self.axes[axis].take(indexer)\n        return self.reindex_indexer(\n            new_axis=new_labels,\n            indexer=indexer,\n            axis=axis,\n            allow_dups=True,\n            consolidate=False,\n        )```\n\nUpdated code:\n```python\ndef take(self: T, indexer, axis: int = 1, verify: bool = True) -> T:\n        \n        \n        \n        # We have 6 tests that get here with a slice\n        indexer = (\n            np.arange(indexer.start, indexer.stop, indexer.step, dtype=\"int64\")\n            if isinstance(indexer, slice)\n            else np.asanyarray(indexer, dtype=\"int64\")\n        )\n\n        n = self.shape[axis]\n        indexer = maybe_convert_indices(indexer, n, verify=verify)\n\n        new_labels = self.axes[axis].take(indexer)\n        return self.reindex_indexer(\n            new_axis=new_labels,\n            indexer=indexer,\n            axis=axis,\n            allow_dups=True,\n            consolidate=False,\n        )\n```\n\nOriginal documentation:\n```python\nTake items along any axis.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Take items along any axis.\n\n        indexer : np.ndarray or slice\n        axis : int, default 1\n        verify : bool, default True\n            Check that all entries are between 0 and len(self) - 1, inclusive.\n            Pass verify=False if this check has been done by the caller.\n\n        Returns\n        -------\n        BlockManager", "file": "pandas_core_internals_managers.py", "function": "BlockManager.take", "project": "pandas", "change_stats": {"code_changes": 12, "doc_changes": 12, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef load_config(fname: str) -> JSON_TYPE:\n    \n    config = load_yaml(fname)\n    # Check if all views and cards have a unique id or else add one\n    updated = False\n    seen_card_ids = set()\n    seen_view_ids = set()\n    index = 0\n    for view in config.get('views', []):\n        view_id = view.get('id')\n        if view_id is None:\n            updated = True\n            view.insert(0, 'id', index,\n                        comment=\"Automatically created id\")\n        else:\n            if view_id in seen_view_ids:\n                raise DuplicateIdError(\n                    'ID `{}` has multiple occurances in views'.format(view_id))\n            seen_view_ids.add(view_id)\n        for card in view.get('cards', []):\n            card_id = card.get('id')\n            if card_id is None:\n                updated = True\n                card.insert(0, 'id', uuid.uuid4().hex,\n                            comment=\"Automatically created id\")\n            else:\n                if card_id in seen_card_ids:\n                    raise DuplicateIdError(\n                        'ID `{}` has multiple occurances in cards'\n                        .format(card_id))\n                seen_card_ids.add(card_id)\n        index += 1\n    if updated:\n        save_yaml(fname, config)\n    return config```\n\nUpdated code:\n```python\ndef load_config(fname: str) -> JSON_TYPE:\n    \n    return load_yaml(fname)\n```\n\nOriginal documentation:\n```python\nLoad a YAML file and adds id to views and cards if not present.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Load a YAML file.", "file": "homeassistant_components_lovelace___init__.py", "function": "load_config", "project": "core", "change_stats": {"code_changes": 36, "doc_changes": 4, "total_changes": 40}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef spsolve_triangular(A, b, lower=True, overwrite_A=False, overwrite_b=False,\n                       unit_diagonal=False):\n    \n    \n    if not cusparse.check_availability('csrsm2'):\n        raise NotImplementedError\n\n    if not sparse.isspmatrix(A):\n        raise TypeError('A must be cupyx.scipy.sparse.spmatrix')\n    if not isinstance(b, cupy.ndarray):\n        raise TypeError('b must be cupy.ndarray')\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('A must be a square matrix (A.shape: {})'.\n                         format(A.shape))\n    if b.ndim not in [1, 2]:\n        raise ValueError('b must be 1D or 2D array (b.shape: {})'.\n                         format(b.shape))\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('The size of dimensions of A must be equal to the '\n                         'size of the first dimension of b '\n                         '(A.shape: {}, b.shape: {})'.format(A.shape, b.shape))\n    if A.dtype.char not in 'fdFD':\n        raise TypeError('unsupported dtype (actual: {})'.format(A.dtype))\n\n    if not (sparse.isspmatrix_csr(A) or sparse.isspmatrix_csc(A)):\n        warnings.warn('CSR or CSC format is required. Converting to CSR '\n                      'format.', sparse.SparseEfficiencyWarning)\n        A = A.tocsr()\n    A.sum_duplicates()\n\n    if (overwrite_b and A.dtype == b.dtype and\n            (b._c_contiguous or b._f_contiguous)):\n        x = b\n    else:\n        x = b.astype(A.dtype, copy=True)\n\n    cusparse.csrsm2(A, x, lower=lower, unit_diag=unit_diagonal)\n\n    if x.dtype.char in 'fF':\n        # Note: This is for compatibility with SciPy.\n        dtype = numpy.promote_types(x.dtype, 'float64')\n        x = x.astype(dtype)\n    return x```\n\nUpdated code:\n```python\ndef spsolve_triangular(A, b, lower=True, overwrite_A=False, overwrite_b=False,\n                       unit_diagonal=False):\n    \n    \n    if not (cusparse.check_availability('spsm') or\n            cusparse.check_availability('csrsm2')):\n        raise NotImplementedError\n\n    if not sparse.isspmatrix(A):\n        raise TypeError('A must be cupyx.scipy.sparse.spmatrix')\n    if not isinstance(b, cupy.ndarray):\n        raise TypeError('b must be cupy.ndarray')\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(f'A must be a square matrix (A.shape: {A.shape})')\n    if b.ndim not in [1, 2]:\n        raise ValueError(f'b must be 1D or 2D array (b.shape: {b.shape})')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('The size of dimensions of A must be equal to the '\n                         'size of the first dimension of b '\n                         f'(A.shape: {A.shape}, b.shape: {b.shape})')\n    if A.dtype.char not in 'fdFD':\n        raise TypeError(f'unsupported dtype (actual: {A.dtype})')\n\n    if cusparse.check_availability('spsm'):\n        if not (sparse.isspmatrix_csr(A) or\n                sparse.isspmatrix_csc(A) or\n                sparse.isspmatrix_coo(A)):\n            warnings.warn('CSR, CSC or COO format is required. Converting to '\n                          'CSR format.', sparse.SparseEfficiencyWarning)\n            A = A.tocsr()\n        A.sum_duplicates()\n        x = cusparse.spsm(A, b, lower=lower, unit_diag=unit_diagonal)\n    elif cusparse.check_availability('csrsm2'):\n        if not (sparse.isspmatrix_csr(A) or sparse.isspmatrix_csc(A)):\n            warnings.warn('CSR or CSC format is required. Converting to CSR '\n                          'format.', sparse.SparseEfficiencyWarning)\n            A = A.tocsr()\n        A.sum_duplicates()\n\n        if (overwrite_b and A.dtype == b.dtype and\n                (b._c_contiguous or b._f_contiguous)):\n            x = b\n        else:\n            x = b.astype(A.dtype, copy=True)\n\n        cusparse.csrsm2(A, x, lower=lower, unit_diag=unit_diagonal)\n    else:\n        assert False\n\n    if x.dtype.char in 'fF':\n        # Note: This is for compatibility with SciPy.\n        dtype = numpy.promote_types(x.dtype, 'float64')\n        x = x.astype(dtype)\n    return x\n```\n\nOriginal documentation:\n```python\nSolves a sparse triangular system ``A x = b``.\n\n    Args:\n        A (cupyx.scipy.sparse.spmatrix):\n            Sparse matrix with dimension ``(M, M)``.\n        b (cupy.ndarray):\n            Dense vector or matrix with dimension ``(M)`` or ``(M, K)``.\n        lower (bool):\n            Whether ``A`` is a lower or upper trinagular matrix.\n            If True, it is lower triangular, otherwise, upper triangular.\n        overwrite_A (bool):\n            (not supported)\n        overwrite_b (bool):\n            Allows overwriting data in ``b``.\n        unit_diagonal (bool):\n            If True, diagonal elements of ``A`` are assumed to be 1 and will\n            not be referencec.\n\n    Returns:\n        cupy.ndarray:\n            Solution to the system ``A x = b``. The shape is the same as ``b``.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Solves a sparse triangular system ``A x = b``.\n\n    Args:\n        A (cupyx.scipy.sparse.spmatrix):\n            Sparse matrix with dimension ``(M, M)``.\n        b (cupy.ndarray):\n            Dense vector or matrix with dimension ``(M)`` or ``(M, K)``.\n        lower (bool):\n            Whether ``A`` is a lower or upper trinagular matrix.\n            If True, it is lower triangular, otherwise, upper triangular.\n        overwrite_A (bool):\n            (not supported)\n        overwrite_b (bool):\n            Allows overwriting data in ``b``.\n        unit_diagonal (bool):\n            If True, diagonal elements of ``A`` are assumed to be 1 and will\n            not be referenced.\n\n    Returns:\n        cupy.ndarray:\n            Solution to the system ``A x = b``. The shape is the same as ``b``.", "file": "cupyx_scipy_sparse_linalg__solve.py", "function": "spsolve_triangular", "project": "cupy", "change_stats": {"code_changes": 49, "doc_changes": 4, "total_changes": 53}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef next_market_minute(self, start):\n        \n        \n        \n        next_minute = start + datetime.timedelta(minutes=1)\n        if self.is_market_hours(next_minute):\n            return next_minute\n        return self.next_open_and_close(start)[0]```\n\nUpdated code:\n```python\ndef next_market_minute(self, start):\n        \n        \n        \n        if self.is_trading_day(start):\n            market_open, market_close = self.get_open_and_close(start)\n            # If start before market open on a trading day, return market open.\n            if start < market_open:\n                return market_open\n            # If start is during trading hours, then get the next minute.\n            elif start < market_close:\n                return start + datetime.timedelta(minutes=1)\n        # If start is not in a trading day, or is after the market close\n        # then return the open of the *next* trading day.\n        return self.next_open_and_close(start)[0]\n```\n\nOriginal documentation:\n```python\nGet the next market minute after @start. This is either the immediate\n        next minute, or the open of the next market day after start.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Get the next market minute after @start. This is either the immediate\n        next minute, the open of the same day if @start is before the market\n        open on a trading day, or the open of the next market day after @start.", "file": "zipline_finance_trading.py", "function": "TradingEnvironment.next_market_minute", "project": "zipline", "change_stats": {"code_changes": 15, "doc_changes": 5, "total_changes": 20}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _replace_regex(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        mask=None,\n        using_cow: bool = False,\n    ) -> list[Block]:\n        \n        \n        \n        if not self._can_hold_element(to_replace):\n            # i.e. only if self.is_object is True, but could in principle include a\n            #  String ExtensionBlock\n            if using_cow:\n                return [self.copy(deep=False)]\n            return [self] if inplace else [self.copy()]\n\n        rx = re.compile(to_replace)\n\n        block = self._maybe_copy_cow_check(using_cow, inplace)\n\n        replace_regex(block.values, rx, value, mask)\n\n        nbs = block.convert(copy=False, using_cow=using_cow)\n        opt = get_option(\"future.no_silent_downcasting\")\n        if (len(nbs) > 1 or nbs[0].dtype != block.dtype) and not opt:\n            warnings.warn(\n                # GH#54710\n                \"Downcasting behavior in `replace` is deprecated and \"\n                \"will be removed in a future version. To retain the old \"\n                \"behavior, explicitly call `result.infer_objects(copy=False)`. \"\n                \"To opt-in to the future \"\n                \"behavior, set \"\n                \"`pd.set_option('future.no_silent_downcasting', True)`\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        return nbs```\n\nUpdated code:\n```python\ndef _replace_regex(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        mask=None,\n    ) -> list[Block]:\n        \n        \n        \n        if not self._can_hold_element(to_replace):\n            # i.e. only if self.is_object is True, but could in principle include a\n            #  String ExtensionBlock\n            return [self.copy(deep=False)]\n\n        rx = re.compile(to_replace)\n\n        block = self._maybe_copy(inplace)\n\n        replace_regex(block.values, rx, value, mask)\n\n        nbs = block.convert()\n        opt = get_option(\"future.no_silent_downcasting\")\n        if (len(nbs) > 1 or nbs[0].dtype != block.dtype) and not opt:\n            warnings.warn(\n                # GH#54710\n                \"Downcasting behavior in `replace` is deprecated and \"\n                \"will be removed in a future version. To retain the old \"\n                \"behavior, explicitly call `result.infer_objects(copy=False)`. \"\n                \"To opt-in to the future \"\n                \"behavior, set \"\n                \"`pd.set_option('future.no_silent_downcasting', True)`\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        return nbs\n```\n\nOriginal documentation:\n```python\nReplace elements by the given value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n        using_cow: bool, default False\n            Specifying if copy on write is enabled.\n\n        Returns\n        -------\n        List[Block]\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Replace elements by the given value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        List[Block]", "file": "pandas_core_internals_blocks.py", "function": "Block._replace_regex", "project": "pandas", "change_stats": {"code_changes": 11, "doc_changes": 4, "total_changes": 15}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef _lazy_init(self) -> None:\n        \n        \n        # Initialize param attributes lazily, in case the param's dtype or\n        # device changes after __init__.\n        for p in self.params:\n            self._init_param_attributes(p)\n\n        # Initialize _is_root and setup streams. These steps would ideally\n        # happen in __init__, but _is_root can only be determined after the\n        # entire model hierarchy is setup, thus we run it lazily.\n        if self._is_root is None:\n            # _is_root means that we are in the outermost module's forward.\n            self._set_is_root()\n            self._setup_streams()\n\n        if self._is_root:\n            # Buffers stay on GPU, and don't get sharded. Since _cast_buffers\n            # applies recursively, we only call this from the root instance.\n            self._cast_buffers(recurse=True)\n\n            # Don't free the full params for the outer-most (root) instance,\n            # In most cases, root instance contains params in the last layers\n            # or has no params. In these cases, those params will be needed\n            # immediately after for the backward pass. Note that this only\n            # applies currently when freeing parameters at end of layer's\n            # forward pass.\n            self.reshard_after_forward = False\n\n            # Due to the use of streams, we need to make sure the previous\n            # ``optim.step()`` is done before we all-gather parameters.\n            self._wait_for_previous_optim_step()\n\n            # Set a flag on every FlatParameter to track whether its post\n            # backward hook has been called, mainly for validation purpose\n            # in wait_for_post_backward.\n            if self._exec_order_data.is_first_iter:\n                for m in self.fsdp_modules(self):\n                    for p in m.params:\n                        p._post_backward_called = False```\n\nUpdated code:\n```python\ndef _lazy_init(self) -> None:\n        \n        \n        \n        if self._is_root is not None:\n            return  # no-op: already initialized\n        # The following logic is only run on the root FSDP instance\n        self._is_root = True\n        self._assert_state(TrainingState_.IDLE)\n        self._init_streams()\n        self._cast_buffers(recurse=True)\n        for param in self.params:\n            self._init_param_attributes(param)\n        # Do not reshard the root's parameters at the end of the forward pass\n        # with the intention that they are immediately used in the backward\n        # pass gradient computation (though this may not be true)\n        self.reshard_after_forward = False\n        self._exec_order_data.init(self)\n        # Initialize non-root FSDP instances and share attributes from the root\n        # to non-root instances (e.g. streams for overlapping)\n        for fsdp_module in self.fsdp_modules(self):\n            if fsdp_module is not self:\n                # Relax the assert for non-root FSDP instances in case the\n                # nested initialized module is wrapped again in FSDP later (e.g.\n                # after training to run inference)\n                assert fsdp_module._is_root is None or not fsdp_module._is_root, (\n                    \"Non-root FSDP instance's `_is_root` should not have been \"\n                    \"set yet or should have been set to `False`\"\n                )\n                fsdp_module._is_root = False\n                fsdp_module._streams = self._streams\n                fsdp_module._fsdp_graph_order = self._fsdp_graph_order\n                fsdp_module._exec_order_data = self._exec_order_data\n                for param in fsdp_module.params:\n                    fsdp_module._init_param_attributes(param)\n\n            # Set a flag on every FlatParameter to track whether its post\n            # backward hook has been called, mainly for validation purpose\n            # in wait_for_post_backward.\n            if self._exec_order_data.is_first_iter:\n                for m in self.fsdp_modules(self):\n                    for p in m.params:\n                        p._post_backward_called = False\n```\n\nOriginal documentation:\n```python\nInitialization steps that should happen lazily, typically right\n        before the first forward pass.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Performs initialization lazily, typically right before the first\n        forward pass. The laziness is needed to ensure that the parameter\n        device/dtype and the FSDP hierarchy have finalized.\n\n        This method's actual logic only runs on the root FSDP instance, which\n        performs initialization for all non-root FSDP instances to avoid\n        partial initialization.", "file": "torch_distributed_fsdp_fully_sharded_data_parallel.py", "function": "FullyShardedDataParallel._lazy_init", "project": "pytorch", "change_stats": {"code_changes": 63, "doc_changes": 11, "total_changes": 74}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef unpackbits(myarray, bitorder='big'):\n    \n    \n    if myarray.dtype != cupy.uint8:\n        raise TypeError('Expected an input array of unsigned byte data type')\n\n    if bitorder not in ['big', 'little']:\n        raise ValueError(\"bitorder must be either 'big' or 'little'\")\n\n    unpacked = cupy.ndarray((myarray.size * 8), dtype=cupy.uint8)\n    return _unpackbits_kernel[bitorder](myarray, unpacked)```\n\nUpdated code:\n```python\ndef unpackbits(a, axis=None, bitorder='big'):\n    \n    \n    if a.dtype != cupy.uint8:\n        raise TypeError('Expected an input array of unsigned byte data type')\n\n    if axis is not None:\n        raise NotImplementedError('axis option is not supported yet')\n\n    if bitorder not in ('big', 'little'):\n        raise ValueError(\"bitorder must be either 'big' or 'little'\")\n\n    unpacked = cupy.ndarray((a.size * 8), dtype=cupy.uint8)\n    return _unpackbits_kernel[bitorder](a, unpacked)\n```\n\nOriginal documentation:\n```python\nUnpacks elements of a uint8 array into a binary-valued output array.\n\n    This function currently does not support ``axis`` option.\n\n    Args:\n        myarray (cupy.ndarray): Input array.\n        bitorder (str, optional): bit order to use when unpacking the array,\n            allowed values are `'little'` and `'big'`. Defaults to `'big'`.\n\n    Returns:\n        cupy.ndarray: The unpacked array.\n\n    .. seealso:: :func:`numpy.unpackbits`\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Unpacks elements of a uint8 array into a binary-valued output array.\n\n    This function currently does not support ``axis`` option.\n\n    Args:\n        a (cupy.ndarray): Input array.\n        bitorder (str, optional): bit order to use when unpacking the array,\n            allowed values are `'little'` and `'big'`. Defaults to `'big'`.\n\n    Returns:\n        cupy.ndarray: The unpacked array.\n\n    .. seealso:: :func:`numpy.unpackbits`", "file": "cupy__binary_packing.py", "function": "unpackbits", "project": "cupy", "change_stats": {"code_changes": 15, "doc_changes": 4, "total_changes": 19}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef save_model(self,\n                   model: nn.Module,\n                   checkpoint: str,\n                   prefix: str = None,\n                   shard: bool = False,\n                   size_per_shard: int = 1024):\n        \n        \n        \n        pass```\n\nUpdated code:\n```python\ndef save_model(self,\n                   model: Union[nn.Module, ModelWrapper],\n                   checkpoint: str,\n                   shard: bool = False,\n                   prefix: str = None,\n                   size_per_shard: int = 1024):\n        \n        \n        \n\n        if isinstance(model, ModelWrapper):\n            model = model.unwrap()\n\n        if shard:\n            self.save_sharded_model(model, checkpoint, prefix, size_per_shard)\n        else:\n            self.save_unsharded_model(model, checkpoint)\n```\n\nOriginal documentation:\n```python\nSave model to checkpoint.\n\n        Examples:\n            >>> from colossalai.checkpoint_io import GeneralCheckpointIO\n            >>> checkpoint_io = CheckpointIO()\n            >>>\n            >>> # save model to a single file\n            >>> save_model(model, 'model.pt')\n            >>>\n            >>> # save model to a sharded checkpoint\n            >>> save_model(model, './checkpoints/', shard=True)\n\n        Args:\n            model (nn.Module): model to be saved.\n            checkpoint: checkpoint path. The checkpoint path can be :\n                1. a file path, e.g. 'model.pt'\n                2. a directory path to save the sharded checkpoint, e.g. './checkpoints/' when shard = True.\n            shard: whether to shard the checkpoint. Default: False. If set to True, the checkpoint will be sharded into\n                multiple files. The model shards will be specificed by a `model.index.json` file. When shard = True, please ensure\n                that the checkpoint path is a directory path instead of a file path.\n            size_per_shard (int): size per shard in MB. Default: 1024. This value is only used when shard is set to True.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Save model to checkpoint.\n\n        Examples:\n            >>> from colossalai.checkpoint_io import GeneralCheckpointIO\n            >>> checkpoint_io = CheckpointIO()\n            >>>\n            >>> # save model to a single file\n            >>> save_model(model, 'model.pt')\n            >>>\n            >>> # save model to a sharded checkpoint\n            >>> save_model(model, './checkpoints/', shard=True)\n\n        Args:\n            model (nn.Module): model to be saved.\n            checkpoint (str): checkpoint path. The checkpoint path can be :\n                1. a file path, e.g. 'model.pt'\n                2. a directory path to save the sharded checkpoint, e.g. './checkpoints/' when shard = True.\n            shard (bool): whether to shard the checkpoint. Default: False. If set to True, the checkpoint will be sharded into\n                multiple files. The model shards will be specificed by a `model.index.json` file. When shard = True, please ensure\n                that the checkpoint path is a directory path instead of a file path.\n            prefix (str): prefix for the model checkpoint file name when shard=True. Default: None.\n            size_per_shard (int): size per shard in MB. Default: 1024. This value is only used when shard = True.", "file": "colossalai_checkpoint_io_checkpoint_io_base.py", "function": "CheckpointIO.save_model", "project": "ColossalAI", "change_stats": {"code_changes": 15, "doc_changes": 9, "total_changes": 24}}
{"instruction": "Improve the following Python documentation to align with the updated code:\n\nOriginal code:\n```python\ndef is_yaml_nlu_file(filename: Text) -> bool:\n        \n        \n        if not rasa.shared.data.is_likely_yaml_file(filename):\n            return False\n\n        try:\n            content = rasa.shared.utils.io.read_yaml_file(filename)\n\n            return any(key in content for key in {KEY_NLU, KEY_RESPONSES})\n        except (YAMLError, Warning) as e:\n            logger.error(\n                f\"Tried to check if '{filename}' is a NLU file, but failed to \"\n                f\"read it. If this file contains NLU data, you should \"\n                f\"investigate this error, otherwise it is probably best to \"\n                f\"move the file to a different location. \"\n                f\"Error: {e}\"\n            )\n            return False```\n\nUpdated code:\n```python\ndef is_yaml_nlu_file(filename: Text) -> bool:\n        \n        \n        if not rasa.shared.data.is_likely_yaml_file(filename):\n            return False\n\n        content = rasa.shared.utils.io.read_yaml_file(filename)\n\n        return any(key in content for key in {KEY_NLU, KEY_RESPONSES})\n```\n\nOriginal documentation:\n```python\nChecks if the specified file possibly contains NLU training data in YAML.\n\n        Args:\n            filename: name of the file to check.\n\n        Returns:\n            `True` if the `filename` is possibly a valid YAML NLU file,\n            `False` otherwise.\n```\n\nPlease provide an improved version of the documentation that reflects the code changes.", "response": "Checks if the specified file possibly contains NLU training data in YAML.\n\n        Args:\n            filename: name of the file to check.\n\n        Returns:\n            `True` if the `filename` is possibly a valid YAML NLU file,\n            `False` otherwise.\n\n        Raises:\n            YamlException: if the file seems to be a YAML file (extension) but\n                can not be read / parsed.", "file": "rasa_shared_nlu_training_data_formats_rasa_yaml.py", "function": "RasaYAMLReader.is_yaml_nlu_file", "project": "rasa", "change_stats": {"code_changes": 16, "doc_changes": 6, "total_changes": 22}}
